<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java网络编程与NIO详解10：深度解读Tomcat中的NIO模型]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8ENIO%E8%AF%A6%E8%A7%A310%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BBTomcat%E4%B8%AD%E7%9A%84NIO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文转自：http://www.sohu.com/a/203838233_827544 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 摘要: I/O复用模型，是同步非阻塞，这里的非阻塞是指I/O读写，对应的是recvfrom操作，因为数据报文已经准备好，无需阻塞。 说它是同步，是因为，这个执行是在一个线程里面执行的。有时候，还会说它又是阻塞的，实际上是指阻塞在select上面，必须等到读就绪、写就绪等网络事件。 一、I/O复用模型解读Tomcat的NIO是基于I/O复用来实现的。对这点一定要清楚，不然我们的讨论就不在一个逻辑线上。下面这张图学习过I/O模型知识的一般都见过，出自《UNIX网络编程》，I/O模型一共有阻塞式I/O，非阻塞式I/O，I/O复用(select/poll/epoll)，信号驱动式I/O和异步I/O。这篇文章讲的是I/O复用。 IO复用.png 这里先来说下用户态和内核态，直白来讲，如果线程执行的是用户代码，当前线程处在用户态，如果线程执行的是内核里面的代码，当前线程处在内核态。更深层来讲，操作系统为代码所处的特权级别分了4个级别。 不过现代操作系统只用到了0和3两个级别。0和3的切换就是用户态和内核态的切换。更详细的可参照《深入理解计算机操作系统》。I/O复用模型，是同步非阻塞，这里的非阻塞是指I/O读写，对应的是recvfrom操作，因为数据报文已经准备好，无需阻塞。 说它是同步，是因为，这个执行是在一个线程里面执行的。有时候，还会说它又是阻塞的，实际上是指阻塞在select上面，必须等到读就绪、写就绪等网络事件。有时候我们又说I/O复用是多路复用，这里的多路是指N个连接，每一个连接对应一个channel，或者说多路就是多个channel。 复用，是指多个连接复用了一个线程或者少量线程(在Tomcat中是Math.min(2,Runtime.getRuntime().availableProcessors()))。 上面提到的网络事件有连接就绪，接收就绪，读就绪，写就绪四个网络事件。I/O复用主要是通过Selector复用器来实现的，可以结合下面这个图理解上面的叙述。 Selector图解.png 二、TOMCAT对IO模型的支持 tomcat支持IO类型图.png tomcat从6以后开始支持NIO模型，实现是基于JDK的java.nio包。这里可以看到对read body 和response body是Blocking的。关于这点在第6.3节源代码阅读有重点介绍。 三、TOMCAT中NIO的配置与使用在Connector节点配置protocol=”org.apache.coyote.http11.Http11NioProtocol”，Http11NioProtocol协议下默认最大连接数是10000，也可以重新修改maxConnections的值，同时我们可以设置最大线程数maxThreads，这里设置的最大线程数就是Excutor的线程池的大小。 在BIO模式下实际上是没有maxConnections，即使配置也不会生效，BIO模式下的maxConnections是保持跟maxThreads大小一致，因为它是一请求一线程模式。 四、NioEndpoint组件关系图解读 tomcatnio组成.png 我们要理解tomcat的nio最主要就是对NioEndpoint的理解。它一共包含LimitLatch、Acceptor、Poller、SocketProcessor、Excutor5个部分。 LimitLatch是连接控制器，它负责维护连接数的计算，nio模式下默认是10000，达到这个阈值后，就会拒绝连接请求。Acceptor负责接收连接，默认是1个线程来执行，将请求的事件注册到事件列表。 有Poller来负责轮询，Poller线程数量是cpu的核数Math.min(2,Runtime.getRuntime().availableProcessors())。由Poller将就绪的事件生成SocketProcessor同时交给Excutor去执行。Excutor线程池的大小就是我们在Connector节点配置的maxThreads的值。 在Excutor的线程中，会完成从socket中读取http request，解析成HttpServletRequest对象，分派到相应的servlet并完成逻辑，然后将response通过socket发回client。 在从socket中读数据和往socket中写数据的过程，并没有像典型的非阻塞的NIO的那样，注册OP_READ或OP_WRITE事件到主Selector，而是直接通过socket完成读写，这时是阻塞完成的，但是在timeout控制上，使用了NIO的Selector机制，但是这个Selector并不是Poller线程维护的主Selector，而是BlockPoller线程中维护的Selector，称之为辅Selector。详细源代码可以参照 第6.3节。 五、NioEndpoint执行序列图 tomcatnio序列图.png 在下一小节NioEndpoint源码解读中我们将对步骤1-步骤11依次找到对应的代码来说明。 六、NioEndpoint源码解读6.1、初始化 无论是BIO还是NIO，开始都会初始化连接限制，不可能无限增大，NIO模式下默认是10000。 6.2、步骤解读 下面我们着重叙述跟NIO相关的流程，共分为11个步骤，分别对应上面序列图中的步骤。 步骤1：绑定IP地址及端口，将ServerSocketChannel设置为阻塞。 这里为什么要设置成阻塞呢，我们一直都在说非阻塞。Tomcat的设计初衷主要是为了操作方便。这样这里就跟BIO模式下一样了。只不过在BIO下这里返回的是 Socket，NIO下这里返回的是SocketChannel。 步骤2：启动接收线程 步骤3：ServerSocketChannel.accept()接收新连接 步骤4：将接收到的链接通道设置为非阻塞 步骤5：构造NioChannel对象 步骤6：register注册到轮询线程 步骤7：构造PollerEvent，并添加到事件队列 步骤8：启动轮询线程 步骤9：取出队列中新增的PollerEvent并注册到Selector 步骤10：Selector.select() 步骤11：根据选择的SelectionKey构造SocketProcessor提交到请求处理线程 6.3、NioBlockingSelector和BlockPoller介绍 上面的序列图有个地方我没有描述，就是NioSelectorPool这个内部类，是因为在整体理解tomcat的nio上面在序列图里面不包括它更好理解。 在有了上面的基础后，我们在来说下NioSelectorPool这个类，对更深层了解Tomcat的NIO一定要知道它的作用。NioEndpoint对象中维护了一个NioSelecPool对象，这个NioSelectorPool中又维护了一个BlockPoller线程，这个线程就是基于辅Selector进行NIO的逻辑。 以执行servlet后，得到response，往socket中写数据为例，最终写的过程调用NioBlockingSelector的write方法。代码如下： 也就是说当socket.write()返回0时，说明网络状态不稳定，这时将socket注册OP_WRITE事件到辅Selector，由BlockPoller线程不断轮询这个辅Selector，直到发现这个socket的写状态恢复了，通过那个倒数计数器，通知Worker线程继续写socket动作。看一下BlockSelector线程的代码逻辑： 使用这个辅Selector主要是减少线程间的切换，同时还可减轻主Selector的负担。 七、关于性能下面这份报告是我们压测的一个结果，跟想象的是不是不太一样？几乎没有差别，实际上NIO优化的是I/O的读写，如果瓶颈不在这里的话，比如传输字节数很小的情况下，BIO和NIO实际上是没有差别的。 NIO的优势更在于用少量的线程hold住大量的连接。还有一点，我们在压测的过程中，遇到在NIO模式下刚开始的一小段时间内容，会有错误，这是因为一般的压测工具是基于一种长连接，也就是说比如模拟1000并发，那么同时建立1000个连接，下一时刻再发送请求就是基于先前的这1000个连接来发送，还有TOMCAT的NIO处理是有POLLER线程来接管的，它的线程数一般等于CPU的核数，如果一瞬间有大量并发过来，POLLER也会顿时处理不过来。 压测1.jpeg 压测2.jpeg 八、总结NIO只是优化了网络IO的读写，如果系统的瓶颈不在这里，比如每次读取的字节说都是500b，那么BIO和NIO在性能上没有区别。NIO模式是最大化压榨CPU，把时间片都更好利用起来。 对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源如内存，有关线程资源可参照这篇文章《一台java服务器可以跑多少个线程》。 因此，使用的线程越少越好。而I/O复用模型正是利用少量的线程来管理大量的连接。在对于维护大量长连接的应用里面更适合用基于I/O复用模型NIO，比如web qq这样的应用。所以我们要清楚系统的瓶颈是I/O还是CPU的计算 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程与NIO详解11：Tomcat中的Connector源码分析（NIO）]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8ENIO%E8%AF%A6%E8%A7%A311%EF%BC%9ATomcat%E4%B8%AD%E7%9A%84Connector%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88NIO%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本文转载 https://www.javadoop.com 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言之前写了两篇关于 NIO 的文章，第一篇介绍了 NIO 的 Channel、Buffer、Selector 使用，第二篇介绍了非阻塞 IO 和异步 IO，并展示了简单的用例。 本文将介绍 Tomcat 中的 NIO 使用，使大家对 Java NIO 的生产使用有更加直观的认识。 虽然本文的源码篇幅也不短，但是 Tomcat 的源码毕竟不像 Doug Lea 的并发源码那么“变态”，对于大部分读者来说，阅读难度比之前介绍的其他并发源码要简单一些，所以读者不要觉得有什么压力。 本文基于 Tomcat 当前（2018-03-20）最新版本 9.0.6。 先简单画一张图示意一下本文的主要内容： 目录 源码环境准备Tomcat 9.0.6 下载地址：https://tomcat.apache.org/download-90.cgi 由于上面下载的 tomcat 的源码并没有使用 maven 进行组织，不方便我们看源码，也不方便我们进行调试。这里我们将使用 maven 仓库中的 tomcat-embed-core，自己编写代码进行启动的方式来进行调试。 首先，创建一个空的 maven 工程，然后添加以下依赖。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; tomcat-embed-core &lt;version&gt;9.0.6&lt;/version&gt;&lt;/dependency&gt; 上面的依赖，只会将 tomcat-embed-core-9.0.6.jar 和 tomcat-annotations-api-9.0.6.jar 两个包引进来，对于本文来说，已经足够了，如果你需要其他功能，需要额外引用其他的依赖，如 Jasper。 然后，使用以下启动方法： 1234567891011public static void main(String[] args) throws LifecycleException &#123; Tomcat tomcat = new Tomcat(); Connector connector = new Connector(&quot;HTTP/1.1&quot;); connector.setPort(8080); tomcat.setConnector(connector); tomcat.start(); tomcat.getServer().await();&#125; 经过以上的代码，我们的 Tomcat 就启动起来了。 Tomcat 中的其他接口感兴趣的读者请自行探索，如设置 webapp 目录，设置 resources 等 这里，介绍第一个重要的概念：Connector。在 Tomcat 中，使用 Connector 来处理连接，一个 Tomcat 可以配置多个 Connector，分别用于监听不同端口，或处理不同协议。 在 Connector 的构造方法中，我们可以传 HTTP/1.1 或 AJP/1.3 用于指定协议，也可以传入相应的协议处理类，毕竟协议不是重点，将不同端口进来的连接对应不同处理类才是正道。典型地，我们可以指定以下几个协议处理类： org.apache.coyote.http11.Http11NioProtocol：对应非阻塞 IO org.apache.coyote.http11.Http11Nio2Protocol：对应异步 IO org.apache.coyote.http2.Http2Protocol：对应 http2 协议，对 http2 感兴趣的读者，赶紧看起来吧。 本文的重点当然是非阻塞 IO 了，之前已经介绍过异步 IO的基础知识了，读者看完本文后，如果对异步 IO 的处理流程感兴趣，可以自行去分析一遍。 如果你使用 9.0 以前的版本，Tomcat 在启动的时候是会自动配置一个 connector 的，我们可以不用显示配置。 9.0 版本的 Tomcat#start() 方法： 12345&gt; public void start() throws LifecycleException &#123;&gt; getServer();&gt; server.start();&gt; &#125;&gt; 8.5 及之前版本的 Tomcat#start() 方法： 1234567&gt; public void start() throws LifecycleException &#123;&gt; getServer();&gt; // 自动配置一个使用非阻塞 IO 的 connector&gt; getConnector();&gt; server.start();&gt; &#125;&gt; endpoint前面我们说过一个 Connector 对应一个协议，当然这描述也不太对，NIO 和 NIO2 就都是处理 HTTP/1.1 的，只不过一个使用非阻塞，一个使用异步。进到指定 protocol 代码，我们就会发现，它们的代码及其简单，只不过是指定了特定的 endpoint。 打开 Http11NioProtocol 和 Http11Nio2Protocol源码，我们可以看到，在构造方法中，它们分别指定了 NioEndpoint 和 Nio2Endpoint。 1234567891011121314151617// 非阻塞模式public class Http11NioProtocol extends AbstractHttp11JsseProtocol&lt;NioChannel&gt; &#123; public Http11NioProtocol() &#123; // NioEndpoint super(new NioEndpoint()); &#125; ...&#125;// 异步模式public class Http11Nio2Protocol extends AbstractHttp11JsseProtocol&lt;Nio2Channel&gt; &#123; public Http11Nio2Protocol() &#123; // Nio2Endpoint super(new Nio2Endpoint()); &#125; ...&#125; 这里介绍第二个重要的概念：endpoint。Tomcat 使用不同的 endpoint 来处理不同的协议请求，今天我们的重点是 NioEndpoint，其使用非阻塞 IO 来进行处理 HTTP/1.1 协议的请求。 NioEndpoint 继承 =&gt; AbstractJsseEndpoint 继承 =&gt; AbstractEndpoint。中间的 AbstractJsseEndpoint 主要是提供了一些关于 HTTPS 的方法，这块我们暂时忽略它，后面所有关于 HTTPS 的我们都直接忽略，感兴趣的读者请自行分析。 init 过程分析下面，我们看看从 tomcat.start() 一直到 NioEndpoint 的过程。 1. AbstractProtocol # init 123456789@Overridepublic void init() throws Exception &#123; ... String endpointName = getName(); endpoint.setName(endpointName.substring(1, endpointName.length()-1)); endpoint.setDomain(domain); // endpoint 的 name=http-nio-8089,domain=Tomcat endpoint.init();&#125; 2. AbstractEndpoint # init 1234567public final void init() throws Exception &#123; if (bindOnInit) &#123; bind(); // 这里对应的当然是子类 NioEndpoint 的 bind() 方法 bindState = BindState.BOUND_ON_INIT; &#125; ...&#125; 3. NioEndpoint # bind 这里就到我们的 NioEndpoint 了，要使用到我们之前学习的 NIO 的知识了。 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic void bind() throws Exception &#123; // initServerSocket(); 原代码是这行，我们 “内联” 过来一起说 // 开启 ServerSocketChannel serverSock = ServerSocketChannel.open(); socketProperties.setProperties(serverSock.socket()); // getPort() 会返回我们最开始设置的 8080，得到我们的 address 是 0.0.0.0:8080 InetSocketAddress addr = (getAddress()!=null?new InetSocketAddress(getAddress(),getPort()):new InetSocketAddress(getPort())); // ServerSocketChannel 绑定地址、端口， // 第二个参数 backlog 默认为 100，超过 100 的时候，新连接会被拒绝(不过源码注释也说了，这个值的真实语义取决于具体实现) serverSock.socket().bind(addr,getAcceptCount()); // ※※※ 设置 ServerSocketChannel 为阻塞模式 ※※※ serverSock.configureBlocking(true); // 设置 acceptor 和 poller 的数量，至于它们是什么角色，待会说 // acceptorThreadCount 默认为 1 if (acceptorThreadCount == 0) &#123; // FIXME: Doesn&apos;t seem to work that well with multiple accept threads // 作者想表达的意思应该是：使用多个 acceptor 线程并不见得性能会更好 acceptorThreadCount = 1; &#125; // poller 线程数，默认值定义如下，所以在多核模式下，默认为 2 // pollerThreadCount = Math.min(2,Runtime.getRuntime().availableProcessors()); if (pollerThreadCount &lt;= 0) &#123; pollerThreadCount = 1; &#125; // setStopLatch(new CountDownLatch(pollerThreadCount)); // 初始化 ssl，我们忽略 ssl initialiseSsl(); // 打开 NioSelectorPool，先忽略它 selectorPool.open();&#125; ServerSocketChannel 已经打开，并且绑定要了之前指定的 8080 端口，设置成了阻塞模式。 设置了 acceptor 的线程数为 1 设置了 poller 的线程数，单核 CPU 为 1，多核为 2 打开了一个 SelectorPool，我们先忽略这个 到这里，我们还不知道 Acceptor 和 Poller 是什么东西，我们只是设置了它们的数量，我们先来看看最后面提到的 SelectorPool。 start 过程分析刚刚我们分析完了 init() 过程，下面是启动过程 start() 分析。 AbstractProtocol # start 1234567891011121314151617@Overridepublic void start() throws Exception &#123; ... // 调用 endpoint 的 start 方法 endpoint.start(); // Start async timeout thread asyncTimeout = new AsyncTimeout(); Thread timeoutThread = new Thread(asyncTimeout, getNameInternal() + &quot;-AsyncTimeout&quot;); int priority = endpoint.getThreadPriority(); if (priority &lt; Thread.MIN_PRIORITY || priority &gt; Thread.MAX_PRIORITY) &#123; priority = Thread.NORM_PRIORITY; &#125; timeoutThread.setPriority(priority); timeoutThread.setDaemon(true); timeoutThread.start();&#125; AbstractEndpoint # start 12345678910public final void start() throws Exception &#123; // 按照我们的流程，刚刚 init 的时候，已经把 bindState 改为 BindState.BOUND_ON_INIT 了， // 所以下面的 if 分支我们就不进去了 if (bindState == BindState.UNBOUND) &#123; bind(); bindState = BindState.BOUND_ON_START; &#125; // 往里看 NioEndpoint 的实现 startInternal();&#125; 下面这个方法还是比较重要的，这里会创建前面说过的 acceptor 和 poller。 NioEndpoint # startInternal 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic void startInternal() throws Exception &#123; if (!running) &#123; running = true; paused = false; // 以下几个是缓存用的，之后我们也会看到很多这样的代码，为了减少 new 很多对象出来 processorCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getProcessorCache()); eventCache = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getEventCache()); nioChannels = new SynchronizedStack&lt;&gt;(SynchronizedStack.DEFAULT_SIZE, socketProperties.getBufferPool()); // 创建【工作线程池】，Tomcat 自己包装了一下 ThreadPoolExecutor， // 1\. 为了在创建线程池以后，先启动 corePoolSize 个线程(这个属于线程池的知识了，不熟悉的读者可以看看我之前的文章) // 2\. 自己管理线程池的增长方式（默认 corePoolSize 10, maxPoolSize 200），不是本文重点，不分析 if ( getExecutor() == null ) &#123; createExecutor(); &#125; // 设置一个栅栏（tomcat 自定义了类 LimitLatch），控制最大的连接数，默认是 10000 initializeConnectionLatch(); // 开启 poller 线程 // 还记得之前 init 的时候，默认地设置了 poller 的数量为 2，所以这里启动 2 个 poller 线程 pollers = new Poller[getPollerThreadCount()]; for (int i=0; i&lt;pollers.length; i++) &#123; pollers[i] = new Poller(); Thread pollerThread = new Thread(pollers[i], getName() + &quot;-ClientPoller-&quot;+i); pollerThread.setPriority(threadPriority); pollerThread.setDaemon(true); pollerThread.start(); &#125; // 开启 acceptor 线程，和开启 poller 线程组差不多。 // init 的时候，默认地，acceptor 的线程数是 1 startAcceptorThreads(); &#125;&#125; 到这里，我们启动了工作线程池、 poller 线程组、acceptor 线程组。同时，工作线程池初始就已经启动了 10 个线程。我们用 jconsole 来看看此时的线程，请看下图： 从 jconsole 中，我们可以看到，此时启动了 BlockPoller、worker、poller、acceptor、AsyncTimeout，大家应该都已经清楚了每个线程是哪里启动的吧。 Tomcat 中并没有 Worker 这个类，此名字是我瞎编。 此时，我们还是不知道 acceptor、poller 甚至 worker 到底是干嘛的，下面，我们从 acceptor 线程开始看起。 Acceptor它的结构非常简单，在构造函数中，已经把 endpoint 传进来了，此外就只有 threadName 和 state 两个简单的属性。 1234567private final AbstractEndpoint&lt;?,U&gt; endpoint;private String threadName;protected volatile AcceptorState state = AcceptorState.NEW;public Acceptor(AbstractEndpoint&lt;?,U&gt; endpoint) &#123; this.endpoint = endpoint;&#125; threadName 就是一个线程名字而已，Acceptor 的状态 state 主要是随着 endpoint 来的。 123public enum AcceptorState &#123; NEW, RUNNING, PAUSED, ENDED&#125; 我们直接来看 acceptor 的 run 方法吧： Acceptor # run 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485@Overridepublic void run() &#123; int errorDelay = 0; // 只要 endpoint 处于 running，这里就一直循环 while (endpoint.isRunning()) &#123; // 如果 endpoint 处于 pause 状态，这边 Acceptor 用一个 while 循环将自己也挂起 while (endpoint.isPaused() &amp;&amp; endpoint.isRunning()) &#123; state = AcceptorState.PAUSED; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; // Ignore &#125; &#125; // endpoint 结束了，Acceptor 自然也要结束嘛 if (!endpoint.isRunning()) &#123; break; &#125; state = AcceptorState.RUNNING; try &#123; // 如果此时达到了最大连接数(之前我们说过，默认是10000)，就等待 endpoint.countUpOrAwaitConnection(); // Endpoint might have been paused while waiting for latch // If that is the case, don&apos;t accept new connections if (endpoint.isPaused()) &#123; continue; &#125; U socket = null; try &#123; // 这里就是接收下一个进来的 SocketChannel // 之前我们设置了 ServerSocketChannel 为阻塞模式，所以这边的 accept 是阻塞的 socket = endpoint.serverSocketAccept(); &#125; catch (Exception ioe) &#123; // We didn&apos;t get a socket endpoint.countDownConnection(); if (endpoint.isRunning()) &#123; // Introduce delay if necessary errorDelay = handleExceptionWithDelay(errorDelay); // re-throw throw ioe; &#125; else &#123; break; &#125; &#125; // accept 成功，将 errorDelay 设置为 0 errorDelay = 0; if (endpoint.isRunning() &amp;&amp; !endpoint.isPaused()) &#123; // setSocketOptions() 是这里的关键方法，也就是说前面千辛万苦都是为了能到这里进行处理 if (!endpoint.setSocketOptions(socket)) &#123; // 如果上面的方法返回 false，关闭 SocketChannel endpoint.closeSocket(socket); &#125; &#125; else &#123; // 由于 endpoint 不 running 了，或者处于 pause 了，将此 SocketChannel 关闭 endpoint.destroySocket(socket); &#125; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); String msg = sm.getString(&quot;endpoint.accept.fail&quot;); // APR specific. // Could push this down but not sure it is worth the trouble. if (t instanceof Error) &#123; Error e = (Error) t; if (e.getError() == 233) &#123; // Not an error on HP-UX so log as a warning // so it can be filtered out on that platform // See bug 50273 log.warn(msg, t); &#125; else &#123; log.error(msg, t); &#125; &#125; else &#123; log.error(msg, t); &#125; &#125; &#125; state = AcceptorState.ENDED;&#125; 大家应该发现了，Acceptor 绕来绕去，都是在调用 NioEndpoint 的方法，我们简单分析一下这个。 在 NioEndpoint init 的时候，我们开启了一个 ServerSocketChannel，后来 start 的时候，我们开启多个 acceptor（实际上，默认是 1 个），每个 acceptor 启动以后就开始循环调用 ServerSocketChannel 的 accept() 方法获取新的连接，然后调用 endpoint.setSocketOptions(socket) 处理新的连接，之后再进入循环 accept 下一个连接。 到这里，大家应该也就知道了，为什么这个叫 acceptor 了吧？接下来，我们来看看 setSocketOptions 方法到底做了什么。 NioEndpoint # setSocketOptions 123456789101112131415161718192021222324252627282930313233343536373839404142@Overrideprotected boolean setSocketOptions(SocketChannel socket) &#123; try &#123; // 设置该 SocketChannel 为非阻塞模式 socket.configureBlocking(false); Socket sock = socket.socket(); // 设置 socket 的一些属性 socketProperties.setProperties(sock); // 还记得 startInternal 的时候，说过了 nioChannels 是缓存用的。 // 限于篇幅，这里的 NioChannel 就不展开了，它包括了 socket 和 buffer NioChannel channel = nioChannels.pop(); if (channel == null) &#123; // 主要是创建读和写的两个 buffer，默认地，读和写 buffer 都是 8192 字节，8k SocketBufferHandler bufhandler = new SocketBufferHandler( socketProperties.getAppReadBufSize(), socketProperties.getAppWriteBufSize(), socketProperties.getDirectBuffer()); if (isSSLEnabled()) &#123; channel = new SecureNioChannel(socket, bufhandler, selectorPool, this); &#125; else &#123; channel = new NioChannel(socket, bufhandler); &#125; &#125; else &#123; channel.setIOChannel(socket); channel.reset(); &#125; // getPoller0() 会选取所有 poller 中的一个 poller getPoller0().register(channel); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); try &#123; log.error(&quot;&quot;,t); &#125; catch (Throwable tt) &#123; ExceptionUtils.handleThrowable(tt); &#125; // Tell to close the socket return false; &#125; return true;&#125; 我们看到，这里又没有进行实际的处理，而是将这个 SocketChannel 注册到了其中一个 poller 上。因为我们知道，acceptor 应该尽可能的简单，只做 accept 的工作，简单处理下就往后面扔。acceptor 还得回到之前的循环去 accept 新的连接呢。 我们只需要明白，此时，往 poller 中注册了一个 NioChannel 实例，此实例包含客户端过来的 SocketChannel 和一个 SocketBufferHandler 实例。 Poller之前我们看到 acceptor 将一个 NioChannel 实例 register 到了一个 poller 中。在看 register 方法之前，我们需要先对 poller 要有个简单的认识。 123456789101112131415161718192021public class Poller implements Runnable &#123; public Poller() throws IOException &#123; // 每个 poller 开启一个 Selector this.selector = Selector.open(); &#125; private Selector selector; // events 队列，此类的核心 private final SynchronizedQueue&lt;PollerEvent&gt; events = new SynchronizedQueue&lt;&gt;(); private volatile boolean close = false; private long nextExpiration = 0;//optimize expiration handling // 这个值后面有用，记住它的初始值为 0 private AtomicLong wakeupCounter = new AtomicLong(0); private volatile int keyCount = 0; ...&#125; 敲重点：每个 poller 关联了一个 Selector。 Poller 内部围着一个 events 队列转，来看看其 events() 方法： 1234567891011121314151617181920public boolean events() &#123; boolean result = false; PollerEvent pe = null; for (int i = 0, size = events.size(); i &lt; size &amp;&amp; (pe = events.poll()) != null; i++ ) &#123; result = true; try &#123; // 逐个执行 event.run() pe.run(); // 该 PollerEvent 还得给以后用，这里 reset 一下(还是之前说过的缓存) pe.reset(); if (running &amp;&amp; !paused) &#123; eventCache.push(pe); &#125; &#125; catch ( Throwable x ) &#123; log.error(&quot;&quot;,x); &#125; &#125; return result;&#125; events() 方法比较简单，就是取出当前队列中的 PollerEvent 对象，逐个执行 event.run() 方法。 然后，现在来看 Poller 的 run() 方法，该方法会一直循环，直到 poller.destroy() 被调用。 Poller # run 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public void run() &#123; while (true) &#123; boolean hasEvents = false; try &#123; if (!close) &#123; // 执行 events 队列中每个 event 的 run() 方法 hasEvents = events(); // wakeupCounter 的初始值为 0，这里设置为 -1 if (wakeupCounter.getAndSet(-1) &gt; 0) &#123; //if we are here, means we have other stuff to do //do a non blocking select keyCount = selector.selectNow(); &#125; else &#123; // timeout 默认值 1 秒 keyCount = selector.select(selectorTimeout); &#125; wakeupCounter.set(0); &#125; // 篇幅所限，我们就不说 close 的情况了 if (close) &#123; events(); timeout(0, false); try &#123; selector.close(); &#125; catch (IOException ioe) &#123; log.error(sm.getString(&quot;endpoint.nio.selectorCloseFail&quot;), ioe); &#125; break; &#125; &#125; catch (Throwable x) &#123; ExceptionUtils.handleThrowable(x); log.error(&quot;&quot;,x); continue; &#125; //either we timed out or we woke up, process events first // 这里没什么好说的，顶多就再执行一次 events() 方法 if ( keyCount == 0 ) hasEvents = (hasEvents | events()); // 如果刚刚 select 有返回 ready keys，进行处理 Iterator&lt;SelectionKey&gt; iterator = keyCount &gt; 0 ? selector.selectedKeys().iterator() : null; // Walk through the collection of ready keys and dispatch // any active event. while (iterator != null &amp;&amp; iterator.hasNext()) &#123; SelectionKey sk = iterator.next(); NioSocketWrapper attachment = (NioSocketWrapper)sk.attachment(); // Attachment may be null if another thread has called // cancelledKey() if (attachment == null) &#123; iterator.remove(); &#125; else &#123; iterator.remove(); // ※※※※※ 处理 ready key ※※※※※ processKey(sk, attachment); &#125; &#125;//while //process timeouts timeout(keyCount,hasEvents); &#125;//while getStopLatch().countDown();&#125; poller 的 run() 方法主要做了调用 events() 方法和处理注册到 Selector 上的 ready key，这里我们暂时不展开 processKey 方法，因为此方法必定是及其复杂的。 我们回过头来看之前从 acceptor 线程中调用的 register 方法。 Poller # register 1234567891011121314151617181920public void register(final NioChannel socket) &#123; socket.setPoller(this); NioSocketWrapper ka = new NioSocketWrapper(socket, NioEndpoint.this); socket.setSocketWrapper(ka); ka.setPoller(this); ka.setReadTimeout(getConnectionTimeout()); ka.setWriteTimeout(getConnectionTimeout()); ka.setKeepAliveLeft(NioEndpoint.this.getMaxKeepAliveRequests()); ka.setSecure(isSSLEnabled()); PollerEvent r = eventCache.pop(); ka.interestOps(SelectionKey.OP_READ);//this is what OP_REGISTER turns into. // 注意第三个参数值 OP_REGISTER if ( r==null) r = new PollerEvent(socket,ka,OP_REGISTER); else r.reset(socket,ka,OP_REGISTER); // 添加 event 到 poller 中 addEvent(r);&#125; 这里将这个 socket（包含 socket 和 buffer 的 NioChannel 实例） 包装为一个 PollerEvent，然后添加到 events 中，此时调用此方法的 acceptor 结束返回，去处理新的 accepted 连接了。 接下来，我们已经知道了，poller 线程在循环过程中会不断调用 events() 方法，那么 PollerEvent 的 run() 方法很快就会被执行，我们就来看看刚刚这个新的连接被注册到这个 poller 后，会发生什么。 PollerEvent # run 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Overridepublic void run() &#123; // 对于新来的连接，前面我们说过，interestOps == OP_REGISTER if (interestOps == OP_REGISTER) &#123; try &#123; // 这步很关键！！！ // 将这个新连接 SocketChannel 注册到该 poller 的 Selector 中， // 设置监听 OP_READ 事件， // 将 socketWrapper 设置为 attachment 进行传递(这个对象可是什么鬼都有，往上看就知道了) socket.getIOChannel().register( socket.getPoller().getSelector(), SelectionKey.OP_READ, socketWrapper); &#125; catch (Exception x) &#123; log.error(sm.getString(&quot;endpoint.nio.registerFail&quot;), x); &#125; &#125; else &#123; /* else 这块不介绍，省得大家头大 */ final SelectionKey key = socket.getIOChannel().keyFor(socket.getPoller().getSelector()); try &#123; if (key == null) &#123; // The key was cancelled (e.g. due to socket closure) // and removed from the selector while it was being // processed. Count down the connections at this point // since it won&apos;t have been counted down when the socket // closed. socket.socketWrapper.getEndpoint().countDownConnection(); &#125; else &#123; final NioSocketWrapper socketWrapper = (NioSocketWrapper) key.attachment(); if (socketWrapper != null) &#123; //we are registering the key to start with, reset the fairness counter. int ops = key.interestOps() | interestOps; socketWrapper.interestOps(ops); key.interestOps(ops); &#125; else &#123; socket.getPoller().cancelledKey(key); &#125; &#125; &#125; catch (CancelledKeyException ckx) &#123; try &#123; socket.getPoller().cancelledKey(key); &#125; catch (Exception ignore) &#123;&#125; &#125; &#125;&#125; 到这里，我们再回顾一下：刚刚在 PollerEvent 的 run() 方法中，我们看到，新的 SocketChannel 注册到了 Poller 内部的 Selector 中，监听 OP_READ 事件，然后我们再回到 Poller 的 run() 看下，一旦该 SocketChannel 是 readable 的状态，那么就会进入到 poller 的 processKey 方法。 processKeyPoller # processKey 12345678910111213141516171819202122232425262728293031323334353637383940414243protected void processKey(SelectionKey sk, NioSocketWrapper attachment) &#123; try &#123; if ( close ) &#123; cancelledKey(sk); &#125; else if ( sk.isValid() &amp;&amp; attachment != null ) &#123; if (sk.isReadable() || sk.isWritable() ) &#123; // 忽略 sendfile if ( attachment.getSendfileData() != null ) &#123; processSendfile(sk,attachment, false); &#125; else &#123; // unregister 相应的 interest set， // 如接下来是处理 SocketChannel 进来的数据，那么就不再监听该 channel 的 OP_READ 事件 unreg(sk, attachment, sk.readyOps()); boolean closeSocket = false; // Read goes before write if (sk.isReadable()) &#123; // 处理读 if (!processSocket(attachment, SocketEvent.OPEN_READ, true)) &#123; closeSocket = true; &#125; &#125; if (!closeSocket &amp;&amp; sk.isWritable()) &#123; // 处理写 if (!processSocket(attachment, SocketEvent.OPEN_WRITE, true)) &#123; closeSocket = true; &#125; &#125; if (closeSocket) &#123; cancelledKey(sk); &#125; &#125; &#125; &#125; else &#123; //invalid key cancelledKey(sk); &#125; &#125; catch ( CancelledKeyException ckx ) &#123; cancelledKey(sk); &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); log.error(&quot;&quot;,t); &#125;&#125; 接下来是 processSocket 方法，注意第三个参数，上面进来的时候是 true。 AbstractEndpoint # processSocket 1234567891011121314151617181920212223242526272829303132public boolean processSocket(SocketWrapperBase&lt;S&gt; socketWrapper, SocketEvent event, boolean dispatch) &#123; try &#123; if (socketWrapper == null) &#123; return false; &#125; SocketProcessorBase&lt;S&gt; sc = processorCache.pop(); if (sc == null) &#123; // 创建一个 SocketProcessor 的实例 sc = createSocketProcessor(socketWrapper, event); &#125; else &#123; sc.reset(socketWrapper, event); &#125; Executor executor = getExecutor(); if (dispatch &amp;&amp; executor != null) &#123; // 将任务放到之前建立的 worker 线程池中执行 executor.execute(sc); &#125; else &#123; sc.run(); // ps: 如果 dispatch 为 false，那么就当前线程自己执行 &#125; &#125; catch (RejectedExecutionException ree) &#123; getLog().warn(sm.getString(&quot;endpoint.executor.fail&quot;, socketWrapper) , ree); return false; &#125; catch (Throwable t) &#123; ExceptionUtils.handleThrowable(t); // This means we got an OOM or similar creating a thread, or that // the pool and its queue are full getLog().error(sm.getString(&quot;endpoint.process.fail&quot;), t); return false; &#125; return true;&#125; NioEndpoint # createSocketProcessor 12345@Overrideprotected SocketProcessorBase&lt;NioChannel&gt; createSocketProcessor( SocketWrapperBase&lt;NioChannel&gt; socketWrapper, SocketEvent event) &#123; return new SocketProcessor(socketWrapper, event);&#125; 我们看到，提交到 worker 线程池中的是 NioEndpoint.SocketProcessor 的实例，至于它的 run() 方法之后的逻辑，我们就不再继续往里分析了。 总结最后，再祭出文章开始的那张图来总结一下： 这里简单梳理下前面我们说的流程，帮大家回忆一下： 指定 Protocol，初始化相应的 Endpoint，我们分析的是 NioEndpoint； init 过程：在 NioEndpoint 中做 bind 操作； start 过程：启动 worker 线程池，启动 1 个 Acceptor 和 2 个 Poller，当然它们都是默认值，可配； Acceptor 获取到新的连接后，getPoller0() 获取其中一个 Poller，然后 register 到 Poller 中； Poller 循环 selector.select(xxx)，如果有通道 readable，那么在 processKey 中将其放到 worker 线程池中。 后续的流程，感兴趣的读者请自行分析，本文就说到这里了。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解9：基于NIO的网络编程框架Netty]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A39%EF%BC%9A%E5%9F%BA%E4%BA%8ENIO%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6Netty%2F</url>
    <content type="text"><![CDATA[本文转自：https://sylvanassun.github.io/2017/11/30/2017-11-30-netty_introduction/ 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Netty概述Netty是一个基于异步与事件驱动的网络应用程序框架，它支持快速与简单地开发可维护的高性能的服务器与客户端。 所谓事件驱动就是由通过各种事件响应来决定程序的流程，在Netty中到处都充满了异步与事件驱动，这种特点使得应用程序可以以任意的顺序响应在任意的时间点产生的事件，它带来了非常高的可伸缩性，让你的应用可以在需要处理的工作不断增长时，通过某种可行的方式或者扩大它的处理能力来适应这种增长。 Netty提供了高性能与易用性，它具有以下特点： 拥有设计良好且统一的API，支持NIO与OIO（阻塞IO）等多种传输类型，支持真正的无连接UDP Socket。 简单而强大的线程模型，可高度定制线程（池）。(定制化的Reactor模型) 良好的模块化与解耦，支持可扩展和灵活的事件模型，可以很轻松地分离关注点以复用逻辑组件（可插拔的）。 性能高效，拥有比Java核心API更高的吞吐量，通过zero-copy功能以实现最少的内存复制消耗。 内置了许多常用的协议编解码器，如HTTP、SSL、WebScoket等常见协议可以通过Netty做到开箱即用。用户也可以利用Netty简单方便地实现自己的应用层协议。 大多数人使用Netty主要还是为了提高应用的性能，而高性能则离不开非阻塞IO。Netty的非阻塞IO是基于Java NIO的，并且对其进行了封装（直接使用Java NIO API在高复杂度下的应用中是一项非常繁琐且容易出错的操作，而Netty帮你封装了这些复杂操作）。 etty简介读完这一章，我们基本上可以了解到Netty所有重要的组件，对Netty有一个全面的认识，这对下一步深入学习Netty是十分重要的，而学完这一章，我们其实已经可以用Netty解决一些常规的问题了。 Netty都有哪些组件？为了更好的理解和进一步深入Netty，我们先总体认识一下Netty用到的组件及它们在整个Netty架构中是怎么协调工作的。Netty应用中必不可少的组件： Bootstrap or ServerBootstrap EventLoop EventLoopGroup ChannelPipeline Channel Future or ChannelFuture ChannelInitializer ChannelHandler Bootstrap，一个Netty应用通常由一个Bootstrap开始，它主要作用是配置整个Netty程序，串联起各个组件。 Handler，为了支持各种协议和处理数据的方式，便诞生了Handler组件。Handler主要用来处理各种事件，这里的事件很广泛，比如可以是连接、数据接收、异常、数据转换等。 ChannelInboundHandler，一个最常用的Handler。这个Handler的作用就是处理接收到数据时的事件，也就是说，我们的业务逻辑一般就是写在这个Handler里面的，ChannelInboundHandler就是用来处理我们的核心业务逻辑。 ChannelInitializer，当一个链接建立时，我们需要知道怎么来接收或者发送数据，当然，我们有各种各样的Handler实现来处理它，那么ChannelInitializer便是用来配置这些Handler，它会提供一个ChannelPipeline，并把Handler加入到ChannelPipeline。 ChannelPipeline，一个Netty应用基于ChannelPipeline机制，这种机制需要依赖于EventLoop和EventLoopGroup，因为它们三个都和事件或者事件处理相关。 EventLoops的目的是为Channel处理IO操作，一个EventLoop可以为多个Channel服务。 EventLoopGroup会包含多个EventLoop。 Channel代表了一个Socket链接，或者其它和IO操作相关的组件，它和EventLoop一起用来参与IO处理。 Future，在Netty中所有的IO操作都是异步的，因此，你不能立刻得知消息是否被正确处理，但是我们可以过一会等它执行完成或者直接注册一个监听，具体的实现就是通过Future和ChannelFutures,他们可以注册一个监听，当操作执行成功或失败时监听会自动触发。总之，所有的操作都会返回一个ChannelFuture。 Netty是如何处理连接请求和业务逻辑的呢？Channels、Events 和 IO Netty是一个非阻塞的、事件驱动的、网络编程框架。当然，我们很容易理解Netty会用线程来处理IO事件，对于熟悉多线程编程的人来说，你或许会想到如何同步你的代码，但是Netty不需要我们考虑这些，具体是这样： 一个Channel会对应一个EventLoop，而一个EventLoop会对应着一个线程，也就是说，仅有一个线程在负责一个Channel的IO操作。 关于这些名词之间的关系，可以见下图： 如图所示：当一个连接到达，Netty会注册一个channel，然后EventLoopGroup会分配一个EventLoop绑定到这个channel,在这个channel的整个生命周期过程中，都会由绑定的这个EventLoop来为它服务，而这个EventLoop就是一个线程。 说到这里，那么EventLoops和EventLoopGroups关系是如何的呢？我们前面说过一个EventLoopGroup包含多个Eventloop，但是我们看一下下面这幅图，这幅图是一个继承树，从这幅图中我们可以看出，EventLoop其实继承自EventloopGroup，也就是说，在某些情况下，我们可以把一个EventLoopGroup当做一个EventLoop来用。 ​ 如何配置一个Netty应用？BootsStrapping 我们利用BootsStrap来配置netty 应用，它有两种类型，一种用于Client端：BootsStrap，另一种用于Server端：ServerBootstrap，要想区别如何使用它们，你仅需要记住一个用在Client端，一个用在Server端。下面我们来详细介绍一下这两种类型的区别： 1.第一个最明显的区别是，ServerBootstrap用于Server端，通过调用bind()方法来绑定到一个端口监听连接；Bootstrap用于Client端，需要调用connect()方法来连接服务器端，但我们也可以通过调用bind()方法返回的ChannelFuture中获取Channel从而去connect服务器端。 2.客户端的Bootstrap一般用一个EventLoopGroup，而服务器端的ServerBootstrap会用到两个（这两个也可以是同一个实例）。为何服务器端要用到两个EventLoopGroup呢？这么设计有明显的好处，如果一个ServerBootstrap有两个EventLoopGroup，那么就可以把第一个EventLoopGroup用来专门负责绑定到端口监听连接事件，而把第二个EventLoopGroup用来处理每个接收到的连接，下面我们用一幅图来展现一下这种模式： PS: 如果仅由一个EventLoopGroup处理所有请求和连接的话，在并发量很大的情况下，这个EventLoopGroup有可能会忙于处理已经接收到的连接而不能及时处理新的连接请求，用两个的话，会有专门的线程来处理连接请求，不会导致请求超时的情况，大大提高了并发处理能力。 我们知道一个Channel需要由一个EventLoop来绑定，而且两者一旦绑定就不会再改变。一般情况下一个EventLoopGroup中的EventLoop数量会少于Channel数量，那么就很有可能出现一个多个Channel公用一个EventLoop的情况，这就意味着如果一个Channel中的EventLoop很忙的话，会影响到这个Eventloop对其它Channel的处理，这也就是为什么我们不能阻塞EventLoop的原因。 当然，我们的Server也可以只用一个EventLoopGroup,由一个实例来处理连接请求和IO事件，请看下面这幅图： Netty是如何处理数据的？Netty核心ChannelHandler 下面我们来看一下netty中是怎样处理数据的，回想一下我们前面讲到的Handler，对了，就是它。说到Handler我们就不得不提ChannelPipeline，ChannelPipeline负责安排Handler的顺序及其执行，下面我们就来详细介绍一下他们： ChannelPipeline and handlers 我们的应用程序中用到的最多的应该就是ChannelHandler，我们可以这么想象，数据在一个ChannelPipeline中流动，而ChannelHandler便是其中的一个个的小阀门，这些数据都会经过每一个ChannelHandler并且被它处理。这里有一个公共接口ChannelHandler: 从上图中我们可以看到，ChannelHandler有两个子类ChannelInboundHandler和ChannelOutboundHandler，这两个类对应了两个数据流向，如果数据是从外部流入我们的应用程序，我们就看做是inbound，相反便是outbound。其实ChannelHandler和Servlet有些类似，一个ChannelHandler处理完接收到的数据会传给下一个Handler，或者什么不处理，直接传递给下一个。下面我们看一下ChannelPipeline是如何安排ChannelHandler的： 从上图中我们可以看到，一个ChannelPipeline可以把两种Handler（ChannelInboundHandler和ChannelOutboundHandler）混合在一起，当一个数据流进入ChannelPipeline时，它会从ChannelPipeline头部开始传给第一个ChannelInboundHandler，当第一个处理完后再传给下一个，一直传递到管道的尾部。与之相对应的是，当数据被写出时，它会从管道的尾部开始，先经过管道尾部的“最后”一个ChannelOutboundHandler，当它处理完成后会传递给前一个ChannelOutboundHandler。 数据在各个Handler之间传递，这需要调用方法中传递的ChanneHandlerContext来操作， 在netty的API中提供了两个基类分ChannelOutboundHandlerAdapter和ChannelInboundHandlerAdapter，他们仅仅实现了调用ChanneHandlerContext来把消息传递给下一个Handler，因为我们只关心处理数据，因此我们的程序中可以继承这两个基类来帮助我们做这些，而我们仅需实现处理数据的部分即可。 我们知道InboundHandler和OutboundHandler在ChannelPipeline中是混合在一起的，那么它们如何区分彼此呢？其实很容易，因为它们各自实现的是不同的接口，对于inbound event，Netty会自动跳过OutboundHandler,相反若是outbound event，ChannelInboundHandler会被忽略掉。 当一个ChannelHandler被加入到ChannelPipeline中时，它便会获得一个ChannelHandlerContext的引用，而ChannelHandlerContext可以用来读写Netty中的数据流。因此，现在可以有两种方式来发送数据，一种是把数据直接写入Channel，一种是把数据写入ChannelHandlerContext，它们的区别是写入Channel的话，数据流会从Channel的头开始传递，而如果写入ChannelHandlerContext的话，数据流会流入管道中的下一个Handler。 如何处理我们的业务逻辑？Encoders, Decoders and Domain Logic Netty中会有很多Handler，具体是哪种Handler还要看它们继承的是InboundAdapter还是OutboundAdapter。当然，Netty中还提供了一些列的Adapter来帮助我们简化开发，我们知道在Channelpipeline中每一个Handler都负责把Event传递给下一个Handler，如果有了这些辅助Adapter，这些额外的工作都可自动完成，我们只需覆盖实现我们真正关心的部分即可。此外，还有一些Adapter会提供一些额外的功能，比如编码和解码。那么下面我们就来看一下其中的三种常用的ChannelHandler： Encoders和Decoders 因为我们在网络传输时只能传输字节流，因此，在发送数据之前，我们必须把我们的message型转换为bytes，与之对应，我们在接收数据后，必须把接收到的bytes再转换成message。我们把bytes to message这个过程称作Decode(解码成我们可以理解的)，把message to bytes这个过程成为Encode。 Netty中提供了很多现成的编码/解码器，我们一般从他们的名字中便可知道他们的用途，如ByteToMessageDecoder、MessageToByteEncoder，如专门用来处理Google Protobuf协议的ProtobufEncoder、 ProtobufDecoder。 我们前面说过，具体是哪种Handler就要看它们继承的是InboundAdapter还是OutboundAdapter，对于Decoders,很容易便可以知道它是继承自ChannelInboundHandlerAdapter或 ChannelInboundHandler，因为解码的意思是把ChannelPipeline传入的bytes解码成我们可以理解的message（即Java Object），而ChannelInboundHandler正是处理Inbound Event，而Inbound Event中传入的正是字节流。Decoder会覆盖其中的“ChannelRead()”方法，在这个方法中来调用具体的decode方法解码传递过来的字节流，然后通过调用ChannelHandlerContext.fireChannelRead(decodedMessage)方法把编码好的Message传递给下一个Handler。与之类似，Encoder就不必多少了。 Domain Logic 其实我们最最关心的事情就是如何处理接收到的解码后的数据，我们真正的业务逻辑便是处理接收到的数据。Netty提供了一个最常用的基类SimpleChannelInboundHandler，其中T就是这个Handler处理的数据的类型（上一个Handler已经替我们解码好了），消息到达这个Handler时，Netty会自动调用这个Handler中的channelRead0(ChannelHandlerContext,T)方法，T是传递过来的数据对象，在这个方法中我们便可以任意写我们的业务逻辑了。 Netty从某方面来说就是一套NIO框架，在Java NIO基础上做了封装，所以要想学好Netty我建议先理解好Java NIO， NIO可以称为New IO也可以称为Non-blocking IO，它比Java旧的阻塞IO在性能上要高效许多（如果让每一个连接中的IO操作都单独创建一个线程，那么阻塞IO并不会比NIO在性能上落后，但不可能创建无限多的线程，在连接数非常多的情况下会很糟糕）。 ByteBuffer：NIO的数据传输是基于缓冲区的，ByteBuffer正是NIO数据传输中所使用的缓冲区抽象。ByteBuffer支持在堆外分配内存，并且尝试避免在执行I/O操作中的多余复制。一般的I/O操作都需要进行系统调用，这样会先切换到内核态，内核态要先从文件读取数据到它的缓冲区，只有等数据准备完毕后，才会从内核态把数据写到用户态，所谓的阻塞IO其实就是说的在等待数据准备好的这段时间内进行阻塞。如果想要避免这个额外的内核操作，可以通过使用mmap（虚拟内存映射）的方式来让用户态直接操作文件。 Channel：它类似于(fd)文件描述符，简单地来说它代表了一个实体（如一个硬件设备、文件、Socket或者一个能够执行一个或多个不同的I/O操作的程序组件）。你可以从一个Channel中读取数据到缓冲区，也可以将一个缓冲区中的数据写入到Channel。 Selector：选择器是NIO实现的关键，NIO采用的是I/O多路复用的方式来实现非阻塞，Selector通过在一个线程中监听每个Channel的IO事件来确定有哪些已经准备好进行IO操作的Channel，因此可以在任何时间检查任意的读操作或写操作的完成状态。这种方式避免了等待IO操作准备数据时的阻塞，使用较少的线程便可以处理许多连接，减少了线程切换与维护的开销。 了解了NIO的实现思想之后，我觉得还很有必要了解一下Unix中的I/O模型，Unix中拥有以下5种I/O模型： 阻塞I/O（Blocking I/O） 非阻塞I/O（Non-blocking I/O） I/O多路复用（I/O multiplexing (select and poll)） 信号驱动I/O（signal driven I/O (SIGIO)） 异步I/O（asynchronous I/O (the POSIX aio_functions)） 阻塞I/O模型是最常见的I/O模型，通常我们使用的InputStream/OutputStream都是基于阻塞I/O模型。在上图中，我们使用UDP作为例子，recvfrom()函数是UDP协议用于接收数据的函数，它需要使用系统调用并一直阻塞到内核将数据准备好，之后再由内核缓冲区复制数据到用户态（即是recvfrom()接收到数据），所谓阻塞就是在等待内核准备数据的这段时间内什么也不干。 举个生活中的例子，阻塞I/O就像是你去餐厅吃饭，在等待饭做好的时间段中，你只能在餐厅中坐着干等（如果你在玩手机那么这就是非阻塞I/O了）。 在非阻塞I/O模型中，内核在数据尚未准备好的情况下回返回一个错误码EWOULDBLOCK，而recvfrom并没有在失败的情况下选择阻塞休眠，而是不断地向内核询问是否已经准备完毕，在上图中，前三次内核都返回了EWOULDBLOCK，直到第四次询问时，内核数据准备完毕，然后开始将内核中缓存的数据复制到用户态。这种不断询问内核以查看某种状态是否完成的方式被称为polling（轮询）。 非阻塞I/O就像是你在点外卖，只不过你非常心急，每隔一段时间就要打电话问外卖小哥有没有到。 I/O多路复用的思想跟非阻塞I/O是一样的，只不过在非阻塞I/O中，是在recvfrom的用户态（或一个线程）中去轮询内核，这种方式会消耗大量的CPU时间。而I/O多路复用则是通过select()或poll()系统调用来负责进行轮询，以实现监听I/O读写事件的状态。如上图中，select监听到一个datagram可读时，就交由recvfrom去发送系统调用将内核中的数据复制到用户态。 这种方式的优点很明显，通过I/O多路复用可以监听多个文件描述符，且在内核中完成监控的任务。但缺点是至少需要两个系统调用（select()与recvfrom()）。 I/O多路复用同样适用于点外卖这个例子，只不过你在等外卖的期间完全可以做自己的事情，当外卖到的时候会通过外卖APP或者由外卖小哥打电话来通知你(因为内核会帮你轮询)。 Unix中提供了两种I/O多路复用函数，select()和poll()。select()的兼容性更好，但它在单个进程中所能监控的文件描述符是有限的，这个值与FD_SETSIZE相关，32位系统中默认为1024，64位系统中为2048。select()还有一个缺点就是他轮询的方式，它采取了线性扫描的轮询方式，每次都要遍历FD_SETSIZE个文件描述符，不管它们是否活不活跃的。poll()本质上与select()的实现没有区别，不过在数据结构上区别很大，用户必须分配一个pollfd结构数组，该数组维护在内核态中，正因如此，poll()并不像select()那样拥有大小上限的限制，但缺点同样也很明显，大量的fd数组会在用户态与内核态之间不断复制，不管这样的复制是否有意义。 还有一种比select()与poll()更加高效的实现叫做epoll()，它是由Linux内核2.6推出的可伸缩的I/O多路复用实现，目的是为了替代select()与poll()。epoll()同样没有文件描述符上限的限制，它使用一个文件描述符来管理多个文件描述符，并使用一个红黑树来作为存储结构。同时它还支持边缘触发（edge-triggered）与水平触发（level-triggered）两种模式（poll()只支持水平触发），在边缘触发模式下，epoll_wait仅会在新的事件对象首次被加入到epoll时返回，而在水平触发模式下，epoll_wait会在事件状态未变更前不断地触发。也就是说，边缘触发模式只会在文件描述符变为就绪状态时通知一次，水平触发模式会不断地通知该文件描述符直到被处理。 关于epoll_wait请参考如下epoll API。 // 创建一个epoll对象并返回它的文件描述符。 // 参数flags允许修改epoll的行为，它只有一个有效值EPOLL_CLOEXEC。 int epoll_create1(int flags); // 配置对象，该对象负责描述监控哪些文件描述符和哪些事件。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // 等待与epoll_ctl注册的任何事件，直至事件发生一次或超时。 // 返回在events中发生的事件，最多同时返回maxevents个。 int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); epoll另一亮点是采用了事件驱动的方式而不是轮询，在epoll_ctl中注册的文件描述符在事件触发的时候会通过一个回调机制来激活该文件描述符，epoll_wait便可以收到通知。这样效率就不会与文件描述符的数量成正比 在Java NIO2（从JDK1.7开始引入）中，只要Linux内核版本在2.6以上，就会采用epoll，如下源码所示（DefaultSelectorProvider.java）。 public static SelectorProvider create() { String osname = AccessController.doPrivileged( new GetPropertyAction("os.name")); if ("SunOS".equals(osname)) { return new sun.nio.ch.DevPollSelectorProvider(); } // use EPollSelectorProvider for Linux kernels >= 2.6 if ("Linux".equals(osname)) { String osversion = AccessController.doPrivileged( new GetPropertyAction("os.version")); String[] vers = osversion.split("\\.", 0); if (vers.length >= 2) { try { int major = Integer.parseInt(vers[0]); int minor = Integer.parseInt(vers[1]); if (major > 2 || (major == 2 && minor >= 6)) { return new sun.nio.ch.EPollSelectorProvider(); } } catch (NumberFormatException x) { // format not recognized } } } return new sun.nio.ch.PollSelectorProvider(); } 信号驱动I/O模型使用到了信号，内核在数据准备就绪时会通过信号来进行通知。我们首先开启了一个信号驱动I/O套接字，并使用sigaction系统调用来安装信号处理程序，内核直接返回，不会阻塞用户态。当datagram准备好时，内核会发送SIGIN信号，recvfrom接收到信号后会发送系统调用开始进行I/O操作。 这种模型的优点是主进程（线程）不会被阻塞，当数据准备就绪时，通过信号处理程序来通知主进程（线程）准备进行I/O操作与对数据的处理。 我们之前讨论的各种I/O模型无论是阻塞还是非阻塞，它们所说的阻塞都是指的数据准备阶段。异步I/O模型同样依赖于信号处理程序来进行通知，但与以上I/O模型都不相同的是，异步I/O模型通知的是I/O操作已经完成，而不是数据准备完成。 可以说异步I/O模型才是真正的非阻塞，主进程只管做自己的事情，然后在I/O操作完成时调用回调函数来完成一些对数据的处理操作即可。 闲扯了这么多，想必大家已经对I/O模型有了一个深刻的认识。之后，我们将会结合部分源码（Netty4.X）来探讨Netty中的各大核心组件，以及如何使用Netty，你会发现实现一个Netty程序是多么简单（而且还伴随了高性能与可维护性）。 ByteBuf 网络传输的基本单位是字节，在Java NIO中提供了ByteBuffer作为字节缓冲区容器，但该类的API使用起来不太方便，所以Netty实现了ByteBuf作为其替代品，下面是使用ByteBuf的优点： 相比ByteBuffer使用起来更加简单。 通过内置的复合缓冲区类型实现了透明的zero-copy。 容量可以按需增长。 读和写使用了不同的索引指针。 支持链式调用。 支持引用计数与池化。 可以被用户自定义的缓冲区类型扩展。 在讨论ByteBuf之前，我们先需要了解一下ByteBuffer的实现，这样才能比较深刻地明白它们之间的区别。 ByteBuffer继承于abstract class Buffer（所以还有LongBuffer、IntBuffer等其他类型的实现），本质上它只是一个有限的线性的元素序列，包含了三个重要的属性。 Capacity：缓冲区中元素的容量大小，你只能将capacity个数量的元素写入缓冲区，一旦缓冲区已满就需要清理缓冲区才能继续写数据。 Position：指向下一个写入数据位置的索引指针，初始位置为0，最大为capacity-1。当写模式转换为读模式时，position需要被重置为0。 Limit：在写模式中，limit是可以写入缓冲区的最大索引，也就是说它在写模式中等价于缓冲区的容量。在读模式中，limit表示可以读取数据的最大索引。 由于Buffer中只维护了position一个索引指针，所以它在读写模式之间的切换需要调用一个flip()方法来重置指针。使用Buffer的流程一般如下： 写入数据到缓冲区。 调用flip()方法。 从缓冲区中读取数据 调用buffer.clear()或者buffer.compact()清理缓冲区，以便下次写入数据。 RandomAccessFile aFile = new RandomAccessFile("data/nio-data.txt", "rw"); FileChannel inChannel = aFile.getChannel(); // 分配一个48字节大小的缓冲区 ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); // 读取数据到缓冲区 while (bytesRead != -1) { buf.flip(); // 将position重置为0 while(buf.hasRemaining()){ System.out.print((char) buf.get()); // 读取数据并输出到控制台 } buf.clear(); // 清理缓冲区 bytesRead = inChannel.read(buf); } aFile.close(); Buffer中核心方法的实现也非常简单，主要就是在操作指针position。 Buffer中核心方法的实现也非常简单，主要就是在操作指针position。 /** * Sets this buffer's mark at its position. * * @return This buffer */ public final Buffer mark() { mark = position; // mark属性是用来标记当前索引位置的 return this; } // 将当前索引位置重置为mark所标记的位置 public final Buffer reset() { int m = mark; if (m < 0) throw new InvalidMarkException(); position = m; return this; } // 翻转这个Buffer，将limit设置为当前索引位置，然后再把position重置为0 public final Buffer flip() { limit = position; position = 0; mark = -1; return this; } // 清理缓冲区 // 说是清理,也只是把postion与limit进行重置,之后再写入数据就会覆盖之前的数据了 public final Buffer clear() { position = 0; limit = capacity; mark = -1; return this; } // 返回剩余空间 public final int remaining() { return limit - position; } Java NIO中的Buffer API操作的麻烦之处就在于读写转换需要手动重置指针。而ByteBuf没有这种繁琐性，它维护了两个不同的索引，一个用于读取，一个用于写入。当你从ByteBuf读取数据时，它的readerIndex将会被递增已经被读取的字节数，同样的，当你写入数据时，writerIndex则会递增。readerIndex的最大范围在writerIndex的所在位置，如果试图移动readerIndex超过该值则会触发异常。 ByteBuf中名称以read或write开头的方法将会递增它们其对应的索引，而名称以get或set开头的方法则不会。ByteBuf同样可以指定一个最大容量，试图移动writerIndex超过该值则会触发异常。 public byte readByte() { this.checkReadableBytes0(1); // 检查readerIndex是否已越界 int i = this.readerIndex; byte b = this._getByte(i); this.readerIndex = i + 1; // 递增readerIndex return b; } private void checkReadableBytes0(int minimumReadableBytes) { this.ensureAccessible(); if(this.readerIndex > this.writerIndex - minimumReadableBytes) { throw new IndexOutOfBoundsException(String.format("readerIndex(%d) + length(%d) exceeds writerIndex(%d): %s", new Object[]{Integer.valueOf(this.readerIndex), Integer.valueOf(minimumReadableBytes), Integer.valueOf(this.writerIndex), this})); } } public ByteBuf writeByte(int value) { this.ensureAccessible(); this.ensureWritable0(1); // 检查writerIndex是否会越过capacity this._setByte(this.writerIndex++, value); return this; } private void ensureWritable0(int minWritableBytes) { if(minWritableBytes > this.writableBytes()) { if(minWritableBytes > this.maxCapacity - this.writerIndex) { throw new IndexOutOfBoundsException(String.format("writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s", new Object[]{Integer.valueOf(this.writerIndex), Integer.valueOf(minWritableBytes), Integer.valueOf(this.maxCapacity), this})); } else { int newCapacity = this.alloc().calculateNewCapacity(this.writerIndex + minWritableBytes, this.maxCapacity); this.capacity(newCapacity); } } } // get与set只对传入的索引进行了检查，然后对其位置进行get或set public byte getByte(int index) { this.checkIndex(index); return this._getByte(index); } public ByteBuf setByte(int index, int value) { this.checkIndex(index); this._setByte(index, value); return this; } ByteBuf同样支持在堆内和堆外进行分配。在堆内分配也被称为支撑数组模式，它能在没有使用池化的情况下提供快速的分配和释放。 ByteBuf heapBuf = Unpooled.copiedBuffer(bytes); if (heapBuf.hasArray()) { // 判断是否有一个支撑数组 byte[] array = heapBuf.array(); // 计算第一个字节的偏移量 int offset = heapBuf.arrayOffset() + heapBuf.readerIndex(); int length = heapBuf.readableBytes(); // 获得可读字节 handleArray(array,offset,length); // 调用你的处理方法 } 另一种模式为堆外分配，Java NIO ByteBuffer类在JDK1.4时就已经允许JVM实现通过JNI调用来在堆外分配内存（调用malloc()函数在JVM堆外分配内存），这主要是为了避免额外的缓冲区复制操作。 ByteBuf directBuf = Unpooled.directBuffer(capacity); if (!directBuf.hasArray()) { int length = directBuf.readableBytes(); byte[] array = new byte[length]; // 将字节复制到数组中 directBuf.getBytes(directBuf.readerIndex(),array); handleArray(array,0,length); } ByteBuf还支持第三种模式，它被称为复合缓冲区，为多个ByteBuf提供了一个聚合视图。在这个视图中，你可以根据需要添加或者删除ByteBuf实例，ByteBuf的子类CompositeByteBuf实现了该模式。 一个适合使用复合缓冲区的场景是HTTP协议，通过HTTP协议传输的消息都会被分成两部分——头部和主体，如果这两部分由应用程序的不同模块产生，将在消息发送时进行组装，并且该应用程序还会为多个消息复用相同的消息主体，这样对于每个消息都将会创建一个新的头部，产生了很多不必要的内存操作。使用CompositeByteBuf是一个很好的选择，它消除了这些额外的复制，以帮助你复用这些消息。 CompositeByteBuf messageBuf = Unpooled.compositeBuffer(); ByteBuf headerBuf = ....; ByteBuf bodyBuf = ....; messageBuf.addComponents(headerBuf,bodyBuf); for (ByteBuf buf : messageBuf) { System.out.println(buf.toString()); } CompositeByteBuf透明的实现了zero-copy，zero-copy其实就是避免数据在两个内存区域中来回的复制。从操作系统层面上来讲，zero-copy指的是避免在内核态与用户态之间的数据缓冲区复制（通过mmap避免），而Netty中的zero-copy更偏向于在用户态中的数据操作的优化，就像使用CompositeByteBuf来复用多个ByteBuf以避免额外的复制，也可以使用wrap()方法来将一个字节数组包装成ByteBuf，又或者使用ByteBuf的slice()方法把它分割为多个共享同一内存区域的ByteBuf，这些都是为了优化内存的使用率。 那么如何创建ByteBuf呢？在上面的代码中使用到了Unpooled，它是Netty提供的一个用于创建与分配ByteBuf的工具类，建议都使用这个工具类来创建你的缓冲区，不要自己去调用构造函数。经常使用的是wrappedBuffer()与copiedBuffer()，它们一个是用于将一个字节数组或ByteBuffer包装为一个ByteBuf，一个是根据传入的字节数组与ByteBuffer/ByteBuf来复制出一个新的ByteBuf。 // 通过array.clone()来复制一个数组进行包装 public static ByteBuf copiedBuffer(byte[] array) { return array.length == 0?EMPTY_BUFFER:wrappedBuffer((byte[])array.clone()); } // 默认是堆内分配 public static ByteBuf wrappedBuffer(byte[] array) { return (ByteBuf)(array.length == 0?EMPTY_BUFFER:new UnpooledHeapByteBuf(ALLOC, array, array.length)); } // 也提供了堆外分配的方法 private static final ByteBufAllocator ALLOC; public static ByteBuf directBuffer(int initialCapacity) { return ALLOC.directBuffer(initialCapacity); } 相对底层的分配方法是使用ByteBufAllocator，Netty实现了PooledByteBufAllocator和UnpooledByteBufAllocator，前者使用了jemalloc（一种malloc()的实现）来分配内存，并且实现了对ByteBuf的池化以提高性能。后者分配的是未池化的ByteBuf，其分配方式与之前讲的一致。 Channel channel = ...; ByteBufAllocator allocator = channel.alloc(); ByteBuf buffer = allocator.directBuffer(); do something....... 为了优化内存使用率，Netty提供了一套手动的方式来追踪不活跃对象，像UnpooledHeapByteBuf这种分配在堆内的对象得益于JVM的GC管理，无需额外操心，而UnpooledDirectByteBuf是在堆外分配的，它的内部基于DirectByteBuffer，DirectByteBuffer会先向Bits类申请一个额度（Bits还拥有一个全局变量totalCapacity，记录了所有DirectByteBuffer总大小），每次申请前都会查看是否已经超过-XX:MaxDirectMemorySize所设置的上限，如果超限就会尝试调用System.gc()，以试图回收一部分内存，然后休眠100毫秒，如果内存还是不足，则只能抛出OOM异常。堆外内存的回收虽然有了这么一层保障，但为了提高性能与使用率，主动回收也是很有必要的。由于Netty还实现了ByteBuf的池化，像PooledHeapByteBuf和PooledDirectByteBuf就必须依赖于手动的方式来进行回收（放回池中）。 Netty使用了引用计数器的方式来追踪那些不活跃的对象。引用计数的接口为ReferenceCounted，它的思想很简单，只要ByteBuf对象的引用计数大于0，就保证该对象不会被释放回收，可以通过手动调用release()与retain()方法来操作该对象的引用计数值递减或递增。用户也可以通过自定义一个ReferenceCounted的实现类，以满足自定义的规则。 package io.netty.buffer; public abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf { // 由于ByteBuf的实例对象会非常多,所以这里没有将refCnt包装为AtomicInteger // 而是使用一个全局的AtomicIntegerFieldUpdater来负责操作refCnt private static final AtomicIntegerFieldUpdater refCntUpdater = AtomicIntegerFieldUpdater.newUpdater(AbstractReferenceCountedByteBuf.class, "refCnt"); // 每个ByteBuf的初始引用值都为1 private volatile int refCnt = 1; public int refCnt() { return this.refCnt; } protected final void setRefCnt(int refCnt) { this.refCnt = refCnt; } public ByteBuf retain() { return this.retain0(1); } // 引用计数值递增increment，increment必须大于0 public ByteBuf retain(int increment) { return this.retain0(ObjectUtil.checkPositive(increment, "increment")); } public static int checkPositive(int i, String name) { if(i 0)"); } else { return i; } } // 使用CAS操作不断尝试更新值 private ByteBuf retain0(int increment) { int refCnt; int nextCnt; do { refCnt = this.refCnt; nextCnt = refCnt + increment; if(nextCnt]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程与NIO详解8：浅析mmap和Direct Buffer]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8ENIO%E8%AF%A6%E8%A7%A38%EF%BC%9A%E6%B5%85%E6%9E%90mmap%E5%92%8CDirect%20Buffer%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.cnblogs.com/huxiao-tee/p/4660352.html 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 阅读目录 mmap基础概念 mmap内存映射原理 mmap和常规文件操作的区别 mmap优点总结 mmap相关函数 mmap使用细节 mmap基础概念mmap是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read,write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示： ![](https://images0.cnblogs.com/blog2015/571793/201507/200501092691998.png)由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。 linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示： ![](https://images0.cnblogs.com/blog2015/571793/201507/200501434261629.png)vm_area_struct结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作需要用要的信息，都可以从vm_area_struct中获得。mmap函数就是要创建一个新的vm_area_struct结构，并将其与文件的物理磁盘地址相连。具体步骤请看下一节。 回到顶部 mmap内存映射原理mmap内存映射的实现过程，总的来说可以分为三个阶段： （一）进程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域 1、进程在用户空间调用库函数mmap，原型：void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset); 2、在当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址 3、为此虚拟区分配一个vm_area_struct结构，接着对这个结构的各个域进行了初始化 4、将新建的虚拟区结构（vm_area_struct）插入进程的虚拟地址区域链表或树中 （二）调用内核空间的系统调用函数mmap（不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系 5、为映射分配了新的虚拟地址区域后，通过待映射的文件指针，在文件描述符表中找到对应的文件描述符，通过文件描述符，链接到内核“已打开文件集”中该文件的文件结构体（struct file），每个文件结构体维护着和这个已打开文件相关各项信息。 6、通过该文件的文件结构体，链接到file_operations模块，调用内核函数mmap，其原型为：int mmap(struct file *filp, struct vm_area_struct *vma)，不同于用户空间库函数。 7、内核mmap函数通过虚拟文件系统inode模块定位到文件磁盘物理地址。 8、通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系。此时，这片虚拟地址并没有任何数据关联到主存中。 （三）进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝 注：前两个阶段仅在于创建虚拟区间并完成地址映射，但是并没有将任何文件数据的拷贝至主存。真正的文件读取是当进程发起读或写操作时。 9、进程的读或写操作访问虚拟地址空间这一段映射地址，通过查询页表，发现这一段地址并不在物理页面上。因为目前只建立了地址映射，真正的硬盘数据还没有拷贝到内存中，因此引发缺页异常。 10、缺页异常进行一系列判断，确定无非法操作后，内核发起请求调页过程。 11、调页过程先在交换缓存空间（swap cache）中寻找需要访问的内存页，如果没有则调用nopage函数把所缺的页从磁盘装入到主存中。 12、之后进程即可对这片主存进行读或者写的操作，如果写操作改变了其内容，一定时间后系统会自动回写脏页面到对应磁盘地址，也即完成了写入到文件的过程。 注：修改过的脏页面并不会立即更新回文件中，而是有一段时间的延迟，可以调用msync()来强制同步, 这样所写的内容就能立即保存到文件里了。 回到顶部 mmap和常规文件操作的区别对linux文件系统不了解的朋友，请参阅我之前写的博文《从内核文件系统看文件读写过程》，我们首先简单的回顾一下常规文件系统操作（调用read/fread等类函数）中，函数的调用过程： 1、进程发起读文件请求。 2、内核通过查找进程文件符表，定位到内核已打开文件集上的文件信息，从而找到此文件的inode。 3、inode在address_space上查找要请求的文件页是否已经缓存在页缓存中。如果存在，则直接返回这片文件页的内容。 4、如果不存在，则通过inode定位到文件磁盘地址，将数据从磁盘复制到页缓存。之后再次发起读页面过程，进而将页缓存中的数据发给用户进程。 总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过了两次数据拷贝过程，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的buffer在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝。 而使用mmap操作文件中，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中，供进程使用。 总而言之，常规文件操作需要从磁盘到页缓存再到用户主存的两次数据拷贝。而mmap操控文件，只需要从磁盘到用户主存的一次数据拷贝过程。说白了，mmap的关键点是实现了用户空间和内核空间的数据直接交互而省去了空间不同数据不通的繁琐过程。因此mmap效率更高。 回到顶部 mmap优点总结由上文讨论可知，mmap优点共有一下几点： 1、对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代I/O读写，提高了文件读取效率。 2、实现了用户空间和内核空间的高效交互方式。两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。 3、提供进程间共享内存及相互通信的方式。不管是父子进程还是无亲缘关系的进程，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的。 同时，如果进程A和进程B都映射了区域C，当A第一次读取C时通过缺页从磁盘复制文件页到内存中；但当B再读C的相同页面时，虽然也会产生缺页异常，但是不再需要从磁盘中复制文件过来，而可直接使用已经保存在内存中的文件数据。4、可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好的解决。换句话说，但凡是需要用磁盘空间代替内存的时候，mmap都可以发挥其功效。 mmap使用细节1、使用mmap需要注意的一个关键点是，mmap映射区域大小必须是物理页大小(page_size)的整倍数（32位系统中通常是4k字节）。原因是，内存的最小粒度是页，而进程虚拟地址空间和内存的映射也是以页为单位。为了匹配内存的操作，mmap从磁盘到虚拟地址空间的映射也必须是页。 2、内核可以跟踪被内存映射的底层对象（文件）的大小，进程可以合法的访问在当前文件大小以内又在内存映射区以内的那些字节。也就是说，如果文件的大小一直在扩张，只要在映射区域范围内的数据，进程都可以合法得到，这和映射建立时文件的大小无关。具体情形参见“情形三”。 3、映射建立之后，即使文件关闭，映射依然存在。因为映射的是磁盘的地址，不是文件本身，和文件句柄无关。同时可用于进程间通信的有效地址空间不完全受限于被映射文件的大小，因为是按页映射。 在上面的知识前提下，我们下面看看如果大小不是页的整倍数的具体情况： 情形一：一个文件的大小是5000字节，mmap函数从一个文件的起始位置开始，映射5000字节到虚拟内存中。 分析：因为单位物理页面的大小是4096字节，虽然被映射的文件只有5000字节，但是对应到进程虚拟地址区域的大小需要满足整页大小，因此mmap函数执行后，实际映射到虚拟内存区域8192个 字节，5000~8191的字节部分用零填充。映射后的对应关系如下图所示： ![](https://images0.cnblogs.com/blog2015/571793/201507/200521495513717.png)此时： （1）读/写前5000个字节（0~4999），会返回操作文件内容。 （2）读字节50008191时，结果全为0。写50008191时，进程不会报错，但是所写的内容不会写入原文件中 。 （3）读/写8192以外的磁盘部分，会返回一个SIGSECV错误。 情形二：一个文件的大小是5000字节，mmap函数从一个文件的起始位置开始，映射15000字节到虚拟内存中，即映射大小超过了原始文件的大小。 分析：由于文件的大小是5000字节，和情形一一样，其对应的两个物理页。那么这两个物理页都是合法可以读写的，只是超出5000的部分不会体现在原文件中。由于程序要求映射15000字节，而文件只占两个物理页，因此8192字节~15000字节都不能读写，操作时会返回异常。如下图所示： ![](https://images0.cnblogs.com/blog2015/571793/201507/200522381763096.png)此时： （1）进程可以正常读/写被映射的前5000字节(0~4999)，写操作的改动会在一定时间后反映在原文件中。 （2）对于5000~8191字节，进程可以进行读写过程，不会报错。但是内容在写入前均为0，另外，写入后不会反映在文件中。 （3）对于8192~14999字节，进程不能对其进行读写，会报SIGBUS错误。 （4）对于15000以外的字节，进程不能对其读写，会引发SIGSEGV错误。 情形三：一个文件初始大小为0，使用mmap操作映射了1000*4K的大小，即1000个物理页大约4M字节空间，mmap返回指针ptr。 分析：如果在映射建立之初，就对文件进行读写操作，由于文件大小为0，并没有合法的物理页对应，如同情形二一样，会返回SIGBUS错误。 但是如果，每次操作ptr读写前，先增加文件的大小，那么ptr在文件大小内部的操作就是合法的。例如，文件扩充4096字节，ptr就能操作ptr ~ [ (char)ptr + 4095]的空间。只要文件扩充的范围在1000个物理页（映射范围）内，ptr都可以对应操作相同的大小。 这样，方便随时扩充文件空间，随时写入文件，不造成空间浪费。 本文转自：https://www.jianshu.com/p/007052ee3773 堆外内存堆外内存是相对于堆内内存的一个概念。堆内内存是由JVM所管控的Java进程内存，我们平时在Java中创建的对象都处于堆内内存中，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理它们的内存。那么堆外内存就是存在于JVM管控之外的一块内存区域，因此它是不受JVM的管控。 在讲解DirectByteBuffer之前，需要先简单了解两个知识点java引用类型，因为DirectByteBuffer是通过虚引用(Phantom Reference)来实现堆外内存的释放的。PhantomReference 是所有“弱引用”中最弱的引用类型。不同于软引用和弱引用，虚引用无法通过 get() 方法来取得目标对象的强引用从而使用目标对象，观察源码可以发现 get() 被重写为永远返回 null。那虚引用到底有什么作用？其实虚引用主要被用来 跟踪对象被垃圾回收的状态，通过查看引用队列中是否包含对象所对应的虚引用来判断它是否 即将被垃圾回收，从而采取行动。它并不被期待用来取得目标对象的引用，而目标对象被回收前，它的引用会被放入一个 ReferenceQueue 对象中，从而达到跟踪对象垃圾回收的作用。关于java引用类型的实现和原理可以阅读之前的文章Reference 、ReferenceQueue 详解 和Java 引用类型简述 关于linux的内核态和用户态 内核态：控制计算机的硬件资源，并提供上层应用程序运行的环境。比如socket I/0操作或者文件的读写操作等 用户态：上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源。 系统调用：为了使上层应用能够访问到这些资源，内核为上层应用提供访问的接口。 因此我们可以得知当我们通过JNI调用的native方法实际上就是从用户态切换到了内核态的一种方式。并且通过该系统调用使用操作系统所提供的功能。 Q：为什么需要用户进程(位于用户态中)要通过系统调用(Java中即使JNI)来调用内核态中的资源，或者说调用操作系统的服务了？A：intel cpu提供Ring0-Ring3四种级别的运行模式，Ring0级别最高，Ring3最低。Linux使用了Ring3级别运行用户态，Ring0作为内核态。Ring3状态不能访问Ring0的地址空间，包括代码和数据。因此用户态是没有权限去操作内核态的资源的，它只能通过系统调用外完成用户态到内核态的切换，然后在完成相关操作后再有内核态切换回用户态。 DirectByteBuffer ———— 直接缓冲DirectByteBuffer是Java用于实现堆外内存的一个重要类，我们可以通过该类实现堆外内存的创建、使用和销毁。 DirectByteBuffer该类本身还是位于Java内存模型的堆中。堆内内存是JVM可以直接管控、操纵。而DirectByteBuffer中的unsafe.allocateMemory(size);是个一个native方法，这个方法分配的是堆外内存，通过C的malloc来进行分配的。分配的内存是系统本地的内存，并不在Java的内存中，也不属于JVM管控范围，所以在DirectByteBuffer一定会存在某种方式来操纵堆外内存。在DirectByteBuffer的父类Buffer中有个address属性： 123// Used only by direct buffers// NOTE: hoisted here for speed in JNI GetDirectBufferAddresslong address; address只会被直接缓存给使用到。之所以将address属性升级放在Buffer中，是为了在JNI调用GetDirectBufferAddress时提升它调用的速率。address表示分配的堆外内存的地址。 unsafe.allocateMemory(size);分配完堆外内存后就会返回分配的堆外内存基地址，并将这个地址赋值给了address属性。这样我们后面通过JNI对这个堆外内存操作时都是通过这个address来实现的了。 在前面我们说过，在linux中内核态的权限是最高的，那么在内核态的场景下，操作系统是可以访问任何一个内存区域的，所以操作系统是可以访问到Java堆的这个内存区域的。Q：那为什么操作系统不直接访问Java堆内的内存区域了？A：这是因为JNI方法访问的内存区域是一个已经确定了的内存区域地质，那么该内存地址指向的是Java堆内内存的话，那么如果在操作系统正在访问这个内存地址的时候，Java在这个时候进行了GC操作，而GC操作会涉及到数据的移动操作[GC经常会进行先标志在压缩的操作。即，将可回收的空间做标志，然后清空标志位置的内存，然后会进行一个压缩，压缩就会涉及到对象的移动，移动的目的是为了腾出一块更加完整、连续的内存空间，以容纳更大的新对象]，数据的移动会使JNI调用的数据错乱。所以JNI调用的内存是不能进行GC操作的。 Q：如上面所说，JNI调用的内存是不能进行GC操作的，那该如何解决了？A：①堆内内存与堆外内存之间数据拷贝的方式(并且在将堆内内存拷贝到堆外内存的过程JVM会保证不会进行GC操作)：比如我们要完成一个从文件中读数据到堆内内存的操作，即FileChannelImpl.read(HeapByteBuffer)。这里实际上File I/O会将数据读到堆外内存中，然后堆外内存再讲数据拷贝到堆内内存，这样我们就读到了文件中的内存。 12345678910111213141516171819202122232425262728static int read(FileDescriptor var0, ByteBuffer var1, long var2, NativeDispatcher var4) throws IOException &#123; if (var1.isReadOnly()) &#123; throw new IllegalArgumentException(&quot;Read-only buffer&quot;); &#125; else if (var1 instanceof DirectBuffer) &#123; return readIntoNativeBuffer(var0, var1, var2, var4); &#125; else &#123; // 分配临时的堆外内存 ByteBuffer var5 = Util.getTemporaryDirectBuffer(var1.remaining()); int var7; try &#123; // File I/O 操作会将数据读入到堆外内存中 int var6 = readIntoNativeBuffer(var0, var5, var2, var4); var5.flip(); if (var6 &gt; 0) &#123; // 将堆外内存的数据拷贝到堆外内存中 var1.put(var5); &#125; var7 = var6; &#125; finally &#123; // 里面会调用DirectBuffer.cleaner().clean()来释放临时的堆外内存 Util.offerFirstTemporaryDirectBuffer(var5); &#125; return var7; &#125;&#125; 而写操作则反之，我们会将堆内内存的数据线写到对堆外内存中，然后操作系统会将堆外内存的数据写入到文件中。② 直接使用堆外内存，如DirectByteBuffer：这种方式是直接在堆外分配一个内存(即，native memory)来存储数据，程序通过JNI直接将数据读/写到堆外内存中。因为数据直接写入到了堆外内存中，所以这种方式就不会再在JVM管控的堆内再分配内存来存储数据了，也就不存在堆内内存和堆外内存数据拷贝的操作了。这样在进行I/O操作时，只需要将这个堆外内存地址传给JNI的I/O的函数就好了。 DirectByteBuffer堆外内存的创建和回收的源码解读堆外内存分配123456789101112131415161718192021222324252627DirectByteBuffer(int cap) &#123; // package-private super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); // 保留总分配内存(按页分配)的大小和实际内存的大小 Bits.reserveMemory(size, cap); long base = 0; try &#123; // 通过unsafe.allocateMemory分配堆外内存，并返回堆外内存的基地址 base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; // 构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，堆外内存也会被释放 cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;&#125; Bits.reserveMemory(size, cap) 方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960static void reserveMemory(long size, int cap) &#123; if (!memoryLimitSet &amp;&amp; VM.isBooted()) &#123; maxMemory = VM.maxDirectMemory(); memoryLimitSet = true; &#125; // optimist! if (tryReserveMemory(size, cap)) &#123; return; &#125; final JavaLangRefAccess jlra = SharedSecrets.getJavaLangRefAccess(); // retry while helping enqueue pending Reference objects // which includes executing pending Cleaner(s) which includes // Cleaner(s) that free direct buffer memory while (jlra.tryHandlePendingReference()) &#123; if (tryReserveMemory(size, cap)) &#123; return; &#125; &#125; // trigger VM&apos;s Reference processing System.gc(); // a retry loop with exponential back-off delays // (this gives VM some time to do it&apos;s job) boolean interrupted = false; try &#123; long sleepTime = 1; int sleeps = 0; while (true) &#123; if (tryReserveMemory(size, cap)) &#123; return; &#125; if (sleeps &gt;= MAX_SLEEPS) &#123; break; &#125; if (!jlra.tryHandlePendingReference()) &#123; try &#123; Thread.sleep(sleepTime); sleepTime &lt;&lt;= 1; sleeps++; &#125; catch (InterruptedException e) &#123; interrupted = true; &#125; &#125; &#125; // no luck throw new OutOfMemoryError(&quot;Direct buffer memory&quot;); &#125; finally &#123; if (interrupted) &#123; // don&apos;t swallow interrupts Thread.currentThread().interrupt(); &#125; &#125;&#125; 该方法用于在系统中保存总分配内存(按页分配)的大小和实际内存的大小。 其中，如果系统中内存( 即，堆外内存 )不够的话： 12345678910final JavaLangRefAccess jlra = SharedSecrets.getJavaLangRefAccess();// retry while helping enqueue pending Reference objects// which includes executing pending Cleaner(s) which includes// Cleaner(s) that free direct buffer memorywhile (jlra.tryHandlePendingReference()) &#123; if (tryReserveMemory(size, cap)) &#123; return; &#125;&#125; jlra.tryHandlePendingReference()会触发一次非堵塞的Reference#tryHandlePending(false)。该方法会将已经被JVM垃圾回收的DirectBuffer对象的堆外内存释放。因为在Reference的静态代码块中定义了： 123456SharedSecrets.setJavaLangRefAccess(new JavaLangRefAccess() &#123; @Override public boolean tryHandlePendingReference() &#123; return tryHandlePending(false); &#125;&#125;); 如果在进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则 12// trigger VM&apos;s Reference processingSystem.gc(); System.gc()会触发一个full gc，当然前提是你没有显示的设置-XX:+DisableExplicitGC来禁用显式GC。并且你需要知道，调用System.gc()并不能够保证full gc马上就能被执行。所以在后面打代码中，会进行最多9次尝试，看是否有足够的可用堆外内存来分配堆外内存。并且每次尝试之前，都对延迟等待时间，已给JVM足够的时间去完成full gc操作。如果9次尝试后依旧没有足够的可用堆外内存来分配本次堆外内存，则抛出OutOfMemoryError(“Direct buffer memory”)异常。 注意，这里之所以用使用full gc的很重要的一个原因是：System.gc()会对新生代的老生代都会进行内存回收，这样会比较彻底地回收DirectByteBuffer对象以及他们关联的堆外内存.DirectByteBuffer对象本身其实是很小的，但是它后面可能关联了一个非常大的堆外内存，因此我们通常称之为冰山对象.我们做ygc的时候会将新生代里的不可达的DirectByteBuffer对象及其堆外内存回收了，但是无法对old里的DirectByteBuffer对象及其堆外内存进行回收，这也是我们通常碰到的最大的问题。( 并且堆外内存多用于生命期中等或较长的对象 )如果有大量的DirectByteBuffer对象移到了old，但是又一直没有做cms gc或者full gc，而只进行ygc，那么我们的物理内存可能被慢慢耗光，但是我们还不知道发生了什么，因为heap明明剩余的内存还很多(前提是我们禁用了System.gc – JVM参数DisableExplicitGC)。 总的来说，Bits.reserveMemory(size, cap)方法在可用堆外内存不足以分配给当前要创建的堆外内存大小时，会实现以下的步骤来尝试完成本次堆外内存的创建：① 触发一次非堵塞的Reference#tryHandlePending(false)。该方法会将已经被JVM垃圾回收的DirectBuffer对象的堆外内存释放。② 如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()。System.gc()会触发一个full gc，但你需要知道，调用System.gc()并不能够保证full gc马上就能被执行。所以在后面打代码中，会进行最多9次尝试，看是否有足够的可用堆外内存来分配堆外内存。并且每次尝试之前，都对延迟等待时间，已给JVM足够的时间去完成full gc操作。注意，如果你设置了-XX:+DisableExplicitGC，将会禁用显示GC，这会使System.gc()调用无效。③ 如果9次尝试后依旧没有足够的可用堆外内存来分配本次堆外内存，则抛出OutOfMemoryError(“Direct buffer memory”)异常。 那么可用堆外内存到底是多少了？，即默认堆外存内存有多大：① 如果我们没有通过-XX:MaxDirectMemorySize来指定最大的堆外内存。则👇② 如果我们没通过-Dsun.nio.MaxDirectMemorySize指定了这个属性，且它不等于-1。则👇③ 那么最大堆外内存的值来自于directMemory = Runtime.getRuntime().maxMemory()，这是一个native方法 1234567891011JNIEXPORT jlong JNICALLJava_java_lang_Runtime_maxMemory(JNIEnv *env, jobject this)&#123; return JVM_MaxMemory();&#125;JVM_ENTRY_NO_ENV(jlong, JVM_MaxMemory(void)) JVMWrapper(&quot;JVM_MaxMemory&quot;); size_t n = Universe::heap()-&gt;max_capacity(); return convert_size_t_to_jlong(n);JVM_END 其中在我们使用CMS GC的情况下也就是我们设置的-Xmx的值里除去一个survivor的大小就是默认的堆外内存的大小了。 堆外内存回收Cleaner是PhantomReference的子类，并通过自身的next和prev字段维护的一个双向链表。PhantomReference的作用在于跟踪垃圾回收过程，并不会对对象的垃圾回收过程造成任何的影响。所以cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); 用于对当前构造的DirectByteBuffer对象的垃圾回收过程进行跟踪。当DirectByteBuffer对象从pending状态 ——&gt; enqueue状态时，会触发Cleaner的clean()，而Cleaner的clean()的方法会实现通过unsafe对堆外内存的释放。 👆虽然Cleaner不会调用到Reference.clear()，但Cleaner的clean()方法调用了remove(this)，即将当前Cleaner从Cleaner链表中移除，这样当clean()执行完后，Cleaner就是一个无引用指向的对象了，也就是可被GC回收的对象。 thunk方法： 通过配置参数的方式来回收堆外内存同时我们可以通过-XX:MaxDirectMemorySize来指定最大的堆外内存大小，当使用达到了阈值的时候将调用System.gc()来做一次full gc，以此来回收掉没有被使用的堆外内存。 堆外内存那些事使用堆外内存的原因 对垃圾回收停顿的改善因为full gc 意味着彻底回收，彻底回收时，垃圾收集器会对所有分配的堆内内存进行完整的扫描，这意味着一个重要的事实——这样一次垃圾收集对Java应用造成的影响，跟堆的大小是成正比的。过大的堆会影响Java应用的性能。如果使用堆外内存的话，堆外内存是直接受操作系统管理( 而不是虚拟机 )。这样做的结果就是能保持一个较小的堆内内存，以减少垃圾收集对应用的影响。 在某些场景下可以提升程序I/O操纵的性能。少去了将数据从堆内内存拷贝到堆外内存的步骤。 什么情况下使用堆外内存 堆外内存适用于生命周期中等或较长的对象。( 如果是生命周期较短的对象，在YGC的时候就被回收了，就不存在大内存且生命周期较长的对象在FGC对应用造成的性能影响 )。 直接的文件拷贝操作，或者I/O操作。直接使用堆外内存就能少去内存从用户内存拷贝到系统内存的操作，因为I/O操作是系统内核内存和设备间的通信，而不是通过程序直接和外设通信的。 同时，还可以使用 池+堆外内存 的组合方式，来对生命周期较短，但涉及到I/O操作的对象进行堆外内存的再使用。( Netty中就使用了该方式 ) 堆外内存 VS 内存池 内存池：主要用于两类对象：①生命周期较短，且结构简单的对象，在内存池中重复利用这些对象能增加CPU缓存的命中率，从而提高性能；②加载含有大量重复对象的大片数据，此时使用内存池能减少垃圾回收的时间。 堆外内存：它和内存池一样，也能缩短垃圾回收时间，但是它适用的对象和内存池完全相反。内存池往往适用于生命期较短的可变对象，而生命期中等或较长的对象，正是堆外内存要解决的。 堆外内存的特点 对于大内存有良好的伸缩性 对垃圾回收停顿的改善可以明显感觉到 在进程间可以共享，减少虚拟机间的复制 堆外内存的一些问题 堆外内存回收问题，以及堆外内存的泄漏问题。这个在上面的源码解析已经提到了 堆外内存的数据结构问题：堆外内存最大的问题就是你的数据结构变得不那么直观，如果数据结构比较复杂，就要对它进行串行化（serialization），而串行化本身也会影响性能。另一个问题是由于你可以使用更大的内存，你可能开始担心虚拟内存（即硬盘）的速度对你的影响了。 参考文章http://lovestblog.cn/blog/2015/05/12/direct-buffer/http://www.infoq.com/cn/news/2014/12/external-memory-heap-memoryhttp://www.jianshu.com/p/85e931636f27圣思园《并发与Netty》课程 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解7：浅谈 Linux 中NIO Selector 的实现原理]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A37%EF%BC%9A%E6%B5%85%E8%B0%88%20Linux%20%E4%B8%ADNIO%20Selector%20%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 概述Selector是NIO中实现I/O多路复用的关键类。Selector实现了通过一个线程管理多个Channel，从而管理多个网络连接的目的。 Channel代表这一个网络连接通道，我们可以将Channel注册到Selector中以实现Selector对其的管理。一个Channel可以注册到多个不同的Selector中。 当Channel注册到Selector后会返回一个SelectionKey对象，该SelectionKey对象则代表这这个Channel和它注册的Selector间的关系。并且SelectionKey中维护着两个很重要的属性：interestOps、readyOpsinterestOps是我们希望Selector监听Channel的哪些事件。 我们将我们感兴趣的事件设置到该字段，这样在selection操作时，当发现该Channel有我们所感兴趣的事件发生时，就会将我们感兴趣的事件再设置到readyOps中，这样我们就能得知是哪些事件发生了以做相应处理。 Selector的中的重要属性Selector中维护3个特别重要的SelectionKey集合，分别是 keys：所有注册到Selector的Channel所表示的SelectionKey都会存在于该集合中。keys元素的添加会在Channel注册到Selector时发生。 selectedKeys：该集合中的每个SelectionKey都是其对应的Channel在上一次操作selection期间被检查到至少有一种SelectionKey中所感兴趣的操作已经准备好被处理。该集合是keys的一个子集。 cancelledKeys：执行了取消操作的SelectionKey会被放入到该集合中。该集合是keys的一个子集。 下面的源码解析会说明上面3个集合的用处 Selector 源码解析下面我们通过一段对Selector的使用流程讲解来进一步深入其实现原理。首先先来段Selector最简单的使用片段 1234567891011121314151617ServerSocketChannel serverChannel = ServerSocketChannel.open();serverChannel.configureBlocking(false);int port = 5566; serverChannel.socket().bind(new InetSocketAddress(port));Selector selector = Selector.open();serverChannel.register(selector, SelectionKey.OP_ACCEPT);while(true)&#123; int n = selector.select(); if(n &gt; 0) &#123; Iterator&lt;SelectionKey&gt; iter = selector.selectedKeys().iterator(); while (iter.hasNext()) &#123; SelectionKey selectionKey = iter.next(); ...... iter.remove(); &#125; &#125;&#125; 1、Selector的构建SocketChannel、ServerSocketChannel和Selector的实例初始化都通过SelectorProvider类实现。 ServerSocketChannel.open(); 123public static ServerSocketChannel open() throws IOException &#123; return SelectorProvider.provider().openServerSocketChannel();&#125; SocketChannel.open(); 123public static SocketChannel open() throws IOException &#123; return SelectorProvider.provider().openSocketChannel();&#125; Selector.open(); 123public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125; 我们来进一步的了解下SelectorProvider.provider() 1234567891011121314151617public static SelectorProvider provider() &#123; synchronized (lock) &#123; if (provider != null) return provider; return AccessController.doPrivileged( new PrivilegedAction&lt;&gt;() &#123; public SelectorProvider run() &#123; if (loadProviderFromProperty()) return provider; if (loadProviderAsService()) return provider; provider = sun.nio.ch.DefaultSelectorProvider.create(); return provider; &#125; &#125;); &#125;&#125; ① 如果配置了“java.nio.channels.spi.SelectorProvider”属性，则通过该属性值load对应的SelectorProvider对象，如果构建失败则抛异常。② 如果provider类已经安装在了对系统类加载程序可见的jar包中，并且该jar包的源码目录META-INF/services包含有一个java.nio.channels.spi.SelectorProvider提供类配置文件，则取文件中第一个类名进行load以构建对应的SelectorProvider对象，如果构建失败则抛异常。③ 如果上面两种情况都不存在，则返回系统默认的SelectorProvider，即，sun.nio.ch.DefaultSelectorProvider.create();④ 随后在调用该方法，即SelectorProvider.provider()。则返回第一次调用的结果。 不同系统对应着不同的sun.nio.ch.DefaultSelectorProvider 这里我们看linux下面的sun.nio.ch.DefaultSelectorProvider 123456789101112131415public class DefaultSelectorProvider &#123; /** * Prevent instantiation. */ private DefaultSelectorProvider() &#123; &#125; /** * Returns the default SelectorProvider. */ public static SelectorProvider create() &#123; return new sun.nio.ch.EPollSelectorProvider(); &#125;&#125; 可以看见，linux系统下sun.nio.ch.DefaultSelectorProvider.create(); 会生成一个sun.nio.ch.EPollSelectorProvider类型的SelectorProvider，这里对应于linux系统的epoll 接下来看下 selector.open()：12345678910111213141516/** * Opens a selector. * * &lt;p&gt; The new selector is created by invoking the &#123;@link * java.nio.channels.spi.SelectorProvider#openSelector openSelector&#125; method * of the system-wide default &#123;@link * java.nio.channels.spi.SelectorProvider&#125; object. &lt;/p&gt; * * @return A new selector * * @throws IOException * If an I/O error occurs */public static Selector open() throws IOException &#123; return SelectorProvider.provider().openSelector();&#125; 在得到sun.nio.ch.EPollSelectorProvider后调用openSelector()方法构建Selector，这里会构建一个EPollSelectorImpl对象。 EPollSelectorImpl12345678910111213class EPollSelectorImpl extends SelectorImpl&#123; // File descriptors used for interrupt protected int fd0; protected int fd1; // The poll object EPollArrayWrapper pollWrapper; // Maps from file descriptors to keys private Map&lt;Integer,SelectionKeyImpl&gt; fdToKey; 1234567891011121314151617181920212223EPollSelectorImpl(SelectorProvider sp) throws IOException &#123; super(sp); long pipeFds = IOUtil.makePipe(false); fd0 = (int) (pipeFds &gt;&gt;&gt; 32); fd1 = (int) pipeFds; try &#123; pollWrapper = new EPollArrayWrapper(); pollWrapper.initInterrupt(fd0, fd1); fdToKey = new HashMap&lt;&gt;(); &#125; catch (Throwable t) &#123; try &#123; FileDispatcherImpl.closeIntFD(fd0); &#125; catch (IOException ioe0) &#123; t.addSuppressed(ioe0); &#125; try &#123; FileDispatcherImpl.closeIntFD(fd1); &#125; catch (IOException ioe1) &#123; t.addSuppressed(ioe1); &#125; throw t; &#125; &#125; EPollSelectorImpl构造函数完成：① EPollArrayWrapper的构建，EpollArrayWapper将Linux的epoll相关系统调用封装成了native方法供EpollSelectorImpl使用。② 通过EPollArrayWrapper向epoll注册中断事件 12345void initInterrupt(int fd0, int fd1) &#123; outgoingInterruptFD = fd1; incomingInterruptFD = fd0; epollCtl(epfd, EPOLL_CTL_ADD, fd0, EPOLLIN);&#125; ③ fdToKey：构建文件描述符-SelectionKeyImpl映射表，所有注册到selector的channel对应的SelectionKey和与之对应的文件描述符都会放入到该映射表中。 EPollArrayWrapperEPollArrayWrapper完成了对epoll文件描述符的构建，以及对linux系统的epoll指令操纵的封装。维护每次selection操作的结果，即epoll_wait结果的epoll_event数组。EPollArrayWrapper操纵了一个linux系统下epoll_event结构的本地数组。 1234567891011* typedef union epoll_data &#123;* void *ptr;* int fd;* __uint32_t u32;* __uint64_t u64;* &#125; epoll_data_t;** struct epoll_event &#123;* __uint32_t events;* epoll_data_t data;* &#125;; epoll_event的数据成员(epoll_data_t data)包含有与通过epoll_ctl将文件描述符注册到epoll时设置的数据相同的数据。这里data.fd为我们注册的文件描述符。这样我们在处理事件的时候持有有效的文件描述符了。 EPollArrayWrapper将Linux的epoll相关系统调用封装成了native方法供EpollSelectorImpl使用。 1234private native int epollCreate();private native void epollCtl(int epfd, int opcode, int fd, int events);private native int epollWait(long pollAddress, int numfds, long timeout, int epfd) throws IOException; 上述三个native方法就对应Linux下epoll相关的三个系统调用 12345678// The fd of the epoll driverprivate final int epfd; // The epoll_event array for results from epoll_waitprivate final AllocatedNativeObject pollArray;// Base address of the epoll_event arrayprivate final long pollArrayAddress; 123// 用于存储已经注册的文件描述符和其注册等待改变的事件的关联关系。在epoll_wait操作就是要检测这里文件描述法注册的事件是否有发生。private final byte[] eventsLow = new byte[MAX_UPDATE_ARRAY_SIZE];private final Map&lt;Integer,Byte&gt; eventsHigh = new HashMap&lt;&gt;(); 123456789EPollArrayWrapper() throws IOException &#123; // creates the epoll file descriptor epfd = epollCreate(); // the epoll_event array passed to epoll_wait int allocationSize = NUM_EPOLLEVENTS * SIZE_EPOLLEVENT; pollArray = new AllocatedNativeObject(allocationSize, true); pollArrayAddress = pollArray.address();&#125; EPoolArrayWrapper构造函数，创建了epoll文件描述符。构建了一个用于存放epoll_wait返回结果的epoll_event数组。 ServerSocketChannel的构建ServerSocketChannel.open(); 返回ServerSocketChannelImpl对象，构建linux系统下ServerSocket的文件描述符。 123456// Our file descriptorprivate final FileDescriptor fd;// fd value needed for dev/poll. This value will remain valid// even after the value in the file descriptor object has been set to -1private int fdVal; 123456ServerSocketChannelImpl(SelectorProvider sp) throws IOException &#123; super(sp); this.fd = Net.serverSocket(true); this.fdVal = IOUtil.fdVal(fd); this.state = ST_INUSE;&#125; 将ServerSocketChannel注册到SelectorserverChannel.register(selector, SelectionKey.OP_ACCEPT); 12345678910111213141516171819202122232425262728public final SelectionKey register(Selector sel, int ops, Object att) throws ClosedChannelException&#123; synchronized (regLock) &#123; if (!isOpen()) throw new ClosedChannelException(); if ((ops &amp; ~validOps()) != 0) throw new IllegalArgumentException(); if (blocking) throw new IllegalBlockingModeException(); SelectionKey k = findKey(sel); if (k != null) &#123; k.interestOps(ops); k.attach(att); &#125; if (k == null) &#123; // New registration synchronized (keyLock) &#123; if (!isOpen()) throw new ClosedChannelException(); k = ((AbstractSelector)sel).register(this, ops, att); addKey(k); &#125; &#125; return k; &#125;&#125; 1234567891011121314protected final SelectionKey register(AbstractSelectableChannel ch, int ops, Object attachment)&#123; if (!(ch instanceof SelChImpl)) throw new IllegalSelectorException(); SelectionKeyImpl k = new SelectionKeyImpl((SelChImpl)ch, this); k.attach(attachment); synchronized (publicKeys) &#123; implRegister(k); &#125; k.interestOps(ops); return k;&#125; ① 构建代表channel和selector间关系的SelectionKey对象② implRegister(k)将channel注册到epoll中③ k.interestOps(int) 完成下面两个操作：a) 会将注册的感兴趣的事件和其对应的文件描述存储到EPollArrayWrapper对象的eventsLow或eventsHigh中，这是给底层实现epoll_wait时使用的。b) 同时该操作还会将设置SelectionKey的interestOps字段，这是给我们程序员获取使用的。 EPollSelectorImpl. implRegister123456789protected void implRegister(SelectionKeyImpl ski) &#123; if (closed) throw new ClosedSelectorException(); SelChImpl ch = ski.channel; int fd = Integer.valueOf(ch.getFDVal()); fdToKey.put(fd, ski); pollWrapper.add(fd); keys.add(ski);&#125; ① 将channel对应的fd(文件描述符)和对应的SelectionKeyImpl放到fdToKey映射表中。② 将channel对应的fd(文件描述符)添加到EPollArrayWrapper中，并强制初始化fd的事件为0 ( 强制初始更新事件为0，因为该事件可能存在于之前被取消过的注册中。)③ 将selectionKey放入到keys集合中。 Selection操作selection操作有3中类型：① select()：该方法会一直阻塞直到至少一个channel被选择(即，该channel注册的事件发生了)为止，除非当前线程发生中断或者selector的wakeup方法被调用。② select(long time)：该方法和select()类似，该方法也会导致阻塞直到至少一个channel被选择(即，该channel注册的事件发生了)为止，除非下面3种情况任意一种发生：a) 设置的超时时间到达；b) 当前线程发生中断；c) selector的wakeup方法被调用③ selectNow()：该方法不会发生阻塞，如果没有一个channel被选择也会立即返回。 我们主要来看看select()的实现 ：int n = selector.select(); 123public int select() throws IOException &#123; return select(0);&#125; 最终会调用到EPollSelectorImpl的doSelect 1234567891011121314151617181920212223protected int doSelect(long timeout) throws IOException &#123; if (closed) throw new ClosedSelectorException(); processDeregisterQueue(); try &#123; begin(); pollWrapper.poll(timeout); &#125; finally &#123; end(); &#125; processDeregisterQueue(); int numKeysUpdated = updateSelectedKeys(); if (pollWrapper.interrupted()) &#123; // Clear the wakeup pipe pollWrapper.putEventOps(pollWrapper.interruptedIndex(), 0); synchronized (interruptLock) &#123; pollWrapper.clearInterrupted(); IOUtil.drain(fd0); interruptTriggered = false; &#125; &#125; return numKeysUpdated;&#125; ① 先处理注销的selectionKey队列② 进行底层的epoll_wait操作③ 再次对注销的selectionKey队列进行处理④ 更新被选择的selectionKey 先来看processDeregisterQueue(): 1234567891011121314151617181920212223void processDeregisterQueue() throws IOException &#123; Set var1 = this.cancelledKeys(); synchronized(var1) &#123; if (!var1.isEmpty()) &#123; Iterator var3 = var1.iterator(); while(var3.hasNext()) &#123; SelectionKeyImpl var4 = (SelectionKeyImpl)var3.next(); try &#123; this.implDereg(var4); &#125; catch (SocketException var12) &#123; IOException var6 = new IOException(&quot;Error deregistering key&quot;); var6.initCause(var12); throw var6; &#125; finally &#123; var3.remove(); &#125; &#125; &#125; &#125;&#125; 从cancelledKeys集合中依次取出注销的SelectionKey，执行注销操作，将处理后的SelectionKey从cancelledKeys集合中移除。执行processDeregisterQueue()后cancelledKeys集合会为空。 1234567891011121314protected void implDereg(SelectionKeyImpl ski) throws IOException &#123; assert (ski.getIndex() &gt;= 0); SelChImpl ch = ski.channel; int fd = ch.getFDVal(); fdToKey.remove(Integer.valueOf(fd)); pollWrapper.remove(fd); ski.setIndex(-1); keys.remove(ski); selectedKeys.remove(ski); deregister((AbstractSelectionKey)ski); SelectableChannel selch = ski.channel(); if (!selch.isOpen() &amp;&amp; !selch.isRegistered()) ((SelChImpl)selch).kill();&#125; 注销会完成下面的操作：① 将已经注销的selectionKey从fdToKey( 文件描述与SelectionKeyImpl的映射表 )中移除② 将selectionKey所代表的channel的文件描述符从EPollArrayWrapper中移除③ 将selectionKey从keys集合中移除，这样下次selector.select()就不会再将该selectionKey注册到epoll中监听④ 也会将selectionKey从对应的channel中注销⑤ 最后如果对应的channel已经关闭并且没有注册其他的selector了，则将该channel关闭完成👆的操作后，注销的SelectionKey就不会出现先在keys、selectedKeys以及cancelKeys这3个集合中的任何一个。 接着我们来看EPollArrayWrapper.poll(timeout)： 123456789101112int poll(long timeout) throws IOException &#123; updateRegistrations(); updated = epollWait(pollArrayAddress, NUM_EPOLLEVENTS, timeout, epfd); for (int i=0; i&lt;updated; i++) &#123; if (getDescriptor(i) == incomingInterruptFD) &#123; interruptedIndex = i; interrupted = true; break; &#125; &#125; return updated;&#125; updateRegistrations()方法会将已经注册到该selector的事件(eventsLow或eventsHigh)通过调用epollCtl(epfd, opcode, fd, events); 注册到linux系统中。这里epollWait就会调用linux底层的epoll_wait方法，并返回在epoll_wait期间有事件触发的entry的个数 再看updateSelectedKeys()： 123456789101112131415161718192021222324private int updateSelectedKeys() &#123; int entries = pollWrapper.updated; int numKeysUpdated = 0; for (int i=0; i&lt;entries; i++) &#123; int nextFD = pollWrapper.getDescriptor(i); SelectionKeyImpl ski = fdToKey.get(Integer.valueOf(nextFD)); // ski is null in the case of an interrupt if (ski != null) &#123; int rOps = pollWrapper.getEventOps(i); if (selectedKeys.contains(ski)) &#123; if (ski.channel.translateAndSetReadyOps(rOps, ski)) &#123; numKeysUpdated++; &#125; &#125; else &#123; ski.channel.translateAndSetReadyOps(rOps, ski); if ((ski.nioReadyOps() &amp; ski.nioInterestOps()) != 0) &#123; selectedKeys.add(ski); numKeysUpdated++; &#125; &#125; &#125; &#125; return numKeysUpdated;&#125; 该方法会从通过EPollArrayWrapper pollWrapper 以及 fdToKey( 构建文件描述符-SelectorKeyImpl映射表 )来获取有事件触发的SelectionKeyImpl对象，然后将SelectionKeyImpl放到selectedKey集合( 有事件触发的selectionKey集合，可以通过selector.selectedKeys()方法获得 )中，即selectedKeys。并重新设置SelectionKeyImpl中相关的readyOps值。 但是，这里要注意两点： ① 如果SelectionKeyImpl已经存在于selectedKeys集合中，并且发现触发的事件已经存在于readyOps中了，则不会使numKeysUpdated++；这样会使得我们无法得知该事件的变化。 👆这点说明了为什么我们要在每次从selectedKey中获取到Selectionkey后，将其从selectedKey集合移除，就是为了当有事件触发使selectionKey能正确到放入selectedKey集合中，并正确的通知给调用者。 再者，如果不将已经处理的SelectionKey从selectedKey集合中移除，那么下次有新事件到来时，在遍历selectedKey集合时又会遍历到这个SelectionKey，这个时候就很可能出错了。比如，如果没有在处理完OP_ACCEPT事件后将对应SelectionKey从selectedKey集合移除，那么下次遍历selectedKey集合时，处理到到该SelectionKey，相应的ServerSocketChannel.accept()将返回一个空(null)的SocketChannel。 ② 如果发现channel所发生I/O事件不是当前SelectionKey所感兴趣，则不会将SelectionKeyImpl放入selectedKeys集合中，也不会使numKeysUpdated++ epoll原理select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 epoll是Linux下的一种IO多路复用技术，可以非常高效的处理数以百万计的socket句柄。 在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。)如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 注意：linux下Selector底层是通过epoll来实现的，当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 先看看使用c封装的3个epoll系统调用: int epoll_create(int size)epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)epoll_ctl可以操作epoll_create创建的epoll，如将socket句柄加入到epoll中让其监控，或把epoll正在监控的某个socket句柄移出epoll。 int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout)epoll_wait在调用时，在给定的timeout时间内，所监控的句柄中有事件发生时，就返回用户态的进程。 大概看看epoll内部是怎么实现的： epoll初始化时，会向内核注册一个文件系统，用于存储被监控的句柄文件，调用epoll_create时，会在这个文件系统中创建一个file节点。同时epoll会开辟自己的内核高速缓存区，以红黑树的结构保存句柄，以支持快速的查找、插入、删除。还会再建立一个list链表，用于存储准备就绪的事件。 当执行epoll_ctl时，除了把socket句柄放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后，就把socket插入到就绪链表里。 当epoll_wait调用时，仅仅观察就绪链表里有没有数据，如果有数据就返回，否则就sleep，超时时立刻返回。 epoll的两种工作模式： LT：level-trigger，水平触发模式，只要某个socket处于readable/writable状态，无论什么时候进行epoll_wait都会返回该socket。 ET：edge-trigger，边缘触发模式，只有某个socket从unreadable变为readable或从unwritable变为writable时，epoll_wait才会返回该socket。 socket读数据 socket写数据 最后顺便说下在Linux系统中JDK NIO使用的是 LT ，而Netty epoll使用的是 ET。 后记因为本人对计算机系统组成以及C语言等知识比较欠缺，因为文中相关知识点的表示也相当“肤浅”，如有不对不妥的地方望读者指出。同时我也会继续加强对该方面知识点的学习~ 参考文章http://www.jianshu.com/p/0d497fe5484ahttp://remcarpediem.com/2017/04/02/Netty源码-三-I-O模型和Java-NIO底层原理/圣思园netty课程 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解6：Linux epoll实现原理详解]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A36%EF%BC%9ALinux%20epoll%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 为什么要 I/O 多路复用当需要从一个叫 r_fd 的描述符不停地读取数据，并把读到的数据写入一个叫 w_fd 的描述符时，我们可以用循环使用阻塞 I/O ： while((n = read(r_fd, buf, BUF_SIZE)) &gt; 0) if(write(w_fd, buf, n) != n) err_sys(&quot;write error&quot;);但是，如果要从两个地方读取数据呢？这时，不能再使用会把程序阻塞住的 read 函数。因为可能在阻塞地等待 r_fd1 的数据时，来不及处理 r_fd2，已经到达的 r_fd2 的数据可能会丢失掉。 这个情况下需要使用非阻塞 I/O。 只要做个标记，把文件描述符标记为非阻塞的，以后再对它使用 read 函数：如果它还没有数据可读，函数会立即返回并把 errorno 这个变量的值设置为 35，于是我们知道它没有数据可读，然后可以立马去对其他描述符使用 read；如果它有数据可读，我们就读取它数据。对所有要读的描述符都调用了一遍 read 之后，我们可以等一个较长的时间（比如几秒），然后再从第一个文件描述符开始调用 read 。这种循环就叫做轮询（polling）。 这样，不会像使用阻塞 I/O 时那样因为一个描述符 read 长时间处于等待数据而使程序阻塞。 轮询的缺点是浪费太多 CPU 时间。大多数时候我们没有数据可读，但是还是用了 read 这个系统调用，使用系统调用时会从用户态切换到内核态。而大多数情况下我们调用 read，然后陷入内核态，内核发现这个描述符没有准备好，然后切换回用户态并且只得到 EAGAIN （errorno 被设置为 35），做的是无用功。描述符非常多的时候，每次的切换过程就是巨大的浪费。 所以，需要 I/O 多路复用。I/O 多路复用通过使用一个系统函数，同时等待多个描述符的可读、可写状态。 为了达到这个目的，我们需要做的是：建立一个描述符列表，以及我们分别关心它们的什么事件（可读还是可写还是发生例外情况）；调用一个系统函数，直到这个描述符列表里有至少一个描述符关联的事件发生时，这个函数才会返回。 select, poll, epoll 就是这样的系统函数。 select我们可以在所有 POSIX 兼容的系统里使用 select 函数来进行 I/O 多路复用。我们需要通过 select 函数的参数传递给内核的信息有： * 我们关心哪些描述符 * 我们关心它们的什么事件 * 我们希望等待多长时间select 的返回时，内核会告诉我们： * 可读的描述符的个数 * 哪些描述符发生了哪些事件 #include &lt;sys/select.h&gt; int select(int maxfdp1, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout); // 返回值: 已就绪的描述符的个数。超时时为 0 ，错误时为 -1maxfdp1 意思是 “max file descriptor plus 1” ，就是把你要监视的所有文件描述符里最大的那个加上 1 。（它实际上决定了内核要遍历文件描述符的次数，比如你监视了文件描述符 5 和 20 并把 maxfdp1 设置为 21 ，内核每次都会从描述符 0 依次检查到 20。） 中间的三个参数是你想监视的文件描述符的集合。可以把 fd_set 类型视为 1024 位的二进制数，这意味着 select 只能监视小于 1024 的文件描述符（1024 是由 Linux 的 sys/select.h 里 FD_SETSIZE 宏设置的值）。在 select 返回后我们通过 FD_ISSET 来判断代表该位的描述符是否是已准备好的状态。 最后一个参数是等待超时的时长：到达这个时长但是没有任一描述符可用时，函数会返回 0 。 用一个代码片段来展示 select 的用法： // 这个例子要监控文件描述符 3, 4 的可读状态，以及 4, 5 的可写状态 // 初始化两个 fd_set 以及 timeval fd_set read_set, write_set; FD_ZERO(read_set); FD_ZERO(write_set); timeval t; t.tv_sec = 5; // 超时为 5 秒 t.tv_usec = 0; // 加 0 微秒 // 设置好两个 fd_set int fd1 = 3; int fd2 = 4; int fd3 = 5; int maxfdp1 = 5 + 1; FD_SET(fd1, &amp;read_set); FD_SET(fd2, &amp;read_set); FD_SET(fd2, &amp;write_set); FD_SET(fd3, &amp;write_set); // 准备备用的 fd_set fd_set r_temp = read_set; fd_set w_temp = write_set; while(true){ // 每次都要重新设置放入 select 的 fd_set read_set = r_temp; write_set = w_temp; // 使用 select int n = select(maxfdp1, &amp;read_set, &amp;write_set, NULL, &amp;t); // 上面的 select 函数会一直阻塞，直到 // 3, 4 可读以及 4, 5 可写这四件事中至少一项发生 // 或者等待时间到达 5 秒，返回 0 for(int i=0; i&lt;maxfdp1 &amp;&amp; n&gt;0; i++){ if(FD_ISSET(i, &amp;read_set)){ n--; if(i==fd1) prinf(&quot;描述符 3 可读&quot;); if(i==fd2) prinf(&quot;描述符 4 可读&quot;); } if(FD_ISSET(i, &amp;write_set)){ n--; if(i==fd2) prinf(&quot;描述符 3 可写&quot;); if(i==fd3) prinf(&quot;描述符 4 可写&quot;); } } // 上面的 printf 语句换成对应的 read 或者 write 函数就 // 可以立即读取或者写入相应的描述符而不用等待 }可以看到，select 的缺点有： 默认能监视的文件描述符不能大于 1024，也代表监视的总数不超过1024。即使你因为需要监视的描述符大于 1024 而改动内核的 FD_SETSIZE 值，但由于 select 是每次都会线性扫描整个fd_set，集合越大速度越慢，所以性能会比较差。 select 函数返回时只能看见已准备好的描述符数量，至于是哪个描述符准备好了需要循环用 FD_ISSET 来检查，当未准备好的描述符很多而准备好的很少时，效率比较低。 select 函数每次执行的时候，都把参数里传入的三个 fd_set 从用户空间复制到内核空间。而每次 fd_set 里要监视的描述符变化不大时，全部重新复制一遍并不划算。同样在每次都是未准备好的描述符很多而准备好的很少时，调用 select 会很频繁，用户/内核间的的数据复制就成了一个大的开销。 还有一个问题是在代码的写法上给我一些困扰的，就是每次调用 select 前必须重新设置三个 fd_set。 fd_set 类型只是 1024 位的二进制数（实际上结构体里是几个 long 变量的数组；比如 64 位机器上 long 是 64 bit，那么 fd_set 里就是 16 个 long 变量的数组），由一位的 1 和 0 代表一个文件描述符的状态，但是其实调用 select 前后位的 1/0 状态意义是不一样的。 先讲一下几个对 fd_set 操作的函数的作用：FD_ZERO 把 fd_set 所有位设置为 0 ；FD_SET 把一个位设置为 1 ；FD_ISSET 判断一个位是否为 1 。 调用 select 前：我们用 FD_ZERO 把 fd_set 先全部初始化，然后用 FD_SET 把我们关心的代表描述符的位设置为 1 。我们这时可以用 FD_ISSET 判断这个位是否被我们设置，这时的含义是我们想要监视的描述符是否被设置为被监视的状态。 调用 select 时：内核判断 fd_set 里的位并把各个 fd_set 里所有值为 1 的位记录下来，然后把 fd_set 全部设置成 0 ；一个描述符上有对应的事件发生时，把对应 fd_set 里代表这个描述符的位设置为 1 。 在 select 返回之后：我们同样用 FD_ISSET 判断各个我们关心的位是 0 还是 1 ，这时的含义是，这个位是否是发生了我们关心的事件。 所以，在下一次调用 select 前，我们不得不把已经被内核改掉的 fd_set 全部重新设置一下。 select 在监视大量描述符尤其是更多的描述符未准备好的情况时性能很差。《Unix 高级编程》里写，用 select 的程序通常只使用 3 到 10 个描述符。 pollpoll 和 select 是相似的，只是给的接口不同。 #include &lt;poll.h&gt; int poll(struct pollfd fdarray[], nfds_t nfds, int timeout); // 返回值: 已就绪的描述符的个数。超时时为 0 ，错误时为 -1fdarray 是 pollfd 的数组。pollfd 结构体是这样的： struct pollfd { int fd; // 文件描述符 short events; // 我期待的事件 short revents; // 实际发生的事件：我期待的事件中发生的；或者异常情况 };nfds 是 fdarray 的长度，也就是 pollfd 的个数。 timeout 代表等待超时的毫秒数。 相比 select ，poll 有这些优点：由于 poll 在 pollfd 里用 int fd 来表示文件描述符而不像 select 里用的 fd_set 来分别表示描述符，所以没有必须小于 1024 的限制，也没有数量限制；由于 poll 用 events 表示期待的事件，通过修改 revents 来表示发生的事件，所以不需要像 select 在每次调用前重新设置描述符和期待的事件。 除此之外，poll 和 select 几乎相同。在 poll 返回后，需要遍历 fdarray 来检查各个 pollfd 里的 revents 是否发生了期待的事件；每次调用 poll 时，把 fdarray 复制到内核空间。在描述符太多而每次准备好的较少时，poll 有同样的性能问题。 epollepoll 是在 Linux 2.5.44 中首度登场的。不像 select 和 poll ，它提供了三个系统函数而不是一个。 epoll_create 用来创建一个 epoll 描述符：#include &lt;sys/epoll.h&gt; int epoll_create(int size); // 返回值：epoll 描述符size 用来告诉内核你想监视的文件描述符的数目，但是它并不是限制了能监视的描述符的最大个数，而是给内核最初分配的空间一个建议。然后系统会在内核中分配一个空间来存放事件表，并返回一个 epoll 描述符，用来操作这个事件表。 epoll_ctl 用来增/删/改内核中的事件表：int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // 返回值：成功时返回 0 ，失败时返回 -1epfd 是 epoll 描述符。 op 是操作类型（增加/删除/修改）。 fd 是希望监视的文件描述符。 event 是一个 epoll_event 结构体的指针。epoll_event 的定义是这样的： typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; // 我期待的事件 epoll_data_t data; // 用户数据变量 };这个结构体里，除了期待的事件外，还有一个 data ，是一个 union，它是用来让我们在得到下面第三个函数的返回值以后方便的定位文件描述符的。 epoll_wait 用来等待事件int epoll_wait(int epfd, struct epoll_event *result_events, int maxevents, int timeout); // 返回值：已就绪的描述符个数。超时时为 0 ，错误时为 -1epfd 是 epoll 描述符。 result_events 是 epoll_event 结构体的指针，它将指向的是所有已经准备好的事件描述符相关联的 epoll_event（在上个步骤里调用 epoll_ctl 时关联起来的）。下面的例子可以让你知道这个参数的意义。 maxevents 是返回的最大事件个数，也就是你能通过 result_events 指针遍历到的最大的次数。 timeout 是等待超时的毫秒数。 用一个代码片段来展示 epoll 的用法： // 这个例子要监控文件描述符 3, 4 的可读状态，以及 4, 5 的可写状态 /* 通过 epoll_create 创建 epoll 描述符 */ int epfd = epoll_create(4); int fd1 = 3; int fd2 = 4; int fd3 = 5; /* 通过 epoll_ctl 注册好四个事件 */ struct epoll_event ev1; ev1.events = EPOLLIN; // 期待它的可读事件发生 ev1.data = fd1; // 我们通常就把 data 设置为 fd ，方便以后查看 epoll_ctl(epfd, EPOLL_CTL_ADD, fd1, &amp;ev1); // 添加到事件表 struct epoll_event ev2; ev2.events = EPOLLIN; ev2.data = fd2; epoll_ctl(epfd, EPOLL_CTL_ADD, fd2, &amp;ev2); struct epoll_event ev3; ev3.events = EPOLLOUT; // 期待它的可写事件发生 ev3.data = fd2; epoll_ctl(epfd, EPOLL_CTL_ADD, fd2, &amp;ev3); struct epoll_event ev4; ev4.events = EPOLLOUT; ev4.data = fd3; epoll_ctl(epfd, EPOLL_CTL_ADD, fd3, &amp;ev4); /* 通过 epoll_wait 等待事件 */ # DEFINE MAXEVENTS 4 struct epoll_event result_events[MAXEVENTS]; while(true){ int n = epoll_wait(epfd, &amp;result_events, MAXEVENTS, 5000); for(int i=0; i&lt;n; n--){ // result_events[i] 一定是 ev1 到 ev4 中的一个 if(result_events[i].events&amp;EPOLLIN) printf(&quot;描述符 %d 可读&quot;, result_events[i].fd); else if(result_events[i].events&amp;EPOLLOUT) printf(&quot;描述符 %d 可写&quot;, result_events[i].fd) } }所以 epoll 解决了 poll 和 select 的问题： 只在 epoll_ctl 的时候把数据复制到内核空间，这保证了每个描述符和事件一定只会被复制到内核空间一次；每次调用 epoll_wait 都不会复制新数据到内核空间。相比之下，select 每次调用都会把三个 fd_set 复制一遍；poll 每次调用都会把 fdarray 复制一遍。 epoll_wait 返回 n ，那么只需要做 n 次循环，可以保证遍历的每一次都是有意义的。相比之下，select 需要做至少 n 次至多 maxfdp1 次循环；poll 需要遍历完 fdarray 即做 nfds 次循环。 在内部实现上，epoll 使用了回调的方法。调用 epoll_ctl 时，就是注册了一个事件：在集合中放入文件描述符以及事件数据，并且加上一个回调函数。一旦文件描述符上的对应事件发生，就会调用回调函数，这个函数会把这个文件描述符加入到就绪队列上。当你调用 epoll_wait 时，它只是在查看就绪队列上是否有内容，有的话就返回给你的程序。select() poll() epoll_wait() 三个函数在操作系统看来，都是睡眠一会儿然后判断一会儿的循环，但是 select 和 poll 在醒着的时候要遍历整个文件描述符集合，而 epoll_wait 只是看看就绪队列是否为空而已。这是 epoll 高性能的理由，使得其 I/O 的效率不会像使用轮询的 select/poll 随着描述符增加而大大降低。 注 1 ：select/poll/epoll_wait 三个函数的等待超时时间都有一样的特性：等待时间设置为 0 时函数不阻塞而是立即返回，不论是否有文件描述符已准备好；poll/epoll_wait 中的 timeout 为 -1，select 中的 timeout 为 NULL 时，则无限等待，直到有描述符已准备好才会返回。 注 2 ：有的新手会把文件描述符是否标记为阻塞 I/O 等同于 I/O 多路复用函数是否阻塞。其实文件描述符是否标记为阻塞，决定了你 read 或 write 它时如果它未准备好是阻塞等待，还是立即返回 EAGAIN ；而 I/O 多路复用函数除非你把 timeout 设置为 0 ，否则它总是会阻塞住你的程序。 注 3 ：上面的例子只是入门，可能是不准确或不全面的：一是数据要立即处理防止丢失；二是 EPOLLIN/EPOLLOUT 不完全等同于可读可写事件，具体要去搜索 poll/epoll 的事件具体有哪些；三是大多数实际例子里，比如一个 tcp server ，都会在运行中不断增加/删除的文件描述符而不是记住固定的 3 4 5 几个描述符（用这种例子更能看出 epoll 的优势）；四是 epoll 的优势更多的体现在处理大量闲连接的情况，如果场景是处理少量短连接，用 select 反而更好，而且用 select 的代码能运行在所有平台上。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解5：Java 非阻塞 IO 和异步 IO]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A35%EF%BC%9AJava%20%E9%9D%9E%E9%98%BB%E5%A1%9E%20IO%20%E5%92%8C%E5%BC%82%E6%AD%A5%20IO%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 上一篇文章介绍了 Java NIO 中 Buffer、Channel 和 Selector 的基本操作，主要是一些接口操作，比较简单。 本文将介绍非阻塞 IO 和异步 IO，也就是大家耳熟能详的 NIO 和 AIO。很多初学者可能分不清楚异步和非阻塞的区别，只是在各种场合能听到异步非阻塞这个词。 本文会先介绍并演示阻塞模式，然后引入非阻塞模式来对阻塞模式进行优化，最后再介绍 JDK7 引入的异步 IO，由于网上关于异步 IO 的介绍相对较少，所以这部分内容我会介绍得具体一些。 希望看完本文，读者可以对非阻塞 IO 和异步 IO 的迷雾看得更清晰些，或者为初学者解开一丝丝疑惑也是好的。 阻塞模式 IO我们已经介绍过使用 Java NIO 包组成一个简单的客户端-服务端网络通讯所需要的 ServerSocketChannel、SocketChannel 和 Buffer，我们这里整合一下它们，给出一个完整的可运行的例子： 1234567891011121314151617181920public class Server &#123; public static void main(String[] args) throws IOException &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 监听 8080 端口进来的 TCP 链接 serverSocketChannel.socket().bind(new InetSocketAddress(8080)); while (true) &#123; // 这里会阻塞，直到有一个请求的连接进来 SocketChannel socketChannel = serverSocketChannel.accept(); // 开启一个新的线程来处理这个请求，然后在 while 循环中继续监听 8080 端口 SocketHandler handler = new SocketHandler(socketChannel); new Thread(handler).start(); &#125; &#125;&#125; 这里看一下新的线程需要做什么，SocketHandler： 12345678910111213141516171819202122232425262728293031323334353637public class SocketHandler implements Runnable &#123; private SocketChannel socketChannel; public SocketHandler(SocketChannel socketChannel) &#123; this.socketChannel = socketChannel; &#125; @Override public void run() &#123; ByteBuffer buffer = ByteBuffer.allocate(1024); try &#123; // 将请求数据读入 Buffer 中 int num; while ((num = socketChannel.read(buffer)) &gt; 0) &#123; // 读取 Buffer 内容之前先 flip 一下 buffer.flip(); // 提取 Buffer 中的数据 byte[] bytes = new byte[num]; buffer.get(bytes); String re = new String(bytes, &quot;UTF-8&quot;); System.out.println(&quot;收到请求：&quot; + re); // 回应客户端 ByteBuffer writeBuffer = ByteBuffer.wrap((&quot;我已经收到你的请求，你的请求内容是：&quot; + re).getBytes()); socketChannel.write(writeBuffer); buffer.clear(); &#125; &#125; catch (IOException e) &#123; IOUtils.closeQuietly(socketChannel); &#125; &#125;&#125; 最后，贴一下客户端 SocketChannel 的使用，客户端比较简单： 1234567891011121314151617181920212223public class SocketChannelTest &#123; public static void main(String[] args) throws IOException &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;localhost&quot;, 8080)); // 发送请求 ByteBuffer buffer = ByteBuffer.wrap(&quot;1234567890&quot;.getBytes()); socketChannel.write(buffer); // 读取响应 ByteBuffer readBuffer = ByteBuffer.allocate(1024); int num; if ((num = socketChannel.read(readBuffer)) &gt; 0) &#123; readBuffer.flip(); byte[] re = new byte[num]; readBuffer.get(re); String result = new String(re, &quot;UTF-8&quot;); System.out.println(&quot;返回值: &quot; + result); &#125; &#125;&#125; 上面介绍的阻塞模式的代码应该很好理解：来一个新的连接，我们就新开一个线程来处理这个连接，之后的操作全部由那个线程来完成。 那么，这个模式下的性能瓶颈在哪里呢？ 首先，每次来一个连接都开一个新的线程这肯定是不合适的。当活跃连接数在几十几百的时候当然是可以这样做的，但如果活跃连接数是几万几十万的时候，这么多线程明显就不行了。每个线程都需要一部分内存，内存会被迅速消耗，同时，线程切换的开销非常大。 其次，阻塞操作在这里也是一个问题。首先，accept() 是一个阻塞操作，当 accept() 返回的时候，代表有一个连接可以使用了，我们这里是马上就新建线程来处理这个 SocketChannel 了，但是，但是这里不代表对方就将数据传输过来了。所以，SocketChannel#read 方法将阻塞，等待数据，明显这个等待是不值得的。同理，write 方法也需要等待通道可写才能执行写入操作，这边的阻塞等待也是不值得的。 非阻塞 IO说完了阻塞模式的使用及其缺点以后，我们这里就可以介绍非阻塞 IO 了。 非阻塞 IO 的核心在于使用一个 Selector 来管理多个通道，可以是 SocketChannel，也可以是 ServerSocketChannel，将各个通道注册到 Selector 上，指定监听的事件。 之后可以只用一个线程来轮询这个 Selector，看看上面是否有通道是准备好的，当通道准备好可读或可写，然后才去开始真正的读写，这样速度就很快了。我们就完全没有必要给每个通道都起一个线程。 NIO 中 Selector 是对底层操作系统实现的一个抽象，管理通道状态其实都是底层系统实现的，这里简单介绍下在不同系统下的实现。 select：上世纪 80 年代就实现了，它支持注册 FD_SETSIZE(1024) 个 socket，在那个年代肯定是够用的，不过现在嘛，肯定是不行了。 poll：1997 年，出现了 poll 作为 select 的替代者，最大的区别就是，poll 不再限制 socket 数量。 select 和 poll 都有一个共同的问题，那就是它们都只会告诉你有几个通道准备好了，但是不会告诉你具体是哪几个通道。所以，一旦知道有通道准备好以后，自己还是需要进行一次扫描，显然这个不太好，通道少的时候还行，一旦通道的数量是几十万个以上的时候，扫描一次的时间都很可观了，时间复杂度 O(n)。所以，后来才催生了以下实现。 epoll：2002 年随 Linux 内核 2.5.44 发布，epoll 能直接返回具体的准备好的通道，时间复杂度 O(1)。 除了 Linux 中的 epoll，2000 年 FreeBSD 出现了 Kqueue，还有就是，Solaris 中有 /dev/poll。 前面说了那么多实现，但是没有出现 Windows，Windows 平台的非阻塞 IO 使用 select，我们也不必觉得 Windows 很落后，在 Windows 中 IOCP 提供的异步 IO 是比较强大的。 我们回到 Selector，毕竟 JVM 就是这么一个屏蔽底层实现的平台，我们面向 Selector 编程就可以了。 之前在介绍 Selector 的时候已经了解过了它的基本用法，这边来一个可运行的实例代码，大家不妨看看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class SelectorServer &#123; public static void main(String[] args) throws IOException &#123; Selector selector = Selector.open(); ServerSocketChannel server = ServerSocketChannel.open(); server.socket().bind(new InetSocketAddress(8080)); // 将其注册到 Selector 中，监听 OP_ACCEPT 事件 server.configureBlocking(false); server.register(selector, SelectionKey.OP_ACCEPT); while (true) &#123; int readyChannels = selector.select(); if (readyChannels == 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; readyKeys = selector.selectedKeys(); // 遍历 Iterator&lt;SelectionKey&gt; iterator = readyKeys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; // 有已经接受的新的到服务端的连接 SocketChannel socketChannel = server.accept(); // 有新的连接并不代表这个通道就有数据， // 这里将这个新的 SocketChannel 注册到 Selector，监听 OP_READ 事件，等待数据 socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; // 有数据可读 // 上面一个 if 分支中注册了监听 OP_READ 事件的 SocketChannel SocketChannel socketChannel = (SocketChannel) key.channel(); ByteBuffer readBuffer = ByteBuffer.allocate(1024); int num = socketChannel.read(readBuffer); if (num &gt; 0) &#123; // 处理进来的数据... System.out.println(&quot;收到数据：&quot; + new String(readBuffer.array()).trim()); ByteBuffer buffer = ByteBuffer.wrap(&quot;返回给客户端的数据...&quot;.getBytes()); socketChannel.write(buffer); &#125; else if (num == -1) &#123; // -1 代表连接已经关闭 socketChannel.close(); &#125; &#125; &#125; &#125; &#125;&#125; 至于客户端，大家可以继续使用上一节介绍阻塞模式时的客户端进行测试。 NIO.2 异步 IOMore New IO，或称 NIO.2，随 JDK 1.7 发布，包括了引入异步 IO 接口和 Paths 等文件访问接口。 异步这个词，我想对于绝大多数开发者来说都很熟悉，很多场景下我们都会使用异步。 通常，我们会有一个线程池用于执行异步任务，提交任务的线程将任务提交到线程池就可以立马返回，不必等到任务真正完成。如果想要知道任务的执行结果，通常是通过传递一个回调函数的方式，任务结束后去调用这个函数。 同样的原理，Java 中的异步 IO 也是一样的，都是由一个线程池来负责执行任务，然后使用回调或自己去查询结果。 大部分开发者都知道为什么要这么设计了，这里再啰嗦一下。异步 IO 主要是为了控制线程数量，减少过多的线程带来的内存消耗和 CPU 在线程调度上的开销。 在 Unix/Linux 等系统中，JDK 使用了并发包中的线程池来管理任务，具体可以查看 AsynchronousChannelGroup 的源码。 在 Windows 操作系统中，提供了一个叫做 I/O Completion Ports 的方案，通常简称为 IOCP，操作系统负责管理线程池，其性能非常优异，所以在 Windows 中 JDK 直接采用了 IOCP 的支持，使用系统支持，把更多的操作信息暴露给操作系统，也使得操作系统能够对我们的 IO 进行一定程度的优化。 在 Linux 中其实也是有异步 IO 系统实现的，但是限制比较多，性能也一般，所以 JDK 采用了自建线程池的方式。 本文还是以实用为主，想要了解更多信息请自行查找其他资料，下面对 Java 异步 IO 进行实践性的介绍。 总共有三个类需要我们关注，分别是 AsynchronousSocketChannel，AsynchronousServerSocketChannel 和 AsynchronousFileChannel，只不过是在之前介绍的 FileChannel、SocketChannel 和 ServerSocketChannel 的类名上加了个前缀 Asynchronous。 Java 异步 IO 提供了两种使用方式，分别是返回 Future 实例和使用回调函数。 1、返回 Future 实例返回 java.util.concurrent.Future 实例的方式我们应该很熟悉，JDK 线程池就是这么使用的。Future 接口的几个方法语义在这里也是通用的，这里先做简单介绍。 future.isDone(); 判断操作是否已经完成，包括了正常完成、异常抛出、取消 future.cancel(true); 取消操作，方式是中断。参数 true 说的是，即使这个任务正在执行，也会进行中断。 future.isCancelled(); 是否被取消，只有在任务正常结束之前被取消，这个方法才会返回 true future.get(); 这是我们的老朋友，获取执行结果，阻塞。 future.get(10, TimeUnit.SECONDS); 如果上面的 get() 方法的阻塞你不满意，那就设置个超时时间。 2、提供 CompletionHandler 回调函数java.nio.channels.CompletionHandler 接口定义： 123456public interface CompletionHandler&lt;V,A&gt; &#123; void completed(V result, A attachment); void failed(Throwable exc, A attachment);&#125; 注意，参数上有个 attachment，虽然不常用，我们可以在各个支持的方法中传递这个参数值 123456789101112AsynchronousServerSocketChannel listener = AsynchronousServerSocketChannel.open().bind(null);// accept 方法的第一个参数可以传递 attachmentlistener.accept(attachment, new CompletionHandler&lt;AsynchronousSocketChannel, Object&gt;() &#123; public void completed( AsynchronousSocketChannel client, Object attachment) &#123; // &#125; public void failed(Throwable exc, Object attachment) &#123; // &#125;&#125;); AsynchronousFileChannel网上关于 Non-Blocking IO 的介绍文章很多，但是 Asynchronous IO 的文章相对就少得多了，所以我这边会多介绍一些相关内容。 首先，我们就来关注异步的文件 IO，前面我们说了，文件 IO 在所有的操作系统中都不支持非阻塞模式，但是我们可以对文件 IO 采用异步的方式来提高性能。 下面，我会介绍 AsynchronousFileChannel 里面的一些重要的接口，都很简单，读者要是觉得无趣，直接滑到下一个标题就可以了。 实例化： 1AsynchronousFileChannel channel = AsynchronousFileChannel.open(Paths.get(&quot;/Users/hongjie/test.txt&quot;)); 一旦实例化完成，我们就可以着手准备将数据读入到 Buffer 中： 12ByteBuffer buffer = ByteBuffer.allocate(1024);Future&lt;Integer&gt; result = channel.read(buffer, 0); 异步文件通道的读操作和写操作都需要提供一个文件的开始位置，文件开始位置为 0 除了使用返回 Future 实例的方式，也可以采用回调函数进行操作，接口如下： 1234public abstract &lt;A&gt; void read(ByteBuffer dst, long position, A attachment, CompletionHandler&lt;Integer,? super A&gt; handler); 顺便也贴一下写操作的两个版本的接口： 123456public abstract Future&lt;Integer&gt; write(ByteBuffer src, long position);public abstract &lt;A&gt; void write(ByteBuffer src, long position, A attachment, CompletionHandler&lt;Integer,? super A&gt; handler); 我们可以看到，AIO 的读写主要也还是与 Buffer 打交道，这个与 NIO 是一脉相承的。 另外，还提供了用于将内存中的数据刷入到磁盘的方法： 1public abstract void force(boolean metaData) throws IOException; 因为我们对文件的写操作，操作系统并不会直接针对文件操作，系统会缓存，然后周期性地刷入到磁盘。如果希望将数据及时写入到磁盘中，以免断电引发部分数据丢失，可以调用此方法。参数如果设置为 true，意味着同时也将文件属性信息更新到磁盘。 还有，还提供了对文件的锁定功能，我们可以锁定文件的部分数据，这样可以进行排他性的操作。 1public abstract Future&lt;FileLock&gt; lock(long position, long size, boolean shared); position 是要锁定内容的开始位置，size 指示了要锁定的区域大小，shared 指示需要的是共享锁还是排他锁 当然，也可以使用回调函数的版本： 12345public abstract &lt;A&gt; void lock(long position, long size, boolean shared, A attachment, CompletionHandler&lt;FileLock,? super A&gt; handler); 文件锁定功能上还提供了 tryLock 方法，此方法会快速返回结果： 12public abstract FileLock tryLock(long position, long size, boolean shared) throws IOException; 这个方法很简单，就是尝试去获取锁，如果该区域已被其他线程或其他应用锁住，那么立刻返回 null，否则返回 FileLock 对象。 AsynchronousFileChannel 操作大体上也就以上介绍的这些接口，还是比较简单的，这里就少一些废话早点结束好了。 AsynchronousServerSocketChannel这个类对应的是非阻塞 IO 的 ServerSocketChannel，大家可以类比下使用方式。 我们就废话少说，用代码说事吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.javadoop.aio;import java.io.IOException;import java.net.InetSocketAddress;import java.net.SocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;public class Server &#123; public static void main(String[] args) throws IOException &#123; // 实例化，并监听端口 AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(8080)); // 自己定义一个 Attachment 类，用于传递一些信息 Attachment att = new Attachment(); att.setServer(server); server.accept(att, new CompletionHandler&lt;AsynchronousSocketChannel, Attachment&gt;() &#123; @Override public void completed(AsynchronousSocketChannel client, Attachment att) &#123; try &#123; SocketAddress clientAddr = client.getRemoteAddress(); System.out.println(&quot;收到新的连接：&quot; + clientAddr); // 收到新的连接后，server 应该重新调用 accept 方法等待新的连接进来 att.getServer().accept(att, this); Attachment newAtt = new Attachment(); newAtt.setServer(server); newAtt.setClient(client); newAtt.setReadMode(true); newAtt.setBuffer(ByteBuffer.allocate(2048)); // 这里也可以继续使用匿名实现类，不过代码不好看，所以这里专门定义一个类 client.read(newAtt.getBuffer(), newAtt, new ChannelHandler()); &#125; catch (IOException ex) &#123; ex.printStackTrace(); &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println(&quot;accept failed&quot;); &#125; &#125;); // 为了防止 main 线程退出 try &#123; Thread.currentThread().join(); &#125; catch (InterruptedException e) &#123; &#125; &#125;&#125; 看一下 ChannelHandler 类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.javadoop.aio;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;public class ChannelHandler implements CompletionHandler&lt;Integer, Attachment&gt; &#123; @Override public void completed(Integer result, Attachment att) &#123; if (att.isReadMode()) &#123; // 读取来自客户端的数据 ByteBuffer buffer = att.getBuffer(); buffer.flip(); byte bytes[] = new byte[buffer.limit()]; buffer.get(bytes); String msg = new String(buffer.array()).toString().trim(); System.out.println(&quot;收到来自客户端的数据: &quot; + msg); // 响应客户端请求，返回数据 buffer.clear(); buffer.put(&quot;Response from server!&quot;.getBytes(Charset.forName(&quot;UTF-8&quot;))); att.setReadMode(false); buffer.flip(); // 写数据到客户端也是异步 att.getClient().write(buffer, att, this); &#125; else &#123; // 到这里，说明往客户端写数据也结束了，有以下两种选择: // 1\. 继续等待客户端发送新的数据过来// att.setReadMode(true);// att.getBuffer().clear();// att.getClient().read(att.getBuffer(), att, this); // 2\. 既然服务端已经返回数据给客户端，断开这次的连接 try &#123; att.getClient().close(); &#125; catch (IOException e) &#123; &#125; &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println(&quot;连接断开&quot;); &#125;&#125; 顺便再贴一下自定义的 Attachment 类： 1234567public class Attachment &#123; private AsynchronousServerSocketChannel server; private AsynchronousSocketChannel client; private boolean isReadMode; private ByteBuffer buffer; // getter &amp; setter&#125; 这样，一个简单的服务端就写好了，接下来可以接收客户端请求了。上面我们用的都是回调函数的方式，读者要是感兴趣，可以试试写个使用 Future 的。 AsynchronousSocketChannel其实，说完上面的 AsynchronousServerSocketChannel，基本上读者也就知道怎么使用 AsynchronousSocketChannel 了，和非阻塞 IO 基本类似。 这边做个简单演示，这样读者就可以配合之前介绍的 Server 进行测试使用了。 12345678910111213141516171819202122232425262728293031323334package com.javadoop.aio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.nio.charset.Charset;import java.util.concurrent.ExecutionException;import java.util.concurrent.Future;public class Client &#123; public static void main(String[] args) throws Exception &#123; AsynchronousSocketChannel client = AsynchronousSocketChannel.open(); // 来个 Future 形式的 Future&lt;?&gt; future = client.connect(new InetSocketAddress(8080)); // 阻塞一下，等待连接成功 future.get(); Attachment att = new Attachment(); att.setClient(client); att.setReadMode(false); att.setBuffer(ByteBuffer.allocate(2048)); byte[] data = &quot;I am obot!&quot;.getBytes(); att.getBuffer().put(data); att.getBuffer().flip(); // 异步发送数据到服务端 client.write(att.getBuffer(), att, new ClientChannelHandler()); // 这里休息一下再退出，给出足够的时间处理数据 Thread.sleep(2000); &#125;&#125; 往里面看下 ClientChannelHandler 类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.javadoop.aio;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.CompletionHandler;import java.nio.charset.Charset;public class ClientChannelHandler implements CompletionHandler&lt;Integer, Attachment&gt; &#123; @Override public void completed(Integer result, Attachment att) &#123; ByteBuffer buffer = att.getBuffer(); if (att.isReadMode()) &#123; // 读取来自服务端的数据 buffer.flip(); byte[] bytes = new byte[buffer.limit()]; buffer.get(bytes); String msg = new String(bytes, Charset.forName(&quot;UTF-8&quot;)); System.out.println(&quot;收到来自服务端的响应数据: &quot; + msg); // 接下来，有以下两种选择: // 1\. 向服务端发送新的数据// att.setReadMode(false);// buffer.clear();// String newMsg = &quot;new message from client&quot;;// byte[] data = newMsg.getBytes(Charset.forName(&quot;UTF-8&quot;));// buffer.put(data);// buffer.flip();// att.getClient().write(buffer, att, this); // 2\. 关闭连接 try &#123; att.getClient().close(); &#125; catch (IOException e) &#123; &#125; &#125; else &#123; // 写操作完成后，会进到这里 att.setReadMode(true); buffer.clear(); att.getClient().read(buffer, att, this); &#125; &#125; @Override public void failed(Throwable t, Attachment att) &#123; System.out.println(&quot;服务器无响应&quot;); &#125;&#125; 以上代码都是可以运行调试的，如果读者碰到问题，请在评论区留言。 Asynchronous Channel Groups为了知识的完整性，有必要对 group 进行介绍，其实也就是介绍 AsynchronousChannelGroup 这个类。之前我们说过，异步 IO 一定存在一个线程池，这个线程池负责接收任务、处理 IO 事件、回调等。这个线程池就在 group 内部，group 一旦关闭，那么相应的线程池就会关闭。 AsynchronousServerSocketChannels 和 AsynchronousSocketChannels 是属于 group 的，当我们调用 AsynchronousServerSocketChannel 或 AsynchronousSocketChannel 的 open() 方法的时候，相应的 channel 就属于默认的 group，这个 group 由 JVM 自动构造并管理。 如果我们想要配置这个默认的 group，可以在 JVM 启动参数中指定以下系统变量： java.nio.channels.DefaultThreadPool.threadFactory 此系统变量用于设置 ThreadFactory，它应该是 java.util.concurrent.ThreadFactory 实现类的全限定类名。一旦我们指定了这个 ThreadFactory 以后，group 中的线程就会使用该类产生。 java.nio.channels.DefaultThreadPool.initialSize 此系统变量也很好理解，用于设置线程池的初始大小。 可能你会想要使用自己定义的 group，这样可以对其中的线程进行更多的控制，使用以下几个方法即可： AsynchronousChannelGroup.withCachedThreadPool(ExecutorService executor, int initialSize) AsynchronousChannelGroup.withFixedThreadPool(int nThreads, ThreadFactory threadFactory) AsynchronousChannelGroup.withThreadPool(ExecutorService executor) 熟悉线程池的读者对这些方法应该很好理解，它们都是 AsynchronousChannelGroup 中的静态方法。 至于 group 的使用就很简单了，代码一看就懂： 1234AsynchronousChannelGroup group = AsynchronousChannelGroup .withFixedThreadPool(10, Executors.defaultThreadFactory());AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open(group);AsynchronousSocketChannel client = AsynchronousSocketChannel.open(group); AsynchronousFileChannels 不属于 group。但是它们也是关联到一个线程池的，如果不指定，会使用系统默认的线程池，如果想要使用指定的线程池，可以在实例化的时候使用以下方法： 123456public static AsynchronousFileChannel open(Path file, Set&lt;? extends OpenOption&gt; options, ExecutorService executor, FileAttribute&lt;?&gt;... attrs) &#123; ...&#125; 到这里，异步 IO 就算介绍完成了。 小结我想，本文应该是说清楚了非阻塞 IO 和异步 IO 了，对于异步 IO，由于网上的资料比较少，所以不免篇幅多了些。 我们也要知道，看懂了这些，确实可以学到一些东西，多了解一些知识，但是我们还是很少在工作中将这些知识变成工程代码。一般而言，我们需要在网络应用中使用 NIO 或 AIO 来提升性能，但是，在工程上，绝不是了解了一些概念，知道了一些接口就可以的，需要处理的细节还非常多。 这也是为什么 Netty/Mina 如此盛行的原因，因为它们帮助封装好了很多细节，提供给我们用户友好的接口，后面有时间我也会对 Netty 进行介绍。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程与NIO详解4：浅析NIO包中的Buffer、Channel 和 Selector]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8ENIO%E8%AF%A6%E8%A7%A34%EF%BC%9A%E6%B5%85%E6%9E%90NIO%E5%8C%85%E4%B8%AD%E7%9A%84Buffer%E3%80%81Channel%20%E5%92%8C%20Selector%2F</url>
    <content type="text"><![CDATA[本文转载自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文将介绍 Java NIO 中三大组件 Buffer、Channel、Selector 的使用。 本来要一起介绍非阻塞 IO 和 JDK7 的异步 IO 的，不过因为之前的文章真的太长了，有点影响读者阅读，所以这里将它们放到另一篇文章中进行介绍。 Buffer一个 Buffer 本质上是内存中的一块，我们可以将数据写入这块内存，之后从这块内存获取数据。 java.nio 定义了以下几个 Buffer 的实现，这个图读者应该也在不少地方见过了吧。 其实核心是最后的 ByteBuffer，前面的一大串类只是包装了一下它而已，我们使用最多的通常也是 ByteBuffer。 我们应该将 Buffer 理解为一个数组，IntBuffer、CharBuffer、DoubleBuffer 等分别对应 int[]、char[]、double[] 等。 MappedByteBuffer 用于实现内存映射文件，也不是本文关注的重点。 我觉得操作 Buffer 和操作数组、类集差不多，只不过大部分时候我们都把它放到了 NIO 的场景里面来使用而已。下面介绍 Buffer 中的几个重要属性和几个重要方法。 position、limit、capacity就像数组有数组容量，每次访问元素要指定下标，Buffer 中也有几个重要属性：position、limit、capacity。 最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值。 position 和 limit 是变化的，我们分别看下读和写操作下，它们是如何变化的。 position 的初始值是 0，每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置。读操作的时候也是类似的，每读一个值，position 就自动加 1。 从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。 Limit：写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了。 初始化 Buffer每个 Buffer 实现类都提供了一个静态方法 allocate(int capacity) 帮助我们快速实例化一个 Buffer。如： 1234ByteBuffer byteBuf = ByteBuffer.allocate(1024);IntBuffer intBuf = IntBuffer.allocate(1024);LongBuffer longBuf = LongBuffer.allocate(1024);// ... 另外，我们经常使用 wrap 方法来初始化一个 Buffer。 123public static ByteBuffer wrap(byte[] array) &#123; ...&#125; 填充 Buffer各个 Buffer 类都提供了一些 put 方法用于将数据填充到 Buffer 中，如 ByteBuffer 中的几个 put 方法： 1234567// 填充一个 byte 值public abstract ByteBuffer put(byte b);// 在指定位置填充一个 int 值public abstract ByteBuffer put(int index, byte b);// 将一个数组中的值填充进去public final ByteBuffer put(byte[] src) &#123;...&#125;public ByteBuffer put(byte[] src, int offset, int length) &#123;...&#125; 上述这些方法需要自己控制 Buffer 大小，不能超过 capacity，超过会抛 java.nio.BufferOverflowException 异常。 对于 Buffer 来说，另一个常见的操作中就是，我们要将来自 Channel 的数据填充到 Buffer 中，在系统层面上，这个操作我们称为读操作，因为数据是从外部（文件或网络等）读到内存中。 1int num = channel.read(buf); 上述方法会返回从 Channel 中读入到 Buffer 的数据大小。 提取 Buffer 中的值前面介绍了写操作，每写入一个值，position 的值都需要加 1，所以 position 最后会指向最后一次写入的位置的后面一个，如果 Buffer 写满了，那么 position 等于 capacity（position 从 0 开始）。 如果要读 Buffer 中的值，需要切换模式，从写入模式切换到读出模式。注意，通常在说 NIO 的读操作的时候，我们说的是从 Channel 中读数据到 Buffer 中，对应的是对 Buffer 的写入操作，初学者需要理清楚这个。 调用 Buffer 的 flip() 方法，可以从写入模式切换到读取模式。其实这个方法也就是设置了一下 position 和 limit 值罢了。 123456public final Buffer flip() &#123; limit = position; // 将 limit 设置为实际写入的数据数量 position = 0; // 重置 position 为 0 mark = -1; // mark 之后再说 return this;&#125; 对应写入操作的一系列 put 方法，读操作提供了一系列的 get 方法： 123456// 根据 position 来获取数据public abstract byte get();// 获取指定位置的数据public abstract byte get(int index);// 将 Buffer 中的数据写入到数组中public ByteBuffer get(byte[] dst) 附一个经常使用的方法： 1new String(buffer.array()).trim(); 当然了，除了将数据从 Buffer 取出来使用，更常见的操作是将我们写入的数据传输到 Channel 中，如通过 FileChannel 将数据写入到文件中，通过 SocketChannel 将数据写入网络发送到远程机器等。对应的，这种操作，我们称之为写操作。 1int num = channel.write(buf); mark() &amp; reset()除了 position、limit、capacity 这三个基本的属性外，还有一个常用的属性就是 mark。 mark 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用。 1234public final Buffer mark() &#123; mark = position; return this;&#125; 那到底什么时候用呢？考虑以下场景，我们在 position 为 5 的时候，先 mark() 一下，然后继续往下读，读到第 10 的时候，我想重新回到 position 为 5 的地方重新来一遍，那只要调一下 reset() 方法，position 就回到 5 了。 1234567public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; rewind() &amp; clear() &amp; compact()rewind()：会重置 position 为 0，通常用于重新从头读写 Buffer。 12345public final Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; clear()：有点重置 Buffer 的意思，相当于重新实例化了一样。 通常，我们会先填充 Buffer，然后从 Buffer 读取数据，之后我们再重新往里填充新的数据，我们一般在重新填充之前先调用 clear()。 123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; compact()：和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用。 前面说的 clear() 方法会重置几个属性，但是我们要看到，clear() 方法并不会将 Buffer 中的数据清空，只不过后续的写入会覆盖掉原来的数据，也就相当于清空了数据了。 而 compact() 方法有点不一样，调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向原来数据的右边。 Channel所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，我们将关心 java.nio 包中实现的以下几个 Channel： FileChannel：文件通道，用于文件的读和写 DatagramChannel：用于 UDP 连接的接收和发送 SocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端 ServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求 这里不是很理解这些也没关系，后面介绍了代码之后就清晰了。还有，我们最应该关注，也是后面将会重点介绍的是 SocketChannel 和 ServerSocketChannel。 Channel 经常翻译为通道，类似 IO 中的流，用于读取和写入。它与前面介绍的 Buffer 打交道，读操作的时候将 Channel 中的数据填充到 Buffer 中，而写操作时将 Buffer 中的数据写入到 Channel 中。 至少读者应该记住一点，这两个方法都是 channel 实例的方法。 FileChannel我想文件操作对于大家来说应该是最熟悉的，不过我们在说 NIO 的时候，其实 FileChannel 并不是关注的重点。而且后面我们说非阻塞的时候会看到，FileChannel 是不支持非阻塞的。 这里算是简单介绍下常用的操作吧，感兴趣的读者瞄一眼就是了。 初始化： 12FileInputStream inputStream = new FileInputStream(new File(&quot;/data.txt&quot;));FileChannel fileChannel = inputStream.getChannel(); 当然了，我们也可以从 RandomAccessFile#getChannel 来得到 FileChannel。 读取文件内容： 123ByteBuffer buffer = ByteBuffer.allocate(1024);int num = fileChannel.read(buffer); 前面我们也说了，所有的 Channel 都是和 Buffer 打交道的。 写入文件内容： 12345678ByteBuffer buffer = ByteBuffer.allocate(1024);buffer.put(&quot;随机写入一些内容到 Buffer 中&quot;.getBytes());// Buffer 切换为读模式buffer.flip();while(buffer.hasRemaining()) &#123; // 将 Buffer 中的内容写入文件 fileChannel.write(buffer);&#125; SocketChannel我们前面说了，我们可以将 SocketChannel 理解成一个 TCP 客户端。虽然这么理解有点狭隘，因为我们在介绍 ServerSocketChannel 的时候会看到另一种使用方式。 打开一个 TCP 连接： 1SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;https://www.javadoop.com&quot;, 80)); 当然了，上面的这行代码等价于下面的两行： 1234// 打开一个通道SocketChannel socketChannel = SocketChannel.open();// 发起连接socketChannel.connect(new InetSocketAddress(&quot;https://www.javadoop.com&quot;, 80)); SocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区。 1234567// 读取数据socketChannel.read(buffer);// 写入数据到网络连接中while(buffer.hasRemaining()) &#123; socketChannel.write(buffer); &#125; 不要在这里停留太久，先继续往下走。 ServerSocketChannel之前说 SocketChannel 是 TCP 客户端，这里说的 ServerSocketChannel 就是对应的服务端。 ServerSocketChannel 用于监听机器端口，管理从这个端口进来的 TCP 连接。 123456789// 实例化ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 监听 8080 端口serverSocketChannel.socket().bind(new InetSocketAddress(8080));while (true) &#123; // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理 SocketChannel socketChannel = serverSocketChannel.accept();&#125; 这里我们可以看到 SocketChannel 的第二个实例化方式 到这里，我们应该能理解 SocketChannel 了，它不仅仅是 TCP 客户端，它代表的是一个网络通道，可读可写。 ServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接。 DatagramChannelUDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端。 科普一下，UDP 是面向无连接的，不需要和对方握手，不需要通知对方，就可以直接将数据包投出去，至于能不能送达，它是不知道的 监听端口： 12DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9090)); 123ByteBuffer buf = ByteBuffer.allocate(48);channel.receive(buf); 发送数据： 12345678String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(&quot;jenkov.com&quot;, 80)); SelectorNIO 三大组件就剩 Selector 了，Selector 建立在非阻塞的基础之上，大家经常听到的 多路复用 在 Java 世界中指的就是它，用于实现一个线程管理多个 Channel。 读者在这一节不能消化 Selector 也没关系，因为后续在介绍非阻塞 IO 的时候还得说到这个，这里先介绍一些基本的接口操作。 首先，我们开启一个 Selector。你们爱翻译成选择器也好，多路复用器也好。 1Selector selector = Selector.open(); 将 Channel 注册到 Selector 上。前面我们说了，Selector 建立在非阻塞模式之上，所以注册到 Selector 的 Channel 必须要支持非阻塞模式，FileChannel 不支持非阻塞，我们这里讨论最常见的 SocketChannel 和 ServerSocketChannel。 1234// 将通道设置为非阻塞模式，因为默认都是阻塞模式的channel.configureBlocking(false);// 注册SelectionKey key = channel.register(selector, SelectionKey.OP_READ); register 方法的第二个 int 型参数（使用二进制的标记位）用于表明需要监听哪些感兴趣的事件，共以下四种事件： SelectionKey.OP_READ 对应 00000001，通道中有数据可以进行读取 SelectionKey.OP_WRITE 对应 00000100，可以往通道中写入数据 SelectionKey.OP_CONNECT 对应 00001000，成功建立 TCP 连接 SelectionKey.OP_ACCEPT 对应 00010000，接受 TCP 连接 我们可以同时监听一个 Channel 中的发生的多个事件，比如我们要监听 ACCEPT 和 READ 事件，那么指定参数为二进制的 00010001 即十进制数值 17 即可。 注册方法返回值是 SelectionKey 实例，它包含了 Channel 和 Selector 信息，也包括了一个叫做 Interest Set 的信息，即我们设置的我们感兴趣的正在监听的事件集合。 调用 select() 方法获取通道信息。用于判断是否有我们感兴趣的事件已经发生了。 Selector 的操作就是以上 3 步，这里来一个简单的示例，大家看一下就好了。之后在介绍非阻塞 IO 的时候，会演示一份可执行的示例代码。 123456789101112131415161718192021222324252627282930313233Selector selector = Selector.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; // 判断是否有事件准备好 int readyChannels = selector.select(); if(readyChannels == 0) continue; // 遍历 Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125;&#125; 对于 Selector，我们还需要非常熟悉以下几个方法： select() 调用此方法，会将上次 select 之后的准备好的 channel 对应的 SelectionKey 复制到 selected set 中。如果没有任何通道准备好，这个方法会阻塞，直到至少有一个通道准备好。 selectNow() 功能和 select 一样，区别在于如果没有准备好的通道，那么此方法会立即返回 0。 select(long timeout) 看了前面两个，这个应该很好理解了，如果没有通道准备好，此方法会等待一会 wakeup() 这个方法是用来唤醒等待在 select() 和 select(timeout) 上的线程的。如果 wakeup() 先被调用，此时没有线程在 select 上阻塞，那么之后的一个 select() 或 select(timeout) 会立即返回，而不会阻塞，当然，它只会作用一次。 小结到此为止，介绍了 Buffer、Channel 和 Selector 的常见接口。 Buffer 和数组差不多，它有 position、limit、capacity 几个重要属性。put() 一下数据、flip() 切换到读模式、然后用 get() 获取数据、clear() 一下清空数据、重新回到 put() 写入数据。 Channel 基本上只和 Buffer 打交道，最重要的接口就是 channel.read(buffer) 和 channel.write(buffer)。 Selector 用于实现非阻塞 IO，这里仅仅介绍接口使用，后续请关注非阻塞 IO 的介绍。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解1：JAVA 中原生的 socket 通信机制]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A31%EF%BC%9AJAVA%20%E4%B8%AD%E5%8E%9F%E7%94%9F%E7%9A%84%20socket%20%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文转自：https://github.com/jasonGeng88/blog 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 当前环境 jdk == 1.8 知识点 socket 的连接处理 IO 输入、输出流的处理 请求数据格式处理 请求模型优化 场景今天，和大家聊一下 JAVA 中的 socket 通信问题。这里采用最简单的一请求一响应模型为例，假设我们现在需要向 baidu 站点进行通信。我们用 JAVA 原生的 socket 该如何实现。 建立 socket 连接首先，我们需要建立 socket 连接（_核心代码_） import java.net.InetSocketAddress; import java.net.Socket; import java.net.SocketAddress; // 初始化 socket Socket socket = new Socket(); // 初始化远程连接地址 SocketAddress remote = new InetSocketAddress(host, port); // 建立连接 socket.connect(remote); 处理 socket 输入输出流成功建立 socket 连接后，我们就能获得它的输入输出流，通信的本质是对输入输出流的处理。通过输入流，读取网络连接上传来的数据，通过输出流，将本地的数据传出给远端。 socket 连接实际与处理文件流有点类似，都是在进行 IO 操作。 获取输入、输出流代码如下： // 输入流 InputStream in = socket.getInputStream(); // 输出流 OutputStream out = socket.getOutputStream(); 关于 IO 流的处理，我们一般会用相应的包装类来处理 IO 流，如果直接处理的话，我们需要对 byte[] 进行操作，而这是相对比较繁琐的。如果采用包装类，我们可以直接以string、int等类型进行处理，简化了 IO 字节操作。 下面以 BufferedReader 与 PrintWriter 作为输入输出的包装类进行处理。 // 获取 socket 输入流 private BufferedReader getReader(Socket socket) throws IOException { InputStream in = socket.getInputStream(); return new BufferedReader(new InputStreamReader(in)); } // 获取 socket 输出流 private PrintWriter getWriter(Socket socket) throws IOException { OutputStream out = socket.getOutputStream(); return new PrintWriter(new OutputStreamWriter(out)); } 数据请求与响应有了 socket 连接、IO 输入输出流，下面就该向发送请求数据，以及获取请求的响应结果。 因为有了 IO 包装类的支持，我们可以直接以字符串的格式进行传输，由包装类帮我们将数据装换成相应的字节流。 因为我们与 baidu 站点进行的是 HTTP 访问，所有我们不需要额外定义输出格式。采用标准的 HTTP 传输格式，就能进行请求响应了（_某些特定的 RPC 框架，可能会有自定义的通信格式_）。 请求的数据内容处理如下： public class HttpUtil { public static String compositeRequest(String host){ return "GET / HTTP/1.1\r\n" + "Host: " + host + "\r\n" + "User-Agent: curl/7.43.0\r\n" + "Accept: */*\r\n\r\n"; } } 发送请求数据代码如下： // 发起请求 PrintWriter writer = getWriter(socket); writer.write(HttpUtil.compositeRequest(host)); writer.flush(); 接收响应数据代码如下： // 读取响应 String msg; BufferedReader reader = getReader(socket); while ((msg = reader.readLine()) != null){ System.out.println(msg); } 结果展示至此，讲完了原生 socket 下的创建连接、发送请求与接收响应的所有核心代码。 完整代码如下： 1import java.io.*;import java.net.InetSocketAddress;import java.net.Socket;import java.net.SocketAddress;import com.test.network.util.HttpUtil; public class SocketHttpClient &#123; public void start(String host, int port) &#123; // 初始化 socket Socket socket = new Socket(); try &#123; // 设置 socket 连接 SocketAddress remote = new InetSocketAddress(host, port); socket.setSoTimeout(5000); socket.connect(remote); // 发起请求 PrintWriter writer = getWriter(socket); System.out.println(HttpUtil.compositeRequest(host)); writer.write(HttpUtil.compositeRequest(host)); writer.flush(); // 读取响应 String msg; BufferedReader reader = getReader(socket); while ((msg = reader.readLine()) != null)&#123; System.out.println(msg); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private BufferedReader getReader(Socket socket) throws IOException &#123; InputStream in = socket.getInputStream(); return new BufferedReader(new InputStreamReader(in)); &#125; private PrintWriter getWriter(Socket socket) throws IOException &#123; OutputStream out = socket.getOutputStream(); return new PrintWriter(new OutputStreamWriter(out)); &#125; &#125; 下面，我们通过实例化一个客户端，来展示 socket 通信的结果。 public class Application { public static void main(String[] args) { new SocketHttpClient().start("www.baidu.com", 80); } } 结果输出： 请求模型优化这种方式，虽然实现功能没什么问题。但是我们细看，发现在 IO 写入与读取过程，是发生了 IO 阻塞的情况。即： 1// 会发生 IO 阻塞writer.write(HttpUtil.compositeRequest(host));reader.readLine(); 所以如果要同时请求10个不同的站点，如下： public class SingleThreadApplication { public static void main(String[] args) { // HttpConstant.HOSTS 为 站点集合 for (String host: HttpConstant.HOSTS) { new SocketHttpClient().start(host, HttpConstant.PORT); } } } 它一定是第一个请求响应结束后，才会发起下一个站点处理。 这在服务端更明显，虽然这里的代码是客户端连接，但是具体的操作和服务端是差不多的。请求只能一个个串行处理，这在响应时间上肯定不能达标。 多线程处理 有人觉得这根本不是问题，JAVA 是多线程的编程语言。对于这种情况，采用多线程的模型再合适不过。 1public class MultiThreadApplication &#123; public static void main(String[] args) &#123; for (final String host: HttpConstant.HOSTS) &#123; Thread t = new Thread(new Runnable() &#123; public void run() &#123; new SocketHttpClient().start(host, HttpConstant.PORT); &#125; &#125;); t.start(); &#125; &#125;&#125; 这种方式起初看起来挺有用的，但并发量一大，应用会起很多的线程。都知道，在服务器上，每一个线程实际都会占据一个文件句柄。而服务器上的句柄数是有限的，而且大量的线程，造成的线程间切换的消耗也会相当的大。所以这种方式在并发量大的场景下，一定是承载不住的。 多线程 + 线程池 处理 既然线程太多不行，那我们控制一下线程创建的数目不就行了。只启动固定的线程数来进行 socket 处理，既利用了多线程的处理，又控制了系统的资源消耗。 public class ThreadPoolApplication { public static void main(String[] args) { ExecutorService executorService = Executors.newFixedThreadPool(8); for (final String host: HttpConstant.HOSTS) { Thread t = new Thread(new Runnable() { public void run() { new SocketHttpClient().start(host, HttpConstant.PORT); } }); executorService.submit(t); new SocketHttpClient().start(host, HttpConstant.PORT); } } } 关于启动的线程数，一般 CPU 密集型会设置在 N+1（N为CPU核数），IO 密集型设置在 2N + 1。 这种方式，看起来是最优的了。那有没有更好的呢，如果一个线程能同时处理多个 socket 连接，并且在每个 socket 输入输出数据没有准备好的情况下，不进行阻塞，那是不是更优呢。这种技术叫做“IO多路复用”。在 JAVA 的 nio 包中，提供了相应的实现。 补充1：TCP客户端与服务端public class TCP客户端 { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { try { Socket s = new Socket("127.0.0.1",1234); //构建IO InputStream is = s.getInputStream(); OutputStream os = s.getOutputStream(); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(os)); //向服务器端发送一条消息 bw.write("测试客户端和服务器通信，服务器接收到消息返回到客户端\n"); bw.flush(); //读取服务器返回的消息 BufferedReader br = new BufferedReader(new InputStreamReader(is)); String mess = br.readLine(); System._out_.println("服务器："+mess); } catch (UnknownHostException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } }).start(); } } public class TCP服务端 { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { try { ServerSocket ss = new ServerSocket(1234); while (true) { System._out_.println("启动服务器...."); Socket s = ss.accept(); System._out_.println("客户端:" + s.getInetAddress().getLocalHost() + "已连接到服务器"); BufferedReader br = new BufferedReader(new InputStreamReader(s.getInputStream())); //读取客户端发送来的消息 String mess = br.readLine(); System._out_.println("客户端：" + mess); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(s.getOutputStream())); bw.write(mess + "\n"); bw.flush(); } } catch (IOException e) { e.printStackTrace(); } } }).start(); } } 补充2：UDP客户端和服务端public class UDP客户端 { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { byte []arr = "Hello Server".getBytes(); try { InetAddress inetAddress = InetAddress.getLocalHost(); DatagramSocket datagramSocket = new DatagramSocket(); DatagramPacket datagramPacket = new DatagramPacket(arr, arr.length, inetAddress, 1234); datagramSocket.send(datagramPacket); System._out_.println("send end"); } catch (UnknownHostException e) { e.printStackTrace(); } catch (SocketException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } }).start(); } } public class UDP服务端 { public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { try { DatagramSocket datagramSocket = new DatagramSocket(1234); byte[] buffer = new byte[1024]; DatagramPacket packet = new DatagramPacket(buffer, buffer.length); datagramSocket.receive(packet); System._out_.println("server recv"); String msg = new String(packet.getData(), "utf-8"); System._out_.println(msg); } catch (SocketException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } } }).start(); } } 后续 JAVA 中是如何实现 IO多路复用 Netty 下的实现异步请求的 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程和NIO详解3：IO模型与Java网络编程模型]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%92%8CNIO%E8%AF%A6%E8%A7%A33%EF%BC%9AIO%E6%A8%A1%E5%9E%8B%E4%B8%8EJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 基本概念说明 用户空间与内核空间 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。 进程的阻塞 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存 IO 缓存 IO 又被称作标准 IO，大多数文件系统的默认 IO 操作都是缓存 IO。在 Linux 的缓存 IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 IO 的缺点： 数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模型介绍作者：cooffeelis链接：https://www.jianshu.com/p/511b9cffbdac來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 常用的5种IO模型:blocking IOnonblocking IOIO multiplexingsignal driven IOasynchronous IO 再说一下IO发生时涉及的对象和步骤: 对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象： 一个是调用这个IO的process (or thread) 一个就是系统内核(kernel) 当一个read操作发生时，它会经历两个阶段： 等待数据准备,比如accept(), recv()等待数据 (Waiting for the data to be ready) 将数据从内核拷贝到进程中, 比如 accept()接受到请求,recv()接收连接发送的数据后需要复制到内核,再从内核复制到进程用户空间(Copying the data from the kernel to the process) 对于socket流而言,数据的流向经历两个阶段： 第一步通常涉及等待网络上的数据分组到达，然后被复制到内核的某个缓冲区。 第二步把数据从内核缓冲区复制到应用进程缓冲区。 记住这两点很重要，因为这些IO Model的区别就是在两个阶段上各有不同的情况。 阻塞 I/O（blocking IO） 在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 阻塞IO流程 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞 I/O（nonblocking IO） linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子： 非阻塞 I/O 流程 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 *值得注意的是,此时的非阻塞IO只是应用到等待数据上,当真正有数据到达执行recvfrom的时候,还是同步阻塞IO来的, 从图中的copy data from kernel to user可以看出 * I/O 多路复用（ IO multiplexing） IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 I/O 多路复用流程 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。 所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） IO复用的实现方式目前主要有select、poll和epoll。 select和poll的原理基本相同： 注册待侦听的fd(这里的fd创建时最好使用非阻塞) 每次调用都去检查这些fd的状态，当有一个或者多个fd就绪的时候返回 返回结果中包括已就绪和未就绪的fd 相比select，poll解决了单个进程能够打开的文件描述符数量有限制这个问题：select受限于FD_SIZE的限制，如果修改则需要修改这个宏重新编译内核；而poll通过一个pollfd数组向内核传递需要关注的事件，避开了文件描述符数量限制。 此外，select和poll共同具有的一个很大的缺点就是包含大量fd的数组被整体复制于用户态和内核态地址空间之间，开销会随着fd数量增多而线性增大。 select和poll就类似于上面说的就餐方式。但当你每次都去询问时，老板会把所有你点的饭菜都轮询一遍再告诉你情况，当大量饭菜很长时间都不能准备好的情况下是很低效的。于是，老板有些不耐烦了，就让厨师每做好一个菜就通知他。这样每次你再去问的时候，他会直接把已经准备好的菜告诉你，你再去端。这就是事件驱动IO就绪通知的方式-epoll。 epoll的出现，解决了select、poll的缺点： 基于事件驱动的方式，避免了每次都要把所有fd都扫描一遍。 epoll_wait只返回就绪的fd。 epoll使用nmap内存映射技术避免了内存复制的开销。 epoll的fd数量上限是操作系统的最大文件句柄数目,这个数目一般和内存有关，通常远大于1024。 目前，epoll是Linux2.6下最高效的IO复用方式，也是Nginx、Node的IO实现方式。而在freeBSD下，kqueue是另一种类似于epoll的IO复用方式。 此外，对于IO复用还有一个水平触发和边缘触发的概念： 水平触发：当就绪的fd未被用户进程处理后，下一次查询依旧会返回，这是select和poll的触发方式。 边缘触发：无论就绪的fd是否被处理，下一次不再返回。理论上性能更高，但是实现相当复杂，并且任何意外的丢失事件都会造成请求处理错误。epoll默认使用水平触发，通过相应选项可以使用边缘触发。 点评：I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。所以, IO多路复用，本质上不会有并发的功能，因为任何时候还是只有一个进程或线程进行工作，它之所以能提高效率是因为select\epoll 把进来的socket放到他们的 ‘监视’ 列表里面，当任何socket有可读可写数据立马处理，那如果select\epoll 手里同时检测着很多socket， 一有动静马上返回给进程处理，总比一个一个socket过来,阻塞等待,处理高效率。当然也可以多线程/多进程方式，一个连接过来开一个进程/线程处理，这样消耗的内存和进程切换页会耗掉更多的系统资源。所以我们可以结合IO多路复用和多进程/多线程 来高性能并发，IO复用负责提高接受socket的通知效率，收到请求后，交给进程池/线程池来处理逻辑。 信号驱动上文的就餐方式还是需要你每次都去问一下饭菜状况。于是，你再次不耐烦了，就跟老板说，哪个饭菜好了就通知我一声吧。然后就自己坐在桌子那里干自己的事情。更甚者，你可以把手机号留给老板，自己出门，等饭菜好了直接发条短信给你。这就类似信号驱动的IO模型。 流程如下： 开启套接字信号驱动IO功能 系统调用sigaction执行信号处理函数（非阻塞，立刻返回） 数据就绪，生成sigio信号，通过信号回调通知应用来读取数据。 此种io方式存在的一个很大的问题：Linux中信号队列是有限制的，如果超过这个数字问题就无法读取数据。 异步非阻塞 异步 I/O（asynchronous IO） linux下的asynchronous IO其实用得很少。先看一下它的流程： 异步IO 流程 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 阻塞IO,非阻塞IO 与 同步IO, 异步IO的区别和联系阻塞IO VS 非阻塞IO： 概念：阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 例子：你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 分析：阻塞IO会一直block住对应的进程直到操作完成，而非阻塞IO在kernel还准备数据的情况下会立刻返回。 同步IO VS 异步IO： 概念：同步与异步同步和异步关注的是_消息通信机制 _(synchronous communication/ asynchronous communication)所谓同步，就是在发出一个_调用_时，在没有得到结果之前，该_调用_就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由_调用者_主动等待这个_调用_的结果。而异步则是相反，_调用_在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在_调用_发出后，_被调用者_通过状态、通知来通知调用者，或通过回调函数处理这个调用。 典型的异步编程模型比如Node.js举个通俗的例子：你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 分析：在说明同步IO和异步IO的区别之前，需要先给出两者的定义。Stevens给出的定义（其实是POSIX的定义）是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于同步IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的阻塞IO,非阻塞IO ，IO复用都属于同步IO。有人可能会说，非阻塞IO 并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。非阻塞IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而异步IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 IO模型的形象举例最后，再举几个不是很恰当的例子来说明这四个IO Model:有A，B，C，D四个人在钓鱼：A用的是最老式的鱼竿，所以呢，得一直守着，等到鱼上钩了再拉杆；B的鱼竿有个功能，能够显示是否有鱼上钩，所以呢，B就和旁边的MM聊天，隔会再看看有没有鱼上钩，有的话就迅速拉杆；C用的鱼竿和B差不多，但他想了一个好办法，就是同时放好几根鱼竿，然后守在旁边，一旦有显示说鱼上钩了，它就将对应的鱼竿拉起来；D是个有钱人，干脆雇了一个人帮他钓鱼，一旦那个人把鱼钓上来了，就给D发个短信。 Select/Poll/Epoll 轮询机制select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的Select/Poll/Epoll 都是IO复用的实现方式， 上面说了使用IO复用，会把socket设置成non-blocking，然后放进Select/Poll/Epoll 各自的监视列表里面，那么，他们的对socket是否有数据到达的监视机制分别是怎样的？效率又如何？我们应该使用哪种方式实现IO复用比较好？下面列出他们各自的实现方式，效率，优缺点： （1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 （2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 Java网络编程模型上文讲述了UNIX环境的五种IO模型。基于这五种模型，在Java中，随着NIO和NIO2.0(AIO)的引入，一般具有以下几种网络编程模型： BIO NIO AIO BIOBIO是一个典型的网络编程模型，是通常我们实现一个服务端程序的过程，步骤如下： 主线程accept请求阻塞 请求到达，创建新的线程来处理这个套接字，完成对客户端的响应。 主线程继续accept下一个请求 这种模型有一个很大的问题是：当客户端连接增多时，服务端创建的线程也会暴涨，系统性能会急剧下降。因此，在此模型的基础上，类似于 tomcat的bio connector，采用的是线程池来避免对于每一个客户端都创建一个线程。有些地方把这种方式叫做伪异步IO(把请求抛到线程池中异步等待处理)。 NIOJDK1.4开始引入了NIO类库，这里的NIO指的是New IO，主要是使用Selector多路复用器来实现。Selector在Linux等主流操作系统上是通过epoll实现的。 NIO的实现流程，类似于select： 创建ServerSocketChannel监听客户端连接并绑定监听端口，设置为非阻塞模式。 创建Reactor线程，创建多路复用器(Selector)并启动线程。 将ServerSocketChannel注册到Reactor线程的Selector上。监听accept事件。 Selector在线程run方法中无线循环轮询准备就绪的Key。 Selector监听到新的客户端接入，处理新的请求，完成tcp三次握手，建立物理连接。 将新的客户端连接注册到Selector上，监听读操作。读取客户端发送的网络消息。 客户端发送的数据就绪则读取客户端请求，进行处理。 相比BIO，NIO的编程非常复杂。 AIOJDK1.7引入NIO2.0，提供了异步文件通道和异步套接字通道的实现。其底层在windows上是通过IOCP，在Linux上是通过epoll来实现的(LinuxAsynchronousChannelProvider.java,UnixAsynchronousServerSocketChannelImpl.java)。 创建AsynchronousServerSocketChannel，绑定监听端口 调用AsynchronousServerSocketChannel的accpet方法，传入自己实现的CompletionHandler。包括上一步，都是非阻塞的 连接传入，回调CompletionHandler的completed方法，在里面，调用AsynchronousSocketChannel的read方法，传入负责处理数据的CompletionHandler。 数据就绪，触发负责处理数据的CompletionHandler的completed方法。继续做下一步处理即可。 写入操作类似，也需要传入CompletionHandler。 其编程模型相比NIO有了不少的简化。 对比 . 同步阻塞IO 伪异步IO NIO AIO 客户端数目 ：IO线程 1 : 1 m : n m : 1 m : 0 IO模型 同步阻塞IO 同步阻塞IO 同步非阻塞IO 异步非阻塞IO 吞吐量 低 中 高 高 编程复杂度 简单 简单 非常复杂 复杂 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程与NIO详解2：JAVA NIO 一步步构建IO多路复用的请求模型]]></title>
    <url>%2F2019%2F12%2F13%2F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%2FJava%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%B8%8ENIO%E8%AF%A6%E8%A7%A32%EF%BC%9AJAVA%20NIO%20%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%9E%84%E5%BB%BAIO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%E7%9A%84%E8%AF%B7%E6%B1%82%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文转载自：https://github.com/jasonGeng88/blog 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《不可轻视的Java网络编程》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从计算机网络的基础知识入手，一步步地学习Java网络基础，从socket到nio、bio、aio和netty等网络编程知识，并且进行实战，网络编程是每一个Java后端工程师必须要学习和理解的知识点，进一步来说，你还需要掌握Linux中的网络编程原理，包括IO模型、网络编程框架netty的进阶原理，才能更完整地了解整个Java网络编程的知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文章一：JAVA 中原生的 socket 通信机制 当前环境 jdk == 1.8 代码地址git 地址：https://github.com/jasonGeng88/java-network-programming 知识点 nio 下 I/O 阻塞与非阻塞实现 SocketChannel 介绍 I/O 多路复用的原理 事件选择器与 SocketChannel 的关系 事件监听类型 字节缓冲 ByteBuffer 数据结构 场景接着上一篇中的站点访问问题，如果我们需要并发访问10个不同的网站，我们该如何处理？ 在上一篇中，我们使用了java.net.socket类来实现了这样的需求，以一线程处理一连接的方式，并配以线程池的控制，貌似得到了当前的最优解。可是这里也存在一个问题，连接处理是同步的，也就是并发数量增大后，大量请求会在队列中等待，或直接异常抛出。 为解决这问题，我们发现元凶处在“一线程一请求”上，如果一个线程能同时处理多个请求，那么在高并发下性能上会大大改善。这里就借住 JAVA 中的 nio 技术来实现这一模型。 nio 的阻塞实现关于什么是 nio，从字面上理解为 New IO，就是为了弥补原本 I/O 上的不足，而在 JDK 1.4 中引入的一种新的 I/O 实现方式。简单理解，就是它提供了 I/O 的阻塞与非阻塞的两种实现方式（_当然，默认实现方式是阻塞的。_）。 下面，我们先来看下 nio 以阻塞方式是如何处理的。 建立连接有了上一篇 socket 的经验，我们的第一步一定也是建立 socket 连接。只不过，这里不是采用 new socket() 的方式，而是引入了一个新的概念 SocketChannel。它可以看作是 socket 的一个完善类，除了提供 Socket 的相关功能外，还提供了许多其他特性，如后面要讲到的向选择器注册的功能。 类图如下： 建立连接代码实现： // 初始化 socket，建立 socket 与 channel 的绑定关系 SocketChannel socketChannel = SocketChannel.open(); // 初始化远程连接地址 SocketAddress remote = new InetSocketAddress(this.host, port); // I/O 处理设置阻塞，这也是默认的方式，可不设置 socketChannel.configureBlocking(true); // 建立连接 socketChannel.connect(remote); 获取 socket 连接因为是同样是 I/O 阻塞的实现，所以后面的关于 socket 输入输出流的处理，和上一篇的基本相同。唯一差别是，这里需要通过 channel 来获取 socket 连接。 获取 socket 连接 Socket socket = socketChannel.socket(); 处理输入输出流 PrintWriter pw = getWriter(socketChannel.socket()); BufferedReader br = getReader(socketChannel.socket()); 完整示例package com.jason.network.mode.nio; import com.jason.network.constant.HttpConstant; import com.jason.network.util.HttpUtil; import java.io.*; import java.net.InetSocketAddress; import java.net.Socket; import java.net.SocketAddress; import java.nio.channels.SocketChannel; public class NioBlockingHttpClient { private SocketChannel socketChannel; private String host; public static void main(String[] args) throws IOException { for (String host: HttpConstant.HOSTS) { NioBlockingHttpClient client = new NioBlockingHttpClient(host, HttpConstant.PORT); client.request(); } } public NioBlockingHttpClient(String host, int port) throws IOException { this.host = host; socketChannel = SocketChannel.open(); socketChannel.socket().setSoTimeout(5000); SocketAddress remote = new InetSocketAddress(this.host, port); this.socketChannel.connect(remote); } public void request() throws IOException { PrintWriter pw = getWriter(socketChannel.socket()); BufferedReader br = getReader(socketChannel.socket()); pw.write(HttpUtil.compositeRequest(host)); pw.flush(); String msg; while ((msg = br.readLine()) != null){ System.out.println(msg); } } private PrintWriter getWriter(Socket socket) throws IOException { OutputStream out = socket.getOutputStream(); return new PrintWriter(out); } private BufferedReader getReader(Socket socket) throws IOException { InputStream in = socket.getInputStream(); return new BufferedReader(new InputStreamReader(in)); } } nio 的非阻塞实现原理分析nio 的阻塞实现，基本与使用原生的 socket 类似，没有什么特别大的差别。 下面我们来看看它真正强大的地方。到目前为止，我们将的都是阻塞 I/O。何为阻塞 I/O，看下图： 我们主要观察图中的前三种 I/O 模型，关于异步 I/O，一般需要依靠操作系统的支持，这里不讨论。 从图中可以发现，阻塞过程主要发生在两个阶段上： 第一阶段：等待数据就绪； 第二阶段：将已就绪的数据从内核缓冲区拷贝到用户空间； 这里产生了一个从内核到用户空间的拷贝，主要是为了系统的性能优化考虑。假设，从网卡读到的数据直接返回给用户空间，那势必会造成频繁的系统中断，因为从网卡读到的数据不一定是完整的，可能断断续续的过来。通过内核缓冲区作为缓冲，等待缓冲区有足够的数据，或者读取完结后，进行一次的系统中断，将数据返回给用户，这样就能避免频繁的中断产生。 了解了 I/O 阻塞的两个阶段，下面我们进入正题。看看一个线程是如何实现同时处理多个 I/O 调用的。从上图中的非阻塞 I/O 可以看出，仅仅只有第二阶段需要阻塞，第一阶段的数据等待过程，我们是不需要关心的。不过该模型是频繁地去检查是否就绪，造成了 CPU 无效的处理，反而效果不好。如果有一种类似的好莱坞原则— “不要给我们打电话，我们会打给你” 。这样一个线程可以同时发起多个 I/O 调用，并且不需要同步等待数据就绪。在数据就绪完成的时候，会以事件的机制，来通知我们。这样不就实现了单线程同时处理多个 IO 调用的问题了吗？即所说的“I/O 多路复用模型”。 废话讲了一大堆，下面就来实际操刀一下。 创建选择器由上面分析可以，我们得有一个选择器，它能监听所有的 I/O 操作，并且以事件的方式通知我们哪些 I/O 已经就绪了。 代码如下： import java.nio.channels.Selector; ... private static Selector selector; static { try { selector = Selector.open(); } catch (IOException e) { e.printStackTrace(); } } 创建非阻塞 I/O下面，我们来创建一个非阻塞的 SocketChannel，代码与阻塞实现类型，唯一不同是socketChannel.configureBlocking(false)。 注意：只有在socketChannel.configureBlocking(false)之后的代码，才是非阻塞的，如果socketChannel.connect()在设置非阻塞模式之前，那么连接操作依旧是阻塞调用的。 SocketChannel socketChannel = SocketChannel.open(); SocketAddress remote = new InetSocketAddress(host, port); // 设置非阻塞模式 socketChannel.configureBlocking(false); socketChannel.connect(remote); 建立选择器与 socket 的关联选择器与 socket 都创建好了，下一步就是将两者进行关联，好让选择器和监听到 Socket 的变化。这里采用了以 SocketChannel 主动注册到选择器的方式进行关联绑定，这也就解释了，为什么不直接new Socket()，而是以SocketChannel的方式来创建 socket。 代码如下： socketChannel.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ | SelectionKey.OP_WRITE); 上面代码，我们将 socketChannel 注册到了选择器中，并且对它的连接、可读、可写事件进行了监听。 具体的事件监听类型如下： 操作类型 值 描述 所属对象 OP_READ 1 &lt;&lt; 0 读操作 SocketChannel OP_WRITE 1 &lt;&lt; 2 写操作 SocketChannel OP_CONNECT 1 &lt;&lt; 3 连接socket操作 SocketChannel OP_ACCEPT 1 &lt;&lt; 4 接受socket操作 ServerSocketChannel 选择器监听 socket 变化现在，选择器已经与我们关心的 socket 进行了关联。下面就是感知事件的变化，然后调用相应的处理机制。 这里与 Linux 下的 selector 有点不同，nio 下的 selecotr 不会去遍历所有关联的 socket。我们在注册时设置了我们关心的事件类型，每次从选择器中获取的，只会是那些符合事件类型，并且完成就绪操作的 socket，减少了大量无效的遍历操作。 123456789101112131415161718192021222324252627282930public void select() throws IOException &#123; // 获取就绪的 socket 个数 while (selector.select() &gt; 0)&#123; // 获取符合的 socket 在选择器中对应的事件句柄 key Set keys = selector.selectedKeys(); // 遍历所有的key Iterator it = keys.iterator(); while (it.hasNext())&#123; // 获取对应的 key，并从已选择的集合中移除 SelectionKey key = (SelectionKey)it.next(); it.remove(); if (key.isConnectable())&#123; // 进行连接操作 connect(key); &#125; else if (key.isWritable())&#123; // 进行写操作 write(key); &#125; else if (key.isReadable())&#123; // 进行读操作 receive(key); &#125; &#125; &#125;&#125; 注意：这里的selector.select()是同步阻塞的，等待有事件发生后，才会被唤醒。这就防止了 CPU 空转的产生。当然，我们也可以给它设置超时时间，selector.select(long timeout)来结束阻塞过程。 处理连接就绪事件下面，我们分别来看下，一个 socket 是如何来处理连接、写入数据和读取数据的（_这些操作都是阻塞的过程，只是我们将等待就绪的过程变成了非阻塞的了_）。 处理连接代码： // SelectionKey 代表 SocketChannel 在选择器中注册的事件句柄 private void connect(SelectionKey key) throws IOException { // 获取事件句柄对应的 SocketChannel SocketChannel channel = (SocketChannel) key.channel(); // 真正的完成 socket 连接 channel.finishConnect(); // 打印连接信息 InetSocketAddress remote = (InetSocketAddress) channel.socket().getRemoteSocketAddress(); String host = remote.getHostName(); int port = remote.getPort(); System.out.println(String.format("访问地址: %s:%s 连接成功!", host, port)); } 处理写入就绪事件// 字符集处理类 private Charset charset = Charset.forName("utf8"); private void write(SelectionKey key) throws IOException { SocketChannel channel = (SocketChannel) key.channel(); InetSocketAddress remote = (InetSocketAddress) channel.socket().getRemoteSocketAddress(); String host = remote.getHostName(); // 获取 HTTP 请求，同上一篇 String request = HttpUtil.compositeRequest(host); // 向 SocketChannel 写入事件 channel.write(charset.encode(request)); // 修改 SocketChannel 所关心的事件 key.interestOps(SelectionKey.OP_READ); } 这里有两个地方需要注意： 第一个是使用 channel.write(charset.encode(request)); 进行数据写入。有人会说，为什么不能像上面同步阻塞那样，通过PrintWriter包装类进行操作。因为PrintWriter的 write() 方法是阻塞的，也就是说要等数据真正从 socket 发送出去后才返回。 这与我们这里所讲的阻塞是不一致的，这里的操作虽然也是阻塞的，但它发生的过程是在数据从用户空间到内核缓冲区拷贝过程。至于系统将缓冲区的数据通过 socket 发送出去，这不在阻塞范围内。也解释了为什么要用 Charset 对写入内容进行编码了，因为缓冲区接收的格式是ByteBuffer。 第二，选择器用来监听事件变化的两个参数是 interestOps 与 readyOps。 interestOps：表示 SocketChannel 所关心的事件类型，也就是告诉选择器，当有这几种事件发生时，才来通知我。这里通过key.interestOps(SelectionKey.OP_READ);告诉选择器，之后我只关心“读就绪”事件，其他的不用通知我了。 readyOps：表示 SocketChannel 当前就绪的事件类型。以key.isReadable()为例，判断依据就是：return (readyOps() &amp; OP_READ) != 0; 处理读取就绪事件private void receive(SelectionKey key) throws IOException { SocketChannel channel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); channel.read(buffer); buffer.flip(); String receiveData = charset.decode(buffer).toString(); // 当再没有数据可读时，取消在选择器中的关联，并关闭 socket 连接 if ("".equals(receiveData)) { key.cancel(); channel.close(); return; } System.out.println(receiveData); } 这里的处理基本与写入一致，唯一要注意的是，这里我们需要自行处理去缓冲区读取数据的操作。首先会分配一个固定大小的缓冲区，然后从内核缓冲区中，拷贝数据至我们刚分配固定缓冲区上。这里存在两种情况： 我们分配的缓冲区过大，那多余的部分以0补充（_初始化时，其实会自动补0_）。 我们分配的缓冲去过小，因为选择器会不停的遍历。只要 SocketChannel 处理读就绪状态，那下一次会继续读取。当然，分配过小，会增加遍历次数。 最后，将一下 ByteBuffer 的结构，它主要有 position, limit,capacity 以及 mark 属性。以 buffer.flip(); 为例，讲下各属性的作用（_mark 主要是用来标记之前 position 的位置，是在当前 postion 无法满足的情况下使用的，这里不作讨论_）。 从图中看出， 容量（capacity）：表示缓冲区可以保存的数据容量； 极限（limit）：表示缓冲区的当前终点，即写入、读取都不可超过该重点； 位置（position）：表示缓冲区下一个读写单元的位置； 完整代码package com.jason.network.mode.nio; import com.jason.network.constant.HttpConstant; import com.jason.network.util.HttpUtil; import java.io.IOException; import java.net.InetSocketAddress; import java.net.SocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SelectionKey; import java.nio.channels.Selector; import java.nio.channels.SocketChannel; import java.nio.charset.Charset; import java.util.Iterator; import java.util.Set; public class NioNonBlockingHttpClient { private static Selector selector; private Charset charset = Charset.forName("utf8"); static { try { selector = Selector.open(); } catch (IOException e) { e.printStackTrace(); } } public static void main(String[] args) throws IOException { NioNonBlockingHttpClient client = new NioNonBlockingHttpClient(); for (String host: HttpConstant.HOSTS) { client.request(host, HttpConstant.PORT); } client.select(); } public void request(String host, int port) throws IOException { SocketChannel socketChannel = SocketChannel.open(); socketChannel.socket().setSoTimeout(5000); SocketAddress remote = new InetSocketAddress(host, port); socketChannel.configureBlocking(false); socketChannel.connect(remote); socketChannel.register(selector, SelectionKey.OP_CONNECT | SelectionKey.OP_READ | SelectionKey.OP_WRITE); } public void select() throws IOException { while (selector.select(500) > 0){ Set keys = selector.selectedKeys(); Iterator it = keys.iterator(); while (it.hasNext()){ SelectionKey key = (SelectionKey)it.next(); it.remove(); if (key.isConnectable()){ connect(key); } else if (key.isWritable()){ write(key); } else if (key.isReadable()){ receive(key); } } } } private void connect(SelectionKey key) throws IOException { SocketChannel channel = (SocketChannel) key.channel(); channel.finishConnect(); InetSocketAddress remote = (InetSocketAddress) channel.socket().getRemoteSocketAddress(); String host = remote.getHostName(); int port = remote.getPort(); System.out.println(String.format("访问地址: %s:%s 连接成功!", host, port)); } private void write(SelectionKey key) throws IOException { SocketChannel channel = (SocketChannel) key.channel(); InetSocketAddress remote = (InetSocketAddress) channel.socket().getRemoteSocketAddress(); String host = remote.getHostName(); String request = HttpUtil.compositeRequest(host); System.out.println(request); channel.write(charset.encode(request)); key.interestOps(SelectionKey.OP_READ); } private void receive(SelectionKey key) throws IOException { SocketChannel channel = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); channel.read(buffer); buffer.flip(); String receiveData = charset.decode(buffer).toString(); if ("".equals(receiveData)) { key.cancel(); channel.close(); return; } System.out.println(receiveData); } } 示例效果 总结本文从 nio 的阻塞方式讲起，介绍了阻塞 I/O 与非阻塞 I/O 的区别，以及在 nio 下是如何一步步构建一个 IO 多路复用的模型的客户端。文中需要理解的内容比较多，如果有理解错误的地方，欢迎指正~ 后续 Netty 下的异步请求实现 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界16：极简配置的SpringBoot]]></title>
    <url>%2F2019%2F10%2F28%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C16%EF%BC%9A%E6%9E%81%E7%AE%80%E9%85%8D%E7%BD%AE%E7%9A%84SpringBoot%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） Spring Boot 概述 Build Anything with Spring Boot：Spring Boot is the starting point for building all Spring-based applications. Spring Boot is designed to get you up and running as quickly as possible, with minimal upfront configuration of Spring. 上面是引自官网的一段话，大概是说： Spring Boot 是所有基于 Spring 开发的项目的起点。Spring Boot 的设计是为了让你尽可能快的跑起来 Spring 应用程序并且尽可能减少你的配置文件。 什么是 Spring Boot 它使用 “习惯优于配置” （项目中存在大量的配置，此外还内置一个习惯性的配置，让你无须）的理念让你的项目快速运行起来。 它并不是什么新的框架，而是默认配置了很多框架的使用方式，就像 Maven 整合了所有的 jar 包一样，Spring Boot 整合了所有框架（引自：springboot(一)：入门篇——纯洁的微笑） 使用 Spring Boot 有什么好处回顾我们之前的 SSM 项目，搭建过程还是比较繁琐的，需要： 1）配置 web.xml，加载 spring 和 spring mvc 2）配置数据库连接、配置日志文件 3）配置家在配置文件的读取，开启注解 4）配置mapper文件 ….. 而使用 Spring Boot 来开发项目则只需要非常少的几个配置就可以搭建起来一个 Web 项目，并且利用 IDEA 可以自动生成生成，这简直是太爽了… 划重点：简单、快速、方便地搭建项目；对主流开发框架的无配置集成；极大提高了开发、部署效率。 Spring Boot 快速搭建第一步：新建项目选择 Spring Initializr ，然后选择默认的 url 点击【Next】： 然后修改一下项目的信息： 勾选上 Web 模板： 选择好项目的位置，点击【Finish】： 如果是第一次配置 Spring Boot 的话可能需要等待一会儿 IDEA 下载相应的 依赖包，默认创建好的项目结构如下： 项目结构还是看上去挺清爽的，少了很多配置文件，我们来了解一下默认生成的有什么： SpringbootApplication： 一个带有 main() 方法的类，用于启动应用程序 SpringbootApplicationTests：一个空的 Junit 测试了，它加载了一个使用 Spring Boot 字典配置功能的 Spring 应用程序上下文 application.properties：一个空的 properties 文件，可以根据需要添加配置属性 pom.xml： Maven 构建说明文件 第二步：HelloController在【cn.wmyskxz.springboot】包下新建一个【HelloController】： 12345678910111213141516171819package cn.wmyskxz.springboot;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;/** * 测试控制器 * * @author: @我没有三颗心脏 * @create: 2018-05-08-下午 16:46 */@RestControllerpublic class HelloController &#123; @RequestMapping(&quot;/hello&quot;) public String hello() &#123; return &quot;Hello Spring Boot!&quot;; &#125;&#125; @RestController 注解： 该注解是 @Controller 和 @ResponseBody 注解的合体版 第三步：利用 IDEA 启动 Spring Boot我们回到 SpringbootApplication 这个类中，然后右键点击运行： 注意：我们之所以在上面的项目中没有手动的去配置 Tomcat 服务器，是因为 Spring Boot 内置了 Tomcat 等待一会儿就会看到下方的成功运行的提示信息： 可以看到我们的 Tomcat 运行在 8080 端口，我们来访问 “/hello” 地址试一下： 可以看到页面成功显示出我们返回的信息。 解析 Spring Boot 项目 这一部分参考自：Spring Boot干货系列（一）优雅的入门篇 ——嘟嘟独立博客 解析 pom.xml 文件让我们来看看默认生成的 pom.xml 文件中到底有一些什么特别： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.wmyskxz&lt;/groupId&gt; springboot &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;springboot&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-parent &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-web &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-test &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-maven-plugin &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 我们可以看到一个比较陌生一些的标签 &lt;parent&gt; ，这个标签是在配置 Spring Boot 的父级依赖： 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-parent &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 有了这个，当前的项目才是 Spring Boot 项目，spring-boot-starter-parent 是一个特殊的 starter ，它用来提供相关的 Maven 默认依赖，使用它之后，常用的包依赖就可以省去 version 标签。 关于具体 Spring Boot 提供了哪些 jar 包的依赖，我们可以查看本地 Maven 仓库下：\repository\org\springframework\boot\spring-boot-dependencies\2.0.1.RELEASE\spring-boot-dependencies-2.0.1.RELEASE.pom 文件来查看，挺长的… 应用入口类Spring Boot 项目通常有一个名为 Application 的入口类，入口类里有一个 main 方法， *这个 main 方法其实就是一个标准的 Javay 应用的入口方法。** @SpringBootApplication 是 Spring Boot 的核心注解，它是一个组合注解，该注解组合了：@Configuration、@EnableAutoConfiguration、@ComponentScan； 若不是用 @SpringBootApplication 注解也可以使用这三个注解代替。 其中，@EnableAutoConfiguration 让 Spring Boot 根据类路径中的 jar 包依赖为当前项目进行自动配置，例如，添加了 spring-boot-starter-web 依赖，会自动添加 Tomcat 和 Spring MVC 的依赖，那么 Spring Boot 会对 Tomcat 和 Spring MVC 进行自动配置。 Spring Boot 还会自动扫描 @SpringBootApplication 所在类的同级包以及下级包里的 Bean ，所以入口类建议就配置在 grounpID + arctifactID 组合的包名下（这里为 cn.wmyskxz.springboot 包） Spring Boot 的配置文件Spring Boot 使用一个全局的配置文件 application.properties 或 application.yml，放置在【src/main/resources】目录或者类路径的 /config 下。 Spring Boot 不仅支持常规的 properties 配置文件，还支持 yaml 语言的配置文件。yaml 是以数据为中心的语言，在配置数据的时候具有面向对象的特征。 Spring Boot 的全局配置文件的作用是对一些默认配置的配置值进行修改。 简单实例一下 我们同样的将 Tomcat 默认端口设置为 8080 ，并将默认的访问路径从 “/” 修改为 “/hello” 时，使用 properties 文件和 yml 文件的区别如上图。 注意： yml 需要在 “:” 后加一个空格，幸好 IDEA 很好地支持了 yml 文件的格式有良好的代码提示； 我们可以自己配置多个属性 我们直接把 .properties 后缀的文件删掉，使用 .yml 文件来进行简单的配置，然后使用 @Value 来获取配置属性： 重启 Spring Boot ，输入地址：localhost:8080/hello 能看到正确的结果： 注意： 我们并没有在 yml 文件中注明属性的类型，而是在使用的时候定义的。 你也可以在配置文件中使用当前配置： 仍然可以得到正确的结果： 问题： 这样写配置文件繁琐而且可能会造成类的臃肿，因为有许许多多的 @Value 注解。 封装配置信息 我们可以把配置信息封装成一个类，首先在我们的 name 和 age 前加一个 student 前缀，然后新建一个 StudentProperties 的类用来封装这些信息，并用上两个注解： @Component：表明当前类是一个 Java Bean @ConfigurationProperties(prefix = “student”)：表示获取前缀为 sutdent 的配置信息 这样我们就可以在控制器中使用，重启得到正确信息： Spring Boot 热部署在目前的 Spring Boot 项目中，当发生了任何修改之后我们都需要重新启动才能够正确的得到效果，这样会略显麻烦，Spring Boot 提供了热部署的方式，当发现任何类发生了改变，就会通过 JVM 类加载的方式，加载最新的类到虚拟机中，这样就不需要重新启动也能看到修改后的效果了。 做法也很简单，修改 pom.xml 即可！ 我们往 pom.xml 中添加一个依赖就可以了： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-devtools &lt;optional&gt;true&lt;/optional&gt; &lt;!-- 这个需要为 true 热部署才有效 --&gt;&lt;/dependency&gt; 重新启动 Spring Boot ，然后修改任意代码，就能观察到控制台的自动重启现象： 关于如何在 IDEA 中配置热部署：传送门 Spring Boot 使用上面已经完成了 Spring Boot 项目的简单搭建，我们仅仅需要进行一些简单的设置，写一个 HelloController 就能够直接运行了，不要太简单…接下来我们再深入了解一下 Spring Boot 的使用。 Spring Boot 支持 JSPSpring Boot 的默认视图支持是 Thymeleaf 模板引擎，但是这个我们不熟悉啊，我们还是想要使用 JSP 怎么办呢？ 第一步：修改 pom.xml 增加对 JSP 文件的支持 1234567891011121314151617&lt;!-- servlet依赖. --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; javax.servlet-api &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; jstl&lt;/dependency&gt;&lt;!-- tomcat的支持.--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; tomcat-embed-jasper &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 第二步：配置试图重定向 JSP 文件的位置 修改 application.yml 文件，将我们的 JSP 文件重定向到 /WEB-INF/views/ 目录下： 第三步：修改 HelloController 修改 @RestController 注解为 @Controller ，然后将 hello 方法修改为： 第四步：新建 hello.jsp 文件 在【src/main】目录下依次创建 webapp、WEB-INF、views 目录，并创建一个 hello.jsp 文件： 第五步：刷新网页 因为我们部署了热部署功能，所以只需要等待控制台重启信息完成之后再刷新网页就可以看到正确效果了： 关于 404，使用 spring-boot:run 运行项目可以解决： 集成 MyBatis 第一步：修改 pom.xml 增加对 MySql和 MyBatis 的支持 12345678910111213&lt;!-- mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; mybatis-spring-boot-starter &lt;version&gt;1.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; mysql-connector-java &lt;version&gt;5.1.21&lt;/version&gt;&lt;/dependency&gt; 第二步：新增数据库链接参数 这里我们就直接使用之前创建好的 student 表了吧： 第三步：创建 Student 实体类和 StudentMapper 映射类 在【cn.wmyskxz.springboot】下新建一个【pojo】包，然后在其下创建一个 Student 类： 1234567891011public class Student &#123; private Integer id; private Integer student_id; private String name; private Integer age; private String sex; private Date birthday; /* getter and setter */&#125; 在【cn.wmyskxz.springboot】下新建一个【mapper】包，然后在其下创建一个 StudentMapper 映射类： 1234567891011121314package cn.wmyskxz.springboot.mapper;import cn.wmyskxz.springboot.pojo.Student;import org.apache.ibatis.annotations.Mapper;import org.apache.ibatis.annotations.Select;import java.util.List;@Mapperpublic interface StudentMapper &#123; @Select(&quot;SELECT * FROM student&quot;) List&lt;Student&gt; findAll();&#125; 第四步：编写 StudentController 在【cn.wmyskxz.springboot】下新建一个【controller】包，然后在其下创建一个 StudentController ： 123456789101112131415161718192021222324252627282930package cn.wmyskxz.springboot.controller;import cn.wmyskxz.springboot.mapper.StudentMapper;import cn.wmyskxz.springboot.pojo.Student;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.RequestMapping;import java.util.List;/** * Student 控制器 * * @author: @我没有三颗心脏 * @create: 2018-05-08-下午 20:25 */@Controllerpublic class StudentController &#123; @Autowired StudentMapper studentMapper; @RequestMapping(&quot;/listStudent&quot;) public String listStudent(Model model) &#123; List&lt;Student&gt; students = studentMapper.findAll(); model.addAttribute(&quot;students&quot;, students); return &quot;listStudent&quot;; &#125;&#125; 第五步：编写 listStudent.jsp 文件 我们简化一下 JSP 的文件，仅显示两个字段的数据： 1234567891011121314151617&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;%@ taglib uri=&quot;http://java.sun.com/jsp/jstl/core&quot; prefix=&quot;c&quot;%&gt;&lt;table align=&apos;center&apos; border=&apos;1&apos; cellspacing=&apos;0&apos;&gt; &lt;tr&gt; &lt;td&gt;id&lt;/td&gt; &lt;td&gt;name&lt;/td&gt; &lt;/tr&gt; &lt;c:forEach items=&quot;$&#123;students&#125;&quot; var=&quot;s&quot; varStatus=&quot;st&quot;&gt; &lt;tr&gt; &lt;td&gt;$&#123;s.id&#125;&lt;/td&gt; &lt;td&gt;$&#123;s.name&#125;&lt;/td&gt; &lt;/tr&gt; &lt;/c:forEach&gt;&lt;/table&gt; 第六步：重启服务器运行 因为往 pom.xml 中新增加了依赖的包，所以自动重启服务器没有作用，我们需要手动重启一次，然后在地址输入：localhost:8080/listStudent 查看效果： 以上。 springMVC和springboot的区别Spring 框架就像一个家族，有众多衍生产品例如 boot、security、jpa等等。但他们的基础都是Spring 的 ioc和 aop ioc 提供了依赖注入的容器 aop ，解决了面向横切面的编程，然后在此两者的基础上实现了其他延伸产品的高级功能。Spring MVC是基于 Servlet 的一个 MVC 框架 主要解决 WEB 开发的问题，因为 Spring 的配置非常复杂，各种XML、 JavaConfig、hin处理起来比较繁琐。于是为了简化开发者的使用，从而创造性地推出了Spring boot，约定优于配置，简化了spring的配置流程。 说得更简便一些：Spring 最初利用“工厂模式”（DI）和“代理模式”（AOP）解耦应用组件。大家觉得挺好用，于是按照这种模式搞了一个 MVC框架（一些用Spring 解耦的组件），用开发 web 应用（ SpringMVC ）。然后有发现每次开发都写很多样板代码，为了简化工作流程，于是开发出了一些“懒人整合包”（starter），这套就是 Spring Boot。 Spring MVC的功能 Spring MVC提供了一种轻度耦合的方式来开发web应用。 Spring MVC是Spring的一个模块，式一个web框架。通过Dispatcher Servlet, ModelAndView 和 View Resolver，开发web应用变得很容易。解决的问题领域是网站应用程序或者服务开发——URL路由、Session、模板引擎、静态Web资源等等。 Spring Boot的功能 Spring Boot实现了自动配置，降低了项目搭建的复杂度。 众所周知Spring框架需要进行大量的配置，Spring Boot引入自动配置的概念，让项目设置变得很容易。Spring Boot本身并不提供Spring框架的核心特性以及扩展功能，只是用于快速、敏捷地开发新一代基于Spring框架的应用程序。也就是说，它并不是用来替代Spring的解决方案，而是和Spring框架紧密结合用于提升Spring开发者体验的工具。同时它集成了大量常用的第三方库配置(例如Jackson, JDBC, Mongo, Redis, Mail等等)，Spring Boot应用中这些第三方库几乎可以零配置的开箱即用(out-of-the-box)，大部分的Spring Boot应用都只需要非常少量的配置代码，开发者能够更加专注于业务逻辑。 Spring Boot只是承载者，辅助你简化项目搭建过程的。如果承载的是WEB项目，使用Spring MVC作为MVC框架，那么工作流程和你上面描述的是完全一样的，因为这部分工作是Spring MVC做的而不是Spring Boot。 对使用者来说，换用Spring Boot以后，项目初始化方法变了，配置文件变了，另外就是不需要单独安装Tomcat这类容器服务器了，maven打出jar包直接跑起来就是个网站，但你最核心的业务逻辑实现与业务流程实现没有任何变化。 所以，用最简练的语言概括就是： Spring 是一个“引擎”； Spring MVC 是基于Spring的一个 MVC 框架 ； Spring Boot 是基于Spring4的条件注册的一套快速开发整合包。 参考文章https://www.cnblogs.com/ThinkVenus/p/8026633.htmlhttps://www.jianshu.com/p/f8b0b8616d4fhttps://blog.csdn.net/qq_29229567/article/details/89209719https://www.jianshu.com/p/ffe5ebe17c3ahttps://blog.csdn.net/qq_30258957/article/details/81700960 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入JavaWeb技术世界15:深入浅出Mybatis基本原理]]></title>
    <url>%2F2019%2F10%2F26%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E6%B7%B1%E5%85%A5JavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C15%EF%BC%9A%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAMybatis%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 引言在mybatis的基础知识中我们已经可以对mybatis的工作方式窥斑见豹（参考：《MyBatis————基础知识》）。 但是，为什么还要要学习mybatis的工作原理？因为，随着mybatis框架的不断发展，如今已经越来越趋于自动化，从代码生成，到基本使用，我们甚至不需要动手写一句SQL就可以完成一个简单应用的全部CRUD操作。 从原生mybatis到mybatis-spring，到mybatis-plus再到mybatis-plus-spring-boot-starter。spring在发展，mybatis同样在随之发展。 万变的外表终将迷惑人们的双眼，只要抓住核心我们永远不会迷茫！ 工作原理原型图用最直观的图，来征服你的心！ 工作原理解析mybatis应用程序通过SqlSessionFactoryBuilder从mybatis-config.xml配置文件（也可以用Java文件配置的方式，需要添加@Configuration）中构建出SqlSessionFactory（SqlSessionFactory是线程安全的）； 然后，SqlSessionFactory的实例直接开启一个SqlSession，再通过SqlSession实例获得Mapper对象并运行Mapper映射的SQL语句，完成对数据库的CRUD和事务提交，之后关闭SqlSession。 说明：SqlSession是单线程对象，因为它是非线程安全的，是持久化操作的独享对象，类似jdbc中的Connection，底层就封装了jdbc连接。 详细流程如下： 1、加载mybatis全局配置文件（数据源、mapper映射文件等），解析配置文件，MyBatis基于XML配置文件生成Configuration，和一个个MappedStatement（包括了参数映射配置、动态SQL语句、结果映射配置），其对应着&lt;select | update | delete | insert&gt;标签项。 2、SqlSessionFactoryBuilder通过Configuration对象生成SqlSessionFactory，用来开启SqlSession。 3、SqlSession对象完成和数据库的交互：a、用户程序调用mybatis接口层api（即Mapper接口中的方法）b、SqlSession通过调用api的Statement ID找到对应的MappedStatement对象c、通过Executor（负责动态SQL的生成和查询缓存的维护）将MappedStatement对象进行解析，sql参数转化、动态sql拼接，生成jdbc Statement对象d、JDBC执行sql。 e、借助MappedStatement中的结果映射关系，将返回结果转化成HashMap、JavaBean等存储结构并返回。 mybatis层次图： MyBatis框架及原理分析MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架，其主要就完成2件事情： 封装JDBC操作 利用反射打通Java类与SQL语句之间的相互转换 MyBatis的主要设计目的就是让我们对执行SQL语句时对输入输出的数据管理更加方便，所以方便地写出SQL和方便地获取SQL的执行结果才是MyBatis的核心竞争力。 MyBatis的配置MyBatis框架和其他绝大部分框架一样，需要一个配置文件，其配置文件大致如下： &lt;configuration&gt; &lt;settings&gt; &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; &lt;setting name=&quot;lazyLoadingEnabled&quot; value=&quot;false&quot;/&gt; &lt;!--&lt;setting name=&quot;logImpl&quot; value=&quot;STDOUT_LOGGING&quot;/&gt; &amp;lt;!&amp;ndash; 打印日志信息 &amp;ndash;&amp;gt;--&gt; &lt;/settings&gt; &lt;typeAliases&gt; &lt;typeAlias type=&quot;com.luo.dao.UserDao&quot; alias=&quot;User&quot;/&gt; &lt;/typeAliases&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;!--事务管理类型--&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;luoxn28&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;123456&quot;/&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://192.168.1.150/ssh_study&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;userMapper.xml&quot;/&gt; &lt;/mappers&gt; &lt;/configuration&gt;以上配置中，最重要的是数据库参数的配置，比如用户名密码等，如果配置了数据表对应的mapper文件，则需要将其加入到节点下。 MyBatis的主要成员 Configuration MyBatis所有的配置信息都保存在Configuration对象之中，配置文件中的大部分配置都会存储到该类中 SqlSession 作为MyBatis工作的主要顶层API，表示和数据库交互时的会话，完成必要数据库增删改查功能 Executor MyBatis执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护 StatementHandler 封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数等 ParameterHandler 负责对用户传递的参数转换成JDBC Statement 所对应的数据类型 ResultSetHandler 负责将JDBC返回的ResultSet结果集对象转换成List类型的集合 TypeHandler 负责java数据类型和jdbc数据类型(也可以说是数据表列类型)之间的映射和转换 MappedStatement MappedStatement维护一条&lt;select|update|delete|insert&gt;节点的封装 SqlSource 负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql 表示动态生成的SQL语句以及相应的参数信息 以上主要成员在一次数据库操作中基本都会涉及，在SQL操作中重点需要关注的是SQL参数什么时候被设置和结果集怎么转换为JavaBean对象的，这两个过程正好对应StatementHandler和ResultSetHandler类中的处理逻辑。 (图片来自《深入理解mybatis原理》 MyBatis的架构设计以及实例分析) 参考文章https://www.jianshu.com/p/e398435fc1c4https://segmentfault.com/a/1190000015117926?utm_source=tag-newest#articleHeader4https://blog.csdn.net/u014745069/article/details/80788127https://blog.csdn.net/u014297148/article/details/78696096https://blog.csdn.net/weixin_43184769/article/details/91126687 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界14：Mybatis入门]]></title>
    <url>%2F2019%2F10%2F26%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C14%EF%BC%9AMybatis%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言学习一个新东西前，如果能对他有一个比较直观的印象与定位，那么接下来的学习过程就会顺畅很多。所以本文主要是我对Mybatis的一个简单入门性的总结介绍（前提还是需要些必要的概念认知）。PS:文末有参考列表 Mybatis是什么Mybatis是一个持久层框架，用于数据的持久化。主要表现为将SQL与POJO进行一个映射，将SQL从代码中解耦。基本概念如图： 使用时，以User为例，UserMapper定义了findById接口，该接口返回一个User对象，接口的实现为一个xml配置文件。该xml文件中定义对应接口中的实现所需要的SQL。从而达到将SQL与代码解耦的目标。 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace=&quot;com.mybatis.UserMapper&quot;&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;User&quot;&gt; select user_id id,user_name userName,user_age age from t_user where user_id=#&#123;id&#125; &lt;/select&gt; &lt;/mapper&gt; MyBatis 是Apache的一个Java开源项目，是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。Mybatis可以将Sql语句配置在XML文件中，避免将Sql语句硬编码在Java类中。特点1.Mybatis通过参数映射方式，可以将参数灵活的配置在SQL语句中的配置文件中，避免在Java类中配置参数（JDBC） 2.Mybatis通过输出映射机制，将结果集的检索自动映射成相应的Java对象，避免对结果集手工检索（JDBC） 3.Mybatis可以通过Xml配置文件对数据库连接进行管理 核心类介绍1.SqlSessionaFactoryBuilder ：该类主要用于创建 SqlSessionFactory, 这个类可以被实例化、使用和丢弃，一旦创建了 SqlSessionFactory，就不再需要它了。 因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。 2.SqlSessionFactory ：该类的作用了创建 SqlSession, 从名字上我们也能看出, 该类使用了工厂模式, 每次应用程序访问数据库, 我们就要通过 SqlSessionFactory 创建 SqlSession, 所以 SqlSessionFactory 和整个 Mybatis 的生命周期是相同的. 这也告诉我们不能创建多个同一个数据的 SqlSessionFactory, 如果创建多个, 会消耗尽数据库的连接资源, 导致服务器夯机. 应当使用单例模式. 避免过多的连接被消耗, 也方便管理。 3.SqlSession ：SqlSession 相当于一个会话, 每次访问数据库都需要这样一个会话, 大家可能会想起了 JDBC 中的 Connection, 很类似, 但还是有区别的, 何况现在几乎所有的连接都是使用的连接池技术, 用完后直接归还而不会像 Session 一样销毁. 注意: 他是一个线程不安全的对象, 在设计多线程的时候我们需要特别的当心, 操作数据库需要注意其隔离级别, 数据库锁等高级特性, 此外, 每次创建的 SqlSession 都必须及时关闭它, 它长期存在就会使数据库连接池的活动资源减少, 对系统性能的影响很大, 我们一般在 finally 块中将其关闭. 还有, SqlSession 存活于一个应用的请求和操作, 可以执行多条 Sql, 保证事务的一致性。SqlSession在执行过程中，有包含了几大对象： 3.1.Executor ：执行器，由它调度 StatementHandler、ParameterHandler、ResultSetHandler 等来执行对应的 SQL。其中 StatementHandler 是最重要的。 3.2.StatementHandler ：作用是使用数据库的 Statement（PreparedStatement）执行操作，它是四大对象的核心，起到承上启下的作用，许多重要的插件都是通过拦截它来实现的。 3.3.ParamentHandler ：用来处理 SQL 参数的。 3.4.ResultSetHandler ：进行数据集的封装返回处理的。4.Mapper ：映射器是一些由你创建的、绑定你映射的语句的接口。映射器接口的实例是从 SqlSession 中获得的, 他的作用是发送 SQL, 然后返回我们需要的结果. 或者执行 SQL 从而更改数据库的数据, 因此它应该在 SqlSession 的事务方法之内, 在 Spring 管理的 Bean 中, Mapper 是单例的。 功能架构：我们把Mybatis的功能架构分为三层(1)API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 (2)数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 (3)基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 框架结构：(1)加载配置：配置来源于两个地方，一处是配置文件，一处是Java代码的注解，将SQL的配置信息加载成为一个个MappedStatement对象（包括了传入参数映射配置、执行的SQL语句、结果映射配置），存储在内存中。 (2)SQL解析：当API接口层接收到调用请求时，会接收到传入SQL的ID和传入对象（可以是Map、JavaBean或者基本数据类型），Mybatis会根据SQL的ID找到对应的MappedStatement，然后根据传入参数对象对MappedStatement进行解析，解析后可以得到最终要执行的SQL语句和参数。 (3)SQL执行：将最终得到的SQL和参数拿到数据库进行执行，得到操作数据库的结果。 (4)结果映射：将操作数据库的结果按照映射的配置进行转换，可以转换成HashMap、JavaBean或者基本数据类型，并将最终结果返回。 执行流程：1.获取SqlsessionFactory：根据配置文件（全局、sql映射）初始化configuration对象， 2.获取sqlSession：创建一个DefaultSqlSession对象，包含Configuration及Executor（根据全局配置文件中defaultExecutorType创建对应的Executor） 3.获取接口代理对象MapperProxy：DefaultSqlSession.getMapper拿到Mapper接口对应的MapperProxy 4.执行增删改查 1、调用DefaultSqlSession增删改查 2、创建StatementHandler （同时创建ParameterHandler,ResultSetHandler） 3、调用StatementHandler预编译参数以及设置参数值，使用ParameterHandler给sql设置参数 4、调用StatementHandler增删改查 5、ResultSetHandler封装结果 与Hibernate的异同Mybatis开始逐渐流行起来，必然有其原因，简单了解了一下它与同为持久层框架的Hibernate的异同。 映射模式从上面的简单概念可以知道Mybatis实际上着力点在POJO与SQL的映射。而Hibernate则主要是POJO与数据库表的对象关系映射。前者掌控力度更细，代码量会相对多一点，后者灵活性则差一点，更为自动化一些，与PHP里的Eloquent属于同类型。 性能Mybatis基于原生JDBC，相比于对JDBC进行二次封装的Hibernate性能会更好一点。 开发与维护Hibernate配置好实体类后，使用起来是比较简洁，舒服的，但是前期学习曲线比较陡，后期调优比较麻烦。Mybatis对SQL掌控的颗粒更细一点，相比较而言看上去简陋些。由于直接映射SQL，迁移性是个问题。 参考文章https://segmentfault.com/a/1190000009707894 https://www.cnblogs.com/hysum/p/7100874.html http://c.biancheng.net/view/939.html https://www.runoob.com/ https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：黄小斜黄小斜是跨考软件工程的 985 硕士，自学 Java 两年，拿到了 BAT 等近十家大厂 offer，从技术小白成长为阿里工程师。 作者专注于 JAVA 后端技术栈，热衷于分享程序员干货、学习经验、求职心得和程序人生，目前黄小斜的CSDN博客有百万+访问量，知乎粉丝2W+，全网已有10W+读者。 黄小斜是一个斜杠青年，坚持学习和写作，相信终身学习的力量，希望和更多的程序员交朋友，一起进步和成长！ 原创电子书:关注公众号【黄小斜】后回复【原创电子书】即可领取我原创的电子书《菜鸟程序员修炼手册：从技术小白到阿里巴巴Java工程师》 程序员3T技术学习资源： 一些程序员学习技术的资源大礼包，关注公众号后，后台回复关键字 “资料” 即可免费无套路获取。 考研复习资料：计算机考研大礼包，都是我自己考研复习时用的一些复习资料,包括公共课和专业的复习视频，这里也推荐给大家，关注公众号后，后台回复关键字 “考研” 即可免费获取。 mybatis新手上路MyBatis简介Mybatis是Apache的一个Java开源项目，是一个支持动态Sql语句的持久层框架。Mybatis可以将Sql语句配置在XML文件中，避免将Sql语句硬编码在Java类中。与JDBC相比： Mybatis通过参数映射方式，可以将参数灵活的配置在SQL语句中的配置文件中，避免在Java类中配置参数（JDBC） Mybatis通过输出映射机制，将结果集的检索自动映射成相应的Java对象，避免对结果集手工检索（JDBC） Mybatis可以通过Xml配置文件对数据库连接进行管理。 MyBatis整体架构及运行流程Mybatis整体构造由 数据源配置文件、Sql映射文件、会话工厂、会话、执行器和底层封装对象组成。 1.数据源配置文件通过配置的方式将数据库的配置信息从应用程序中独立出来，由独立的模块管理和配置。Mybatis的数据源配置文件包含数据库驱动、数据库连接地址、用户名密码、事务管理等，还可以配置连接池的连接数、空闲时间等。 一个SqlMapConfig.xml基本的配置信息如下： 2.Sql映射文件Mybatis中所有数据库的操作都会基于该映射文件和配置的sql语句，在这个配置文件中可以配置任何类型的sql语句。框架会根据配置文件中的参数配置，完成对sql语句以及输入输出参数的映射配置。 Mapper.xml配置文件大致如下： select * from products where id = #{id} 3.会话工厂与会话Mybatis中会话工厂SqlSessionFactory类可以通过加载资源文件，读取数据源配置SqlMapConfig.xml信息，从而产生一种可以与数据库交互的会话实例SqlSession，会话实例SqlSession根据Mapper.xml文件中配置的sql,对数据库进行操作。 4.运行流程会话工厂SqlSessionFactory通过加载资源文件获取SqlMapConfig.xml配置文件信息，然后生成可以与数据库交互的会话实例SqlSession。 会话实例可以根据Mapper配置文件中的Sql配置去执行相应的增删改查操作。 在SqlSession会话实例内部，通过执行器Executor对数据库进行操作，Executor依靠封装对象Mappered Statement，它分装了从mapper.xml文件中读取的信息（sql语句，参数，结果集类型）。 Mybatis通过执行器与Mappered Statement的结合实现与数据库的交互。 执行流程图： 测试工程搭建 新建maven工程 2. 添加依赖pom.xml 4.0.0 com.sl mybatis-demo 0.0.1-SNAPSHOT 4.12 3.4.1 5.1.32 1.2.17 junit junit ${junit.version} org.mybatis mybatis ${mybatis.version} mysql mysql-connector-java ${mysql.version} log4j log4j ${log4j.version} 3.编写数据源配置文件SqlMapConfig.xml 4.编写SQL映射配置文件productMapper.xml select * from products 5.编写测试代码TestClient.java //使用productMapper.xml配置文件 public class TestClient { //定义会话SqlSession SqlSession session =null; @Before public void init() throws IOException { //定义mabatis全局配置文件 String resource = "SqlMapConfig.xml"; //加载mybatis全局配置文件 //InputStream inputStream = TestClient.class.getClassLoader().getResourceAsStream(resource); InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); SqlSessionFactory factory = builder.build(inputStream); //根据sqlSessionFactory产生会话sqlsession session = factory.openSession(); } //查询所有user表所有数据 @Test public void testSelectAllUser() { String statement = "com.sl.mapper.ProductMapper.selectAllProduct"; List listProduct =session.selectList(statement); for(Product product:listProduct) { System.out.println(product); } //关闭会话 session.close(); } } public class Product { private int Id; private String Name; private String Description; private BigDecimal UnitPrice; private String ImageUrl; private Boolean IsNew; public int getId() { return Id; } public void setId(int id) { this.Id = id; } public String getName() { return Name; } public void setName(String name) { this.Name = name; } public String getDescription() { return Description; } public void setDescription(String description) { this.Description = description; } public BigDecimal getUnitPrice() { return UnitPrice; } public void setUnitPrice(BigDecimal unitprice) { this.UnitPrice = unitprice; } public String getImageUrl() { return Name; } public void setImageUrl(String imageurl) { this.ImageUrl = imageurl; } public boolean getIsNew() { return IsNew; } public void setIsNew(boolean isnew) { this.IsNew = isnew; } @Override public String toString() { return "Product [id=" + Id + ", Name=" + Name + ", Description=" + Description + ", UnitPrice=" + UnitPrice + ", ImageUrl=" + ImageUrl + ", IsNew=" + IsNew+ "]"; } } 6.运行测试用例 参考文章https://www.jianshu.com/p/06f68617fc04https://blog.csdn.net/tao5375/article/details/81774161https://blog.csdn.net/qq_42897427/article/details/89191524https://www.cnblogs.com/cailijia52o/p/8725850.htmlhttps://www.jianshu.com/p/f35598c769d9 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界13：Hibernate入门经典与注解式开发]]></title>
    <url>%2F2019%2F10%2F25%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C13%EF%BC%9AHibernate%E5%85%A5%E9%97%A8%E7%BB%8F%E5%85%B8%E4%B8%8E%E6%B3%A8%E8%A7%A3%E5%BC%8F%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 前言本博文主要讲解介绍Hibernate框架，ORM的概念和Hibernate入门，相信你们看了就会使用Hibernate了! 什么是Hibernate框架？Hibernate是一种ORM框架，全称为 Object_Relative DateBase-Mapping，在Java对象与关系数据库之间建立某种映射，以实现直接存取Java对象！ 为什么要使用Hibernate？既然Hibernate是关于Java对象和关系数据库之间的联系的话，也就是我们MVC中的数据持久层-&gt;在编写程序中的DAO层… 首先，我们来回顾一下我们在DAO层写程序的历程吧： 在DAO层操作XML，将数据封装到XML文件上，读写XML文件数据实现CRUD在DAO层使用原生JDBC连接数据库，实现CRUD嫌弃JDBC的ConnectionStatementResultSet等对象太繁琐，使用对原生JDBC的封装组件–&gt;DbUtils组件我们来看看使用DbUtils之后，程序的代码是怎么样的： public class CategoryDAOImpl implements zhongfucheng.dao.CategoryDao { @Override public void addCategory(Category category) { QueryRunner queryRunner = new QueryRunner(Utils2DB.getDataSource()); String sql = &quot;INSERT INTO category (id, name, description) VALUES(?,?,?)&quot;; try { queryRunner.update(sql, new Object[]{category.getId(), category.getName(), category.getDescription()}); } catch (SQLException e) { throw new RuntimeException(e); } } @Override public Category findCategory(String id) { QueryRunner queryRunner = new QueryRunner(Utils2DB.getDataSource()); String sql = &quot;SELECT * FROM category WHERE id=?&quot;; try { Category category = (Category) queryRunner.query(sql, id, new BeanHandler(Category.class)); return category; } catch (SQLException e) { throw new RuntimeException(e); } } @Override public List&lt;Category&gt; getAllCategory() { QueryRunner queryRunner = new QueryRunner(Utils2DB.getDataSource()); String sql = &quot;SELECT * FROM category&quot;; try { List&lt;Category&gt; categories = (List&lt;Category&gt;) queryRunner.query(sql, new BeanListHandler(Category.class)); return categories; } catch (SQLException e) { throw new RuntimeException(e); } } }其实使用DbUtils时，DAO层中的代码编写是很有规律的。 当插入数据的时候，就将JavaBean对象拆分，拼装成SQL语句当查询数据的时候，用SQL把数据库表中的列组合，拼装成JavaBean对象也就是说：javaBean对象和数据表中的列存在映射关系!如果程序能够自动生成SQL语句就好了….那么Hibernate就实现了这个功能！ 简单来说：我们使用Hibernate框架就不用我们写很多繁琐的SQL语句，从而简化我们的开发！ ORM概述在介绍Hibernate的时候，说了Hibernate是一种ORM的框架。那什么是ORM呢？ORM是一种思想 O代表的是ObjcetR代表的是RelativeM代表的是MappingORM-&gt;对象关系映射….ORM关注是对象与数据库中的列的关系 Hibernate快速入门学习一个框架无非就是三个步骤： 引入jar开发包配置相关的XML文件熟悉API引入相关jar包我们使用的是Hibernate3.6的版本 hibernate3.jar核心 + required 必须引入的(6个) + jpa 目录 + 数据库驱动包 编写对象和对象映射编写一个User对象-&gt;User.java public class User { private int id; private String username; private String password; private String cellphone; //各种setter和getter }编写对象映射-&gt;User.hbm.xml。一般它和JavaBean对象放在同一目录下 我们是不知道该XML是怎么写的，可以搜索一下Hibernate文件夹中后缀为.hbm.xml。看看它们是怎么写的。然后复制一份过来 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE hibernate-mapping PUBLIC &quot;-//Hibernate/Hibernate Mapping DTD 3.0//EN&quot; &quot;http://www.hibernate.org/dtd/hibernate-mapping-3.0.dtd&quot;&gt; &lt;!-- This mapping demonstrates content-based discrimination for the table-per-hierarchy mapping strategy, using a formula discriminator. --&gt; &lt;hibernate-mapping package=&quot;org.hibernate.test.array&quot;&gt; &lt;class name=&quot;A&quot; lazy=&quot;true&quot; table=&quot;aaa&quot;&gt; &lt;id name=&quot;id&quot;&gt; &lt;generator class=&quot;native&quot;/&gt; &lt;/id&gt; &lt;key column=&quot;a_id&quot;/&gt; &lt;list-index column=&quot;idx&quot;/&gt; &lt;one-to-many class=&quot;B&quot;/&gt; &lt;/class&gt; &lt;class name=&quot;B&quot; lazy=&quot;true&quot; table=&quot;bbb&quot;&gt; &lt;id name=&quot;id&quot;&gt; &lt;generator class=&quot;native&quot;/&gt; &lt;/id&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt;在上面的模板上修改～下面会具体讲解这个配置文件! &lt;!--在domain包下--&gt; &lt;hibernate-mapping package=&quot;zhongfucheng.domain&quot;&gt; &lt;!--类名为User，表名也为User--&gt; &lt;class name=&quot;User&quot; table=&quot;user&quot;&gt; &lt;!--主键映射，属性名为id，列名也为id--&gt; &lt;id name=&quot;id&quot; column=&quot;id&quot;&gt; &lt;!--根据底层数据库主键自动增长--&gt; &lt;generator class=&quot;native&quot;/&gt; &lt;/id&gt; &lt;!--非主键映射，属性和列名一一对应--&gt; &lt;property name=&quot;username&quot; column=&quot;username&quot;/&gt; &lt;property name=&quot;cellphone&quot; column=&quot;cellphone&quot;/&gt; &lt;property name=&quot;password&quot; column=&quot;password&quot;/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt;如果使用Intellij Idea生成的Hibernate可以指定生成出主配置文件hibernate.cfg.xml，它是要放在src目录下的 如果不是自动生成的，我们可以在Hibernate的hibernate-distribution-3.6.0.Final\project\etc这个目录下可以找到 它长得这个样子： &lt;?xml version=&apos;1.0&apos; encoding=&apos;utf-8&apos;?&gt; &lt;!DOCTYPE hibernate-configuration PUBLIC &quot;-//Hibernate/Hibernate Configuration DTD//EN&quot; &quot;http://www.hibernate.org/dtd/hibernate-configuration-3.0.dtd&quot;&gt; &lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name=&quot;connection.url.&quot;/&gt; &lt;property name=&quot;connection.driver_class&quot;/&gt; &lt;property name=&quot;connection.username&quot;/&gt; &lt;property name=&quot;connection.password&quot;/&gt; &lt;!-- DB schema will be updated if needed --&gt; &lt;!-- &lt;property name=&quot;hbm2ddl.auto&quot;&gt;update&lt;/property&gt; --&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt;通过上面的模板进行修改，后面会有对该配置文件进行讲解！ &lt;hibernate-configuration&gt; &lt;!-- 通常，一个session-factory节点代表一个数据库 --&gt; &lt;session-factory&gt; &lt;!-- 1\. 数据库连接配置 --&gt; &lt;property name=&quot;hibernate.connection.driver_class&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt; &lt;property name=&quot;hibernate.connection.url&quot;&gt;jdbc:mysql:///zhongfucheng&lt;/property&gt; &lt;property name=&quot;hibernate.connection.username&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;hibernate.connection.password&quot;&gt;root&lt;/property&gt; &lt;!-- 数据库方法配置， hibernate在运行的时候，会根据不同的方言生成符合当前数据库语法的sql --&gt; &lt;property name=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.MySQL5Dialect&lt;/property&gt; &lt;!-- 2\. 其他相关配置 --&gt; &lt;!-- 2.1 显示hibernate在运行时候执行的sql语句 --&gt; &lt;property name=&quot;hibernate.show_sql&quot;&gt;true&lt;/property&gt; &lt;!-- 2.2 格式化sql --&gt; &lt;property name=&quot;hibernate.format_sql&quot;&gt;true&lt;/property&gt; &lt;!-- 2.3 自动建表 --&gt; &lt;property name=&quot;hibernate.hbm2ddl.auto&quot;&gt;create&lt;/property&gt; &lt;!--3\. 加载所有映射--&gt; &lt;mapping resource=&quot;zhongfucheng/domain/User.hbm.xml&quot;/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt;测试package zhongfucheng.domain; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import org.hibernate.classic.Session; /** * Created by ozc on 2017/5/6. */ public class App { public static void main(String[] args) { //创建对象 User user = new User(); user.setPassword(&quot;123&quot;); user.setCellphone(&quot;122222&quot;); user.setUsername(&quot;nihao&quot;); //获取加载配置管理类 Configuration configuration = new Configuration(); //不给参数就默认加载hibernate.cfg.xml文件， configuration.configure(); //创建Session工厂对象 SessionFactory factory = configuration.buildSessionFactory(); //得到Session对象 Session session = factory.openSession(); //使用Hibernate操作数据库，都要开启事务,得到事务对象 Transaction transaction = session.getTransaction(); //开启事务 transaction.begin(); //把对象添加到数据库中 session.save(user); //提交事务 transaction.commit(); //关闭Session session.close(); } }值得注意的是：JavaBean的主键类型只能是int类型，因为在映射关系中配置是自动增长的，String类型是不能自动增长的。如果是你设置了String类型，又使用了自动增长，那么就会报出下面的错误！ Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table &apos;zhongfucheng.user&apos; does执行完程序后，Hibernate就为我们创建对应的表，并把数据存进了数据库了 我们看看快速入门案例的代码用到了什么对象吧，然后一个一个讲解 public static void main(String[] args) { //创建对象 User user = new User(); user.setPassword(&quot;123&quot;); user.setCellphone(&quot;122222&quot;); user.setUsername(&quot;nihao&quot;); //获取加载配置管理类 Configuration configuration = new Configuration(); //不给参数就默认加载hibernate.cfg.xml文件， configuration.configure(); //创建Session工厂对象 SessionFactory factory = configuration.buildSessionFactory(); //得到Session对象 Session session = factory.openSession(); //使用Hibernate操作数据库，都要开启事务,得到事务对象 Transaction transaction = session.getTransaction(); //开启事务 transaction.begin(); //把对象添加到数据库中 session.save(user); //提交事务 transaction.commit(); //关闭Session session.close(); }相关类Configuration配置管理类：主要管理配置文件的一个类 它拥有一个子类AnnotationConfiguration，也就是说：我们可以使用注解来代替XML配置文件来配置相对应的信息 configure方法configure()方法用于加载配置文件 加载主配置文件的方法 如果指定参数，那么加载参数的路径配置文件如果不指定参数，默认加载src/目录下的hibernate.cfg.xml buildSessionFactory方法buildSessionFactory()用于创建Session工厂 SessionFactorySessionFactory–&gt;Session的工厂，也可以说代表了hibernate.cfg.xml这个文件…hibernate.cfg.xml的就有这么一个节点 openSession方法创建一个Session对象 getCurrentSession方法创建Session对象或取出Session对象 SessionSession是Hibernate最重要的对象，Session维护了一个连接（Connection），只要使用Hibernate操作数据库，都需要用到Session对象 通常我们在DAO层中都会有以下的方法，Session也为我们提供了对应的方法来实现！ public interface IEmployeeDao { void save(Employee emp); void update(Employee emp); Employee findById(Serializable id); List&lt;Employee&gt; getAll(); List&lt;Employee&gt; getAll(String employeeName); List&lt;Employee&gt; getAll(int index, int count); void delete(Serializable id); }更新操作 我们在快速入门中使用到了save(Objcet o)方法，调用了这个方法就把对象保存在数据库之中了。Session对象还提供着其他的方法来进行对数据库的更新 session.save(obj); 【保存一个对象】session.update(obj); 【更新一个对象】session.saveOrUpdate(obj); 【保存或者更新的方法】 没有设置主键，执行保存；有设置主键，执行更新操作;如果设置主键不存在报错！我们来使用一下update()方法吧….既然是更新操作了，那么肯定需要设置主键的，不设置主键，数据库怎么知道你要更新什么。将id为1的记录修改成如下： user.setId(1); user.setPassword(&quot;qwer&quot;); user.setCellphone(&quot;1111&quot;); user.setUsername(&quot;zhongfucheng&quot;);主键查询 通过主键来查询数据库的记录，从而返回一个JavaBean对象 session.get(javaBean.class, int id); 【传入对应的class和id就可以查询】session.load(javaBean.class, int id); 【支持懒加载】User重写toString()来看一下效果： User user1 = (User) session.get(User.class, 1); System.out.println(user1);HQL查询 HQL:hibernate query language 即hibernate提供的面向对象的查询语言 查询的是对象以及对象的属性【它查询的是对象以及属性，因此是区分大小写的！】。 SQL：Struct query language 结构化查询语言查询的是表以及列【不区分大小写】HQL是面向对象的查询语言，可以用来查询全部的数据！ Query query = session.createQuery(&quot;FROM User&quot;); List list = query.list(); System.out.println(list);当然啦，它也可以传递参数进去查询 Query query = session.createQuery(&quot;FROM User WHERE id=?&quot;); //这里的？号是从0开始的，并不像JDBC从1开始的！ query.setParameter(0, user.getId()); List list = query.list(); System.out.println(list);QBC查询 QBC查询: query by criteria 完全面向对象的查询 从上面的HQL查询，我们就可以发现：HQL查询是需要SQL的基础的，因为还是要写少部分的SQL代码….QBC查询就是完全的面向对象查询…但是呢，我们用得比较少 我们来看一下怎么使用吧： //创建关于user对象的criteria对象 Criteria criteria = session.createCriteria(User.class); //添加条件 criteria.add(Restrictions.eq(&quot;id&quot;, 1)); //查询全部数据 List list = criteria.list(); System.out.println(list);本地SQL查询 有的时候，如果SQL是非常复杂的，我们不能靠HQL查询来实现功能的话，我们就需要使用原生的SQL来进行复杂查询了！ 但是呢，它有一个缺陷：它是不能跨平台的…因此我们在主配置文件中已经配置了数据库的“方言“了。 我们来简单使用一下把： //将所有的记录封装成User对象存进List集合中 SQLQuery sqlQuery = session.createSQLQuery(&quot;SELECT * FROM user&quot;).addEntity(User.class); List list = sqlQuery.list(); System.out.println(list);beginTransaction方法 开启事务，返回的是一个事务对象….Hibernate规定所有的数据库操作都必须在事务环境下进行，否则报错！ Hibernate注解开发 在Hibernate中我们一般都会使用注解，这样可以帮助我们大大简化hbm映射文件的配置。下面我就来为大家详细介绍。 PO类注解配置 首先肯定是搭建好Hibernate的开发环境啦，我在此也不过多赘述，读者自行实践。接着在src目录下创建一个cn.itheima.domain包，并在该包下创建一个Book实体类，由于Book实体类中写有注解配置，所以就不用编写那个映射配置文件啦！ @Entity // 定义了一个实体 @Table(name=&quot;t_book&quot;,catalog=&quot;hibernateTest&quot;) public class Book { @Id // 这表示一个主键 // @GeneratedValue 相当于native主键生成策略 @GeneratedValue(strategy=GenerationType.IDENTITY) // 相当于identity主键生成策略 private Integer id; // 主键 @Column(name=&quot;c_name&quot;, length=30, nullable=true) private String name; @Temporal(TemporalType.TIMESTAMP) // 是用来定义日期类型 private Date publicationDate; // 出版日期 @Type(type=&quot;double&quot;) // 允许你去指定Hibernate里面的一些类型 private Double price; // 价格，如果没有添加注解，也会自动的生成在表中 public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Date getPublicationDate() { return publicationDate; } public void setPublicationDate(Date publicationDate) { this.publicationDate = publicationDate; } public Double getPrice() { return price; } public void setPrice(Double price) { this.price = price; }} 下面我就来详细说一下Book实体类中的注解。 @Entity：声明一个实体。 @Table：来描述类与表之间的对应关系。 @Entity // 定义了一个实体 @Table(name=&quot;t_book&quot;,catalog=&quot;hibernateTest&quot;) public class Book { ...... } @id：声明一个主键。 @GeneratedValue：用它来声明一个主键生成策略。默认情况是native主键生成策略。可以选择的主键生成策略有：AUTO、IDENTITY、SEQUENCE @Id // 这表示一个主键 // @GeneratedValue 相当于native主键生成策略 @GeneratedValue(strategy=GenerationType.IDENTITY) // 相当于identity主键生成策略 private Integer id; // 主键 @Column：定义列。 @Column(name=&quot;c_name&quot;, length=30, nullable=true) private String name;注意：对于PO类中所有属性，如果你不写注解，默认情况下也会在表中生成对应的列，列的名称就是属性的名称，列的类型也即属性的类型。 @Temporal：声明日期类型。 @Temporal(TemporalType.TIMESTAMP) // 是用来定义日期类型 private Date publicationDate; // 出版日期日期类型可以选择的有： * TemporalType.DATA：只有年月日。 * TemporalType.TIME：只有小时分钟秒。 * TemporalType.TIMESTAMP：有年月日小时分钟秒。 @Type：可允许你去指定Hibernate里面的一些类型。 @Type(type=&quot;double&quot;) // 允许你去指定Hibernate里面的一些类型 private Double price; // 价格，如果没有添加注解，也会自动的生成在表中最后我们在src目录下创建一个cn.itheima.test包，在该包下编写一个HibernateAnnotationTest单元测试类，并在该类中编写一个用于测试PO类的注解开发的方法： public class HibernateAnnotationTest { // 测试PO的注解开发 @Test public void test1() { Session session = HibernateUtils.openSession(); session.beginTransaction(); Book b = new Book(); b.setName(&quot;情书&quot;); b.setPrice(56.78); b.setPublicationDate(new Date()); session.save(b); session.getTransaction().commit(); session.close(); } }现在来思考两个问题： 如果主键生成策略我们想使用UUID类型呢？如何设定类的属性不在表中映射？这两个问题我们一起解决。废话不多说，直接上例子。在cn.itheima.domain包下再编写一个Person实体类，同样使用注解配置。 @Entity @Table(name=&quot;t_person&quot;, catalog=&quot;hibernateTest&quot;) public class Person { // 生成UUID的主键生成策略 @Id @GenericGenerator(name=&quot;myuuid&quot;, strategy=&quot;uuid&quot;) // 声明一种主键生成策略(uuid) @GeneratedValue(generator=&quot;myuuid&quot;) // 引用uuid主键生成策略 private String id; @Type(type=&quot;string&quot;) // 允许你去指定Hibernate里面的一些类型 private String name; @Transient private String msg; // 现在这个属性不想生成在表中 public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } }最后在HibernateAnnotationTest单元测试类中编写如下一个方法： public class HibernateAnnotationTest { // 测试uuid的主键生成策略及不生成表中映射 @Test public void test2() { Session session = HibernateUtils.openSession(); session.beginTransaction(); Person p = new Person(); p.setName(&quot;李四&quot;); p.setMsg(&quot;这是一个好人&quot;); session.save(p); session.getTransaction().commit(); session.close(); } }至此，两个问题就解决了。注意：对于我们以上讲解的关于属性配置的注解，我们也可以在其对应的getXxx方法去使用。 Hibernate关联映射——一对多（多对一）仍以客户(Customer)和订单(Order)为例来开始我的表演。在src目录下创建一个cn.itheima.oneToMany包，并在该包编写这两个实体类： 客户(Customer)类 // 客户 ---- 一的一方 @Entity @Table(name=&quot;t_customer&quot;) public class Customer { @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; // 主键 private String name; // 姓名 // 描述客户可以有多个订单 /* * targetEntity=&quot;...&quot;：相当于&lt;one-to-many &gt; */ @OneToMany(targetEntity=Order.class,mappedBy=&quot;c&quot;) private Set&lt;Order&gt; orders = new HashSet&lt;Order&gt;(); public Set&lt;Order&gt; getOrders() { return orders; } public void setOrders(Set&lt;Order&gt; orders) { this.orders = orders; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } } 订单(Order)类 // 订单 ---- 多的一方 @Entity @Table(name=&quot;t_order&quot;) public class Order { @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; private Double money; private String receiverInfo; // 收货地址 // 订单与客户关联 @ManyToOne(targetEntity=Customer.class) @JoinColumn(name=&quot;c_customer_id&quot;) // 指定外键列 private Customer c; // 描述订单属于某一个客户 public Customer getC() { return c; } public void setC(Customer c) { this.c = c; } public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public Double getMoney() { return money; } public void setMoney(Double money) { this.money = money; } public String getReceiverInfo() { return receiverInfo; } public void setReceiverInfo(String receiverInfo) { this.receiverInfo = receiverInfo; } }这儿用到了@OneToMany和@ManyToOne这两个注解。以上两个实体类编写好之后，可以很明显的看出我们不需要写它们对应的映射配置文件了，是不是很爽呢！接下来，我就要编写测试程序测试一下了。现在我的需求是保存客户时，顺便保存订单，对于这种情况我们需要在Customer类中配置cascade操作，即配置cascade=”save-update”，配置的方式有两种，下面我细细说来： 第一种方式，可以使用JPA提供的注解。那么@OneToMany注解就应修改为： @OneToMany(targetEntity=Order.class,mappedBy=&quot;c&quot;,cascade=CascadeType.ALL) private Set&lt;Order&gt; orders = new HashSet&lt;Order&gt;();第二种方式，可以使用Hibernate提供的注解。那么@OneToMany注解就应修改为： @OneToMany(targetEntity=Order.class,mappedBy=&quot;c&quot;) @Cascade(CascadeType.SAVE_UPDATE) private Set&lt;Order&gt; orders = new HashSet&lt;Order&gt;();两种方式都可以，口味任君选择，不过我倾向于第二种方式。接下来在HibernateAnnotationTest单元测试类中编写如下方法进行测试： public class HibernateAnnotationTest { // 测试one-to-many注解操作(保存客户时级联保存订单) @Test public void test3() { Session session = HibernateUtils.openSession(); session.beginTransaction(); // 1.创建一个客户 Customer c = new Customer(); c.setName(&quot;叶子&quot;); // 2.创建两个订单 Order o1 = new Order(); o1.setMoney(1000d); o1.setReceiverInfo(&quot;武汉&quot;); Order o2 = new Order(); o2.setMoney(2000d); o2.setReceiverInfo(&quot;天门&quot;); // 3.建立关系 c.getOrders().add(o1); c.getOrders().add(o2); // 4.保存客户，并级联保存订单 session.save(c); session.getTransaction().commit(); session.close(); } }这时运行以上方法，会发现虽然客户表的那条记录插进去了，但是订单表就变成这个鬼样了： 订单表中没有关联客户的id，这是为什么呢？原因是我们在Customer类中配置了mappedBy=”c”，它代表的是外键的维护由Order方来维护，而Customer不维护，这时你在保存客户时，级联保存订单，是可以的，但是不能维护外键，所以，我们必须在代码中添加订单与客户之间的关系。所以须将test3方法修改为： public class HibernateAnnotationTest { // 测试one-to-many注解操作(保存客户时级联保存订单) @Test public void test3() { Session session = HibernateUtils.openSession(); session.beginTransaction(); // 1.创建一个客户 Customer c = new Customer(); c.setName(&quot;叶子&quot;); // 2.创建两个订单 Order o1 = new Order(); o1.setMoney(1000d); o1.setReceiverInfo(&quot;武汉&quot;); Order o2 = new Order(); o2.setMoney(2000d); o2.setReceiverInfo(&quot;天门&quot;); // 3.建立关系 // 原因：是为了维护外键，不然的话，外键就不能正确的生成！！！ o1.setC(c); o2.setC(c); // 原因：是为了进行级联操作 c.getOrders().add(o1); c.getOrders().add(o2); // 4.保存客户，并级联保存订单 session.save(c); session.getTransaction().commit(); session.close(); } }这时再测试，就没有任何问题啦！ 扩展Hibernate注解@Cascade中的DELETE_ORPHAN已经过时了，如下： 可使用下面方案来替换过时方案： Hibernate关联映射——多对多以学生与老师为例开始我的表演，我是使用注解完成这种多对多的配置。使用@ManyToMany注解来配置多对多，只需要在一端配置中间表，另一端使用mappedBy表示放置外键的维护权。在src目录下创建一个cn.itheima.manyToMany包，并在该包编写这两个实体类： 学生类 @Entity @Table(name=&quot;t_student&quot;) public class Student { @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; private String name; @ManyToMany(targetEntity=Teacher.class) // @JoinTable：使用@JoinTable来描述中间表，并描述中间表中外键与Student、Teacher的映射关系 // joinColumns：它是用来描述Student与中间表的映射关系 // inverseJoinColumns：它是用来描述Teacher与中间表的映射关系 @JoinTable(name=&quot;s_t&quot;, joinColumns={@JoinColumn(name=&quot;c_student_id&quot;,referencedColumnName=&quot;id&quot;)}, inverseJoinColumns={@JoinColumn(name=&quot;c_teacher_id&quot;)}) private Set&lt;Teacher&gt; teachers = new HashSet&lt;Teacher&gt;(); public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Set&lt;Teacher&gt; getTeachers() { return teachers; } public void setTeachers(Set&lt;Teacher&gt; teachers) { this.teachers = teachers; } }老师类 @Entity @Table(name=&quot;t_teacher&quot;) public class Teacher { @Id @GeneratedValue(strategy=GenerationType.IDENTITY) private Integer id; private String name; @ManyToMany(targetEntity=Student.class, mappedBy=&quot;teachers&quot;) // 代表由对方来维护外键 private Set&lt;Student&gt; students = new HashSet&lt;Student&gt;(); public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Set&lt;Student&gt; getStudents() { return students; } public void setStudents(Set&lt;Student&gt; students) { this.students = students; } }接下来，我就要编写测试程序测试一下了。 从上面可看出我们将外键的维护权利交由Student类来维护，现在我们演示保存学生时，将老师也级联保存，对于这种情况我们需要在Student类中配置cascade操作，即配置cascade=”save-update”，如下： @JoinTable(name=&quot;s_t&quot;, joinColumns={@JoinColumn(name=&quot;c_student_id&quot;,referencedColumnName=&quot;id&quot;)}, inverseJoinColumns={@JoinColumn(name=&quot;c_teacher_id&quot;)}) @Cascade(CascadeType.SAVE_UPDATE) private Set&lt;Teacher&gt; teachers = new HashSet&lt;Teacher&gt;();接下来在HibernateAnnotationTest单元测试类中编写如下方法进行测试： public class HibernateAnnotationTest { // 测试多对多级联保存(保存学生时同时保存老师) @Test public void test4() { Session session = HibernateUtils.openSession(); session.beginTransaction(); // 1.创建两个老师 Teacher t1 = new Teacher(); t1.setName(&quot;Tom&quot;); Teacher t2 = new Teacher(); t2.setName(&quot;Fox&quot;); // 2.创建两个学生 Student s1 = new Student(); s1.setName(&quot;张丹&quot;); Student s2 = new Student(); s2.setName(&quot;叶紫&quot;); // 3.学生关联老师 s1.getTeachers().add(t1); s1.getTeachers().add(t2); s2.getTeachers().add(t1); s2.getTeachers().add(t2); // 保存学生同时保存老师 session.save(s1); session.save(s2); session.getTransaction().commit(); session.close(); } }运行以上方法，一切正常。接着我们测试级联删除操作。见下图：这里写图片描述 可在HibernateAnnotationTest单元测试类中编写如下方法进行测试： public class HibernateAnnotationTest { // 测试多对多级联删除(前提是建立了双向的级联) @Test public void test5() { Session session = HibernateUtils.openSession(); session.beginTransaction(); Student s = session.get(Student.class, 1); session.delete(s); session.getTransaction().commit(); session.close(); } }参考文章https://segmentfault.com/a/1190000009707894 https://www.cnblogs.com/hysum/p/7100874.html http://c.biancheng.net/view/939.html https://www.runoob.com/ https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>hibernate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界12：从手动编译打包到项目构建工具Maven]]></title>
    <url>%2F2019%2F10%2F24%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C12%EF%BC%9A%E4%BB%8E%E6%89%8B%E5%8A%A8%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85%E5%88%B0%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7Maven%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） maven简介1.1 Maven是什么Maven是一个项目管理和综合工具。 Maven提供了开发人员构建一个完整的生命周期框架。开发者团队可以自动完成项目的基础工具建设， Maven使用标准的目录结构和默认构建生命周期。 在多个开发者团队环境时， Maven可以设置按标准在非常短的时间里完成配置工作。 由于大部分项目的设置都很简单， 并且可重复使用， Maven让开发人员的工作更轻松， 同时创建报表， 检查， 构建和测试自动化设置。 用过GitHub的同学看到这里应该感觉似曾相识，对，Maven和git的作用很相似，都是为了方便项目的创建与管理。 概括地说， Maven简化和标准化项目建设过程。 处理编译， 分配， 文档， 团队协作和其他任务的无缝连接。 Maven增加可重用性并负责建立相关的任务。 1.2 Maven发展史Maven设计之初， 是为了简化Jakarta Turbine项目的建设。 在几个项目， 每个项目包含了不同的Ant构建文件。 JAR检查到CVS。 Apache组织开发Maven可以建立多个项目， 发布项目信息， 项目部署， 在几个项目中JAR文件提供团队合作和帮助。 Maven的经历了Maven-&gt; Maven2 -&gt; Maven3的发展。 1.3 为什么要用MavenMaven之前我们经常使用Ant来进行Java项目的构建， 然后Ant仅是一个构建工具， 它并未对项目的中的工程依赖以及项目本身进行管理， 并且Ant作为构建工具未能消除软件构建的重复性， 因为不同的项目需要编写对应的Ant任务。 Maven作为后来者， 继承了Ant的项目构建功能， 并且提供了依赖关系， 项目管理的功能， 因此它是一个项目管理和综合工具， 其核心的依赖管理， 项目信息管理， 中央仓库， 约定大于配置的核心功能使得Maven成为当前Java项目构建和管理工具的标准选择。 学习Maven的理由是非常多： 主流IDE（Eclipse,IDEA,Netbean） 够内置了Maven SpringFramework已经不再提供jar的下载， 直接通过Maven进行依赖下载。 在github， 开源社区几乎所有流行的Java项目都是通过Maven进行构建和管理的。 Maven 新手入门Maven概念Maven作为一个构建工具，不仅能帮我们自动化构建，还能够抽象构建过程，提供构建任务实现;它跨平台，对外提供了一致的操作接口，这一切足以使它成为优秀的、流行的构建工具。 Maven不仅是构建工具，还是一个依赖管理工具和项目管理工具，它提供了中央仓库，能帮我自动下载构件。 maven的安装一：因为本人是window系统，所以这里只介绍window下如何安装，在安装Maven之前，先确认已经安装了JDK. 二：接着去Maven官网下载界面下载想要的版本解压到你想要的目录就行 三：最后设置一下环境变量，将Maven安装配置到操作系统环境中，主要就是配置M2_HOME 和PATH两项，如图 都搞定后，验证一下，打开doc输入 mvn -v如何得到下面信息就说明配置成功了 maven目录 bin目录：该目录包含了mvn运行的脚本，这些脚本用来配置java命令，准备好classpath和相关的Java系统属性，然后执行Java命令。 boot目录:该目录只包含一个文件，该文件为plexus-classworlds-2.5.2.jar。plexus-classworlds是一个类加载器框架，相对于默认的java类加载器，它提供了更加丰富的语法以方便配置，Maven使用该框架加载自己的类库。 conf目录:该目录包含了一个非常重要的文件settings.xml。直接修改该文件，就能在机器上全局地定制Maven的行为，一般情况下，我们更偏向于复制该文件至/.m2/目录下（表示用户目录），然后修改该文件，在用户范围定制Maven的行为。 lib目录:该目录包含了所有Maven运行时需要的Java类库，Maven本身是分模块开发的，因此用户能看到诸如maven-core-3.0.jar、maven-model-3.0.jar之类的文件，此外这里还包含一些Maven用到的第三方依赖如commons-cli-1.2.jar、commons-lang-2.6.jar等等。 Maven常用命令说明mvn clean：表示运行清理操作（会默认把target文件夹中的数据清理）。 mvn clean compile：表示先运行清理之后运行编译，会将代码编译到target文件夹中。 mvn clean test：运行清理和测试。 mvn clean package：运行清理和打包。 mvn clean install：运行清理和安装，会将打好的包安装到本地仓库中，以便其他的项目可以调用。 mvn clean deploy：运行清理和发布（发布到私服上面）。上面的命令大部分都是连写的，大家也可以拆分分别执行，这是活的，看个人喜好以及使用需求，Eclipse Run as对maven项目会提供常用的命令。 Maven使用&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.tengj&lt;/groupId&gt; &lt;artifactId&gt;springBootDemo1&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springBootDemo1&lt;/name&gt; &lt;/project&gt;代码的第一行是XML头，指定了该xml文档的版本和编码方式。project是所有pom.xml的根元素，它还声明了一些POM相关的命名空间及xsd元素。根元素下的第一个子元素modelVersion指定了当前的POM模型的版本，对于Maven3来说，它只能是4.0.0代码中最重要是包含了groupId,artifactId和version了。这三个元素定义了一个项目基本的坐标，在Maven的世界，任何的jar、pom或者jar都是以基于这些基本的坐标进行区分的。 groupId定义了项目属于哪个组，随意命名，比如谷歌公司的myapp项目，就取名为 com.google.myapp artifactId定义了当前Maven项目在组中唯一的ID,比如定义hello-world。 version指定了项目当前的版本0.0.1-SNAPSHOT,SNAPSHOT意为快照，说明该项目还处于开发中，是不稳定的。 name元素生命了一个对于用户更为友好的项目名称，虽然这不是必须的，但还是推荐为每个POM声明name,以方便信息交流 依赖的配置&lt;project&gt; ... &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;实际项目&lt;/groupId&gt; &lt;artifactId&gt;模块&lt;/artifactId&gt; &lt;version&gt;版本&lt;/version&gt; &lt;type&gt;依赖类型&lt;/type&gt; &lt;scope&gt;依赖范围&lt;/scope&gt; &lt;optional&gt;依赖是否可选&lt;/optional&gt; &lt;!—主要用于排除传递性依赖--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;…&lt;/groupId&gt; &lt;artifactId&gt;…&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependencies&gt; ... &lt;/project&gt;根元素project下的dependencies可以包含一个或者多个dependency元素，以声明一个或者多个项目依赖。每个依赖可以包含的元素有： grounpId、artifactId和version:以来的基本坐标，对于任何一个依赖来说，基本坐标是最重要的，Maven根据坐标才能找到需要的依赖。 type:依赖的类型，对于项目坐标定义的packaging。大部分情况下，该元素不必声明，其默认值为jar scope:依赖的范围 optional:标记依赖是否可选 exclusions:用来排除传递性依赖 依赖范围依赖范围就是用来控制依赖和三种classpath(编译classpath，测试classpath、运行classpath)的关系，Maven有如下几种依赖范围： compile:编译依赖范围。如果没有指定，就会默认使用该依赖范围。使用此依赖范围的Maven依赖，对于编译、测试、运行三种classpath都有效。典型的例子是spring-code,在编译、测试和运行的时候都需要使用该依赖。 test: 测试依赖范围。使用次依赖范围的Maven依赖，只对于测试classpath有效，在编译主代码或者运行项目的使用时将无法使用此依赖。典型的例子是Jnuit,它只有在编译测试代码及运行测试的时候才需要。 provided:已提供依赖范围。使用此依赖范围的Maven依赖，对于编译和测试classpath有效，但在运行时候无效。典型的例子是servlet-api,编译和测试项目的时候需要该依赖，但在运行项目的时候，由于容器以及提供，就不需要Maven重复地引入一遍。 runtime:运行时依赖范围。使用此依赖范围的Maven依赖，对于测试和运行classpath有效，但在编译主代码时无效。典型的例子是JDBC驱动实现，项目主代码的编译只需要JDK提供的JDBC接口，只有在执行测试或者运行项目的时候才需要实现上述接口的具体JDBC驱动。 system:系统依赖范围。该依赖与三种classpath的关系，和provided依赖范围完全一致，但是，使用system范围的依赖时必须通过systemPath元素显示地指定依赖文件的路径。由于此类依赖不是通过Maven仓库解析的，而且往往与本机系统绑定，可能构成构建的不可移植，因此应该谨慎使用。systemPath元素可以引用环境变量，如： &lt;dependency&gt; &lt;groupId&gt;javax.sql&lt;/groupId&gt; &lt;artifactId&gt;jdbc-stdext&lt;/artifactId&gt; &lt;Version&gt;2.0&lt;/Version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${java.home}/lib/rt.jar&lt;/systemPath&gt; &lt;/dependency&gt; import:导入依赖范围。该依赖范围不会对三种classpath产生实际的影响。上述除import以外的各种依赖范围与三种classpath的关系如下: 传递性依赖比如一个account-email项目为例，account-email有一个compile范围的spring-code依赖，spring-code有一个compile范围的commons-logging依赖，那么commons-logging就会成为account-email的compile的范围依赖，commons-logging是account-email的一个传递性依赖 有了传递性依赖机制，在使用Spring Framework的时候就不用去考虑它依赖了什么，也不用担心引入多余的依赖。Maven会解析各个直接依赖的POM，将那些必要的间接依赖，以传递性依赖的形式引入到当前的项目中。 依赖范围假设A依赖于B,B依赖于C，我们说A对于B是第一直接依赖，B对于C是第二直接依赖，A对于C是传递性依赖。第一直接依赖和第二直接依赖的范围决定了传递性依赖的范围，如下图所示，最左边一行表示第一直接依赖范围，最上面一行表示第二直接依赖范围，中间的交叉单元格则表示传递依赖范围。 从上图中，我们可以发现这样的规律： 当第二直接依赖的范围是compile的时候，传递性依赖的范围与第一直接依赖的范围一致； 当第二直接依赖的范围是test的时候，依赖不会得以传递； 当第二直接依赖的范围是provided的时候，只传递第一直接依赖范围也为provided的依赖，切传递依赖的范围同样为provided; 当第二直接依赖的范围是runtime的时候，传递性依赖的范围与第一直接依赖的范围一致，但compile列外，此时传递性依赖范围为runtime. Maven和Gradle的比较Java生态体系中有三大构建工具：Ant、Maven和Gradle。其中，Ant是由Apache软件基金会维护；Maven这个单词来自于意第绪语（犹太语），意为知识的积累，最初在Jakata Turbine项目中用来简化构建过程；Gradle是一个基于Apache Ant和Apache Maven概念的项目自动化构建开源工具，它使用一种基于Groovy的特定领域语言(DSL)来声明项目设置，抛弃了基于XML的各种繁琐配置。 经过几年的发展，Ant几乎销声匿迹，而Maven由于较为不灵活的配置也渐渐被遗忘，而由于Gradle是基于Ant和Maven的一个优化版本，变得如日中天。 Maven的主要功能主要分为依赖管理系统、多模块构建、一致的项目结构、一致的构建模型和插件机制。这里通过这五个方面介绍两者的不同： 依赖管理系统在Maven的管理体系中，用GroupID、ArtifactID和Version组成的Coordination唯一标识一个依赖项。任何基于Maven构建的项目自身也必须定义这三项属性，生成的包可以是Jar包，也可以是War包或Ear包。 一个典型的引用如下： 123456789101112131415&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-data-jpa &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-thymeleaf &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; spring-boot-starter-test &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 这里 GroupID类似于C#中的namespace或者Java中的package，而ArtifactID相当于Class，Version相当于不同版本，如果Version忽略掉，将选择最新的版本链接。 同时，存储这些组件的仓库有远程仓库和本地仓库之分，远程仓库可以是使用世界公用的central仓库，也可以使用Apache Nexus自建的私有仓库；本地仓库则在本地计算机上。通过Maven安装目录下的settings.xml文件可以配置本地仓库的路径，以及采用的远程仓库地址。Gradle在设计时沿用了Maven这种依赖管理体系，同时也引入了改进，让依赖变得更加简洁： dependencies { // This dependency is exported to consumers, that is to say found on their compile classpath. api &apos;org.apache.commons:commons-math3:3.6.1&apos; // This dependency is used internally, and not exposed to consumers on their own compile classpath. implementation &apos;com.google.guava:guava:23.0&apos; // Use JUnit test framework testImplementation &apos;junit:junit:4.12&apos; compile &apos;org.hibernate:hibernate-core:3.6.7.Final&apos; testCompile ‘junit:junit:4.+&apos; }另外，Maven和Gradle对依赖项的审视也有所不同。在Maven中，一个依赖项有6种scope，分别是compile、provided、runtime、test、system、import。其中compile为默认。而gradle将其简化为4种，compile、runtime、testCompile、testRuntime。如上述代码“testCompile ‘junit:junit:4.+’”，在Gradle中支持动态的版本依赖，在版本号后面使用+号可以实现动态的版本管理。在解决依赖冲突方面Gradle的实现机制更加明确，两者都采用的是传递性依赖，而如果多个依赖项指向同一个依赖项的不同版本时可能会引起依赖冲突，Maven处理起来较为繁琐，而Gradle先天具有比较明确的策略。 多模块构建在面向服务的架构中，通常将一个项目分解为多个模块。在Maven中需要定义parent POM(Project Object Model)作为一组module的通用配置模型，在POM文件中可以使用标签来定义一组子模块。parent POM中的build配置以及依赖配置会自动继承给子module。 Gradle也支持多模块构建，在parent的build.gradle中可以使用allprojects和subprojects代码块分别定义应用于所有项目或子项目中的配置。对于子模块中的定义放置在settings.gradle文件中，每一个模块代表project的对象实例，在parent的build.gradle中通过allproject或subprojects对这些对象进行操作，相比Maven更显灵活。 allprojects { task nice &lt;&lt; { task -&gt; println &quot;I&apos;m $task.project.name&quot; } }执行命令gradle -q nice会依次打印出各模块的项目名称。 一致的项目结构Maven指定了一套项目目录结构作为标准的java项目结构，Gradle也沿用了这一标准的目录结构。如果在Gradle项目中使用了Maven项目结构的话，在Gradle中无需进行多余的配置，只需在文件中包括apply plugin:’java’，系统会自动识别source、resource、test source、test resource等相应资源。 同时，Gradle作为JVM上的构建工具，也支持Groovy、Scala等源代码的构建，同样功能Maven通过一些插件也能达到目的，但配置方面Gradle更灵活。 一致的构建模型为了解决Ant中对项目构建缺乏标准化的问题，Maven设置了标准的项目周期，构建周期：验证、初始化、生成原始数据、处理原始数据、生成资源、处理资源、编译、处理类、生成测试原始数据、处理测试原始数据、生成测试资源、处理测试资源、测试编译、处理测试类、测试、预定义包、生成包文件、预集成测试、集成测试、后集成测试、核实、安装、部署。但这种构建周期也是Maven应用的劣势。因为Maven将项目的构建周期限制过严，无法在构建周期中添加新的阶段，只能将插件绑定到已有的阶段上。而Gradle在构建模型上非常灵活，可以创建一个task，并随时通过depends建立与已有task的依赖关系。 插件机制两者都采用了插件机制，Maven是基于XML进行配置，而在Gradle中更加灵活。 参考文章http://www.pianshen.com/article/4537698845https://www.jianshu.com/p/7248276d3bb5https://www.cnblogs.com/lykbk/p/erwerwerwerwerwerwe.htmlhttps://blog.csdn.net/u012131888/article/details/78209514https://blog.csdn.net/belvine/article/details/81073365https://blog.csdn.net/u012131888/article/details/78209514 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); ​ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界11：单元测试框架Junit]]></title>
    <url>%2F2019%2F10%2F23%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C11%EF%BC%9A%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6Junit%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 简介测试 在软件开发中是一个很重要的方面，良好的测试可以在很大程度决定一个应用的命运。软件测试中，主要有3大种类： 单元测试单元测试主要是用于测试程序模块，确保代码运行正确。单元测试是由开发者编写并进行运行测试。一般使用的测试框架是 JUnit 或者 TestNG。测试用例一般是针对方法 级别的测试。 集成测试集成测试用于检测系统是否能正常工作。集成测试也是由开发者共同进行测试，与单元测试专注测试个人代码组件不同的是，集成测试是系统进行跨组件测试。 功能性测试功能性测试是一种质量保证过程以及基于测试软件组件的规范下的由输入得到输出的一种黑盒测试。功能性测试通常由不同的测试团队进行测试，测试用例的编写要遵循组件规范，然后根据测试输入得到的实际输出与期望值进行对比，判断功能是否正确运行。 概述本文只对 单元测试 进行介绍，主要介绍如何在 Android Studio 下进行单元测试，单元测试使用的测试框架为 JUnit 好处可能目前仍有很大一部分开发者未使用 单元测试 对他们的代码进行测试，一方面可能是觉得没有必要，因为即使没有进行单元测试，程序照样运行得很好；另一方面，也许有些人也认同单元测试的好处，但是由于需要额外的学习成本，所以很多人也是没有时间或者说是没有耐心进行学习······这里我想说的是，如果大家去看下 github 上目前主流的开源框架，star 数比较多的项目，一般都有很详尽的测试用例。所以说，单元测试对于我们的项目开发，还是挺有好处的。至于单元测试的好处，我这里提及几点： 保证代码运行与我们预想的一样，代码正确性可以得到保证 程序运行出错时，有利于我们对错误进行查找（因为我们忽略我们测试通过的代码） 有利于提升代码架构设计（用于测试的用例应力求简单低耦合，因此编写代码的时候，开发者往往会为了对代码进行测试，将其他耦合的部分进行解耦处理）······ Junit单元测试本文实例讲述了java单元测试JUnit框架原理与用法。分享给大家供大家参考，具体如下： 1 简介JUnit是一个Java语言的单元测试框架，它由 Kent Beck 和 Erich Gamma 建立，逐渐成为 xUnit 家族中最为成功的一个。 JUnit有它自己的JUnit扩展生态圈，多数Java的开发环境都已经集成了JUnit作为单元测试的工具。在这里，一个单元可以是一个方法、类、包或者子系统。 因此，单元测试是指对代码中的最小可测试单元进行检查和验证，以便确保它们正常工作。例如，我们可以给予一定的输入测试输出是否是所希望得到的结果。在本篇博客中，作者将着重介绍 JUnit 4.X 版本的特性，这也是我们在日常开发中使用最多的版本。 2 特点JUnit提供了注释以及确定的测试方法； JUnit提供了断言用于测试预期的结果； JUnit测试优雅简洁不需要花费太多的时间； JUnit测试让大家可以更快地编写代码并且提高质量； JUnit测试可以组织成测试套件包含测试案例，甚至其他测试套件； Junit显示测试进度，如果测试是没有问题条形是绿色的，测试失败则会变成红色； JUnit测试可以自动运行，检查自己的结果，并提供即时反馈，没有必要通过测试结果报告来手动梳理。3 内容3.1 注解@Test ：该注释表示，用其附着的公共无效方法（即用public修饰的void类型的方法 ）可以作为一个测试用例； @Before ：该注释表示，用其附着的方法必须在类中的每个测试之前执行，以便执行测试某些必要的先决条件； @BeforeClass ：该注释表示，用其附着的静态方法必须执行一次并在类的所有测试之前，发生这种情况时一般是测试计算共享配置方法，如连接到数据库； @After ：该注释表示，用其附着的方法在执行每项测试后执行，如执行每一个测试后重置某些变量，删除临时变量等； @AfterClass ：该注释表示，当需要执行所有的测试在JUnit测试用例类后执行，AfterClass注解可以使用以清理建立方法，如断开数据库连接，注意：附有此批注（类似于BeforeClass）的方法必须定义为静态； @Ignore ：该注释表示，当想暂时禁用特定的测试执行可以使用忽略注释，每个被注解为@Ignore的方法将不被执行。 / * JUnit 注解示例 */ @Test public void testYeepay(){ Syetem.out.println(&quot;用@Test标示测试方法！&quot;); } @AfterClass public static void paylus(){ Syetem.out.println(&quot;用@AfterClass标示的方法在测试用例类执行完之后！&quot;); }3.2 断言在这里，作者将介绍一些断言方法，所有这些方法都来自 org.junit.Assert 类，其扩展了 java.lang.Object 类并为它们提供编写测试，以便检测故障。简而言之，我们就是通过断言方法来判断实际结果与我们预期的结果是否相同，如果相同，则测试成功，反之，则测试失败。 void assertEquals([String message], expected value, actual value) ：断言两个值相等，值的类型可以为int、short、long、byte、char 或者 java.lang.Object，其中第一个参数是一个可选的字符串消息； void assertTrue([String message], boolean condition) ：断言一个条件为真； void assertFalse([String message],boolean condition) ：断言一个条件为假； void assertNotNull([String message], java.lang.Object object) ：断言一个对象不为空(null)； void assertNull([String message], java.lang.Object object) ：断言一个对象为空(null)； void assertSame([String message], java.lang.Object expected, java.lang.Object actual) ：断言两个对象引用相同的对象； void assertNotSame([String message], java.lang.Object unexpected, java.lang.Object actual) ：断言两个对象不是引用同一个对象； void assertArrayEquals([String message], expectedArray, resultArray) ：断言预期数组和结果数组相等，数组的类型可以为int、long、short、char、byte 或者 java.lang.Object4 JUnit 3.X 和 JUnit 4.X 的区别4.1 JUnit 3.X（1）使用 JUnit 3.X 版本进行单元测试时，测试类必须要继承于 TestCase 父类；（2）测试方法需要遵循的原则： ① public的；② void的；③ 无方法参数；④方法名称必须以 test 开头； （3）不同的测试用例之间一定要保持完全的独立性，不能有任何的关联； （4）要掌握好测试方法的顺序，不能依赖于测试方法自己的执行顺序。 / * 用 JUnit 3.X 进行测试 */ import junit.framework.Assert; import junit.framework.TestCase; public class TestOperation extends TestCase { private Operation operation; public TestOperation(String name) { // 构造函数 super(name); } @Override public void setUp() throws Exception { // 在每个测试方法执行 [之前] 都会被调用，多用于初始化 System.out.println(&quot;欢迎使用Junit进行单元测试...&quot;); operation = new Operation(); } @Override public void tearDown() throws Exception { // 在每个测试方法执行 [之后] 都会被调用，多用于释放资源 System.out.println(&quot;Junit单元测试结束...&quot;); } public void testDivideByZero() { Throwable te = null; try { operation.divide(6, 0); Assert.fail(&quot;测试失败&quot;); //断言失败 } catch (Exception e) { e.printStackTrace(); te = e; } Assert.assertEquals(Exception.class, te.getClass()); Assert.assertEquals(&quot;除数不能为 0 &quot;, te.getMessage()); } }4.2 JUnit 4.X（1）使用 JUnit 4.X 版本进行单元测试时，不用测试类继承TestCase父类；（2）JUnit 4.X 版本，引用了注解的方式进行单元测试；（3）JUnit 4.X 版本我们常用的注解包括： @Before 注解：与JUnit 3.X 中的 setUp() 方法功能一样，在每个测试方法之前执行，多用于初始化； @After 注解：与 JUnit 3.X 中的 tearDown() 方法功能一样，在每个测试方法之后执行，多用于释放资源； @Test(timeout = xxx) 注解：设置当前测试方法在一定时间内运行完，否则返回错误； @Test(expected = Exception.class) 注解：设置被测试的方法是否有异常抛出。抛出异常类型为：Exception.class； 此外，我们可以通过阅读上面的第二部分“2 注解”了解更多的注解。 / * 用 JUnit 4.X 进行测试 */ import static org.junit.Assert.*; import org.junit.After; import org.junit.AfterClass; import org.junit.Before; import org.junit.BeforeClass; import org.junit.Test; public class TestOperation { private Operation operation; @BeforeClass public static void globalInit() { // 在所有方法执行之前执行 System.out.println(&quot;@BeforeClass标注的方法，在所有方法执行之前执行...&quot;); } @AfterClass public static void globalDestory() { // 在所有方法执行之后执行 System.out.println(&quot;@AfterClass标注的方法，在所有方法执行之后执行...&quot;); } @Before public void setUp() { // 在每个测试方法之前执行 System.out.println(&quot;@Before标注的方法，在每个测试方法之前执行...&quot;); operation = new Operation(); } @After public void tearDown() { // 在每个测试方法之后执行 System.out.println(&quot;@After标注的方法，在每个测试方法之后执行...&quot;); } @Test(timeout=600) public void testAdd() { // 设置限定测试方法的运行时间 如果超出则返回错误 System.out.println(&quot;测试 add 方法...&quot;); int result = operation.add(2, 3); assertEquals(5, result); } @Test public void testSubtract() { System.out.println(&quot;测试 subtract 方法...&quot;); int result = operation.subtract(1, 2); assertEquals(-1, result); } @Test public void testMultiply() { System.out.println(&quot;测试 multiply 方法...&quot;); int result = operation.multiply(2, 3); assertEquals(6, result); } @Test public void testDivide() { System.out.println(&quot;测试 divide 方法...&quot;); int result = 0; try { result = operation.divide(6, 2); } catch (Exception e) { fail(); } assertEquals(3, result); } @Test(expected = Exception.class) public void testDivideAgain() throws Exception { System.out.println(&quot;测试 divide 方法，除数为 0 的情况...&quot;); operation.divide(6, 0); fail(&quot;test Error&quot;); } public static void main(String[] args) { } }4.3 特别提醒通过以上两个例子，我们已经可以大致知道 JUnit 3.X 和 JUnit 4.X 两个版本的区别啦！ 首先，如果我们使用 JUnit 3.X，那么在我们写的测试类的时候，一定要继承 TestCase 类，但是如果我们使用 JUnit 4.X，则不需继承 TestCase 类，直接使用注解就可以啦！ 在 JUnit 3.X 中，还强制要求测试方法的命名为“ testXxxx ”这种格式； 在 JUnit 4.X 中，则不要求测试方法的命名格式，但作者还是建议测试方法统一命名为“ testXxxx ”这种格式，简洁明了。 此外，在上面的两个示例中，我们只给出了测试类，但是在这之前，还应该有一个被测试类，也就是我们真正要实现功能的类。现在，作者将给出上面示例中被测试的类，即 Operation 类： / * 定义了加减乘除的法则 */ public class Operation { public static void main(String[] args) { System.out.println(&quot;a + b = &quot; + add(1,2)); System.out.println(&quot;a - b = &quot; + subtract(1,2)); System.out.println(&quot;a * b = &quot; + multiply(1,2)); System.out.println(&quot;a / b = &quot; + divide(4,2)); System.out.println(&quot;a / b = &quot; + divide(1,0)); } public static int add(int a, int b) { return a + b; } public static int subtract(int a, int b) { return a - b; } public static int multiply(int a, int b) { return a * b; } public static int divide(int a, int b) { return a / b; } }5 测试示例5.1 示例一：简单的 JUnit 3.X 测试import junit.framework.Test; import junit.framework.TestCase; import junit.framework.TestSuite; import java.util.ArrayList; import java.util.Collection; / * 1、创建一个测试类，继承TestCase类 */ public class SimpleTestDemo extends TestCase { public SimpleTestDemo(String name) { super(name); } / * 2、写一个测试方法，断言期望的结果 */ public void testEmptyCollection(){ Collection collection = new ArrayList(); assertTrue(collection.isEmpty()); } / * 3、写一个suite()方法，它会使用反射动态的创建一个包含所有的testXxxx方法的测试套件 */ public static Test suit(){ return new TestSuite(SimpleTestDemo.class); } / * 4、写一个main()方法，以文本运行器的方式方便的运行测试 */ public static void main(String[] args) { junit.textui.TestRunner.run(suit()); } }6 个人建议有些童鞋可能会有一些误解，认为写测试代码没有用，而且还会增大自己的压力，浪费时间。但事实上，写测试代码与否，还是有很大区别的，如果是在小的项目中，或许这种区别还不太明显，但如果在大型项目中，一旦出现错误或异常，用人力去排查的话，那将会浪费很多时间，而且还不一定排查的出来，但是如果用测试代码的话，JUnit 就是自动帮我们判断一些代码的结果正确与否，从而节省的时间将会远远超过你写测试代码的时间。 因此，个人建议：要养成编写测试代码的习惯，码一点、测一点；再码一点，再测一点，如此循环。在我们不断编写与测试代码的过程中，我们将会对类的行为有一个更为深入的了解，从而可以有效的提高我们的工作效率。下面，作者就给出一些具体的编写测试代码的技巧和较好的实践方法： 1. 不要用 TestCase 的构造函数初始化 Fixture，而要用 setUp() 和 tearDown() 方法；2. 不要依赖或假定测试运行的顺序，因为 JUnit 会利用 Vector 保存测试方法，所以不同的平台会按不同的顺序从 Vector 中取出测试方法；3. 避免编写有副作用的 TestCase，例如：如果随后的测试依赖于某些特定的交易数据，就不要提交交易数据，只需要简单的回滚就可以了；4. 当继承一个测试类时，记得调用父类的 setUp() 和 tearDown() 方法；5. 将测试代码和工作代码放在一起，同步编译和更新；6. 测试类和测试方法应该有一致的命名方案，如在工作类名前加上 test 从而形成测试类名；7. 确保测试与时间无关，不要使用过期的数据进行测试，以至于导致在随后的维护过程中很难重现测试；8. 如果编写的软件面向国际市场，那么编写测试时一定要考虑国际化的因素；9. 尽可能地利用 JUnit 提供地 assert 和 fail 方法以及异常处理的方法，其可以使代码更为简洁；10. 测试要尽可能地小，执行速度快；11. 不要硬性规定数据文件的路径；12. 使用文档生成器做测试文档。 8 大单元测试框架 1.Arquillian Arquillian是一个基于JVM的高度可扩展的测试平台，允许开发人员创建Java的自动化集成，功能和验收测试。Arquillian允许你在运行态时执行测试。Arquillian可用于管理容器（或容器）的生命周期，绑定测试用例，依赖类和资源。它还能够将压缩包部署到容器中，并在容器中执行测试并捕获结果并创建报告。 Arquillian集成了熟悉的测试框架，如JUnit 4、TestNG 5，并允许使用现有的IDE启动测试。并且由于其模块化设计，它能够运行Ant和Maven测试插件。Arquillian目的是简化项目集成测试和功能测试的编写，让它们能像单元测试一样简单。 2.JTEST JTest也被称为“Parasoft JTest”，是Parasoft公司生产的自动化Java软件测试和静态分析软件。 JTest包括用于单元测试用例生成和执行，静态代码分析，数据流静态分析和度量分析，回归测试，运行时错误检测的功能。 还可以进行结对的代码审查流程自动化和运行时错误检测，例如：条件，异常，资源和内存泄漏，安全攻击漏洞等。 3.The Grinder “The Grinder”是一个Java负载测试框架。并且通过使用大量负载注射器来为分布式测试提供便利。Grinder可以对具有Java API的任何内容加载测试。这包括HTTP Web服务器，SOAP、REST Web服务、应用程序服务器，包括自定义协议。测试脚本用强大的Jython和Clojure语言编写。Grinder的GUI控制台允许对多个负载注射器进行监控和控制，并自动管理客户端连接和Cookie，SSL，代理感知和连接限制。您可以在这里找到关于磨床功能的更多深入信息。 4.TestNG TestNG受JUnit和NUnit的启发，是为Java编程语言而设计的测试框架。TestNG主要设计用于覆盖更广泛的测试类别，如单元，功能，端到端，集成等。它还引入了一些新功能，使其更强大，更易于使用，如：注解，运行在大线程池中进行各种策略测试，多线程安全验证代码测试，灵活的测试配置，数据驱动的参数测试支持等等。 TestNG有各种工具和插件（如Eclipse，IDEA，Maven等）支持。 5.JUnit JUnit是为Java编程语言设计的单元测试框架。JUnit在测试驱动开发框架的开发中发挥了重要作用。它是单元测试框架之一，统称为由SUnit起源的xUnit。 6.JWalk JWalk被设计为用于Java编程语言的单元测试工具包。它被设计为支持称为“Lazy系统单元测试”的测试范例。 JWalkTester工具对任何由程序员提供的编译的Java类执行任何测试。它能够通过静态和动态分析以及来自程序员的提示来测试懒惰Lazy规范的一致性。 7.Mockito Mockito被设计为用于Java的开源测试框架，MIT许可证。Mockito允许程序员为了测试驱动开发（TDD）或行为驱动开发（BDD）而在自动化单元测试中创建和测试双对象（Mock对象）。 8 Powermock PowerMock是用于对源代码进行单元测试的Java框架，它可以作为其他模拟框架的扩展，比如原型Mockito或EasyMock，但具有更强大的功能。PowerMock利用自定义的类加载器和字节码操纵器来实现静态方法，构造函数，最终类和方法以及私有方法等的模拟。它主要是为了扩展现有的API，使用少量的方法和注解来实现额外的功能。 参考文章https://www.jianshu.com/p/0530cb31c3b2https://blog.csdn.net/Dream_Weave/article/details/83859750https://blog.csdn.net/qq_26295547/article/details/83145642https://www.jianshu.com/p/0530cb31c3b2http://www.sohu.com/a/145107423_731023 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>JUnit</tag>
        <tag>单元测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界10：从JavaBean讲到Spring]]></title>
    <url>%2F2019%2F10%2F22%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C10%EF%BC%9A%E4%BB%8EJavaBean%E8%AE%B2%E5%88%B0Spring%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） Java BeanJava语言欠缺属性、事件、多重继承功能。所以，如果要在Java程序中实现一些面向对象编程的常见需求，只能手写大量胶水代码。Java Bean正是编写这套胶水代码的惯用模式或约定。这些约定包括getXxx、setXxx、isXxx、addXxxListener、XxxEvent等。遵守上述约定的类可以用于若干工具或库。 举个例子，假如有人要用Java实现一个单向链表类，可能会这样写： 上述实现为了能够快速获取链表的大小，把链表大小缓存在size变量中。用法如下： JavaIntList myList = new JavaIntList( ); System.out.println(myList.size); 要节省内存，不要缓存size变量了，把代码改成这样： 发现找不到什么size变量。如果要找到size变量，你就必须保持向后兼容性。所以Java标准库中，绝对不会出现public int size这样的代码，而一定会一开始就写成： private int size; public int getSize( ){return size;} 让用户一开始就使用getSize，以便有朝一日修改getSize实现时，不破坏向后兼容性。这种public int getSize() { return size; }的惯用手法，就是Java Bean。 JSP + Java Bean在jsp上， 可以用java bean 来封装业务逻辑，保存数据到数据库， 像这样： 其中jsp 直接用来接受用户的请求， 然后通过java bean 来处理业务， 具体的使用方法是： 这就能把HTTP request中的所有参数都设置到 user 这个java bean 对应的属性上去。 只要保证 http request中的参数名和 java bean 中的属性名是一样的。 这个叫做JSP Model 1 的模型受到了很多Java程序员的欢迎 , 因为他们的应用规模都很小， 用Model 1 使得开发很快速，实际上， 这种方式和微软的asp , 以及和开源的php 几乎一样。 但在项目中频繁使用了Model 1 导致整个系统的崩溃，因为系统中有好几千个jsp， 这些jsp互相调用(通过GET/POST), 到了最后调用关系无人能搞懂。 为了解决这个问题，又推出了 ：JSP Model 2 , 这是个模型真正的体现了Model-View-Controller的思想： Servlet 充当Controller , jsp 充当 View ，Java bean 当然就是Model 了！ 业务逻辑， 页面显示， 和处理过程做了很好的分离。 基于这个模型的扩展和改进， 很多Web开发框架开始如雨后春笋一样出现， 其中最著名的就是 SpringMVC了。 Enterprise Java bean越来越多企业程序员提出诉求：要分布式、要安全、要事务、要高可用性。 诉求可以归结为：“我们只想关注我们的业务逻辑， 我们不想， 也不应该由我们来处理‘低级’的事务， 多线程，连接池，以及其他各种各种的‘低级’API， 此外Java帝国一定得提供集群功能， 这样我们的一台机器死机以后，整个系统还能运转。 ” 于是推出了J2EE， 像Java bean 一样， 这还是一个规范， 但是比Java bean 复杂的多， 其中有： JDBC: Java 数据库连接 JNDI : Java 命名和目录接口， 通过一个名称就可以定位到一个数据源， 连jdbc连接都不用了 RMI： 远程过程调用， 让一个机器上的java 对象可以调用另外一个机器上的java 对象 JMS : Java 消息服务， 可以使用消息队列了 JTA： Java 事务管理， 支持分布式事务， 能在访问、更新多个数据库的时候，仍然保证事务， 还是分布式。 Java mail : 收发邮件 J2EE 后来改成了Java EE。 当然最重要的是， java bean 变成了 Enterprise Java bean *, 简称 *EJB。 使用了EJB， 你就可以把精力只放在业务上了， 那些烦人的事务管理， 安全管理，线程 统统交给容器（应用服务器）来处理吧。 我们还提供了额外的福利， 只要你的应用服务器是由多个机器组成的集群， EJB就可以无缝的运行在这个集群上， 你完全不用考虑一个机器死掉了应用该怎么办。我们都帮你搞定了。 使用Session Bean ， 可以轻松的处理你的业务。 使用实体Bean (Entity bean ) , 你和数据库打交道会变得极为轻松， 甚至sql 都不用写了。 使用消息驱动Bean(Message Driven bean ) , 你可以轻松的和一个消息队列连接， 处理消息。 Spring然而，大部分的程序员就发现， EJB中用起来极为繁琐和笨重， 性能也不好， 为了获得所谓的分布式，反而背上了沉重的枷锁。 实体Bean很快没人用了， 就连简单的无状态Session bean 也被大家所诟病， 其中一条罪状就是“代码的侵入性”。 在定义EJB的时候没考虑那么多，程序员在定义一个Session bean的时候，需要写一大堆和业务完全没有关系的类。 还需要被迫实现一些根本不应该实现的接口及其方法： 他们希望这个样子： public class HelloworldBean{ public String hello(){ return &quot;hello world&quot; } } 与此同时，他们还过分的要求保留事务、 安全这些必备的东西。 Spring 框架顺应了POJO的潮流， 提供了一个spring 的容器来管理这些POJO, 也叫bean 。 对于一个Bean 来说，如果你依赖别的Bean , 只需要声明即可， spring 容器负责把依赖的bean 给“注入进去“， 起初大家称之为控制反转(IoC)。 后来 Martin flower 给这种方式起来个更好的名字，叫“依赖注入”（DI）。 如果一个Bean 需要一些像事务，日志，安全这样的通用的服务， 也是只需要声明即可， spring 容器在运行时能够动态的“织入”这些服务， 这叫面向切面（AOP）。 总之，spring和spring mvc极大的增加了Java对web开发领地的统治力。 JavaBean 和 Spring中Bean的区别先了解一下各自是什么吧! Jave beanjavaBean简单的讲就是实体类，用来封装对象，这个类里面全部都是属性值，和get，set方法。简单笼统的说就是一个类，一个可复用的类。javaBean在MVC设计模型中是model，又称模型层，在一般的程序中，我们称它为数据层，就是用来设置数据的属性和一些行为，然后我会提供获取属性和设置属性的get/set方法JavaBean是一种JAVA语言写成的可重用组件。为写成JavaBean，类必须是具体的和公共的，并且具有无参数的构造器。 spring bean对于使用Spring框架的开发人员来说，我们主要做的主要有两件事情：①开发Bean;②配置Bean;而Spring帮我们做的就是根据配置文件来创建Bean实例，并调用Bean实例的方法来完成“依赖注入”，可以把Spring容器理解成一个大型工厂，Bean就是该工厂的产品，工厂(Spirng容器)里能生产出来什么样的产品（Bean），完全取决于我们在配置文件中的配置。其实就是根据配置文件产生对象,而不需要人为的手动去创造对象,降低了耦合. 用处不同：传统javabean更多地作为值传递参数，而spring中的bean用处几乎无处不在，任何组件都可以被称为bean。 写法不同：传统javabean作为值对象，要求每个属性都提供getter和setter方法；但spring中的bean只需为接受设值注入的属性提供setter方法。 javabean的写法: public class A{ private String a; private void setA(String a){ this.a = a; } private String getA(){ return a; } } spring bean的写法 &lt;bean id=&quot;p1&quot; class=&quot;com.zking.Pojo.Person&quot; scope=&quot;prototype&quot;&gt; //及时加载 加载你的xml配置文件 ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;ApplicationContext.xml&quot;); //getbean输入你配置类的别名得到 person对象 Person p = (Person) applicationContext.getBean(&quot;p1&quot;);id是给这个对象定的别名 class是这个实体类的全路径名 根据配置文件来创建Bean实例，并调用Bean实例的方法 bean里面还有很多属性 生命周期不同：传统javabean作为值对象传递，不接受任何容器管理其生命周期；spring中的bean有spring管理其生命周期行为。 所有可以被spring容器实例化并管理的java类都可以称为bean。 原来服务器处理页面返回的值都是直接使用request对象，后来增加了javabean来管理对象，所有页面值只要是和javabean对应，就可以用类.GET属性方法来获取值。javabean不只可以传参数，也可以处理数据，相当与把一个服务器执行的类放到了页面上，使对象管理相对不那么乱（对比asp的时候所有内容都在页面上完成）。 spring中的bean，是通过配置文件、javaconfig等的设置，有spring自动实例化，用完后自动销毁的对象。让我们只需要在用的时候使用对象就可以，不用考虑如果创建类对象（这就是spring的注入）。一般是用在服务器端代码的执行上。 参考文章微信公众号【码农翻身】https://blog.csdn.net/hmh13548571896/article/details/100628104https://www.cnblogs.com/xll1025/p/11366413.htmlhttps://blog.csdn.net/qqqnzhky/article/details/82747333https://www.cnblogs.com/mike-mei/p/9712836.htmlhttps://blog.csdn.net/qq_42245219/article/details/82748460 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界9：Java日志系统的诞生与发展]]></title>
    <url>%2F2019%2F10%2F21%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C9%EF%BC%9AJava%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%9E%E7%94%9F%E4%B8%8E%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） Java日志系统的演变史我们先看一个故事。项目经理A带着一帮兄弟开发了一套复杂的企业ERP系统，这个系统一连开发了好几年，开发人员也换了好几拨。 阶段一最开始的时候，项目经理A安排小B在系统中添加日志功能，在控制台上打印一些必要的信息。最开始的时候，由于项目的功能比较少，于是小B就是用System.out.println的方式打印日志信息。经理A感觉这样使用比较方便，也便于项目小组人员的使用，于是就沿用了下来。 阶段二此时小B被借调到其他项目，小C加入到了项目组中。此时项目经理A要求改造日志系统，要求能把日志写到一个文件中，方便以后分析用户行为。小C在查看了以前的日志方式之后，感觉特别low，于是自己写了一个日志框架，命名为xiaoC-logging.jar，此举收到了项目经理A的好评。 阶段三项目组中加入了一个大牛老D，老D发现xiaoC-logging.jar这个日志框架虽然可以满足基本的日志要求，但是还不够高大上，没有一些诸如自动归档，异步写入文件，把日志文件写入NoSQL数据库中等功能。于是老D开发了一个更高级的日志框架叫oldD-logging.jar。 阶段四oldD-logging.jar开发完成之后，需要把原来的xiaoC-logging.jar中的日志API做修改，把之前的日志实现写下来，换上高大上的oldD-logging.jar。 阶段五在这个卸载与上新的过程中，老D的工作量陡增，他感觉很累。不过姜还是老的辣，他参考了JDBC和spring中面向接口的编程方式，制定了一个日志的门面（一系列的接口），以后所有的日志的记录，都只面向接口编程，至于今后怎么去实现，都要遵循这个接口就可以了。 那么在JAVA开发中，这正的日志系统是怎么演变的呢？简短地描述下日志发展，最先出现的是apache开源社区的log4j，这个日志确实是应用最广泛的日志工具，成为了java日志的事实上的标准。然而，当时Sun公司在jdk1.4中增加了JUL日志实现，企图对抗log4j，但是却造成了混乱，这个也是被人诟病的一点。当然也有其他日志工具的出现，这样必然造成开发者的混乱，因为这些日志系统互相没有关联，替换和统一也就变成了比较棘手的一件事。想象下你的应用使用log4j，然后使用了一个其他团队的库，他们使用了JUL，你的应用就得使用两个日志系统了，然后又有第二个库出现了，使用了simplelog。 这个时候估计让你崩溃了，这是要闹哪样？这个状况交给你来想想办法，你该如何解决呢？进行抽象，抽象出一个接口层，对每个日志实现都适配或者转接，这样这些提供给别人的库都直接使用抽象层即可。不错，开源社区提供了commons-logging抽象，被称为JCL，也就是日志框架了，确实出色地完成了兼容主流的日志实现（log4j、JUL、simplelog），基本一统江湖，就连顶顶大名的spring也是依赖了JCL。 看起来事物确实是美好，但是美好的日子不长，接下来另一个优秀的日志框架slf4j的加入导致了更加混乱的场面。比较巧的是slf4j的作者(Ceki Gülcü)就是log4j的作者，他觉得JCL不够优秀，所以他要自己搞一套更优雅的出来，于是slf4j日志体系诞生了，并为slf4j实现了一个亲子——logback，确实更加优雅，但是由于之前很多代码库已经使用JCL，虽然出现slf4j和JCL之间的桥接转换，但是集成的时候问题依然多多，对很多新手来说确实会很懊恼，因为比单独的log4j时代“复杂”多了，抱怨声确实很多。 到此本来应该完了，但是Ceki Gülcü觉得还是得回头拯救下自己的“大阿哥”——log4j，于是log4j2诞生了，同样log4j2也参与到了slf4j日志体系中，想必将来会更加混乱。接下来详细解读日志系统的配合使用问题。slf4j的设计确实比较优雅，采用比较熟悉的方式——接口和实现分离，有个纯粹的接口层——slf4j-api工程，这个里边基本完全定义了日志的接口，所以对于开发来说，只需要使用这个即可。 有接口就要有实现，比较推崇的实现是logback，logback完全实现了slf4j-api的接口，并且性能也比log4j更好，同时实现了变参占位符日志输出方式等等新特性。刚刚也提到log4j的使用比较普遍，所以支持这批用户依然是必须的，slf4j-log4j12也实现了slf4j-api，这个算是对log4j的适配器。同样推理，也会有对JUL的适配器slf4j-jdk14等等。为了使使用JCL等等其他日志系统后者实现的用户可以很简单地切换到slf4j上来，给出了各种桥接工程，比如：jcl-over-slf4j会把对JCL的调用都桥接到slf4j上来，可以看出jcl-over-slf4j的api和JCL是相同的，所以这两个jar是不能共存的。jul-to-slf4j是把对jul的调用桥接到slf4j上，log4j-over-slf4j是把对log4j的调用桥接到slf4j。 一、日志框架的分类 门面型日志框架： JCL： Apache基金会所属的项目，是一套Java日志接口，之前叫Jakarta Commons Logging，后更名为Commons Logging SLF4J： 是一套简易Java日志门面，本身并无日志的实现。（Simple Logging Facade for Java，缩写Slf4j） 记录型日志框架: JUL： JDK中的日志记录工具，也常称为JDKLog、jdk-logging，自Java1.4以来的官方日志实现。 Log4j： 一个具体的日志实现框架。 Log4j2： 一个具体的日志实现框架，是LOG4J1的下一个版本，与Log4j 1发生了很大的变化，Log4j 2不兼容Log4j 1。 Logback：一个具体的日志实现框架，和Slf4j是同一个作者，但其性能更好。 二、发展历程要搞清楚它们的关系，就要从它们是在什么情况下产生的说起。我们按照时间的先后顺序来介绍。 Log4j在JDK 1.3及以前，Java打日志依赖System.out.println(), System.err.println()或者e.printStackTrace()，Debug日志被写到STDOUT流，错误日志被写到STDERR流。这样打日志有一个非常大的缺陷，即无法定制化，且日志粒度不够细。于是， Gülcü 于2001年发布了Log4j，后来成为Apache 基金会的顶级项目。Log4j 在设计上非常优秀，对后续的 Java Log 框架有长久而深远的影响，它定义的Logger、Appender、Level等概念如今已经被广泛使用。Log4j 的短板在于性能，在Logback 和 Log4j2 出来之后，Log4j的使用也减少了。 J.U.L受Logj启发，Sun在Java1.4版本中引入了java.util.logging，但是j.u.l功能远不如log4j完善，开发者需要自己编写Appenders（Sun称之为Handlers），且只有两个Handlers可用（Console和File），j.u.l在Java1.5以后性能和可用性才有所提升。 JCL（commons-logging）由于项目的日志打印必然选择两个框架中至少一个，这时候，Apache的JCL（commons-logging）诞生了。JCL 是一个Log Facade，只提供 Log API，不提供实现，然后有 Adapter 来使用 Log4j 或者 JUL 作为Log Implementation。在程序中日志创建和记录都是用JCL中的接口，在真正运行时，会看当前ClassPath中有什么实现，如果有Log4j 就是用 Log4j, 如果啥都没有就是用 JDK 的 JUL。这样，在你的项目中，还有第三方的项目中，大家记录日志都使用 JCL 的接口，然后最终运行程序时，可以按照自己的需求(或者喜好)来选择使用合适的Log Implementation。如果用Log4j, 就添加 Log4j 的jar包进去，然后写一个 Log4j 的配置文件；如果喜欢用JUL，就只需要写个 JUL 的配置文件。如果有其他的新的日志库出现，也只需要它提供一个Adapter，运行的时候把这个日志库的 jar 包加进去。不过，commons-logging对Log4j和j.u.l的配置问题兼容的并不好，使用commons-loggings还可能会遇到类加载问题，导致NoClassDefFoundError的错误出现。 到这个时候一切看起来都很简单，很美好。接口和实现做了良好的分离，在统一的JCL之下，不改变任何代码，就可以通过配置就换用功能更强大，或者性能更好的日志库实现。 这种简单美好一直持续到SLF4J出现。 SLF4J &amp; LogbackSLF4J（Simple Logging Facade for Java）和 Logback 也是Gülcü 创立的项目，目的是为了提供更高性能的实现。从设计模式的角度说，SLF4J 是用来在log和代码层之间起到门面作用，类似于 JCL 的 Log Facade。对于用户来说只要使用SLF4J提供的接口，即可隐藏日志的具体实现，SLF4J提供的核心API是一些接口和一个LoggerFactory的工厂类，用户只需按照它提供的统一纪录日志接口，最终日志的格式、纪录级别、输出方式等可通过具体日志系统的配置来实现，因此可以灵活的切换日志系统。 Logback是log4j的升级版，当前分为三个目标模块： logback-core：核心模块，是其它两个模块的基础模块 logback-classic：是log4j的一个改良版本，同时完整实现 SLF4J API 使你可以很方便地更换成其它日记系统如log4j 或 JDK14 Logging logback-access：访问模块与Servlet容器集成提供通过Http来访问日记的功能，是logback不可或缺的组成部分 Logback相较于log4j有更多的优点： 更快的执行速度 更充分的测试 logback-classic 非常自然的实现了SLF4J 使用XML配置文件或者Groovy 自动重新载入配置文件 优雅地从I/O错误中恢复 自动清除旧的日志归档文件 自动压缩归档日志文件 谨慎模式 Lilith 配置文件中的条件处理 更丰富的过滤 更详细的解释参见官网：https://logback.qos.ch/reasonsToSwitch.html 到这里，你可能会问：Apache 已经有了个JCL，用来做各种Log lib统一的接口，如果 Gülcü 要搞一个更好的 Log 实现的话，直接写一个实现就好了，为啥还要搞一个和SLF4J呢? 原因是Gülcü 认为 JCL 的 API 设计得不好，容易让使用者写出性能有问题的代码。关于这点，你可以参考这篇文章获得更详细的介绍：https://zhuanlan.zhihu.com/p/24272450 现在事情就变复杂了。我们有了两个流行的 Log Facade，以及三个流行的 Log Implementation。Gülcü 是个追求完美的人，他决定让这些Log之间都能够方便的互相替换，所以做了各种 Adapter 和 Bridge 来连接: 可以看到甚至 Log4j 和 JUL 都可以桥接到SLF4J，再通过 SLF4J 适配到到 Logback！需要注意的是不能有循环的桥接，比如下面这些依赖就不能同时存在: jcl-over-slf4j 和 slf4j-jcl log4j-over-slf4j 和 slf4j-log4j12 jul-to-slf4j 和 slf4j-jdk14 然而，事情在变得更麻烦！ Log4j2 现在有了更好的 SLF4J 和 Logback，慢慢取代JCL 和 Log4j ，事情到这里总该大统一圆满结束了吧。然而维护 Log4j 的人不这样想，他们不想坐视用户一点点被 SLF4J / Logback 蚕食，继而搞出了 Log4j2。 Log4j2 和 Log4j1.x 并不兼容，设计上很大程度上模仿了 SLF4J/Logback，性能上也获得了很大的提升。Log4j2 也做了 Facade/Implementation 分离的设计，分成了 log4j-api 和 log4j-core。 现在好了，我们有了三个流行的Log 接口和四个流行的Log实现，如果画出桥接关系的图来回事什么样子呢? 看到这里是不是感觉有点晕呢？是的，我也有这种感觉。同样，在添加依赖的时候，要小心不要有循环依赖。 参考文章https://segmentfault.com/a/1190000009707894 https://www.cnblogs.com/hysum/p/7100874.html http://c.biancheng.net/view/939.html https://www.runoob.com/ https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界8：浅析Tomcat9请求处理流程与启动部署过程]]></title>
    <url>%2F2019%2F10%2F20%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C8%EF%BC%9A%E6%B5%85%E6%9E%90Tomcat9%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E4%B8%8E%E5%90%AF%E5%8A%A8%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 很多东西在时序图中体现的已经非常清楚了，没有必要再一步一步的作介绍，所以本文以图为主，然后对部分内容加以简单解释。 绘制图形使用的工具是 PlantUML + Visual Studio Code + PlantUML Extension 本文对 Tomcat 的介绍以 Tomcat-9.0.0.M22 为标准。 Tomcat-9.0.0.M22 是 Tomcat 目前最新的版本，但尚未发布，它实现了 Servlet4.0 及 JSP2.3 并提供了很多新特性，需要 1.8 及以上的 JDK 支持等等，详情请查阅 Tomcat-9.0-doc。 https://tomcat.apache.org/tomcat-9.0-doc/index.html Overview Connector 启动以后会启动一组线程用于不同阶段的请求处理过程。 Acceptor 线程组。用于接受新连接，并将新连接封装一下，选择一个 Poller 将新连接添加到 Poller 的事件队列中。 Poller 线程组。用于监听 Socket 事件，当 Socket 可读或可写等等时，将 Socket 封装一下添加到 worker 线程池的任务队列中。 worker 线程组。用于对请求进行处理，包括分析请求报文并创建 Request 对象，调用容器的 pipeline 进行处理。 Acceptor、Poller、worker 所在的 ThreadPoolExecutor 都维护在 NioEndpoint 中。 Connector Init and Start initServerSocket()，通过 ServerSocketChannel.open() 打开一个 ServerSocket，默认绑定到 8080 端口，默认的连接等待队列长度是 100， 当超过 100 个时会拒绝服务。我们可以通过配置 conf/server.xml 中 Connector 的 acceptCount 属性对其进行定制。 createExecutor() 用于创建 Worker 线程池。默认会启动 10 个 Worker 线程，Tomcat 处理请求过程中，Woker 最多不超过 200 个。我们可以通过配置 conf/server.xml 中 Connector 的 minSpareThreads 和 maxThreads 对这两个属性进行定制。 Pollor 用于检测已就绪的 Socket。默认最多不超过 2 个，Math.min(2,Runtime.getRuntime().availableProcessors());。我们可以通过配置 pollerThreadCount 来定制。 Acceptor 用于接受新连接。默认是 1 个。我们可以通过配置 acceptorThreadCount 对其进行定制。 Request ProcessAcceptor Acceptor 在启动后会阻塞在 ServerSocketChannel.accept(); 方法处，当有新连接到达时，该方法返回一个 SocketChannel。 配置完 Socket 以后将 Socket 封装到 NioChannel 中，并注册到 Poller,值的一提的是，我们一开始就启动了多个 Poller 线程，注册的时候，连接是公平的分配到每个 Poller 的。NioEndpoint 维护了一个 Poller 数组，当一个连接分配给 pollers[index] 时，下一个连接就会分配给 pollers[(index+1)%pollers.length]. addEvent() 方法会将 Socket 添加到该 Poller 的 PollerEvent 队列中。到此 Acceptor 的任务就完成了。 Poller selector.select(1000)。当 Poller 启动后因为 selector 中并没有已注册的 Channel，所以当执行到该方法时只能阻塞。所有的 Poller 共用一个 Selector，其实现类是 sun.nio.ch.EPollSelectorImpl events() 方法会将通过 addEvent() 方法添加到事件队列中的 Socket 注册到 EPollSelectorImpl，当 Socket 可读时，Poller 才对其进行处理 createSocketProcessor() 方法将 Socket 封装到 SocketProcessor 中，SocketProcessor 实现了 Runnable 接口。worker 线程通过调用其 run() 方法来对 Socket 进行处理。 execute(SocketProcessor) 方法将 SocketProcessor 提交到线程池，放入线程池的 workQueue 中。workQueue 是 BlockingQueue 的实例。到此 Poller 的任务就完成了。 Worker worker 线程被创建以后就执行 ThreadPoolExecutor 的 runWorker() 方法，试图从 workQueue 中取待处理任务，但是一开始 workQueue 是空的，所以 worker 线程会阻塞在 workQueue.take() 方法。 当新任务添加到 workQueue后，workQueue.take() 方法会返回一个 Runnable，通常是 SocketProcessor,然后 worker 线程调用 SocketProcessor 的 run() 方法对 Socket 进行处理。 createProcessor() 会创建一个 Http11Processor, 它用来解析 Socket，将 Socket 中的内容封装到 Request 中。注意这个 Request 是临时使用的一个类，它的全类名是 org.apache.coyote.Request， postParseRequest() 方法封装一下 Request，并处理一下映射关系(从 URL 映射到相应的 Host、Context、Wrapper)。 CoyoteAdapter 将 Rquest 提交给 Container 处理之前，并将 org.apache.coyote.Request 封装到 org.apache.catalina.connector.Request，传递给 Container 处理的 Request 是 org.apache.catalina.connector.Request。 connector.getService().getMapper().map()，用来在 Mapper 中查询 URL 的映射关系。映射关系会保留到 org.apache.catalina.connector.Request 中，Container 处理阶段 request.getHost() 是使用的就是这个阶段查询到的映射主机，以此类推 request.getContext()、request.getWrapper() 都是。 connector.getService().getContainer().getPipeline().getFirst().invoke() 会将请求传递到 Container 处理，当然了 Container 处理也是在 Worker 线程中执行的，但是这是一个相对独立的模块，所以单独分出来一节。 Container 需要注意的是，基本上每一个容器的 StandardPipeline 上都会有多个已注册的 Valve，我们只关注每个容器的 Basic Valve。其他 Valve 都是在 Basic Valve 前执行。 request.getHost().getPipeline().getFirst().invoke() 先获取对应的 StandardHost，并执行其 pipeline。 request.getContext().getPipeline().getFirst().invoke() 先获取对应的 StandardContext,并执行其 pipeline。 request.getWrapper().getPipeline().getFirst().invoke() 先获取对应的 StandardWrapper，并执行其 pipeline。 最值得说的就是 StandardWrapper 的 Basic Valve，StandardWrapperValve allocate() 用来加载并初始化 Servlet，值的一提的是 Servlet 并不都是单例的，当 Servlet 实现了 SingleThreadModel 接口后，StandardWrapper 会维护一组 Servlet 实例，这是享元模式。当然了 SingleThreadModel在 Servlet 2.4 以后就弃用了。 createFilterChain() 方法会从 StandardContext 中获取到所有的过滤器，然后将匹配 Request URL 的所有过滤器挑选出来添加到 filterChain 中。 doFilter() 执行过滤链,当所有的过滤器都执行完毕后调用 Servlet 的 service() 方法。 Reference 《How Tomcat works》 https://www.amazon.com/How-Tomcat-Works-Budi-Kurniawan/dp/097521280X 《Tomcat 架构解析》– 刘光瑞 http://product.dangdang.com/25084132.html Tomcat-9.0-doc https://tomcat.apache.org/tomcat-9.0-doc/index.html apache-tomcat-9.0.0.M22-src http://www-eu.apache.org/dist/tomcat/tomcat-9/v9.0.0.M22/src/ tomcat架构分析 (connector NIO 实现) http://gearever.iteye.com/blog/1844203 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界7：Tomcat和其他WEB容器的区别]]></title>
    <url>%2F2019%2F10%2F19%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C7%EF%BC%9ATomcat%E5%92%8C%E5%85%B6%E4%BB%96WEB%E5%AE%B9%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 下面主要介绍下tomcat 与 nginx，apache的定义、区别及优缺点。 Tomcat和物理服务器的区别Tomcat： 本质：软件 Web 应用服务器—-一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。 用途： a． 当在一台机器（即物理服务器，也就是物理机）上配置好Apache 服务器，可利用它响应HTML页面的访问请求。实际上Tomcat是Apache 服务器的扩展，但运行时它是独立运行的，所以当你运行tomcat 时，它实际上作为一个与Apache 独立的进程单独运行的，Tomcat 实际上运行JSP 页面和Servlet b． Tomcat和IIS等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。 物理服务器：1．本质：硬件，也就是我们经常讲的服务器或者物理机，我们的PC就是一台性能较低的网络服务器，常见的有 云服务器（例如阿里云ECS）等 2．组成：处理器、硬盘、内存、系统总线等，和通用的计算机架构类似，但是由于需要提供高可靠的服务，因此在处理能力、稳定性、可靠性、安全性、可扩展性、可管理性等方面要求较高。 详解tomcat 与 nginx，apache的区别及优缺点定义：1. Apache Apache HTTP服务器是一个模块化的服务器，可以运行在几乎所有广泛使用的计算机平台上。其属于应用服务器。Apache支持支持模块多，性能稳定，Apache本身是静态解析，适合静态HTML、图片等，但可以通过扩展脚本、模块等支持动态页面等。 (Apche可以支持PHPcgiperl,但是要使用Java的话，你需要Tomcat在Apache后台支撑，将Java请求由Apache转发给Tomcat处理。) 缺点：配置相对复杂，自身不支持动态页面。 2. Tomcat： Tomcat是应用(Java)服务器，它只是一个Servlet(JSP也翻译成Servlet)容器，可以认为是Apache的扩展，但是可以独立于Apache运行。 3. Nginx Nginx是俄罗斯人编写的十分轻量级的HTTP服务器,Nginx，它的发音为“engine X”，是一个高性能的HTTP和反向代理服务器，同时也是一个IMAP/POP3/SMTP 代理服务器。 区别1. Apache与Tomcat的比较 相同点： 两者都是Apache组织开发的两者都有HTTP服务的功能两者都是免费的 不同点： Apache是专门用了提供HTTP服务的，以及相关配置的(例如虚拟主机、URL转发等等)，而Tomcat是Apache组织在符合Java EE的JSP、Servlet标准下开发的一个JSP服务器. Apache是一个Web服务器环境程序,启用他可以作为Web服务器使用,不过只支持静态网页如(ASP,PHP,CGI,JSP)等动态网页的就不行。如果要在Apache环境下运行JSP的话就需要一个解释器来执行JSP网页,而这个JSP解释器就是Tomcat。 Apache:侧重于HTTPServer ，Tomcat:侧重于Servlet引擎，如果以Standalone方式运行，功能上与Apache等效，支持JSP，但对静态网页不太理想; Apache是Web服务器，Tomcat是应用(Java)服务器，它只是一个Servlet(JSP也翻译成Servlet)容器，可以认为是Apache的扩展，但是可以独立于Apache运行。 实际使用中Apache与Tomcat常常是整合使用： 如果客户端请求的是静态页面，则只需要Apache服务器响应请求。 如果客户端请求动态页面，则是Tomcat服务器响应请求。 因为JSP是服务器端解释代码的，这样整合就可以减少Tomcat的服务开销。 可以理解Tomcat为Apache的一种扩展。 2. Nginx与Apache比较 1) nginx相对于apache的优点 轻量级，同样起web 服务，比apache占用更少的内存及资源 抗并发，nginx 处理请求是异步非阻塞的，而apache 则是阻塞型的，在高并发下nginx 能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单提供负载均衡 社区活跃，各种高性能模块出品迅速 2) apache 相对于nginx 的优点 apache的 rewrite 比nginx 的强大 ; 支持动态页面; 支持的模块多，基本涵盖所有应用; 性能稳定，而nginx相对bug较多。 3) 两者优缺点比较 Nginx 配置简洁, Apache 复杂 ; Nginx 静态处理性能比 Apache 高 3倍以上 ; Apache 对 PHP 支持比较简单，Nginx 需要配合其他后端用;Apache 的组件比 Nginx 多 ; apache是同步多进程模型，一个连接对应一个进程;nginx是异步的，多个连接(万级别)可以对应一个进程; nginx处理静态文件好,耗费内存少; 动态请求由apache去做，nginx只适合静态和反向; Nginx适合做前端服务器，负载性能很好; Nginx本身就是一个反向代理服务器 ，且支持负载均衡 总结Nginx优点：负载均衡、反向代理、处理静态文件优势。nginx处理静态请求的速度高于apache; Apache优点：相对于Tomcat服务器来说处理静态文件是它的优势，速度快。Apache是静态解析，适合静态HTML、图片等。 Tomcat：动态解析容器，处理动态请求，是编译JSPServlet的容器，Nginx有动态分离机制，静态请求直接就可以通过Nginx处理，动态请求才转发请求到后台交由Tomcat进行处理。 Apache在处理动态有优势，Nginx并发性比较好，CPU内存占用低，如果rewrite频繁，那还是Apache较适合。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界6：Tomcat5总体架构剖析]]></title>
    <url>%2F2019%2F10%2F18%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C6%EF%BC%9ATomcat5%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） Tomcat 总体结构Tomcat 的结构很复杂，但是 Tomcat 也非常的模块化，找到了 Tomcat 最核心的模块，您就抓住了 Tomcat 的“七寸”。下面是 Tomcat 的总体结构图： 从上图中可以看出 Tomcat 的心脏是两个组件：Connector 和 Container，关于这两个组件将在后面详细介绍。Connector 组件是可以被替换，这样可以提供给服务器设计者更多的选择，因为这个组件是如此重要，不仅跟服务器的设计的本身，而且和不同的应用场景也十分相关，所以一个 Container 可以选择对应多个 Connector。多个 Connector 和一个 Container 就形成了一个 Service，Service 的概念大家都很熟悉了，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了。所以整个 Tomcat 的生命周期由 Server 控制。 以 Service 作为“婚姻”我们将 Tomcat 中 Connector、Container 作为一个整体比作一对情侣的话，Connector 主要负责对外交流，可以比作为 Boy，Container 主要处理 Connector 接受的请求，主要是处理内部事务，可以比作为 Girl。那么这个 Service 就是连接这对男女的结婚证了。是 Service 将它们连接在一起，共同组成一个家庭。当然要组成一个家庭还要很多其它的元素。 说白了，Service 只是在 Connector 和 Container 外面多包一层，把它们组装在一起，向外面提供服务，一个 Service 可以设置多个 Connector，但是只能有一个 Container 容器。这个 Service 接口的方法列表如下： 图 2. Service 接口 从 Service 接口中定义的方法中可以看出，它主要是为了关联 Connector 和 Container，同时会初始化它下面的其它组件，注意接口中它并没有规定一定要控制它下面的组件的生命周期。所有组件的生命周期在一个 Lifecycle 的接口中控制，这里用到了一个重要的设计模式，关于这个接口将在后面介绍。 Tomcat 中 Service 接口的标准实现类是 StandardService 它不仅实现了 Service 借口同时还实现了 Lifecycle 接口，这样它就可以控制它下面的组件的生命周期了。StandardService 类结构图如下： 图 3. StandardService 的类结构图 从上图中可以看出除了 Service 接口的方法的实现以及控制组件生命周期的 Lifecycle 接口的实现，还有几个方法是用于在事件监听的方法的实现，不仅是这个 Service 组件，Tomcat 中其它组件也同样有这几个方法，这也是一个典型的设计模式，将在后面介绍。 下面看一下 StandardService 中主要的几个方法实现的代码，下面是 setContainer 和 addConnector 方法的源码： 清单 1. StandardService. SetContainerpublic void setContainer(Container container) { Container oldContainer = this.container; if ((oldContainer != null) &amp;&amp; (oldContainer instanceof Engine)) ((Engine) oldContainer).setService(null); this.container = container; if ((this.container != null) &amp;&amp; (this.container instanceof Engine)) ((Engine) this.container).setService(this); if (started &amp;&amp; (this.container != null) &amp;&amp; (this.container instanceof Lifecycle)) { try { ((Lifecycle) this.container).start(); } catch (LifecycleException e) { ; } } synchronized (connectors) { for (int i = 0; i &lt; connectors.length; i++) connectors[i].setContainer(this.container); } if (started &amp;&amp; (oldContainer != null) &amp;&amp; (oldContainer instanceof Lifecycle)) { try { ((Lifecycle) oldContainer).stop(); } catch (LifecycleException e) { ; } } support.firePropertyChange(&quot;container&quot;, oldContainer, this.container); }这段代码很简单，其实就是先判断当前的这个 Service 有没有已经关联了 Container，如果已经关联了，那么去掉这个关联关系—— oldContainer.setService(null)。如果这个 oldContainer 已经被启动了，结束它的生命周期。然后再替换新的关联、再初始化并开始这个新的 Container 的生命周期。最后将这个过程通知感兴趣的事件监听程序。这里值得注意的地方就是，修改 Container 时要将新的 Container 关联到每个 Connector，还好 Container 和 Connector 没有双向关联，不然这个关联关系将会很难维护。 清单 2. StandardService. addConnectorpublic void addConnector(Connector connector) { synchronized (connectors) { connector.setContainer(this.container); connector.setService(this); Connector results[] = new Connector[connectors.length + 1]; System.arraycopy(connectors, 0, results, 0, connectors.length); results[connectors.length] = connector; connectors = results; if (initialized) { try { connector.initialize(); } catch (LifecycleException e) { e.printStackTrace(System.err); } } if (started &amp;&amp; (connector instanceof Lifecycle)) { try { ((Lifecycle) connector).start(); } catch (LifecycleException e) { ; } } support.firePropertyChange(&quot;connector&quot;, null, connector); } }上面是 addConnector 方法，这个方法也很简单，首先是设置关联关系，然后是初始化工作，开始新的生命周期。这里值得一提的是，注意 Connector 用的是数组而不是 List 集合，这个从性能角度考虑可以理解，有趣的是这里用了数组但是并没有向我们平常那样，一开始就分配一个固定大小的数组，它这里的实现机制是：重新创建一个当前大小的数组对象，然后将原来的数组对象 copy 到新的数组中，这种方式实现了类似的动态数组的功能，这种实现方式，值得我们以后拿来借鉴。 最新的 Tomcat6 中 StandardService 也基本没有变化，但是从 Tomcat5 开始 Service、Server 和容器类都继承了 MBeanRegistration 接口，Mbeans 的管理更加合理。 以 Server 为“居”前面说一对情侣因为 Service 而成为一对夫妻，有了能够组成一个家庭的基本条件，但是它们还要有个实体的家，这是它们在社会上生存之本，有了家它们就可以安心的为人民服务了，一起为社会创造财富。 Server 要完成的任务很简单，就是要能够提供一个接口让其它程序能够访问到这个 Service 集合、同时要维护它所包含的所有 Service 的生命周期，包括如何初始化、如何结束服务、如何找到别人要访问的 Service。还有其它的一些次要的任务，如您住在这个地方要向当地政府去登记啊、可能还有要配合当地公安机关日常的安全检查什么的。 Server 的类结构图如下： 图 4. Server 的类结构图 它的标准实现类 StandardServer 实现了上面这些方法，同时也实现了 Lifecycle、MbeanRegistration 两个接口的所有方法，下面主要看一下 StandardServer 重要的一个方法 addService 的实现： 清单 3. StandardServer.addServicepublic void addService(Service service) { service.setServer(this); synchronized (services) { Service results[] = new Service[services.length + 1]; System.arraycopy(services, 0, results, 0, services.length); results[services.length] = service; services = results; if (initialized) { try { service.initialize(); } catch (LifecycleException e) { e.printStackTrace(System.err); } } if (started &amp;&amp; (service instanceof Lifecycle)) { try { ((Lifecycle) service).start(); } catch (LifecycleException e) { ; } } support.firePropertyChange(&quot;service&quot;, null, service); } }从上面第一句就知道了 Service 和 Server 是相互关联的，Server 也是和 Service 管理 Connector 一样管理它，也是将 Service 放在一个数组中，后面部分的代码也是管理这个新加进来的 Service 的生命周期。Tomcat6 中也是没有什么变化的。 组件的生命线“Lifecycle”前面一直在说 Service 和 Server 管理它下面组件的生命周期，那它们是如何管理的呢？ Tomcat 中组件的生命周期是通过 Lifecycle 接口来控制的，组件只要继承这个接口并实现其中的方法就可以统一被拥有它的组件控制了，这样一层一层的直到一个最高级的组件就可以控制 Tomcat 中所有组件的生命周期，这个最高的组件就是 Server，而控制 Server 的是 Startup，也就是您启动和关闭 Tomcat。 下面是 Lifecycle 接口的类结构图： 图 5. Lifecycle 类结构图 除了控制生命周期的 Start 和 Stop 方法外还有一个监听机制，在生命周期开始和结束的时候做一些额外的操作。这个机制在其它的框架中也被使用，如在 Spring 中。关于这个设计模式会在后面介绍。 Lifecycle 接口的方法的实现都在其它组件中，就像前面中说的，组件的生命周期由包含它的父组件控制，所以它的 Start 方法自然就是调用它下面的组件的 Start 方法，Stop 方法也是一样。如在 Server 中 Start 方法就会调用 Service 组件的 Start 方法，Server 的 Start 方法代码如下： 清单 4. StandardServer.Startpublic void start() throws LifecycleException { if (started) { log.debug(sm.getString(&quot;standardServer.start.started&quot;)); return; } lifecycle.fireLifecycleEvent(BEFORE_START_EVENT, null); lifecycle.fireLifecycleEvent(START_EVENT, null); started = true; synchronized (services) { for (int i = 0; i &lt; services.length; i++) { if (services[i] instanceof Lifecycle) ((Lifecycle) services[i]).start(); } } lifecycle.fireLifecycleEvent(AFTER_START_EVENT, null); }监听的代码会包围 Service 组件的启动过程，就是简单的循环启动所有 Service 组件的 Start 方法，但是所有 Service 必须要实现 Lifecycle 接口，这样做会更加灵活。 Server 的 Stop 方法代码如下： 清单 5. StandardServer.Stoppublic void stop() throws LifecycleException { if (!started) return; lifecycle.fireLifecycleEvent(BEFORE_STOP_EVENT, null); lifecycle.fireLifecycleEvent(STOP_EVENT, null); started = false; for (int i = 0; i &lt; services.length; i++) { if (services[i] instanceof Lifecycle) ((Lifecycle) services[i]).stop(); } lifecycle.fireLifecycleEvent(AFTER_STOP_EVENT, null); }它所要做的事情也和 Start 方法差不多。 Connector 组件Connector 组件是 Tomcat 中两个核心组件之一，它的主要任务是负责接收浏览器的发过来的 tcp 连接请求，创建一个 Request 和 Response 对象分别用于和请求端交换数据，然后会产生一个线程来处理这个请求并把产生的 Request 和 Response 对象传给处理这个请求的线程，处理这个请求的线程就是 Container 组件要做的事了。 由于这个过程比较复杂，大体的流程可以用下面的顺序图来解释： 图 6. Connector 处理一次请求顺序图 （查看清晰大图） Tomcat5 中默认的 Connector 是 Coyote，这个 Connector 是可以选择替换的。Connector 最重要的功能就是接收连接请求然后分配线程让 Container 来处理这个请求，所以这必然是多线程的，多线程的处理是 Connector 设计的核心。Tomcat5 将这个过程更加细化，它将 Connector 划分成 Connector、Processor、Protocol, 另外 Coyote 也定义自己的 Request 和 Response 对象。 下面主要看一下 Tomcat 中如何处理多线程的连接请求，先看一下 Connector 的主要类图： 图 7. Connector 的主要类图 （查看清晰大图） 看一下 HttpConnector 的 Start 方法： 清单 6. HttpConnector.Startpublic void start() throws LifecycleException { if (started) throw new LifecycleException (sm.getString(&quot;httpConnector.alreadyStarted&quot;)); threadName = &quot;HttpConnector[&quot; + port + &quot;]&quot;; lifecycle.fireLifecycleEvent(START_EVENT, null); started = true; threadStart(); while (curProcessors &lt; minProcessors) { if ((maxProcessors &gt; 0) &amp;&amp; (curProcessors &gt;= maxProcessors)) break; HttpProcessor processor = newProcessor(); recycle(processor); } }threadStart() 执行就会进入等待请求的状态，直到一个新的请求到来才会激活它继续执行，这个激活是在 HttpProcessor 的 assign 方法中，这个方法是代码如下 ： 清单 7. HttpProcessor.assignsynchronized void assign(Socket socket) { while (available) { try { wait(); } catch (InterruptedException e) { } } this.socket = socket; available = true; notifyAll(); if ((debug &gt;= 1) &amp;&amp; (socket != null)) log(&quot; An incoming request is being assigned&quot;); }创建 HttpProcessor 对象是会把 available 设为 false，所以当请求到来时不会进入 while 循环，将请求的 socket 赋给当期处理的 socket，并将 available 设为 true，当 available 设为 true 是 HttpProcessor 的 run 方法将被激活，接下去将会处理这次请求。 Run 方法代码如下： 清单 8. HttpProcessor.Runpublic void run() { while (!stopped) { Socket socket = await(); if (socket == null) continue; try { process(socket); } catch (Throwable t) { log(&quot;process.invoke&quot;, t); } connector.recycle(this); } synchronized (threadSync) { threadSync.notifyAll(); } }解析 socket 的过程在 process 方法中，process 方法的代码片段如下： 清单 9. HttpProcessor.processprivate void process(Socket socket) { boolean ok = true; boolean finishResponse = true; SocketInputStream input = null; OutputStream output = null; try { input = new SocketInputStream(socket.getInputStream(),connector.getBufferSize()); } catch (Exception e) { log(&quot;process.create&quot;, e); ok = false; } keepAlive = true; while (!stopped &amp;&amp; ok &amp;&amp; keepAlive) { finishResponse = true; try { request.setStream(input); request.setResponse(response); output = socket.getOutputStream(); response.setStream(output); response.setRequest(request); ((HttpServletResponse) response.getResponse()) .setHeader(&quot;Server&quot;, SERVER_INFO); } catch (Exception e) { log(&quot;process.create&quot;, e); ok = false; } try { if (ok) { parseConnection(socket); parseRequest(input, output); if (!request.getRequest().getProtocol().startsWith(&quot;HTTP/0&quot;)) parseHeaders(input); if (http11) { ackRequest(output); if (connector.isChunkingAllowed()) response.setAllowChunking(true); } } 。。。。。。 try { ((HttpServletResponse) response).setHeader (&quot;Date&quot;, FastHttpDateFormat.getCurrentDate()); if (ok) { connector.getContainer().invoke(request, response); } 。。。。。。 } try { shutdownInput(input); socket.close(); } catch (IOException e) { ; } catch (Throwable e) { log(&quot;process.invoke&quot;, e); } socket = null; }当 Connector 将 socket 连接封装成 request 和 response 对象后接下来的事情就交给 Container 来处理了。 Servlet 容器“Container”Container 是容器的父接口，所有子容器都必须实现这个接口，Container 容器的设计用的是典型的责任链的设计模式，它有四个子容器组件构成，分别是：Engine、Host、Context、Wrapper，这四个组件不是平行的，而是父子关系，Engine 包含 Host,Host 包含 Context，Context 包含 Wrapper。通常一个 Servlet class 对应一个 Wrapper，如果有多个 Servlet 就可以定义多个 Wrapper，如果有多个 Wrapper 就要定义一个更高的 Container 了，如 Context，Context 通常就是对应下面这个配置： 清单 10. Server.xml&lt;Context path=&quot;/library&quot; docBase=&quot;D:\projects\library\deploy\target\library.war&quot; reloadable=&quot;true&quot; /&gt;容器的总体设计Context 还可以定义在父容器 Host 中，Host 不是必须的，但是要运行 war 程序，就必须要 Host，因为 war 中必有 web.xml 文件，这个文件的解析就需要 Host 了，如果要有多个 Host 就要定义一个 top 容器 Engine 了。而 Engine 没有父容器了，一个 Engine 代表一个完整的 Servlet 引擎。 那么这些容器是如何协同工作的呢？先看一下它们之间的关系图： 图 8. 四个容器的关系图 （查看清晰大图） 当 Connector 接受到一个连接请求时，将请求交给 Container，Container 是如何处理这个请求的？这四个组件是怎么分工的，怎么把请求传给特定的子容器的呢？又是如何将最终的请求交给 Servlet 处理。下面是这个过程的时序图： 图 9. Engine 和 Host 处理请求的时序图 （查看清晰大图） 这里看到了 Valve 是不是很熟悉，没错 Valve 的设计在其他框架中也有用的，同样 Pipeline 的原理也基本是相似的，它是一个管道，Engine 和 Host 都会执行这个 Pipeline，您可以在这个管道上增加任意的 Valve，Tomcat 会挨个执行这些 Valve，而且四个组件都会有自己的一套 Valve 集合。您怎么才能定义自己的 Valve 呢？在 server.xml 文件中可以添加，如给 Engine 和 Host 增加一个 Valve 如下： 清单 11. Server.xml&lt;Engine defaultHost=&quot;localhost&quot; name=&quot;Catalina&quot;&gt; &lt;Valve className=&quot;org.apache.catalina.valves.RequestDumperValve&quot;/&gt; ……… &lt;Host appBase=&quot;webapps&quot; autoDeploy=&quot;true&quot; name=&quot;localhost&quot; unpackWARs=&quot;true&quot; xmlNamespaceAware=&quot;false&quot; xmlValidation=&quot;false&quot;&gt; &lt;Valve className=&quot;org.apache.catalina.valves.FastCommonAccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log.&quot; suffix=&quot;.txt&quot; pattern=&quot;common&quot; resolveHosts=&quot;false&quot;/&gt; ………… &lt;/Host&gt; &lt;/Engine&gt;StandardEngineValve 和 StandardHostValve 是 Engine 和 Host 的默认的 Valve，它们是最后一个 Valve 负责将请求传给它们的子容器，以继续往下执行。 前面是 Engine 和 Host 容器的请求过程，下面看 Context 和 Wrapper 容器时如何处理请求的。下面是处理请求的时序图： 图 10. Context 和 wrapper 的处理请求时序图 （查看清晰大图） 从 Tomcat5 开始，子容器的路由放在了 request 中，request 中保存了当前请求正在处理的 Host、Context 和 wrapper。 Engine 容器Engine 容器比较简单，它只定义了一些基本的关联关系，接口类图如下： 图 11. Engine 接口的类结构 它的标准实现类是 StandardEngine，这个类注意一点就是 Engine 没有父容器了，如果调用 setParent 方法时将会报错。添加子容器也只能是 Host 类型的，代码如下： 清单 12. StandardEngine. addChildpublic void addChild(Container child) { if (!(child instanceof Host)) throw new IllegalArgumentException (sm.getString(&quot;standardEngine.notHost&quot;)); super.addChild(child); } public void setParent(Container container) { throw new IllegalArgumentException (sm.getString(&quot;standardEngine.notParent&quot;)); }它的初始化方法也就是初始化和它相关联的组件，以及一些事件的监听。 Host 容器Host 是 Engine 的字容器，一个 Host 在 Engine 中代表一个虚拟主机，这个虚拟主机的作用就是运行多个应用，它负责安装和展开这些应用，并且标识这个应用以便能够区分它们。它的子容器通常是 Context，它除了关联子容器外，还有就是保存一个主机应该有的信息。 下面是和 Host 相关的类关联图： 图 12. Host 相关的类图 （查看清晰大图） 从上图中可以看出除了所有容器都继承的 ContainerBase 外，StandardHost 还实现了 Deployer 接口，上图清楚的列出了这个接口的主要方法，这些方法都是安装、展开、启动和结束每个 web application。 Deployer 接口的实现是 StandardHostDeployer，这个类实现了的最要的几个方法，Host 可以调用这些方法完成应用的部署等。 Context 容器Context 代表 Servlet 的 Context，它具备了 Servlet 运行的基本环境，理论上只要有 Context 就能运行 Servlet 了。简单的 Tomcat 可以没有 Engine 和 Host。 Context 最重要的功能就是管理它里面的 Servlet 实例，Servlet 实例在 Context 中是以 Wrapper 出现的，还有一点就是 Context 如何才能找到正确的 Servlet 来执行它呢？ Tomcat5 以前是通过一个 Mapper 类来管理的，Tomcat5 以后这个功能被移到了 request 中，在前面的时序图中就可以发现获取子容器都是通过 request 来分配的。 Context 准备 Servlet 的运行环境是在 Start 方法开始的，这个方法的代码片段如下： 清单 13. StandardContext.startsynchronized void start() throws LifecycleException { ……… if( !initialized ) { try { init(); } catch( Exception ex ) { throw new LifecycleException(&quot;Error initializaing &quot;, ex); } } ……… lifecycle.fireLifecycleEvent(BEFORE_START_EVENT, null); setAvailable(false); setConfigured(false); boolean ok = true; File configBase = getConfigBase(); if (configBase != null) { if (getConfigFile() == null) { File file = new File(configBase, getDefaultConfigFile()); setConfigFile(file.getPath()); try { File appBaseFile = new File(getAppBase()); if (!appBaseFile.isAbsolute()) { appBaseFile = new File(engineBase(), getAppBase()); } String appBase = appBaseFile.getCanonicalPath(); String basePath = (new File(getBasePath())).getCanonicalPath(); if (!basePath.startsWith(appBase)) { Server server = ServerFactory.getServer(); ((StandardServer) server).storeContext(this); } } catch (Exception e) { log.warn(&quot;Error storing config file&quot;, e); } } else { try { String canConfigFile = (new File(getConfigFile())).getCanonicalPath(); if (!canConfigFile.startsWith (configBase.getCanonicalPath())) { File file = new File(configBase, getDefaultConfigFile()); if (copy(new File(canConfigFile), file)) { setConfigFile(file.getPath()); } } } catch (Exception e) { log.warn(&quot;Error setting config file&quot;, e); } } } ……… Container children[] = findChildren(); for (int i = 0; i &lt; children.length; i++) { if (children[i] instanceof Lifecycle) ((Lifecycle) children[i]).start(); } if (pipeline instanceof Lifecycle) ((Lifecycle) pipeline).start(); ……… }它主要是设置各种资源属性和管理组件，还有非常重要的就是启动子容器和 Pipeline。 我们知道 Context 的配置文件中有个 reloadable 属性，如下面配置： 清单 14. Server.xml&lt;Context path=&quot;/library&quot; docBase=&quot;D:\projects\library\deploy\target\library.war&quot; reloadable=&quot;true&quot; /&gt;当这个 reloadable 设为 true 时，war 被修改后 Tomcat 会自动的重新加载这个应用。如何做到这点的呢 ? 这个功能是在 StandardContext 的 backgroundProcess 方法中实现的，这个方法的代码如下： 清单 15. StandardContext. backgroundProcesspublic void backgroundProcess() { if (!started) return; count = (count + 1) % managerChecksFrequency; if ((getManager() != null) &amp;&amp; (count == 0)) { try { getManager().backgroundProcess(); } catch ( Exception x ) { log.warn(&quot;Unable to perform background process on manager&quot;,x); } } if (getLoader() != null) { if (reloadable &amp;&amp; (getLoader().modified())) { try { Thread.currentThread().setContextClassLoader (StandardContext.class.getClassLoader()); reload(); } finally { if (getLoader() != null) { Thread.currentThread().setContextClassLoader (getLoader().getClassLoader()); } } } if (getLoader() instanceof WebappLoader) { ((WebappLoader) getLoader()).closeJARs(false); } } }它会调用 reload 方法，而 reload 方法会先调用 stop 方法然后再调用 Start 方法，完成 Context 的一次重新加载。可以看出执行 reload 方法的条件是 reloadable 为 true 和应用被修改，那么这个 backgroundProcess 方法是怎么被调用的呢？ 这个方法是在 ContainerBase 类中定义的内部类 ContainerBackgroundProcessor 被周期调用的，这个类是运行在一个后台线程中，它会周期的执行 run 方法，它的 run 方法会周期调用所有容器的 backgroundProcess 方法，因为所有容器都会继承 ContainerBase 类，所以所有容器都能够在 backgroundProcess 方法中定义周期执行的事件。 Wrapper 容器Wrapper 代表一个 Servlet，它负责管理一个 Servlet，包括的 Servlet 的装载、初始化、执行以及资源回收。Wrapper 是最底层的容器，它没有子容器了，所以调用它的 addChild 将会报错。 Wrapper 的实现类是 StandardWrapper，StandardWrapper 还实现了拥有一个 Servlet 初始化信息的 ServletConfig，由此看出 StandardWrapper 将直接和 Servlet 的各种信息打交道。 下面看一下非常重要的一个方法 loadServlet，代码片段如下： 清单 16. StandardWrapper.loadServletpublic synchronized Servlet loadServlet() throws ServletException { ……… Servlet servlet; try { ……… ClassLoader classLoader = loader.getClassLoader(); ……… Class classClass = null; ……… servlet = (Servlet) classClass.newInstance(); if ((servlet instanceof ContainerServlet) &amp;&amp; (isContainerProvidedServlet(actualClass) || ((Context)getParent()).getPrivileged() )) { ((ContainerServlet) servlet).setWrapper(this); } classLoadTime=(int) (System.currentTimeMillis() -t1); try { instanceSupport.fireInstanceEvent(InstanceEvent.BEFORE_INIT_EVENT,servlet); if( System.getSecurityManager() != null) { Class[] classType = new Class[]{ServletConfig.class}; Object[] args = new Object[]{((ServletConfig)facade)}; SecurityUtil.doAsPrivilege(&quot;init&quot;,servlet,classType,args); } else { servlet.init(facade); } if ((loadOnStartup &gt;= 0) &amp;&amp; (jspFile != null)) { ……… if( System.getSecurityManager() != null) { Class[] classType = new Class[]{ServletRequest.class, ServletResponse.class}; Object[] args = new Object[]{req, res}; SecurityUtil.doAsPrivilege(&quot;service&quot;,servlet,classType,args); } else { servlet.service(req, res); } } instanceSupport.fireInstanceEvent(InstanceEvent.AFTER_INIT_EVENT,servlet); ……… return servlet; }它基本上描述了对 Servlet 的操作，当装载了 Servlet 后就会调用 Servlet 的 init 方法，同时会传一个 StandardWrapperFacade 对象给 Servlet，这个对象包装了 StandardWrapper，ServletConfig 与它们的关系图如下： 图 13. ServletConfig 与 StandardWrapperFacade、StandardWrapper 的关系 Servlet 可以获得的信息都在 StandardWrapperFacade 封装，这些信息又是在 StandardWrapper 对象中拿到的。所以 Servlet 可以通过 ServletConfig 拿到有限的容器的信息。 当 Servlet 被初始化完成后，就等着 StandardWrapperValve 去调用它的 service 方法了，调用 service 方法之前要调用 Servlet 所有的 filter。 Tomcat 中其它组件Tomcat 还有其它重要的组件，如安全组件 security、logger 日志组件、session、mbeans、naming 等其它组件。这些组件共同为 Connector 和 Container 提供必要的服务。 参考文章https://www.cnblogs.com/xll1025/p/11366290.htmlhttp://www.imooc.com/article/291184https://blog.csdn.net/jingle1882010/article/details/80557808https://www.jianshu.com/p/c5494b63d564https://blog.csdn.net/w1992wishes/article/details/79242797 微信公众号微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界5：初探Tomcat的HTTP请求过程]]></title>
    <url>%2F2019%2F10%2F17%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C5%EF%BC%9A%E5%88%9D%E6%8E%A2Tomcat%E7%9A%84HTTP%E8%AF%B7%E6%B1%82%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 走进JavaWeb技术世界5：初探Tomcat的HTTP请求过程初探Tomcat的HTTP请求过程 前言：1.作为Java开发人员，大多都对Tomcat不陌生，由Apache基金会提供技术支持与维护，因为其免费开源且易用，作为Web服务器深受市场欢迎，所以有必要对其进行深入的研究，本系列皆以Tomcat 8.5为研究课题，下载地址：https://tomcat.apache.org/download-80.cgi 2.下图为 apache-tomcat-8.5.23.zip 在windows解压后的目录。 下面是解压后的一些关键目录: 1234* /bin - 启动和停止服务等批处理文件. ( *.sh) 文件 (为Unix系统)、 (*.bat) 文件 (for Windows系统)是一个功能性的复制文件. 自从Win32 command-line 开始是一些单一的，缺乏功能的组件, 现在有一些拓展性的功能* /conf - 配置文件和一些相关的DTD文件. 最重要的是 server.xml. 它是这个容器最主要的配置文件.* /logs - 日志文件会打印到这里* /webapps - 这里是你的应用程序部署的地方. 3.从最本质上讲，tomcat为一个servlet容器，首先研究一下Tomcat的架构，如下图： 架构诠释： 1.Server(服务器)是Tomcat构成的顶级构成元素，所有一切均包含在Server中，Server的实现类StandardServer可以包含一个到多个Services,Service的实现类为StandardService调用了容器(Container)接口，其实是调用了Servlet Engine(引擎)，而且StandardService类中也指明了该Service归属的Server; 2.Container: 引擎(Engine)、主机(Host)、上下文(Context)和Wraper均继承自Container接口，所以它们都是容器。但是，它们是有父子关系的，在主机(Host)、上下文(Context)和引擎(Engine)这三类容器中，引擎是顶级容器，直接包含是主机容器，而主机容器又包含上下文容器，所以引擎、主机和上下文从大小上来说又构成父子关系,虽然它们都继承自Container接口。 3.连接器(Connector)将Service和Container连接起来，首先它需要注册到一个Service，它的作用就是把来自客户端的请求转发到Container(容器)，这就是它为什么称作连接器的原因。 从功能的角度将Tomcat源代码分成5个子模块，分别是: Jsper模块: 这个子模块负责jsp页面的解析、jsp属性的验证，同时也负责将jsp页面动态转换为java代码并编译成class文件。在Tomcat源代码中，凡是属于org.apache.jasper包及其子包中的源代码都属于这个子模块; Servlet和Jsp模块: 这个子模块的源代码属于javax.servlet包及其子包，如我们非常熟悉的javax.servlet.Servlet接口、javax.servet.http.HttpServlet类及javax.servlet.jsp.HttpJspPage就位于这个子模块中; Catalina模块: 这个子模块包含了所有以org.apache.catalina开头的java源代码。该子模块的任务是规范了Tomcat的总体架构，定义了Server、Service、Host、Connector、Context、Session及Cluster等关键组件及这些组件的实现，这个子模块大量运用了Composite设计模式。同时也规范了Catalina的启动及停止等事件的执行流程。从代码阅读的角度看，这个子模块应该是我们阅读和学习的重点。 Connector模块: 如果说上面三个子模块实现了Tomcat应用服务器的话，那么这个子模块就是Web服务器的实现。所谓连接器(Connector)就是一个连接客户和应用服务器的桥梁，它接收用户的请求，并把用户请求包装成标准的Http请求(包含协议名称，请求头Head，请求方法是Get还是Post等等)。同时，这个子模块还按照标准的Http协议，负责给客户端发送响应页面，比如在请求页面未发现时，connector就会给客户端浏览器发送标准的Http 404错误响应页面。 Resource模块: 这个子模块包含一些资源文件，如Server.xml及Web.xml配置文件。严格说来，这个子模块不包含java源代码，但是它还是Tomcat编译运行所必需的。 Tomcat的组织结构 Tomcat是一个基于组件的服务器，它的构成组件都是可配置的，其中最外层的是Catalina servlet容器，其他组件按照一定的格式要求配置在这个顶层容器中。Tomcat的各种组件都是在Tomcat安装目录下的/conf/server.xml文件中配置的。 由Server.xml的结构看Tomcat的体系结构&lt;Server&gt; //顶层类元素，可以包括多个Service &lt;Service&gt; //顶层类元素，可包含一个Engine，多个Connecter &lt;Connector&gt; //连接器类元素，代表通信接口 &lt;Engine&gt; //容器类元素，为特定的Service组件处理客户请求，要包含多个Host &lt;Host&gt; //容器类元素，为特定的虚拟主机组件处理客户请求，可包含多个Context &lt;Context&gt; //容器类元素，为特定的Web应用处理所有的客户请求 &lt;/Context&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Connector&gt; &lt;/Service&gt; &lt;/Server&gt;实际源码如下： &lt;?xml version=&apos;1.0&apos; encoding=&apos;utf-8&apos;?&gt; &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot; /&gt; &lt;!-- Security listener. Documentation at /docs/config/listeners.html &lt;Listener className=&quot;org.apache.catalina.security.SecurityListener&quot; /&gt; --&gt; &lt;!--APR library loader. Documentation at /docs/apr.html --&gt; &lt;Listener className=&quot;org.apache.catalina.core.AprLifecycleListener&quot; SSLEngine=&quot;on&quot; /&gt; &lt;!--Initialize Jasper prior to webapps are loaded. Documentation at /docs/jasper-howto.html --&gt; &lt;Listener className=&quot;org.apache.catalina.core.JasperListener&quot; /&gt; &lt;!-- Prevent memory leaks due to use of particular java/javax APIs--&gt; &lt;Listener className=&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot; /&gt; &lt;!-- Global JNDI resources Documentation at /docs/jndi-resources-howto.html --&gt; &lt;GlobalNamingResources&gt; &lt;!-- Editable user database that can also be used by UserDatabaseRealm to authenticate users --&gt; &lt;Resource name=&quot;UserDatabase&quot; auth=&quot;Container&quot; type=&quot;org.apache.catalina.UserDatabase&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; pathname=&quot;conf/tomcat-users.xml&quot; /&gt; &lt;/GlobalNamingResources&gt; &lt;!-- A &quot;Service&quot; is a collection of one or more &quot;Connectors&quot; that share a single &quot;Container&quot; Note: A &quot;Service&quot; is not itself a &quot;Container&quot;, so you may not define subcomponents such as &quot;Valves&quot; at this level. Documentation at /docs/config/service.html --&gt; &lt;Service name=&quot;Catalina&quot;&gt; &lt;!--The connectors can use a shared executor, you can define one or more named thread pools--&gt; &lt;!-- &lt;Executor name=&quot;tomcatThreadPool&quot; namePrefix=&quot;catalina-exec-&quot; maxThreads=&quot;150&quot; minSpareThreads=&quot;4&quot;/&gt; --&gt; &lt;!-- A &quot;Connector&quot; represents an endpoint by which requests are received and responses are returned. Documentation at : Java HTTP Connector: /docs/config/http.html (blocking &amp; non-blocking) Java AJP Connector: /docs/config/ajp.html APR (HTTP/AJP) Connector: /docs/apr.html Define a non-SSL HTTP/1.1 Connector on port 8080 --&gt; &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; &lt;!-- A &quot;Connector&quot; using the shared thread pool--&gt; &lt;!-- &lt;Connector executor=&quot;tomcatThreadPool&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; --&gt; &lt;!-- Define a SSL HTTP/1.1 Connector on port 8443 This connector uses the BIO implementation that requires the JSSE style configuration. When using the APR/native implementation, the OpenSSL style configuration is required as described in the APR/native documentation --&gt; &lt;!-- &lt;Connector port=&quot;8443&quot; protocol=&quot;org.apache.coyote.http11.Http11Protocol&quot; maxThreads=&quot;150&quot; SSLEnabled=&quot;true&quot; scheme=&quot;https&quot; secure=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; /&gt; --&gt; &lt;!-- Define an AJP 1.3 Connector on port 8009 --&gt; &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; &lt;!-- An Engine represents the entry point (within Catalina) that processes every request. The Engine implementation for Tomcat stand alone analyzes the HTTP headers included with the request, and passes them on to the appropriate Host (virtual host). Documentation at /docs/config/engine.html --&gt; &lt;!-- You should set jvmRoute to support load-balancing via AJP ie : &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot; jvmRoute=&quot;jvm1&quot;&gt; --&gt; &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; &lt;!--For clustering, please take a look at documentation at: /docs/cluster-howto.html (simple how to) /docs/config/cluster.html (reference documentation) --&gt; &lt;!-- &lt;Cluster className=&quot;org.apache.catalina.ha.tcp.SimpleTcpCluster&quot;/&gt; --&gt; &lt;!-- Use the LockOutRealm to prevent attempts to guess user passwords via a brute-force attack --&gt; &lt;Realm className=&quot;org.apache.catalina.realm.LockOutRealm&quot;&gt; &lt;!-- This Realm uses the UserDatabase configured in the global JNDI resources under the key &quot;UserDatabase&quot;. Any edits that are performed against this UserDatabase are immediately available for use by the Realm. --&gt; &lt;Realm className=&quot;org.apache.catalina.realm.UserDatabaseRealm&quot; resourceName=&quot;UserDatabase&quot;/&gt; &lt;/Realm&gt; &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;!-- SingleSignOn valve, share authentication between web applications Documentation at: /docs/config/valve.html --&gt; &lt;!-- &lt;Valve className=&quot;org.apache.catalina.authenticator.SingleSignOn&quot; /&gt; --&gt; &lt;!-- Access log processes all example. Documentation at: /docs/config/valve.html Note: The pattern used is equivalent to using pattern=&quot;common&quot; --&gt; &lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log.&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &quot;%r&quot; %s %b&quot; /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt; &lt;/Server&gt;由上可得出Tomcat的体系结构：图一：Tomcat的体系结构 由上图可看出Tomca的心脏是两个组件：Connecter和Container。一个Container可以选择多个Connecter，多个Connector和一个Container就形成了一个Service。Service可以对外提供服务，而Server服务器控制整个Tomcat的生命周期。 Tomcat Server处理一个HTTP请求的过程图三：Tomcat Server处理一个HTTP请求的过程 Tomcat Server处理一个HTTP请求的过程 1、用户点击网页内容，请求被发送到本机端口8080，被在那里监听的Coyote HTTP/1.1 Connector获得。 2、Connector把该请求交给它所在的Service的Engine来处理，并等待Engine的回应。 3、Engine获得请求localhost/test/index.jsp，匹配所有的虚拟主机Host。 4、Engine匹配到名为localhost的Host（即使匹配不到也把请求交给该Host处理，因为该Host被定义为该Engine的默认主机），名为localhost的Host获得请求/test/index.jsp，匹配它所拥有的所有的Context。Host匹配到路径为/test的Context（如果匹配不到就把该请求交给路径名为“ ”的Context去处理）。 5、path=“/test”的Context获得请求/index.jsp，在它的mapping table中寻找出对应的Servlet。Context匹配到URL PATTERN为*.jsp的Servlet,对应于JspServlet类。 6、构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用JspServlet的doGet（）或doPost（）.执行业务逻辑、数据存储等程序。 7、Context把执行完之后的HttpServletResponse对象返回给Host。 8、Host把HttpServletResponse对象返回给Engine。 9、Engine把HttpServletResponse对象返回Connector。 10、Connector把HttpServletResponse对象返回给客户Browser。 参考文章http://www.360doc.com/content/10/0730/19/61151_42573873.shtmlhttps://my.oschina.net/leamon/blog/210133https://www.cnblogs.com/xll1025/p/11366264.htmlhttps://www.cnblogs.com/small-boy/p/8042860.html 微信公众号微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界3：JDBC的进化与连接池技术]]></title>
    <url>%2F2019%2F10%2F16%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C4%EF%BC%9AServlet%20%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） 什么是ServletServlet的作用是为Java程序提供一个统一的web应用的规范，方便程序员统一的使用这种规范来编写程序，应用容器可以使用提供的规范来实现自己的特性。比如tomcat的代码和jetty的代码就不一样，但作为程序员你只需要了解servlet规范就可以从request中取值，你可以操作session等等。不用在意应用服务器底层的实现的差别而影响你的开发。 HTTP 协议只是一个规范，定义服务请求和响应的大致式样。Java servlet 类将HTTP中那些低层的结构包装在 Java 类中，这些类所包含的便利方法使其在 Java 语言环境中更易于处理。 正如您正使用的特定 servlet 容器的配置文件中所定义的，当用户通过 URL 发出一个请求时，这些 Java servlet 类就将之转换成一个 HttpServletRequest，并发送给 URL 所指向的目标。当服务器端完成其工作时，Java 运行时环境（Java Runtime Environment）就将结果包装在一个 HttpServletResponse 中，然后将原 HTTP 响应送回给发出该请求的客户机。在与 Web 应用程序进行交互时，通常会发出多个请求并获得多个响应。所有这些都是在一个会话语境中，Java 语言将之包装在一个 HttpSession 对象中。在处理响应时，您可以访问该对象，并在创建响应时向其添加事件。它提供了一些跨请求的语境。 容器（如 Tomcat）将为 servlet 管理运行时环境。您可以配置该容器，定制 J2EE 服务器的工作方式，以便将 servlet 暴露给外部世界。正如我们将看到的，通过该容器中的各种配置文件，您在 URL（由用户在浏览器中输入）与服务器端组件之间搭建了一座桥梁，这些组件将处理您需要该 URL 转换的请求。在运行应用程序时，该容器将加载并初始化 servlet，管理其生命周期。 Servlet体系结构 Servlet顶级类关联图 Servlet Servlet的框架是由两个Java包组成的：javax.servlet与javax.servlet.http。在javax.servlet包中定义了所有的Servlet类都必须实现或者扩展的通用接口和类。在javax.servlet.http包中定义了采用Http协议通信的HttpServlet类。Servlet的框架的核心是javax.servlet.Servlet接口，所有的Servlet都必须实现这个接口。 Servlet接口 在Servlet接口中定义了5个方法： 123451\. init(ServletConfig)方法：负责初始化Servlet对象，在Servlet的生命周期中，该方法执行一次；该方法执行在单线程的环境下，因此开发者不用考虑线程安全的问题；2\. service(ServletRequest req,ServletResponse res)方法：负责响应客户的请求；为了提高效率，Servlet规范要求一个Servlet实例必须能够同时服务于多个客户端请求，即service()方法运行在多线程的环境下，Servlet开发者必须保证该方法的线程安全性；3\. destroy()方法：当Servlet对象退出生命周期时，负责释放占用的资源；4\. getServletInfo：就是字面意思，返回Servlet的描述；5\. getServletConfig：这个方法返回由Servlet容器传给init方法的ServletConfig。 ServletRequest &amp; ServletResponse 对于每一个HTTP请求，servlet容器会创建一个封装了HTTP请求的ServletRequest实例传递给servlet的service方法，ServletResponse则表示一个Servlet响应，其隐藏了将响应发给浏览器的复杂性。通过ServletRequest的方法你可以获取一些请求相关的参数，而ServletResponse则可以将设置一些返回参数信息，并且设置返回内容。 ServletConfig ServletConfig封装可以通过@WebServlet或者web.xml传给一个Servlet的配置信息，以这种方式传递的每一条信息都称做初始化信息，初始化信息就是一个个K-V键值对。为了从一个Servlet内部获取某个初始参数的值，init方法中调用ServletConfig的getinitParameter方法或getinitParameterNames方法获取，除此之外，还可以通过getServletContext获取ServletContext对象。 ServletContext ServletContext是代表了Servlet应用程序。每个Web应用程序只有一个context。在分布式环境中，一个应用程序同时部署到多个容器中，并且每台Java虚拟机都有一个ServletContext对象。有了ServletContext对象后，就可以共享能通过应用程序的所有资源访问的信息，促进Web对象的动态注册，共享的信息通过一个内部Map中的对象保存在ServiceContext中来实现。保存在ServletContext中的对象称作属性。操作属性的方法： GenericServlet 前面编写的Servlet应用中通过实现Servlet接口来编写Servlet，但是我们每次都必须为Servlet中的所有方法都提供实现，还需要将ServletConfig对象保存到一个类级别的变量中，GenericServlet抽象类就是为了为我们省略一些模板代码，实现了Servlet和ServletConfig，完成了一下几个工作： 将init方法中的ServletConfig赋给一个类级变量，使的可以通过getServletConfig来获取。 1234public void init(ServletConfig config) throws ServletException &#123; this.config = config; this.init();&#125; 同时为避免覆盖init方法后在子类中必须调用super.init(servletConfig)，GenericServlet还提供了一个不带参数的init方法，当ServletConfig赋值完成就会被第带参数的init方法调用。这样就可以通过覆盖不带参数的init方法编写初始化代码，而ServletConfig实例依然得以保存 为Servlet接口中的所有方法提供默认实现。 提供方法来包装ServletConfig中的方法。 HTTPServlet 在编写Servlet应用程序时，大多数都要用到HTTP，也就是说可以利用HTTP提供的特性，javax.servlet.http包含了编写Servlet应用程序的类和接口，其中很多覆盖了javax.servlet中的类型，我们自己在编写应用时大多时候也是继承的HttpServlet。 Servlet工作原理当Web服务器接收到一个HTTP请求时，它会先判断请求内容——如果是静态网页数据，Web服务器将会自行处理，然后产生响应信息；如果牵涉到动态数据，Web服务器会将请求转交给Servlet容器。此时Servlet容器会找到对应的处理该请求的Servlet实例来处理，结果会送回Web服务器，再由Web服务器传回用户端。 针对同一个Servlet，Servlet容器会在第一次收到http请求时建立一个Servlet实例，然后启动一个线程。第二次收到http请求时，Servlet容器无须建立相同的Servlet实例，而是启动第二个线程来服务客户端请求。所以多线程方式不但可以提高Web应用程序的执行效率，也可以降低Web服务器的系统负担。 Web服务器工作流程 接着我们描述一下Tomcat与Servlet是如何工作的，首先看下面的时序图： Servlet工作原理时序图 Web Client 向Servlet容器（Tomcat）发出Http请求； Servlet容器接收Web Client的请求； Servlet容器创建一个HttpRequest对象，将Web Client请求的信息封装到这个对象中； Servlet容器创建一个HttpResponse对象； Servlet容器调用HttpServlet对象的service方法，把HttpRequest对象与HttpResponse对象作为参数传给 HttpServlet对象； HttpServlet调用HttpRequest对象的有关方法，获取Http请求信息； HttpServlet调用HttpResponse对象的有关方法，生成响应数据； Servlet容器把HttpServlet的响应结果传给Web Client； Servlet生命周期在Servlet接口中定义了5个方法，其中3个方法代表了Servlet的生命周期： 1231\. init(ServletConfig)方法：负责初始化Servlet对象，在Servlet的生命周期中，该方法执行一次；该方法执行在单线程的环境下，因此开发者不用考虑线程安全的问题；2\. service(ServletRequest req,ServletResponse res)方法：负责响应客户的请求；为了提高效率，Servlet规范要求一个Servlet实例必须能够同时服务于多个客户端请求，即service()方法运行在多线程的环境下，Servlet开发者必须保证该方法的线程安全性；3\. destroy()方法：当Servlet对象退出生命周期时，负责释放占用的资源； 编程注意事项说明： 当Server Thread线程执行Servlet实例的init()方法时，所有的Client Service Thread线程都不能执行该实例的service()方法，更没有线程能够执行该实例的destroy()方法，因此Servlet的init()方法是工作在单线程的环境下，开发者不必考虑任何线程安全的问题。 当服务器接收到来自客户端的多个请求时，服务器会在单独的Client Service Thread线程中执行Servlet实例的service()方法服务于每个客户端。此时会有多个线程同时执行同一个Servlet实例的service()方法，因此必须考虑线程安全的问题。 虽然service()方法运行在多线程的环境下，并不一定要同步该方法。而是要看这个方法在执行过程中访问的资源类型及对资源的访问方式。分析如下： 1234567891\. 如果service()方法没有访问Servlet的成员变量也没有访问全局的资源比如静态变量、文件、数据库连接等，而是只使用了当前线程自己的资源，比如非指向全局资源的临时变量、request和response对象等。该方法本身就是线程安全的，不必进行任何的同步控制。2\. 如果service()方法访问了Servlet的成员变量，但是对该变量的操作是只读操作，该方法本身就是线程安全的，不必进行任何的同步控制。3\. 如果service()方法访问了Servlet的成员变量，并且对该变量的操作既有读又有写，通常需要加上同步控制语句。4\. 如果service()方法访问了全局的静态变量，如果同一时刻系统中也可能有其它线程访问该静态变量，如果既有读也有写的操作，通常需要加上同步控制语句。5\. 如果service()方法访问了全局的资源，比如文件、数据库连接等，通常需要加上同步控制语句。 在创建一个 Java servlet 时，一般需要子类 HttpServlet。该类中的方法允许您访问请求和响应包装器（wrapper），您可以用这个包装器来处理请求和创建响应。Servlet的生命周期，简单的概括这就分为四步： 1Servlet类加载---&gt;实例化---&gt;服务---&gt;销毁； Servlet生命周期 创建Servlet对象的时机： 默认情况下，在Servlet容器启动后：客户首次向Servlet发出请求，Servlet容器会判断内存中是否存在指定的Servlet对象，如果没有则创建它，然后根据客户的请求创建HttpRequest、HttpResponse对象，从而调用Servlet对象的service方法； Servlet容器启动时：当web.xml文件中如果元素中指定了子元素时，Servlet容器在启动web服务器时，将按照顺序创建并初始化Servlet对象； Servlet的类文件被更新后，重新创建Servlet。Servlet容器在启动时自动创建Servlet，这是由在web.xml文件中为Servlet设置的属性决定的。从中我们也能看到同一个类型的Servlet对象在Servlet容器中以单例的形式存在； 注意：在web.xml文件中，某些Servlet只有元素，没有元素，这样我们无法通过url的方式访问这些Servlet，这种Servlet通常会在元素中配置一个子元素，让容器在启动的时候自动加载这些Servlet并调用init(ServletConfig config)方法来初始化该Servlet。其中方法参数config中包含了Servlet的配置信息，比如初始化参数，该对象由服务器创建。 销毁Servlet对象的时机： Servlet容器停止或者重新启动：Servlet容器调用Servlet对象的destroy方法来释放资源。以上所讲的就是Servlet对象的生命周期。那么Servlet容器如何知道创建哪一个Servlet对象？Servlet对象如何配置？实际上这些信息是通过读取web.xml配置文件来实现的。 12345678910111213141516171819202122232425262728&lt;servlet&gt; &lt;!-- Servlet对象的名称 --&gt; &lt;servlet-name&gt;action&lt;servlet-name&gt; &lt;!-- 创建Servlet对象所要调用的类 --&gt; &lt;servlet-class&gt;org.apache.struts.action.ActionServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;!-- 参数名称 --&gt; &lt;param-name&gt;config&lt;/param-name&gt; &lt;!-- 参数值 --&gt; &lt;param-value&gt;/WEB-INF/struts-config.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;detail&lt;/param-name&gt; &lt;param-value&gt;2&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;2&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- Servlet容器启动时加载Servlet对象的顺序 --&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- 要与servlet中的servlet-name配置节内容对应 --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;action&lt;/servlet-name&gt; &lt;!-- 客户访问的Servlet的相对URL路径 --&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当Servlet容器启动的时候读取配置节信息，根据配置节信息创建Servlet对象，同时根据配置节信息创建HttpServletConfig对象，然后执行Servlet对象的init方法，并且根据配置节信息来决定创建Servlet对象的顺序，如果此配置节信息为负数或者没有配置，那么在Servlet容器启动时，将不加载此Servlet对象。当客户访问Servlet容器时，Servlet容器根据客户访问的URL地址，通过配置节中的配置节信息找到指定的Servlet对象，并调用此Servlet对象的service方法。 在整个Servlet的生命周期过程中，创建Servlet实例、调用实例的init()和destroy()方法都只进行一次，当初始化完成后，Servlet容器会将该实例保存在内存中，通过调用它的service()方法，为接收到的请求服务。下面给出Servlet整个生命周期过程的UML序列图，如图所示： Servlet生命周期 如果需要让Servlet容器在启动时即加载Servlet，可以在web.xml文件中配置元素。 Servlet中的ListenerListener 使用的非常广泛，它是基于观察者模式设计的，Listener 的设计对开发 Servlet 应用程序提供了一种快捷的手段，能够方便的从另一个纵向维度控制程序和数据。目前 Servlet 中提供了 5 种两类事件的观察者接口，它们分别是：4 个 EventListeners 类型的，ServletContextAttributeListener、ServletRequestAttributeListener、ServletRequestListener、HttpSessionAttributeListener 和 2 个 LifecycleListeners 类型的，ServletContextListener、HttpSessionListener。如下图所示： Servlet中的Listener 它们基本上涵盖了整个 Servlet 生命周期中，你感兴趣的每种事件。这些 Listener 的实现类可以配置在 web.xml 中的 标签中。当然也可以在应用程序中动态添加 Listener，需要注意的是 ServletContextListener 在容器启动之后就不能再添加新的，因为它所监听的事件已经不会再出现。掌握这些 Listener 的使用，能够让我们的程序设计的更加灵活。 Cookie与SessionServlet 能够给我们提供两部分数据，一个是在 Servlet 初始化时调用 init 方法时设置的 ServletConfig，这个类基本上含有了 Servlet 本身和 Servlet 所运行的 Servlet 容器中的基本信息。还有一部分数据是由 ServletRequest 类提供，从提供的方法中发现主要是描述这次请求的 HTTP 协议的信息。关于这一块还有一个让很多人迷惑的 Session 与 Cookie。 Session 与 Cookie 的作用都是为了保持访问用户与后端服务器的交互状态。它们有各自的优点也有各自的缺陷。然而具有讽刺意味的是它们优点和它们的使用场景又是矛盾的，例如使用 Cookie 来传递信息时，随着 Cookie 个数的增多和访问量的增加，它占用的网络带宽也也会越来越大。所以大访问量的时候希望用 Session，但是 Session 的致命弱点是不容易在多台服务器之间共享，所以这也限制了 Session 的使用。 不管 Session 和 Cookie 有什么不足，我们还是要用它们。下面详细讲一下，Session 如何基于 Cookie 来工作。实际上有三种方式能可以让 Session 正常工作： 基于 URL Path Parameter，默认就支持 基于 Cookie，如果你没有修改 Context 容器个 cookies 标识的话，默认也是支持的 基于 SSL，默认不支持，只有 connector.getAttribute(“SSLEnabled”) 为 TRUE 时才支持 第一种情况下，当浏览器不支持 Cookie 功能时，浏览器会将用户的 SessionCookieName 重写到用户请求的 URL 参数中，它的传递格式如： 1/path/Servlet?name=value&amp;name2=value2&amp;JSESSIONID=value3 接着 Request 根据这个 JSESSIONID 参数拿到 Session ID 并设置到 request.setRequestedSessionId 中。 请注意如果客户端也支持 Cookie 的话，Tomcat 仍然会解析 Cookie 中的 Session ID，并会覆盖 URL 中的 Session ID。 如果是第三种情况的话将会根据 javax.servlet.request.ssl_session 属性值设置 Session ID。 有了 Session ID 服务器端就可以创建 HttpSession 对象了，第一次触发是通过 request. getSession() 方法，如果当前的 Session ID 还没有对应的 HttpSession 对象那么就创建一个新的，并将这个对象加到 org.apache.catalina. Manager 的 sessions 容器中保存，Manager 类将管理所有 Session 的生命周期，Session 过期将被回收，服务器关闭，Session 将被序列化到磁盘等。只要这个 HttpSession 对象存在，用户就可以根据 Session ID 来获取到这个对象，也就达到了状态的保持。 Session相关类图 上从图中可以看出从 request.getSession 中获取的 HttpSession 对象实际上是 StandardSession 对象的门面对象，这与前面的 Request 和 Servlet 是一样的原理。下图是 Session 工作的时序图： Session工作的时序图 还有一点与 Session 关联的 Cookie 与其它 Cookie 没有什么不同，这个配置的配置可以通过 web.xml 中的 session-config 配置项来指定。 参考文章https://segmentfault.com/a/1190000009707894 https://www.cnblogs.com/hysum/p/7100874.html http://c.biancheng.net/view/939.html https://www.runoob.com/ https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>JDBC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界2：JSP与Servlet的曾经与现在]]></title>
    <url>%2F2019%2F10%2F15%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C2%EF%BC%9AJSP%E4%B8%8EServlet%E7%9A%84%E6%9B%BE%E7%BB%8F%E4%B8%8E%E7%8E%B0%E5%9C%A8%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） jsp作为Servlet技术的扩展，经常会有人将jsp和Servlet搞混。本文，将为大家带来servlet和jsp的区别，希望对大家有所帮助。 servlet和jsp的区别1、Servlet在Java代码中可以通过HttpServletResponse对象动态输出HTML内容。 2、JSP是在静态HTML内容中嵌入Java代码，然后Java代码在被动态执行后生成HTML内容。 servlet和jsp各自的特点1、Servlet虽然能够很好地组织业务逻辑代码，但是在Java源文件中，因为是通过字符串拼接的方式生成动态HTML内容，这样就容易导致代码维护困难、可读性差。 2、JSP虽然规避了Servlet在生成HTML内容方面的劣势，但是在HTML中混入大量、复杂的业务逻辑。 通过MVC双剑合璧JSP和Servlet都有自身的适用环境，那么有没有什么办法能够让它们发挥各自的优势呢？答案是肯有的，MVC模式就能够完美解决这一问题。 MVC模式，是Model-View-Controller的简称，是软件工程中的一种软件架构模式，分为三个基本部分，分别是：模型（Model）、视图（View）和控制器（Controller）： Controller——负责转发请求，对请求进行处理 View——负责界面显示 Model——业务功能编写（例如算法实现）、数据库设计以及数据存取操作实现 在JSP/Servlet开发的软件系统中，这三个部分的描述如下所示： 1、Web浏览器发送HTTP请求到服务端，然后被Controller(Servlet)获取并进行处理（例如参数解析、请求转发） 2、Controller(Servlet)调用核心业务逻辑——Model部分，获得结果 3、Controller(Servlet)将逻辑处理结果交给View（JSP），动态输出HTML内容 4、动态生成的HTML内容返回到浏览器显示 MVC模式在Web开发中有很大的优势，它完美规避了JSP与Servlet各自的缺点，让Servlet只负责业务逻辑部分，而不会生成HTML代码；同时JSP中也不会充斥着大量的业务代码，这样能大提高了代码的可读性和可维护性。 JavaWeb基础知识一、Servlet 是什么？Java Servlet 是运行在 Web 服务器或应用服务器上的程序，它是作为来自 Web 浏览器或其他 HTTP 客户端的请求和 HTTP 服务器上的数据库或应用程序之间的中间层。 使用 Servlet，您可以收集来自网页表单的用户输入，呈现来自数据库或者其他源的记录，还可以动态创建网页。 Java Servlet 通常情况下与使用 CGI（Common Gateway Interface，公共网关接口）实现的程序可以达到异曲同工的效果。但是相比于 CGI，Servlet 有以下几点优势： 1、性能明显更好。 2、Servlet 在 Web 服务器的地址空间内执行。这样它就没有必要再创建一个单独的进程来处理每个客户端请求。 3、Servlet 是独立于平台的，因为它们是用 Java 编写的。 4、服务器上的 Java 安全管理器执行了一系列限制，以保护服务器计算机上的资源。因此，Servlet 是可信的。 5、Java 类库的全部功能对 Servlet 来说都是可用的。它可以通过 sockets 和 RMI 机制与 applets、数据库或其他软件进行交互。 二、Servlet的生命周期Servlet 生命周期可被定义为从创建直到毁灭的整个过程。以下是 Servlet 遵循的过程： 1、Servlet 通过调用 init () 方法进行初始化。 2、Servlet 调用 service() 方法来处理客户端的请求。 3、Servlet 通过调用 destroy() 方法终止（结束）。 4、最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。 init() 方法init 方法被设计成只调用一次。它在第一次创建 Servlet 时被调用，在后续每次用户请求时不再调用。因此，它是用于一次性初始化，就像 Applet 的 init 方法一样。 Servlet 创建于用户第一次调用对应于该 Servlet 的 URL 时，但是您也可以指定 Servlet 在服务器第一次启动时被加载。 service() 方法service() 方法是执行实际任务的主要方法。Servlet 容器（即 Web 服务器）调用 service() 方法来处理来自客户端（浏览器）的请求，并把格式化的响应写回给客户端。 每次服务器接收到一个 Servlet 请求时，服务器会产生一个新的线程并调用服务。service() 方法检查 HTTP 请求类型（GET、POST、PUT、DELETE 等），并在适当的时候调用 doGet、doPost、doPut，doDelete 等方法。 destroy() 方法destroy() 方法只会被调用一次，在 Servlet 生命周期结束时被调用。destroy() 方法可以让您的 Servlet 关闭数据库连接、停止后台线程、把 Cookie 列表或点击计数器写入到磁盘，并执行其他类似的清理活动。 在调用 destroy() 方法之后，servlet 对象被标记为垃圾回收。 示例 执行后： 以后继续请求时： 可见，就绪请求时只有service()方法执行！ 相关面试题怎样理解Servlet的单实例多线程？**不同的用户同时对同一个业务（如注册）发出请求，那这个时候容器里产生的有是几个servlet实例呢？ 答案是：只有一个servlet实例。一个servlet是在第一次被访问时加载到内存并实例化的。同样的业务请求共享一个servlet实例。不同的业务请求一般对应不同的servlet。 由于Servlet/JSP默认是以多线程模式执行的，所以，在编写代码时需要非常细致地考虑多线程的安全性问题。 JSP的中存在的多线程问题：当客户端第一次请求某一个JSP文件时，服务端把该JSP编译成一个CLASS文件，并创建一个该类的实例，然后创建一个线程处理CLIENT端的请求。如果有多个客户端同时请求该JSP文件，则服务端会创建多个线程。每个客户端请求对应一个线程。以多线程方式执行可大大降低对系统的资源需求,提高系统的并发量及响应时间。 对JSP中可能用的的变量说明如下: 实例变量: 实例变量是在堆中分配的,并被属于该实例的所有线程共享，所以不是线程安全的。 JSP系统提供的8个类变量 JSP中用到的OUT,REQUEST,RESPONSE,SESSION,CONFIG,PAGE,PAGECONXT是线程安全的(因为每个线程对应的request，respone对象都是不一样的，不存在共享问题),APPLICATION在整个系统内被使用,所以不是线程安全的。 局部变量: 局部变量在堆栈中分配,因为每个线程都有它自己的堆栈空间,所以是线程安全的 静态类: 静态类不用被实例化,就可直接使用,也不是线程安全的 外部资源: 在程序中可能会有多个线程或进程同时操作同一个资源(如:多个线程或进程同时对一个文件进行写操作).此时也要注意同步问题. Servlet单实例多线程机制： Servlet采用多线程来处理多个请求同时访问。servlet依赖于一个线程池来服务请求。线程池实际上是一系列的工作者线程集合。Servlet使用一个调度线程来管理工作者线程。 当容器收到一个Servlet请求，调度线程从线程池中选出一个工作者线程,将请求传递给该工作者线程，然后由该线程来执行Servlet的service方法。 当这个线程正在执行的时候,容器收到另外一个请求,调度线程同样从线程池中选出另一个工作者线程来服务新的请求,容器并不关心这个请求是否访问的是同一个Servlet.当容器同时收到对同一个Servlet的多个请求的时候，那么这个Servlet的service()方法将在多线程中并发执行。 Servlet容器默认采用单实例多线程的方式来处理请求，这样减少产生Servlet实例的开销，提升了对请求的响应时间，对于Tomcat可以在server.xml中通过元素设置线程池中线程的数目。 如何开发线程安全的Servlet1、实现 SingleThreadModel 接口 该接口指定了系统如何处理对同一个Servlet的调用。如果一个Servlet被这个接口指定,那么在这个Servlet中的service方法将不会有两个线程被同时执行，当然也就不存在线程安全的问题。这种方法只要将前面的Concurrent Test类的类头定义更改为： Public class Concurrent Test extends HttpServlet implements SingleThreadModel { ………… } 同步对共享数据的操作使用synchronized 关键字能保证一次只有一个线程可以访问被保护的区段 避免使用实例变量 本实例中的线程安全问题是由实例变量造成的，只要在Servlet里面的任何方法里面都不使用实例变量，那么该Servlet就是线程安全的。 1) Struts2的Action是原型，非单实例的；会对每一个请求,产生一个Action的实例来处理 Struts1 Action是单实例的 mvc的controller也是如此。因此开发时要求必须是线程安全的，因为仅有Action的一个实例来处理所有的请求。单例策略限制了Struts1 Action能作的事，并且要在开发时特别小心。Action资源必须是线程安全的或同步的。 2) Struts1的Action,Spring的Ioc容器管理的bean 默认是单实例的. Spring的Ioc容器管理的bean 默认是单实例的。 Struts2 Action对象为每一个请求产生一个实例，因此没有线程安全问题。（实际上，servlet容器给每个请求产生许多可丢弃的对象，并且不会导致性能和垃圾回收问题）。 当Spring管理Struts2的Action时，bean默认是单实例的，可以通过配置参数将其设置为原型。(scope=”prototype ） 五、servlet与jsp的区别1.jsp经编译后就变成了Servlet.(JSP的本质就是Servlet，JVM只能识别java的类，不能识别JSP的代码,Web容器将JSP的代码编译成JVM能够识别的java类) 2.jsp更擅长表现于页面显示,servlet更擅长于逻辑控制. 3.Servlet中没有内置对象，内置对象都是必须通过HttpServletRequest对象，HttpServletResponse对象以及HttpServlet对象得到.Jsp是Servlet的一种简化，使用Jsp只需要完成程序员需要输出到客户端的内容，Jsp中的Java脚本如何镶嵌到一个类中，由Jsp容器完成。而Servlet则是个完整的Java类，这个类的Service方法用于生成对客户端的响应。 4.对于静态HTML标签，Servlet都必须使用页面输出流逐行输出 参考文章https://www.w3cschool.cn/servlet/servlet-sxoy2p19.htmlhttps://blog.csdn.net/qq_19782019/article/details/80292110https://blog.csdn.net/qiuhuang_123/article/details/83617647https://blog.csdn.net/zt15732625878/article/details/79951933https://blog.csdn.net/android_hl/article/details/53228348 微信公众号微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Jsp</tag>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南15：Fork join并发框架与工作窃取算法剖析]]></title>
    <url>%2F2019%2F10%2F14%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9715%EF%BC%9AFork%20join%E5%B9%B6%E5%8F%91%E6%A1%86%E6%9E%B6%E4%B8%8E%E5%B7%A5%E4%BD%9C%E7%AA%83%E5%8F%96%E7%AE%97%E6%B3%95%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.imooc.com/article/24822 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Fork/Join框架介绍Fork/Join框架是Java7提供的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架。使用工作窃取（work-stealing）算法，主要用于实现“分而治之”。 简介通常，使用Java来开发一个简单的并发应用程序时，会创建一些Runnable对象，然后创建对应的Thread 对象来控制程序中这些线程的创建、执行以及线程的状态。自从Java 5开始引入了Executor和ExecutorService接口以及实现这两个接口的类（比如ThreadPoolExecutor）之后，使得Java在并发支持上得到了进一步的提升。 执行器框架（Executor Framework）将任务的创建和执行进行了分离，通过这个框架，只需要实现Runnable接口的对象和使用Executor对象，然后将Runnable对象发送给执行器。执行器再负责运行这些任务所需要的线程，包括线程的创建，线程的管理以及线程的结束。 Java 7则又更进了一步，它包括了ExecutorService接口的另一种实现，用来解决特殊类型的问题，它就是Fork/Join框架，有时也称分解/合并框架。 Fork/Join框架是用来解决能够通过分治技术（Divide and Conquer Technique）将问题拆分成小任务的问题。在一个任务中，先检查将要解决的问题的大小，如果大于一个设定的大小，那就将问题拆分成可以通过框架来执行的小任务。如果问题的大小比设定的大小要小，就可以直接在任务里解决这个问题，然后，根据需要返回任务的结果。下面的图形总结了这个原理。 没有固定的公式来决定问题的参考大小（Reference Size），从而决定一个任务是需要进行拆分或不需要拆分，拆分与否仍是依赖于任务本身的特性。可以使用在任务中将要处理的元素的数目和任务执行所需要的时间来决定参考大小。测试不同的参考大小来决定解决问题最好的一个方案，将ForkJoinPool类看作一个特殊的 Executor 执行器类型。这个框架基于以下两种操作。 分解（Fork）操作：当需要将一个任务拆分成更小的多个任务时，在框架中执行这些任务； 合并（Join）操作：当一个主任务等待其创建的多个子任务的完成执行。 Fork/Join框架和执行器框架（Executor Framework）主要的区别在于工作窃取算法（Work-Stealing Algorithm）。与执行器框架不同，使用Join操作让一个主任务等待它所创建的子任务的完成，执行这个任务的线程称之为工作者线程（Worker Thread）。工作者线程寻找其他仍未被执行的任务，然后开始执行。通过这种方式，这些线程在运行时拥有所有的优点，进而提升应用程序的性能。 为了达到这个目标，通过Fork/Join框架执行的任务有以下限制。 任务只能使用fork()和join() 操作当作同步机制。如果使用其他的同步机制，工作者线程就不能执行其他任务，当然这些任务是在同步操作里时。比如，如果在Fork/Join 框架中将一个任务休眠，正在执行这个任务的工作者线程在休眠期内不能执行另一个任务。 任务不能执行I/O操作，比如文件数据的读取与写入。 任务不能抛出非运行时异常（Checked Exception），必须在代码中处理掉这些异常。 Fork/Join**框架**的核心是由下列两个类组成的。 ForkJoinPool：这个类实现了ExecutorService接口和工作窃取算法（Work-Stealing Algorithm）。它管理工作者线程，并提供任务的状态信息，以及任务的执行信息。 ForkJoinTask：这个类是一个将在ForkJoinPool中执行的任务的基类。 Fork/Join框架提供了在一个任务里执行fork()和join()操作的机制和控制任务状态的方法。通常，为了实现Fork/Join任务，需要实现一个以下两个类之一的子类。 RecursiveAction：用于任务没有返回结果的场景。 RecursiveTask：用于任务有返回结果的场景。 工作窃取算法介绍工作窃取（work-stealing）算法优点是充分利用线程进行并行计算，并减少了线程间的竞争，其缺点是在某些情况下还是存在竞争，比如双端队列里只有一个任务时。并且消耗了更多的系统资源，比如创建多个线程和多个双端队列。 Fork/Join框架基础类 ForkJoinPool： 用来执行Task，或生成新的ForkJoinWorkerThread，执行 ForkJoinWorkerThread 间的 work-stealing 逻辑。ForkJoinPool 不是为了替代 ExecutorService，而是它的补充，在某些应用场景下性能比 ExecutorService 更好。 ForkJoinTask： 执行具体的分支逻辑，声明以同步/异步方式进行执行 ForkJoinWorkerThread： 是 ForkJoinPool 内的 worker thread，执行 ForkJoinTask, 内部有 ForkJoinPool.WorkQueue来保存要执行的ForkJoinTask。 ForkJoinPool.WorkQueue：保存要执行的ForkJoinTask。 基本思想 ForkJoinPool 的每个工作线程都维护着一个工作队列（WorkQueue），这是一个双端队列（Deque），里面存放的对象是任务（ForkJoinTask）。 每个工作线程在运行中产生新的任务（通常是因为调用了 fork()）时，会放入工作队列的队尾，并且工作线程在处理自己的工作队列时，使用的是 LIFO 方式，也就是说每次从队尾取出任务来执行。 每个工作线程在处理自己的工作队列同时，会尝试窃取一个任务（或是来自于刚刚提交到 pool 的任务，或是来自于其他工作线程的工作队列），窃取的任务位于其他线程的工作队列的队首，也就是说工作线程在窃取其他工作线程的任务时，使用的是 FIFO 方式。 在遇到 join() 时，如果需要 join 的任务尚未完成，则会先处理其他任务，并等待其完成。 在既没有自己的任务，也没有可以窃取的任务时，进入休眠。 代码演示 大家学习时，通常借助的例子都类似于下面这段： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Slf4jpublic class ForkJoinTaskExample extends RecursiveTask&lt;Integer&gt; &#123; public static final int threshold = 2; private int start; private int end; public ForkJoinTaskExample(int start, int end) &#123; this.start = start; this.end = end; &#125; @Override protected Integer compute() &#123; int sum = 0; //如果任务足够小就计算任务 boolean canCompute = (end - start) &lt;= threshold; if (canCompute) &#123; for (int i = start; i &lt;= end; i++) &#123; sum += i; &#125; &#125; else &#123; // 如果任务大于阈值，就分裂成两个子任务计算 int middle = (start + end) / 2; ForkJoinTaskExample leftTask = new ForkJoinTaskExample(start, middle); ForkJoinTaskExample rightTask = new ForkJoinTaskExample(middle + 1, end); // 执行子任务 leftTask.fork(); rightTask.fork(); // 等待任务执行结束合并其结果 int leftResult = leftTask.join(); int rightResult = rightTask.join(); // 合并子任务 sum = leftResult + rightResult; &#125; return sum; &#125; public static void main(String[] args) &#123; ForkJoinPool forkjoinPool = new ForkJoinPool(); //生成一个计算任务，计算1+2+3+4 ForkJoinTaskExample task = new ForkJoinTaskExample(1, 100); //执行一个任务 Future&lt;Integer&gt; result = forkjoinPool.submit(task); try &#123; log.info(&quot;result:&#123;&#125;&quot;, result.get()); &#125; catch (Exception e) &#123; log.error(&quot;exception&quot;, e); &#125; &#125;&#125; 重点注意 需要特别注意的是： ForkJoinPool 使用submit 或 invoke 提交的区别：invoke是同步执行，调用之后需要等待任务完成，才能执行后面的代码；submit是异步执行，只有在Future调用get的时候会阻塞。 这里继承的是RecursiveTask，还可以继承RecursiveAction。前者适用于有返回值的场景，而后者适合于没有返回值的场景 这一点是最容易忽略的地方，其实这里执行子任务调用fork方法并不是最佳的选择，最佳的选择是invokeAll方法。 123456leftTask.fork(); rightTask.fork();替换为invokeAll(leftTask, rightTask); 具体说一下原理：对于Fork/Join模式，假如Pool里面线程数量是固定的，那么调用子任务的fork方法相当于A先分工给B，然后A当监工不干活，B去完成A交代的任务。所以上面的模式相当于浪费了一个线程。那么如果使用invokeAll相当于A分工给B后，A和B都去完成工作。这样可以更好的利用线程池，缩短执行的时间。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南14：JUC中常用的Unsafe和Locksupport]]></title>
    <url>%2F2019%2F10%2F14%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9714%EF%BC%9AJUC%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84Unsafe%E5%92%8CLocksupport%2F</url>
    <content type="text"><![CDATA[本文转自网络，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言最近在看Java并发包的源码，发现了神奇的Unsafe类，仔细研究了一下，在这里跟大家分享一下。 Unsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。 Unsafe类使Java拥有了像C语言的指针一样操作内存空间的能力，同时也带来了指针的问题。过度的使用Unsafe类会使得出错的几率变大，因此Java官方并不建议使用的，官方文档也几乎没有。Oracle正在计划从Java 9中去掉Unsafe类，如果真是如此影响就太大了。 通常我们最好也不要使用Unsafe类，除非有明确的目的，并且也要对它有深入的了解才行。要想使用Unsafe类需要用一些比较tricky的办法。Unsafe类使用了单例模式，需要通过一个静态方法getUnsafe()来获取。但Unsafe类做了限制，如果是普通的调用的话，它会抛出一个SecurityException异常；只有由主类加载器加载的类才能调用这个方法。其源码如下： 1 public static Unsafe getUnsafe() { 2 Class var0 = Reflection.getCallerClass(); 3 if(!VM.isSystemDomainLoader(var0.getClassLoader())) { 4 throw new SecurityException(&quot;Unsafe&quot;); 5 } else { 6 return theUnsafe; 7 } 8 }网上也有一些办法来用主类加载器加载用户代码，比如设置bootclasspath参数。但更简单方法是利用Java反射，方法如下： 1 Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); 2 f.setAccessible(true); 3 Unsafe unsafe = (Unsafe) f.get(null);获取到Unsafe实例之后，我们就可以为所欲为了。Unsafe类提供了以下这些功能： 一、内存管理。包括分配内存、释放内存等。 该部分包括了allocateMemory（分配内存）、reallocateMemory（重新分配内存）、copyMemory（拷贝内存）、freeMemory（释放内存 ）、getAddress（获取内存地址）、addressSize、pageSize、getInt（获取内存地址指向的整数）、getIntVolatile（获取内存地址指向的整数，并支持volatile语义）、putInt（将整数写入指定内存地址）、putIntVolatile（将整数写入指定内存地址，并支持volatile语义）、putOrderedInt（将整数写入指定内存地址、有序或者有延迟的方法）等方法。getXXX和putXXX包含了各种基本类型的操作。 利用copyMemory方法，我们可以实现一个通用的对象拷贝方法，无需再对每一个对象都实现clone方法，当然这通用的方法只能做到对象浅拷贝。 二、非常规的对象实例化。 allocateInstance()方法提供了另一种创建实例的途径。通常我们可以用new或者反射来实例化对象，使用allocateInstance()方法可以直接生成对象实例，且无需调用构造方法和其它初始化方法。 这在对象反序列化的时候会很有用，能够重建和设置final字段，而不需要调用构造方法。 三、操作类、对象、变量。 这部分包括了staticFieldOffset（静态域偏移）、defineClass（定义类）、defineAnonymousClass（定义匿名类）、ensureClassInitialized（确保类初始化）、objectFieldOffset（对象域偏移）等方法。 通过这些方法我们可以获取对象的指针，通过对指针进行偏移，我们不仅可以直接修改指针指向的数据（即使它们是私有的），甚至可以找到JVM已经认定为垃圾、可以进行回收的对象。 四、数组操作。 这部分包括了arrayBaseOffset（获取数组第一个元素的偏移地址）、arrayIndexScale（获取数组中元素的增量地址）等方法。arrayBaseOffset与arrayIndexScale配合起来使用，就可以定位数组中每个元素在内存中的位置。 由于Java的数组最大值为Integer.MAX_VALUE，使用Unsafe类的内存分配方法可以实现超大数组。实际上这样的数据就可以认为是C数组，因此需要注意在合适的时间释放内存。 五、多线程同步。包括锁机制、CAS操作等。 这部分包括了monitorEnter、tryMonitorEnter、monitorExit、compareAndSwapInt、compareAndSwap等方法。 其中monitorEnter、tryMonitorEnter、monitorExit已经被标记为deprecated，不建议使用。 Unsafe类的CAS操作可能是用的最多的，它为Java的锁机制提供了一种新的解决办法，比如AtomicInteger等类都是通过该方法来实现的。compareAndSwap方法是原子的，可以避免繁重的锁机制，提高代码效率。这是一种乐观锁，通常认为在大部分情况下不出现竞态条件，如果操作失败，会不断重试直到成功。 六、挂起与恢复。 这部分包括了park、unpark等方法。 将一个线程进行挂起是通过park方法实现的，调用 park后，线程将一直阻塞直到超时或者中断等条件出现。unpark可以终止一个挂起的线程，使其恢复正常。整个并发框架中对线程的挂起操作被封装在 LockSupport类中，LockSupport类中有各种版本pack方法，但最终都调用了Unsafe.park()方法。 七、内存屏障。 这部分包括了loadFence、storeFence、fullFence等方法。这是在Java 8新引入的，用于定义内存屏障，避免代码重排序。 loadFence() 表示该方法之前的所有load操作在内存屏障之前完成。同理storeFence()表示该方法之前的所有store操作在内存屏障之前完成。fullFence()表示该方法之前的所有load、store操作在内存屏障之前完成。 Unsafe类是啥？Java最初被设计为一种安全的受控环境。尽管如此，Java HotSpot还是包含了一个“后门”，提供了一些可以直接操控内存和线程的低层次操作。这个后门类——sun.misc.Unsafe——被JDK广泛用于自己的包中，如java.nio和java.util.concurrent。但是丝毫不建议在生产环境中使用这个后门。因为这个API十分不安全、不轻便、而且不稳定。这个不安全的类提供了一个观察HotSpot JVM内部结构并且可以对其进行修改。有时它可以被用来在不适用C++调试的情况下学习虚拟机内部结构，有时也可以被拿来做性能监控和开发工具。 为什么叫Unsafe？Java官方不推荐使用Unsafe类，因为官方认为，这个类别人很难正确使用，非正确使用会给JVM带来致命错误。而且未来Java可能封闭丢弃这个类。 1、简单介绍 LockSupport是JDK中比较底层的类，用来创建锁和其他同步工具类的基本线程阻塞原语。可以做到与join() 、wait()/notifyAll() 功能一样，使线程自由的阻塞、释放。 Java锁和同步器框架的核心AQS(AbstractQueuedSynchronizer 抽象队列同步器)，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的。 补充：AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch…。 2、简单原理 LockSupport方法底层都是调用Unsafe的方法实现。全名sun.misc.Unsafe，该类可以直接操控内存，被JDK广泛用于自己的包中，如java.nio和java.util.concurrent。但是不建议在生产环境中使用这个类。因为这个API十分不安全、不轻便、而且不稳定。 LockSupport提供park()和unpark()方法实现阻塞线程和解除线程阻塞，LockSupport和每个使用它的线程都与一个许可(permit)关联。permit是相当于1，0的开关，默认是0，调用一次unpark就加1变成1，调用一次park会消费permit, 也就会将1变成0，同时park立即返回。 再次调用park会变成block（因为permit为0了，会阻塞在这里，直到permit变为1）, 这时调用unpark会把permit置为1。每个线程都有一个相关的permit, permit最多只有一个，重复调用unpark也不会积累。意思就是说unpark 之后，如果permit 已经变为1，之后，再执行unpark ,permit 依旧是1。下边有例子会说到。 3、简单例子 以下边的做饭例子，正常来说，做饭 之前，要有锅、有菜才能开始做饭 。具体如下：（1）先假设已经有了锅 ，那只需要买菜就可以做饭。如下，即注释掉了买锅的步骤： public class LockSupportTest { public static void main(String[] args) throws InterruptedException { //买锅 // Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); // t1.start(); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); // LockSupport.park(); // System.out.println(&quot;锅买回来了...&quot;); LockSupport.park(); System.out.println(&quot;菜买回来 了...&quot;); System.out.println(&quot;开始做饭&quot;); } } class BuyGuo implements Runnable{ private Object threadObj; public BuyGuo(Object threadObj) { this.threadObj = threadObj; } @Override public void run() { System.out.println(&quot;去买锅...&quot;); LockSupport.unpark((Thread)threadObj); } } class BuyCai implements Runnable{ private Object threadObj; public BuyCai(Object threadObj) { this.threadObj = threadObj; } @Override public void run() { System.out.println(&quot;买菜去...&quot;); LockSupport.unpark((Thread)threadObj); } }执行后，可出现下面的结果： 买菜去…菜买回来了…开始做饭 如上所述，可以达到阻塞主线程等到买完菜之后才开始做饭。这即是park()、unpark() 的用法。简单解释一下上述的步骤： main 方法启动后，主线程 和 买菜线程 同时开始执行。因为两者同时进行，当主线程 走到park() 时，发现permit 还为0 ，即会等待在这里。当买菜线程执行进去后，走到unpark() 会将permit 变为1 。主线程 park() 处发现permit 已经变成1 ，就可以继续往下执行了，同时消费掉permit ，重新变成0 。 以上permit 只是park/unpark 执行的一种逻辑开关，执行的步骤大致如此。 4、注意点及思考（1）必须将park()与uppark() 配对使用才更高效。 如果上边也把买锅的线程放开，main 方法改为如下： //买锅 Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); t1.start(); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); LockSupport.park(); System.out.println(&quot;锅买回来了...&quot;); LockSupport.park(); System.out.println(&quot;菜买回来了...&quot;); System.out.println(&quot;开始做饭&quot;); 即调用了两次park() 和unpark() ，发现有时候可以，有时候会使线程卡在那里，然后我又换了下顺序，如下： //买锅 Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); t1.start(); LockSupport.park(); System.out.println(&quot;锅买回来了...&quot;); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); LockSupport.park(); System.out.println(&quot;菜买回来了...&quot;); System.out.println(&quot;开始做饭&quot;); 原理没有详细去研究,不过想了想，上边两种其实并无区别，只是执行顺序有了影响，park() 和unpark() 既然是成对配合使用，通过标识permit 来控制，如果像前边那个例子那样，出现阻塞的情况原因，我分析可能是这么个原因： 当买锅的时候，通过unpark()将permit 置为1，但是还没等到外边的main方法执行第一个park() ,买菜的线程又调了一次unpark(),但是这时候permit 还是从1变成了1，等回到主线程调用park()的时候，因为还有两个park()需要执行，也就是需要两个消费permit ,因为permit 只有1个，所以，可能会剩下一个park()卡在那里了。 （2）使用park(Object blocker) 方法更能明确问题 其实park() 有个重载方法park(Object blocker) ,这俩方法效果差不多，但是有blocker的可以传递给开发人员更多的现场信息，可以查看到当前线程的阻塞对象，方便定位问题。所以java6新增加带blocker入参的系列park方法，替代原有的park方法。 5、与wait()/notifyAll() 的比较LockSupport 的 park/unpark 方法，虽然与平时Object 中wait/notify 同样达到阻塞线程的效果。但是它们之间还是有区别的。 面向的对象主体不同。LockSupport() 操作的是线程对象，直接传入的就是Thread ,而wait() 属于具体对象，notifyAll() 也是针对所有线程进行唤醒。wait/notify 需要获取对象的监视器，即synchronized修饰，而park/unpark 不需要获取对象的监视器。实现的机制不同，因此两者没有交集。也就是说 LockSupport 阻塞的线程，notify/notifyAll 没法唤醒。但是 park 之后，同样可以被中断(interrupt()) ! JAVA高并发—LockSupport的学习及简单使用1、简单介绍 LockSupport是JDK中比较底层的类，用来创建锁和其他同步工具类的基本线程阻塞原语。可以做到与join() 、wait()/notifyAll() 功能一样，使线程自由的阻塞、释放。 Java锁和同步器框架的核心AQS(AbstractQueuedSynchronizer 抽象队列同步器)，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的。 补充：AQS定义了一套多线程访问共享资源的同步器框架，许多同步类实现都依赖于它，如常用的ReentrantLock/Semaphore/CountDownLatch…。 2、简单原理 LockSupport方法底层都是调用Unsafe的方法实现。全名sun.misc.Unsafe，该类可以直接操控内存，被JDK广泛用于自己的包中，如java.nio和java.util.concurrent。但是不建议在生产环境中使用这个类。因为这个API十分不安全、不轻便、而且不稳定。 LockSupport提供park()和unpark()方法实现阻塞线程和解除线程阻塞，LockSupport和每个使用它的线程都与一个许可(permit)关联。 permit是相当于1，0的开关，默认是0，调用一次unpark就加1变成1，调用一次park会消费permit, 也就会将1变成0，同时park立即返回。再次调用park会变成block（因为permit为0了，会阻塞在这里，直到permit变为1）, 这时调用unpark会把permit置为1。 每个线程都有一个相关的permit, permit最多只有一个，重复调用unpark也不会积累。意思就是说unpark 之后，如果permit 已经变为1，之后，再执行unpark ,permit 依旧是1。下边有例子会说到。 3、简单例子 以下边的做饭例子，正常来说，做饭 之前，要有锅、有菜才能开始做饭 。具体如下：（1）先假设已经有了锅 ，那只需要买菜就可以做饭。如下，即注释掉了买锅的步骤： public class LockSupportTest { public static void main(String[] args) throws InterruptedException { //买锅 // Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); // t1.start(); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); // LockSupport.park(); // System.out.println(&quot;锅买回来了...&quot;); LockSupport.park(); System.out.println(&quot;菜买回来 了...&quot;); System.out.println(&quot;开始做饭&quot;); } } class BuyGuo implements Runnable{ private Object threadObj; public BuyGuo(Object threadObj) { this.threadObj = threadObj; } @Override public void run() { System.out.println(&quot;去买锅...&quot;); LockSupport.unpark((Thread)threadObj); } } class BuyCai implements Runnable{ private Object threadObj; public BuyCai(Object threadObj) { this.threadObj = threadObj; } @Override public void run() { System.out.println(&quot;买菜去...&quot;); LockSupport.unpark((Thread)threadObj); } } 执行后，可出现下面的结果： 买菜去…菜买回来了…开始做饭 如上所述，可以达到阻塞主线程等到买完菜之后才开始做饭。这即是park()、unpark() 的用法。简单解释一下上述的步骤： main 方法启动后，主线程 和 买菜线程 同时开始执行。因为两者同时进行，当主线程 走到park() 时，发现permit 还为0 ，即会等待在这里。当买菜线程执行进去后，走到unpark() 会将permit 变为1 。主线程 park() 处发现permit 已经变成1 ，就可以继续往下执行了，同时消费掉permit ，重新变成0 。 以上permit 只是park/unpark 执行的一种逻辑开关，执行的步骤大致如此。 4、注意点及思考（1）必须将park()与uppark() 配对使用才更高效。 如果上边也把买锅的线程放开，main 方法改为如下： //买锅 Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); t1.start(); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); LockSupport.park(); System.out.println(&quot;锅买回来了...&quot;); LockSupport.park(); System.out.println(&quot;菜买回来了...&quot;); System.out.println(&quot;开始做饭&quot;); 即调用了两次park() 和unpark() ，发现有时候可以，有时候会使线程卡在那里，然后我又换了下顺序，如下： //买锅 Thread t1 = new Thread(new BuyGuo(Thread.currentThread())); t1.start(); LockSupport.park(); System.out.println(&quot;锅买回来了...&quot;); //买菜 Thread t2 = new Thread(new BuyCai(Thread.currentThread())); t2.start(); LockSupport.park(); System.out.println(&quot;菜买回来了...&quot;); System.out.println(&quot;开始做饭&quot;); 原理没有详细去研究,不过想了想，上边两种其实并无区别，只是执行顺序有了影响，park() 和unpark() 既然是成对配合使用，通过标识permit 来控制，如果像前边那个例子那样，出现阻塞的情况原因，我分析可能是这么个原因： 当买锅的时候，通过unpark()将permit 置为1，但是还没等到外边的main方法执行第一个park() ,买菜的线程又调了一次unpark(),但是这时候permit 还是从1变成了1，等回到主线程调用park()的时候，因为还有两个park()需要执行，也就是需要两个消费permit ,因为permit 只有1个，所以，可能会剩下一个park()卡在那里了。 （2）使用park(Object blocker) 方法更能明确问题 其实park() 有个重载方法park(Object blocker) ,这俩方法效果差不多，但是有blocker的可以传递给开发人员更多的现场信息，可以查看到当前线程的阻塞对象，方便定位问题。所以java6新增加带blocker入参的系列park方法，替代原有的park方法。 5、与wait()/notifyAll() 的比较LockSupport 的 park/unpark 方法，虽然与平时Object 中wait/notify 同样达到阻塞线程的效果。但是它们之间还是有区别的。 面向的对象主体不同。LockSupport() 操作的是线程对象，直接传入的就是Thread ,而wait() 属于具体对象，notifyAll() 也是针对所有线程进行唤醒。wait/notify 需要获取对象的监视器，即synchronized修饰，而park/unpark 不需要获取对象的监视器。实现的机制不同，因此两者没有交集。也就是说 LockSupport 阻塞的线程，notify/notifyAll 没法唤醒。但是 park 之后，同样可以被中断(interrupt()) ! 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界1：JavaWeb的由来和基础知识]]></title>
    <url>%2F2019%2F10%2F14%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C1%EF%BC%9AJavaWeb%E7%9A%84%E7%94%B1%E6%9D%A5%E5%92%8C%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个JavaWeb技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 什么是 Java Web如果你是70、80后的程序员，你一定要看一看这篇文章，保证满满的回忆。如果你是90后，那你更要看看这篇文章，因为你能找到java web发展的历史。 言归正传，Java语言能长期霸占语言排行榜一个重要的原因就是强大的web开发能力，web开发是java的基石（在EJB推出的时候当时的Sun用基石来描述EJB），所以了解java web开发原理是非常重要的。如果仅仅跟大家聊java web开发原理未免有点单薄，今天我将把java web开发包含的主体内容跟头条的读者一起分享一下（一直计划写关于java web的文章，一直也没时间写，今天就当时开个头吧）。 Web开发的历史web开发的历史其实并不久远，要搞清楚java web开发的特点（主要是优点），首先要了解web开发的历史（简单的回归一下）。早期的web是非常简单的结构，用户发出请求（request），服务器给出回应（response），这个时期的web应用，我们称为web site（网站），特点是一些列静态内容的集合。看一个图示： 图中的服务器保持了一系列html脚本来响应用户的请求，可以说这个时期的web应用还是比较简单的，但是却确立了两个重要的对象：一个request（代表请求），另一个是response（代表回应）。如果把web开发的历史比喻成一部美国大片的话，那么request和response绝对是这部大片的那女主角，而且每一部都是不可或缺的主角（简单的说就是死不了）。 看到这个图，不知道第一批从事web开发的80后是否和我一样，已经有点感触了，当年为了搞清楚这个结构，曾经连续多少个通宵做实验（实验环境比较恶劣）。没关系，这仅仅是个开始，我想当你看完这篇文章的时候，你会泪流满面的（相信我）。为了配合一下这张结构图，在web发展过程中，有一个小插曲，就是在web site向web application发展的过程中，出现了一个小“玩意儿”，就是applet，很多人了解java都是从使用java applet开始的（70，80后那一批程序员）。当时风靡校园（我当时在读大一）的网易聊天室，哎呀那个火啊（大家回忆一下你在学校机房上网时的兴奋），这个聊天室就是采用了applet构建的，当时applet给静态页面一个动态交互的可能，着实火了一段时间。现在知道applet的程序员，你已经暴露年龄了。看一张图片吧： 我想这个时候，泪点低的70、80后已经有点湿润了吧。湿润的，自觉在这里停留一分钟，对着屏幕来张合影，发个朋友圈。 过了这个插曲，真正的三层web开发来了，一个里程碑式的web处理方式CGI，看一张图： CGI的推出，使得web开发正式进入了动态处理时代，服务器能与客户有真正意义上的交流了，有能存储数据的数据库了，虽然CGI的使用周期并不长，但是一定要纪念一下它，毕竟它是里程碑式的变革。java web技术正是踩着CGI的肩膀来到了广大程序员的面前，java web解决了CGI的性能问题。CGI是以进程为单位管理请求的，而java web则是以线程为单位，处理能力更强，占用的资源更少，这个核心的组件就是Servlet。看一组资源占用图，先看CGI的： 再看一下java web中的servlet资源图： 孰优孰劣一目了然，Servlet解决相同数量的请求，却占用较少的系统资源，这就是为什么广大程序员抛弃了CGI转向java web的原因。 另外，开发一个Servlet并不复杂，看一个Servlet编写的HelloWorld应用： 这个代码结构是不是很亲切，是不是很有Coding的感觉，其实Servlet就是个java 类而已，只不过增加了几个限制而已，所以开发一个Servlet并不复杂。然后就是把它部署到web服务器上（Tomcat这个老人家现在身体依然硬朗！），然后就等待客户的请求就可以了。这是Servlet的三层部署图： java web开发的技术体系还包括javabean和jsp，采用MVC结构来组合这三个技术是java web开发的基础内容，先看一下MVC的功能图： 再看一下组合使用Servlet+javaBean+JSP的Model2开发结构： 这个结构是标准的Java web开发结构，现在是不是很少能看到这么“干净”的描述图了？以上就是标准的java web 开发的历史描述，当然这并不是说这些内容已经过时了，反而它一直是官方的标准解决方案。只不过web发展迎来了另一个阶段，繁荣的开源架构时代来了。。。 开源框架时代这个时代的典型代表就是Struts、Spring和Hibernate，简称SSH。 严格的说，这部分内容并不是官方解决方案，但是这些方案却得到了广大程序员的拥护，一方面原因是EJB的方案太重了，另一方面开源架构使用起来非常方便和灵活，所以从03年以后这些开源框架得到了普通的使用。 下面我简单描述一下这三个框架： Struts基于MVC结构的解决方案，分为struts1（已经淘汰了，用过Struts1的程序员已经老了）和struts2两个版本，和Python一样，这两个版本不兼容，目前Struts2的最新版本是2.5.14.1，简单的说Struts就是构建了现成的MVC框架，程序员往这个框架里加代码就可以了，使用起来非常方便。 Hibernate框架完成了面向对象与面向关系的映射，让java程序以面向对象的方式操作面向关系的数据库。整体结构基于DAO进行扩展，很多操作只需要配置一下就可以了，极其方便。 Spring提供了javaBean的容器，池化了javabean，提高了性能，而且核心代码不到2M，小巧且强大。 关于这三个框架我在头条将写专门的文章介绍，今天就不再进行扩展了。 今天看到这个问题，有感而发，原来我们80后真的老了，80后的程序员，看到这篇文章，有没有所感触？关注我吧，我们一起回忆，再一起继续奋斗！ Java Web基础知识一、HTTP协议HTTP(超文本传输协议)，它是一种主流B/S架构中应用的通信协议。具有以下特点： 1、无状态 服务端不会记录客户端每次提交的请求，服务器一旦相应客户端之后，就会结束本次的通信过程。客户端下一次的请求是一个新的 连接，和上一次通信没有任何关系。 2、简单灵活 HTTP是基于请求（request）和响应（response）的模型 3、支持客户端与服务端 支持主流的B/S架构的通信以及C/S架构的通信。 注意：C/S架构可选的协议有多种，例如：TCP/IP,UDP,HTTP ​ 而B/S架构通常只支持HTTP协议 二、服务器1、概念服务器通常由硬件和软件部分构成，统一对用户提供多种不同的服务。 1、硬件：包括响应的CPU、内存、磁盘等等 2、软件：包括操作系统、运行环境、服务器软件、数据库等等 2、web服务器web服务器是提供服务端程序运行的一个环境，它本身也是一个软件。 例如：将我们编写HTML文件放入到web服务器中，那么外界就可以通过浏览器访问我们的html页面 常见的web服务器有Apache，Tomcat、Jetty、Nginx等等。 而Tomcat、Jetty这些web服务器更准确的说是一个Servlet容器。 三、JavaWeb项目结构 项目根目录，例如：myweb、ch01 通常存放静态资源文件（如：html等等） WEB-INF 这个目录是当前项目私有的一个文件夹，只能提供给项目内部访问，对于客户端来说是访问不到了，通常这个目录下存放的是Java源代码、编译后的字节码文件以及Servlet的核心配置文件web.xml src 存放java源代码的目录 classes 存放编译后的字节码文件 lib lib目录存放当前项目所需要的jar文件 JSP 用于存放JSP动态页面 web.xml 项目的配置文件，用于配置Servlet的请求映射、过滤器、监听器等等信息。每一个web项目都对应一个web.xml配置文件 META-INF 配置应用程序、扩展程序、类加载服务等等 参考文章https://blog.csdn.net/shanhanyu/article/details/80515791https://www.jianshu.com/p/d9b770a78da1https://www.cnblogs.com/albertrui/p/8427661.htmlhttps://blog.csdn.net/qq_41911570/article/details/83279327 本文由博客一文多发平台 OpenWrite 发布！ 参考文章https://blog.csdn.net/shanhanyu/article/details/80515791https://www.jianshu.com/p/d9b770a78da1https://www.cnblogs.com/albertrui/p/8427661.htmlhttps://blog.csdn.net/qq_41911570/article/details/83279327 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走进JavaWeb技术世界1：JavaWeb的由来和基础知识]]></title>
    <url>%2F2019%2F10%2F14%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C%2F%E8%B5%B0%E8%BF%9BJavaWeb%E6%8A%80%E6%9C%AF%E4%B8%96%E7%95%8C3%EF%BC%9AJDBC%E7%9A%84%E8%BF%9B%E5%8C%96%E4%B8%8E%E8%BF%9E%E6%8E%A5%E6%B1%A0%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，从servlet到框架，从ssm再到SpringBoot，一步步地学习JavaWeb基础知识，并上手进行实战，接着了解JavaWeb项目中经常要使用的技术和组件，包括日志组件、Maven、Junit，等等内容，以便让你更完整地了解整个Java Web技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） JDBC数据库连接池谈谈连接池、线程池技术原理做互联网研发，最早接触使用jdbc技术，为了数据库连接能够复用，会用到c3p0、dbcp等数据库连接池。应该是研发人员最早接触的数据库连接池，再到httpclient http连接池，再到微服务netty连接池，redis客户端连接池，以及jdk中线程池技术。 这么多数据库、http、netty连接池，jdk线程池，本质上都是连接池技术，连接池技术核心是连接或者说创建的资源复用。 连接池技术核心：通过减少对于连接创建、关闭来提升性能。用于用户后续使用，好处是后续使用不用在创建连接以及线程，因为这些都需要相关很多文件、连接资源、操作系统内核资源支持来完成构建，会消耗大量资源，并且创建、关闭会消耗应用程序大量性能。 网络连接本身会消耗大量内核资源，在linux系统下，网络连接创建本身tcp/ip协议栈在内核里面，连接创建关闭会消耗大量文件句柄（linux中万物皆文件，一种厉害抽象手段）系统资源。当下更多是应用tcp技术完成网络传输，反复打开关闭，需要操作系统维护大量tcp协议栈状态。 连接池本质上是构建一个容器，容器来存储创建好的线程、http连接、数据库连接、netty连接等。对于使用方相当于黑盒，按照接口进行使用就可以了。各个连接池构建、使用管理详细过程大概分成以下三部分。 第一部分：首先初始化连接池，根据设置相应参数，连接池大小、核心线程数、核心连接数等参数，初始化创建数据库、http、netty连接以及jdk线程。 第二部分：连接池使用，前边初始化好的连接池、线程池，直接从连接池、线程中取出资源即可进行使用，使用完后要记得交还连接池、线程池，通过池容器来对资源进行管理。 第三部分：对于连接池维护，连接池、线程池来维护连接、线程状态，不可用连接、线程进行销毁，正在使用连接、线程进行状态标注，连接、线程不够后并且少于设置最大连接、线程数，要进行新连接、线程创建。 通过上边可以了解到各种连接池技术以及线程池原理或者说套路，理解原理才能不被纷繁复杂表象掩盖。 下面谈谈构建自己连接池，其实理解了连接池、线程原理，可以使用ArrayList来构建自己连接池、线程池。初始化时创建配置连接数、线程，存储在ArrayList容器中，使用时从ArrayList从取出连接、线程进行使用，执行完任务后，提交回ArrayList容器。前提条件是单线程，在多线程状态下要用线程安全容器。 前边根据原理介绍了一个简单连接池、线程池怎样构建，实际工业级别线程池还要考虑到连接状态，短连接重连，线程池维护管理高效，线程池稳定等多个因素。 需要用到连接池而又没有相关开源产品可用时，java连接池可以使用common-pool2来构建，比如google开源gRPC技术，本身是高性能跨平台技术，但目前作为微服务使用，没有连接池、负载均衡等相应配套，这时可以根据需要自己基于Java容器构建自己连接池。也可以利用common-pool2构建连接池来提升应用性能，以及保持高可用。common-pool2本身不仅仅可以构建连接池使用，还可以用来构建对象池。 连接池还有一个副作用就是实现了高可用，在微服务场景下一个连接不可用，那么再从netty连接池中取出一个进行使用，避免了连接不可用问题。 掌握原理从比较全面掌握各种池技术，避免数据库连接池，再到httpclient http连接池，再到微服务netty连接池，redis客户端连接池，以及jdk中线程池，对象池各种各样池技术，使我们眼花缭乱，花费过多时间，掌握原理机制以不变应万变。 推广一下这个方法，其他技术也是类似，深入掌握其原理，就可以明白其他类似技术相似原理，避免疲于应对各种新技术。但每一种架构设计与实现又与领域有着关系，也不可讲原理不顾实际情况扩展。理论与架构设计、源码学习相结合才是最好的，希望有帮助。JDBC 数据库连接池转自： 什么情况下使用连接池?对于一个简单的数据库应用，由于对于数据库的访问不是很频繁。这时可以简单地在需要访问数据库时，就新创建一个连接，用完后就关闭它，这样做也不会带来什么明显的性能上的开销。但是对于一个复杂的数据库应用，情况就完全不同了。频繁的建立、关闭连接，会极大的减低系统的性能，因为对于连接的使用成了系统性能的瓶颈。 使用连接池的好处 连接复用。通过建立一个数据库连接池以及一套连接使用管理策略，使得一个数据库连接可以得到高效、安全的复用，避免了数据库连接频繁建立、关闭的开销。 对于共享资源，有一个很著名的设计模式：资源池。该模式正是为了解决资源频繁分配、释放所造成的问题的。把该模式应用到数据库连接管理领域，就是建立一个数据库连接池，提供一套高效的连接分配、使用策略，最终目标是实现连接的高效、安全的复用。 连接池的实现数据库连接池的基本原理是在内部对象池中维护一定数量的数据库连接，并对外暴露数据库连接获取和返回方法。 外部使用者可通过 getConnection 方法获取连接，使用完毕后再通过 close 方法将连接返回，注意此时连接并没有关闭，而是由连接池管理器回收，并为下一次使用做好准备。 Java 中有一个 DataSource 接口, 数据库连接池就是 DataSource 的一个实现 常用数据库连接池 Apache DBCP 官网 C3P0 官网 Druid GitHub 一、JDBC数据库连接池的必要性 在使用开发基于数据库的web程序时，传统的模式基本是按以下步骤： ①在主程序（如servlet、beans）中建立数据库连接。 ②进行sql操作 ③断开数据库连接。 这种模式开发，存在的问题: ①普通的JDBC数据库连接使用 DriverManager 来获取，每次向数据库建立连接的时候都要将 Connection 加载到内存中，再验证用户名和密码(得花费0.05s～1s的时间)。需要数据库连接的时候，就向数据库要求一个，执行完成后再断开连接。这样的方式将会消耗大量的资源和时间。数据库的连接资源并没有得到很好的重复利用.若同时有几百人甚至几千人在线，频繁的进行数据库连接操作将占用很多的系统资源，严重的甚至会造成服务器的崩溃。 ②对于每一次数据库连接，使用完后都得断开。否则，如果程序出现异常而未能关闭，将会导致数据库系统中的内存泄漏，最终将导致重启数据库。 ③这种开发不能控制被创建的连接对象数，系统资源会被毫无顾及的分配出去，如连接过多，也可能导致内存泄漏，服务器崩溃。 二、数据库连接池（connection pool） 数据库连接池简单介绍 为解决传统开发中的数据库连接问题，可以采用数据库连接池技术。 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。 数据库连接池负责分配、管理和释放数据库连接，它允许应用程序重复使用一个现有的数据库连接，而不是重新建立一个。 数据库连接池在初始化时将创建一定数量的数据库连接放到连接池中，这些数据库连接的数量是由最小数据库连接数来设定的。无论这些数据库连接是否被使用，连接池都将一直保证至少拥有这么多的连接数量。连接池的最大数据库连接数量限定了这个连接池能占有的最大连接数，当应用程序向连接池请求的连接数超过最大连接数量时，这些请求将被加入到等待队列中。 数据库连接池工作原理： 数据库连接池技术的优点 资源重用： ①由于数据库连接得以重用，避免了频繁创建，释放连接引起的大量性能开销。在减少系统消耗的基础上，另一方面也增加了系统运行环境的平稳性。 更快的系统反应速度： 数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于连接池中备用。此时连接的初始化工作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而减少了系统的响应时间 新的资源分配手段： 对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接池的配置，实现某一应用最大可用数据库连接数的限制，避免某一应用独占所有的数据库资源 统一的连接管理，避免数据库连接泄露： 在较为完善的数据库连接池实现中，可根据预先的占用超时设定，强制回收被占用连接，从而避免了常规数据库连接操作中可能出现的资源泄露 三、两种开源的数据库连接池 JDBC 的数据库连接池使用 javax.sql.DataSource 来表示，DataSource 只是一个接口，该接口通常由服务器(Weblogic, WebSphere, Tomcat)提供实现，也有一些开源组织提供实现： ①DBCP 数据库连接池 ②C3P0 数据库连接池 DataSource 通常被称为数据源，它包含连接池和连接池管理两个部分，习惯上也经常把 DataSource称为连接池 数据源和数据库连接不同，数据源无需创建多个，它是产生数据库连接的工厂，因此整个应用只需要一个数据源即可。 当数据库访问结束后，程序还是像以前一样关闭数据库连接：conn.close(); 但上面的代码并没有关闭数据库的物理连接，它仅仅把数据库连接释放，归还给了数据库连接池。 参考文章https://blog.csdn.net/u010028461/article/details/78932109https://www.cnblogs.com/shaoxiaohuan/p/7755953.htmlhttps://www.cnblogs.com/albertrui/p/8421791.htmlhttps://blog.csdn.net/weixin_43124134/article/details/82586062https://www.jianshu.com/p/0737ac60c7df 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>JavaWeb</category>
      </categories>
      <tags>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南13：Java 中的 HashMap 和 ConcurrentHashMap 全解析]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9713%EF%BC%9AJava%20%E4%B8%AD%E7%9A%84%20HashMap%20%E5%92%8C%20ConcurrentHashMap%20%E5%85%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/ 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言今天发一篇”水文”，可能很多读者都会表示不理解，不过我想把它作为并发序列文章中不可缺少的一块来介绍。本来以为花不了多少时间的，不过最终还是投入了挺多时间来完成这篇文章的。 网上关于 HashMap 和 ConcurrentHashMap 的文章确实不少，不过缺斤少两的文章比较多，所以才想自己也写一篇，把细节说清楚说透，尤其像 Java8 中的 ConcurrentHashMap，大部分文章都说不清楚。终归是希望能降低大家学习的成本，不希望大家到处找各种不是很靠谱的文章，看完一篇又一篇，可是还是模模糊糊。 阅读建议：四节基本上可以进行独立阅读，建议初学者可按照 Java7 HashMap -&gt; Java7 ConcurrentHashMap -&gt; Java8 HashMap -&gt; Java8 ConcurrentHashMap 顺序进行阅读，可适当降低阅读门槛。 阅读前提：本文分析的是源码，所以至少读者要熟悉它们的接口使用，同时，对于并发，读者至少要知道 CAS、ReentrantLock、UNSAFE 操作这几个基本的知识，文中不会对这些知识进行介绍。Java8 用到了红黑树，不过本文不会进行展开，感兴趣的读者请自行查找相关资料。 Java7 HashMapHashMap 是最简单的，一来我们非常熟悉，二来就是它不支持并发操作，所以源码也非常简单。 首先，我们用下面这张图来介绍 HashMap 的结构。 这个仅仅是示意图，因为没有考虑到数组要扩容的情况，具体的后面再说。 大方向上，HashMap 里面是一个数组，然后数组中每个元素是一个单向链表。 上图中，每个绿色的实体是嵌套类 Entry 的实例，Entry 包含四个属性：key, value, hash 值和用于单向链表的 next。 capacity：当前数组容量，始终保持 2^n，可以扩容，扩容后数组大小为当前的 2 倍。 loadFactor：负载因子，默认为 0.75。 threshold：扩容的阈值，等于 capacity * loadFactor put 过程分析还是比较简单的，跟着代码走一遍吧。 1234567891011121314151617181920212223242526272829public V put(K key, V value) &#123; // 当插入第一个元素的时候，需要先初始化数组大小 if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 如果 key 为 null，感兴趣的可以往里看，最终会将这个 entry 放到 table[0] 中 if (key == null) return putForNullKey(value); // 1\. 求 key 的 hash 值 int hash = hash(key); // 2\. 找到对应的数组下标 int i = indexFor(hash, table.length); // 3\. 遍历一下对应下标处的链表，看是否有重复的 key 已经存在， // 如果有，直接覆盖，put 方法返回旧值就结束了 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 4\. 不存在重复的 key，将此 entry 添加到链表中，细节后面说 addEntry(hash, key, value, i); return null;&#125; 数组初始化在第一个元素插入 HashMap 的时候做一次数组的初始化，就是先确定初始的数组大小，并计算数组扩容的阈值。 12345678910private void inflateTable(int toSize) &#123; // 保证数组大小一定是 2 的 n 次方。 // 比如这样初始化：new HashMap(20)，那么处理成初始数组大小是 32 int capacity = roundUpToPowerOf2(toSize); // 计算扩容阈值：capacity * loadFactor threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); // 算是初始化数组吧 table = new Entry[capacity]; initHashSeedAsNeeded(capacity); //ignore&#125; 这里有一个将数组大小保持为 2 的 n 次方的做法，Java7 和 Java8 的 HashMap 和 ConcurrentHashMap 都有相应的要求，只不过实现的代码稍微有些不同，后面再看到的时候就知道了。 计算具体数组位置这个简单，我们自己也能 YY 一个：使用 key 的 hash 值对数组长度进行取模就可以了。 1234static int indexFor(int hash, int length) &#123; // assert Integer.bitCount(length) == 1 : &quot;length must be a non-zero power of 2&quot;; return hash &amp; (length-1);&#125; 这个方法很简单，简单说就是取 hash 值的低 n 位。如在数组长度为 32 的时候，其实取的就是 key 的 hash 值的低 5 位，作为它在数组中的下标位置。 添加节点到链表中找到数组下标后，会先进行 key 判重，如果没有重复，就准备将新值放入到链表的表头。 12345678910111213141516171819void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果当前 HashMap 大小已经达到了阈值，并且新值要插入的数组位置已经有元素了，那么要扩容 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 扩容，后面会介绍一下 resize(2 * table.length); // 扩容以后，重新计算 hash 值 hash = (null != key) ? hash(key) : 0; // 重新计算扩容后的新的下标 bucketIndex = indexFor(hash, table.length); &#125; // 往下看 createEntry(hash, key, value, bucketIndex);&#125;// 这个很简单，其实就是将新值放到链表的表头，然后 size++void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 这个方法的主要逻辑就是先判断是否需要扩容，需要的话先扩容，然后再将这个新的数据插入到扩容后的数组的相应位置处的链表的表头。 数组扩容前面我们看到，在插入新值的时候，如果当前的 size 已经达到了阈值，并且要插入的数组位置上已经有元素，那么就会触发扩容，扩容后，数组大小为原来的 2 倍。 1234567891011121314void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新的数组 Entry[] newTable = new Entry[newCapacity]; // 将原来数组中的值迁移到新的更大的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 扩容就是用一个新的大数组替换原来的小数组，并将原来数组中的值迁移到新的数组中。 由于是双倍扩容，迁移过程中，会将原来 table[i] 中的链表的所有节点，分拆到新的数组的 newTable[i] 和 newTable[i + oldLength] 位置上。如原来数组长度是 16，那么扩容后，原来 table[0] 处的链表中的所有元素会被分配到新数组中 newTable[0] 和 newTable[16] 这两个位置。代码比较简单，这里就不展开了。 get 过程分析相对于 put 过程，get 过程是非常简单的。 根据 key 计算 hash 值。 找到相应的数组下标：hash &amp; (length - 1)。 遍历该数组位置处的链表，直到找到相等(==或equals)的 key。 123456789public V get(Object key) &#123; // 之前说过，key 为 null 的话，会被放到 table[0]，所以只要遍历下 table[0] 处的链表就可以了 if (key == null) return getForNullKey(); // Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125; getEntry(key): 1234567891011121314151617final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; int hash = (key == null) ? 0 : hash(key); // 确定数组下标，然后从头开始遍历链表，直到找到为止 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null;&#125; Java7 ConcurrentHashMapConcurrentHashMap 和 HashMap 思路是差不多的，但是因为它支持并发操作，所以要复杂一些。 整个 ConcurrentHashMap 由一个个 Segment 组成，Segment 代表”部分“或”一段“的意思，所以很多地方都会将其描述为分段锁。注意，行文中，我很多地方用了“槽”来代表一个 segment。 简单理解就是，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 concurrencyLevel：并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的。 再具体到每个 Segment 内部，其实每个 Segment 很像之前介绍的 HashMap，不过它要保证线程安全，所以处理起来要麻烦些。 初始化initialCapacity：初始容量，这个值指的是整个 ConcurrentHashMap 的初始容量，实际操作的时候需要平均分给每个 Segment。 loadFactor：负载因子，之前我们说了，Segment 数组不可以扩容，所以这个负载因子是给每个 Segment 内部使用的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; // 计算并行级别 ssize，因为要保持并行级别是 2 的 n 次方 while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; // 我们这里先不要那么烧脑，用默认值，concurrencyLevel 为 16，sshift 为 4 // 那么计算出 segmentShift 为 28，segmentMask 为 15，后面会用到这两个值 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // initialCapacity 是设置整个 map 初始的大小， // 这里根据 initialCapacity 计算 Segment 数组中每个位置可以分到的大小 // 如 initialCapacity 为 64，那么每个 Segment 或称之为&quot;槽&quot;可以分到 4 个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组， // 并创建数组的第一个元素 segment[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入 segment[0] UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss;&#125; 初始化完成，我们得到了一个 Segment 数组。 我们就当是用 new ConcurrentHashMap() 无参构造函数进行初始化的，那么初始化完成后： Segment 数组长度为 16，不可以扩容 Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，后面的代码会介绍 当前 segmentShift 的值为 32 - 4 = 28，segmentMask 为 16 - 1 = 15，姑且把它们简单翻译为移位数和掩码，这两个值马上就会用到 put 过程分析我们先看 put 的主流程，对于其中的一些关键细节操作，后面会进行详细介绍。 123456789101112131415161718public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1\. 计算 key 的 hash 值 int hash = hash(key); // 2\. 根据 hash 值找到 Segment 数组中的位置 j // hash 是 32 位，无符号右移 segmentShift(28) 位，剩下高 4 位， // 然后和 segmentMask(15) 做一次与操作，也就是说 j 是 hash 值的高 4 位，也就是槽的数组下标 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 刚刚说了，初始化的时候初始化了 segment[0]，但是其他位置还是 null， // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3\. 插入新值到 槽 s 中 return s.put(key, hash, value, false);&#125; 第一层皮很简单，根据 hash 值很快就能找到相应的 Segment，之后就是 Segment 内部的 put 操作了。 Segment 内部是由 数组+链表 组成的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; // 在往该 segment 写入前，需要先获取该 segment 的独占锁 // 先看主流程，后面还会具体介绍这部分内容 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; // 这个是 segment 内部的数组 HashEntry&lt;K,V&gt;[] tab = table; // 再利用 hash 值，求应该放置的数组下标 int index = (tab.length - 1) &amp; hash; // first 是数组该位置处的链表的表头 HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 下面这串 for 循环虽然很长，不过也很好理解，想想该位置没有任何元素和已经存在一个链表这两种情况 for (HashEntry&lt;K,V&gt; e = first;;) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; // 覆盖旧值 e.value = value; ++modCount; &#125; break; &#125; // 继续顺着链表走 e = e.next; &#125; else &#123; // node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。 // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值，这个 segment 需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 扩容后面也会具体分析 else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， // 其实就是将新的节点设置成原链表的表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; // 解锁 unlock(); &#125; return oldValue;&#125; 整体流程还是比较简单的，由于有独占锁的保护，所以 segment 内部的操作并不复杂。至于这里面的并发问题，我们稍后再进行介绍。 到这里 put 操作就结束了，接下来，我们说一说其中几步关键的操作。 初始化槽: ensureSegmentConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0]，对于其他槽来说，在插入第一个值的时候进行初始化。 这里需要考虑并发，因为很可能会有多个线程同时进来初始化同一个槽 segment[k]，不过只要有一个成功了就可以。 1234567891011121314151617181920212223242526272829private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg;&#125; 总的来说，ensureSegment(int k) 比较简单，对于并发操作使用 CAS 进行控制。 我没搞懂这里为什么要搞一个 while 循环，CAS 失败不就代表有其他线程成功了吗，为什么要再进行判断？ 感谢评论区的李子木，如果当前线程 CAS 失败，这里的 while 循环是为了将 seg 赋值返回。 获取写入锁: scanAndLockForPut前面我们看到，在往某个 segment 中 put 的时候，首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value)，也就是说先进行一次 tryLock() 快速获取该 segment 的独占锁，如果失败，那么进入到 scanAndLockForPut 这个方法来获取锁。 下面我们来具体分析这个方法中是怎么控制加锁的。 123456789101112131415161718192021222324252627282930313233343536373839private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; &#125; // 重试次数如果超过 MAX_SCAN_RETRIES（单核1多核64），那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 这个方法有两个出口，一个是 tryLock() 成功了，循环终止，另一个就是重试次数超过了 MAX_SCAN_RETRIES，进到 lock() 方法，此方法会阻塞等待，直到成功拿到独占锁。 这个方法就是看似复杂，但是其实就是做了一件事，那就是获取该 segment 的独占锁，如果需要的话顺便实例化了一下 node。 扩容: rehash重复一下，segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry&lt;K,V&gt;[] 进行扩容，扩容后，容量为原来的 2 倍。 首先，我们要回顾一下触发扩容的地方，put 的时候，如果判断该值的插入会导致该 segment 的元素个数超过阈值，那么先进行扩容，再插值，读者这个时候可以回去 put 方法看一眼。 该方法不需要考虑并发，因为到这里的时候，是持有该 segment 的独占锁的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) &#123; // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头结点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 这里的扩容比之前的 HashMap 要复杂一些，代码难懂一点。上面有两个挨着的 for 循环，第一个 for 有什么用呢？ 仔细一看发现，如果没有第一个 for 循环，也是可以工作的，但是，这个 for 循环下来，如果 lastRun 的后面还有比较多的节点，那么这次就是值得的。因为我们只需要克隆 lastRun 前面的节点，后面的一串节点跟着 lastRun 走就是了，不需要做任何操作。 我觉得 Doug Lea 的这个想法也是挺有意思的，不过比较坏的情况就是每次 lastRun 都是链表的最后一个元素或者很靠后的元素，那么这次遍历就有点浪费了。不过 Doug Lea 也说了，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆。 get 过程分析相对于 put 来说，get 真的不要太简单。 计算 hash 值，找到 segment 数组中的具体位置，或我们前面用的“槽” 槽中也是一个数组，根据 hash 找到数组中具体的位置 到这里是链表了，顺着链表进行查找即可 1234567891011121314151617181920public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 1\. hash 值 int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // 2\. 根据 hash 找到对应的 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; // 3\. 找到segment 内部数组相应位置的链表，遍历 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 并发问题分析现在我们已经说完了 put 过程和 get 过程，我们可以看到 get 过程中是没有加锁的，那自然我们就需要去考虑并发问题。 添加节点的操作 put 和删除节点的操作 remove 都是要加 segment 上的独占锁的，所以它们之间自然不会有问题，我们需要考虑的问题就是 get 的时候在同一个 segment 中发生了 put 或 remove 操作。 put 操作的线程安全性。 初始化槽，这个我们之前就说过了，使用了 CAS 来初始化 Segment 中的数组。 添加节点到链表的操作是插入到表头的，所以，如果这个时候 get 操作在链表遍历的过程已经到了中间，是不会影响的。当然，另一个并发问题就是 get 操作在 put 之后，需要保证刚刚插入表头的节点被读取，这个依赖于 setEntryAt 方法中使用的 UNSAFE.putOrderedObject。 扩容。扩容是新创建了数组，然后进行迁移数据，最后面将 newTable 设置给属性 table。所以，如果 get 操作此时也在进行，那么也没关系，如果 get 先行，那么就是在旧的 table 上做查询操作；而 put 先行，那么 put 操作的可见性保证就是 table 使用了 volatile 关键字。 remove 操作的线程安全性。 remove 操作我们没有分析源码，所以这里说的读者感兴趣的话还是需要到源码中去求实一下的。 get 操作需要遍历链表，但是 remove 操作会”破坏”链表。 如果 remove 破坏的节点 get 操作已经过去了，那么这里不存在任何问题。 如果 remove 先破坏了一个节点，分两种情况考虑。 1、如果此节点是头结点，那么需要将头结点的 next 设置为数组该位置的元素，table 虽然使用了 volatile 修饰，但是 volatile 并不能提供数组内部操作的可见性保证，所以源码中使用了 UNSAFE 来操作数组，请看方法 setEntryAt。2、如果要删除的节点不是头结点，它会将要删除节点的后继节点接到前驱节点中，这里的并发保证就是 next 属性是 volatile 的。 Java8 HashMapJava8 对 HashMap 进行了一些修改，最大的不同就是利用了红黑树，所以其由 数组+链表+红黑树 组成。 根据 Java7 HashMap 的介绍，我们知道，查找的时候，根据 hash 值我们能够快速定位到数组的具体下标，但是之后的话，需要顺着链表一个个比较下去才能找到我们需要的，时间复杂度取决于链表的长度，为 O(n)。 为了降低这部分的开销，在 Java8 中，当链表中的元素达到了 8 个时，会将链表转换为红黑树，在这些位置进行查找的时候可以降低时间复杂度为 O(logN)。 来一张图简单示意一下吧： 注意，上图是示意图，主要是描述结构，不会达到这个状态的，因为这么多数据的时候早就扩容了。 下面，我们还是用代码来介绍吧，个人感觉，Java8 的源码可读性要差一些，不过精简一些。 Java7 中使用 Entry 来代表每个 HashMap 中的数据节点，Java8 中使用 Node，基本没有区别，都是 key，value，hash 和 next 这四个属性，不过，Node 只能用于链表的情况，红黑树的情况需要使用 TreeNode。 我们根据数组元素中，第一个节点数据类型是 Node 还是 TreeNode 来判断该位置下是链表还是红黑树的。 put 过程分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;// 第三个参数 onlyIfAbsent 如果是 true，那么只有在不存在该 key 时才会进行 put 操作// 第四个参数 evict 我们这里不关心final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 第一次 put 值的时候，会触发下面的 resize()，类似 java7 的第一次 put 也要初始化数组长度 // 第一次 resize 和后续的扩容有些不一样，因为这次是数组从 null 初始化到默认的 16 或自定义的初始容量 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 找到具体的数组下标，如果此位置没有值，那么直接初始化一下 Node 并放置在这个位置就可以了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123;// 数组该位置有数据 Node&lt;K,V&gt; e; K k; // 首先，判断该位置的第一个数据和我们要插入的数据，key 是不是&quot;相等&quot;，如果是，取出这个节点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果该节点是代表红黑树的节点，调用红黑树的插值方法，本文不展开说红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 到这里，说明数组该位置上是一个链表 for (int binCount = 0; ; ++binCount) &#123; // 插入到链表的最后面(Java7 是插入到链表的最前面) if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // TREEIFY_THRESHOLD 为 8，所以，如果新插入的值是链表中的第 8 个 // 会触发下面的 treeifyBin，也就是将链表转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果在该链表中找到了&quot;相等&quot;的 key(== 或 equals) if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 此时 break，那么 e 为链表中[与要插入的新值的 key &quot;相等&quot;]的 node break; p = e; &#125; &#125; // e!=null 说明存在旧值的key与要插入的key&quot;相等&quot; // 对于我们分析的put操作，下面这个 if 其实就是进行 &quot;值覆盖&quot;，然后返回旧值 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 如果 HashMap 由于新插入这个值导致 size 已经超过了阈值，需要进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 和 Java7 稍微有点不一样的地方就是，Java7 是先扩容后插入新值的，Java8 先插值再扩容，不过这个不重要。 数组扩容resize() 方法用于初始化数组或数组扩容，每次扩容后，容量为原来的 2 倍，并进行数据迁移。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 对应数组扩容 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 将数组大小扩大一倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 将阈值扩大一倍 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // 对应使用 new HashMap(int initialCapacity) 初始化后，第一次 put 的时候 newCap = oldThr; else &#123;// 对应使用 new HashMap() 初始化后，第一次 put 的时候 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 用新的数组大小初始化新的数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 如果是初始化数组，到这里就结束了，返回 newTab 即可 if (oldTab != null) &#123; // 开始遍历原数组，进行数据迁移。 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 如果该数组位置上只有单个元素，那就简单了，简单迁移这个元素就可以了 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果是红黑树，具体我们就不展开了 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 这块是处理链表的情况， // 需要将此链表拆成两个链表，放到新的数组中，并且保留原来的先后顺序 // loHead、loTail 对应一条链表，hiHead、hiTail 对应另一条链表，代码还是比较简单的 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; // 第一条链表 newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; // 第二条链表的新的位置是 j + oldCap，这个很好理解 newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; get 过程分析相对于 put 来说，get 真的太简单了。 计算 key 的 hash 值，根据 hash 值找到对应数组下标: hash &amp; (length-1) 判断数组该位置处的元素是否刚好就是我们要找的，如果不是，走第三步 判断该元素类型是否是 TreeNode，如果是，用红黑树的方法取数据，如果不是，走第四步 遍历链表，直到找到相等(==或equals)的 key 1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 1234567891011121314151617181920212223final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 判断第一个节点是不是就是需要的 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; // 判断是否是红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 链表遍历 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; Java8 ConcurrentHashMapJava7 中实现的 ConcurrentHashMap 说实话还是比较复杂的，Java8 对 ConcurrentHashMap 进行了比较大的改动。建议读者可以参考 Java8 中 HashMap 相对于 Java7 HashMap 的改动，对于 ConcurrentHashMap，Java8 也引入了红黑树。 说实话，Java8 ConcurrentHashMap 源码真心不简单，最难的在于扩容，数据迁移操作不容易看懂。 我们先用一个示意图来描述下其结构： 结构上和 Java8 的 HashMap 基本上一样，不过它要保证线程安全性，所以在源码上确实要复杂一些。 初始化123// 这构造函数里，什么都不干public ConcurrentHashMap() &#123;&#125; 12345678public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;&#125; 这个初始化方法有点意思，通过提供初始容量，计算了 sizeCtl，sizeCtl = 【 (1.5 * initialCapacity + 1)，然后向上取最近的 2 的 n 次方】。如 initialCapacity 为 10，那么得到 sizeCtl 为 16，如果 initialCapacity 为 11，得到 sizeCtl 为 32。 sizeCtl 这个属性使用的场景很多，不过只要跟着文章的思路来，就不会被它搞晕了。 如果你爱折腾，也可以看下另一个有三个参数的构造方法，这里我就不说了，大部分时候，我们会使用无参构造函数进行实例化，我们也按照这个思路来进行源码分析吧。 put 过程分析仔细地一行一行代码看下去： 123public V put(K key, V value) &#123; return putVal(key, value, false);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 如果数组&quot;空&quot;，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else &#123; // 到这里就是说，f 是该位置的头结点，而且不为空 V oldVal = null; // 获取数组该位置的头结点的监视器锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; // 头结点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 如果发现了&quot;相等&quot;的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; // 到了链表的最末端，将这个新值放到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; // 红黑树 Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount &gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // addCount(1L, binCount); return null;&#125; put 的主流程看完了，但是至少留下了几个问题，第一个是初始化，第二个是扩容，第三个是帮助数据迁移，这些我们都会在后面进行一一介绍。 初始化数组：initTable这个比较简单，主要就是初始化一个合适大小的数组，然后会设置 sizeCtl。 初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制的。 1234567891011121314151617181920212223242526272829private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // 初始化的&quot;功劳&quot;被其他线程&quot;抢去&quot;了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 链表转红黑树: treeifyBin前面我们在 put 源码分析也说过，treeifyBin 不一定就会进行红黑树转换，也可能是仅仅做数组扩容。我们还是进行源码分析吧。 123456789101112131415161718192021222324252627282930313233private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // MIN_TREEIFY_CAPACITY 为 64 // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 后面我们再详细分析这个方法 tryPresize(n &lt;&lt; 1); // b 是头结点 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 加锁 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; // 下面就是遍历链表，建立一颗红黑树 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; // 将红黑树设置到数组相应位置中 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 扩容：tryPresize如果说 Java8 ConcurrentHashMap 的源码不简单，那么说的就是扩容操作和迁移操作。 这个方法要完完全全看懂还需要看之后的 transfer 方法，读者应该提前知道这点。 这里的扩容也是做翻倍扩容的，扩容后数组容量为原来的 2 倍。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) &#123; // c：size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 0.75 * n &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) &#123; // 我没看懂 rs 的真正含义是什么，不过也关系不大 int rs = resizeStamp(n); if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 2\. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法 // 此时 nextTab 不为 null if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 1\. 将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 我是没看懂这个值真正的意义是什么？不过可以计算出来的是，结果是一个比较大的负数 // 调用 transfer 方法，此时 nextTab 参数为 null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125; 这个方法的核心在于 sizeCtl 值的操作，首先将其设置为一个负数，然后执行 transfer(tab, null)，再下一个循环将 sizeCtl 加 1，并执行 transfer(tab, nt)，之后可能是继续 sizeCtl 加 1，并执行 transfer(tab, nt)。 所以，可能的操作就是执行 1 次 transfer(tab, null) + 多次 transfer(tab, nt)，这里怎么结束循环的需要看完 transfer 源码才清楚。 数据迁移：transfer下面这个方法有点长，将原来的 tab 数组的元素迁移到新的 nextTab 数组中。 虽然我们之前说的 tryPresize 方法中多次调用 transfer 不涉及多线程，但是这个 transfer 方法可以在其他地方被调用，典型地，我们之前在说 put 方法的时候就说过了，请往上看 put 方法，是不是有个地方调用了 helpTransfer 方法，helpTransfer 方法会调用 transfer 方法的。 此方法支持多线程执行，外围调用此方法的时候，会保证第一个发起数据迁移的线程，nextTab 参数为 null，之后再调用此方法的时候，nextTab 不会为 null。 阅读源码之前，先要理解并发操作的机制。原数组长度为 n，所以我们有 n 个迁移任务，让每个线程每次负责一个小任务是最简单的，每做完一个任务再检测是否有其他没做完的任务，帮助迁移就可以了，而 Doug Lea 使用了一个 stride，简单理解就是步长，每个线程每次负责迁移其中的一部分，如每次迁移 16 个小任务。所以，我们就需要一个全局的调度者来安排哪个线程执行哪几个任务，这个就是属性 transferIndex 的作用。 第一个发起数据迁移的线程会将 transferIndex 指向原数组最后的位置，然后从后往前的 stride 个任务属于第一个线程，然后将 transferIndex 指向新的位置，再往前的 stride 个任务属于第二个线程，依此类推。当然，这里说的第二个线程不是真的一定指代了第二个线程，也可以是同一个线程，这个读者应该能理解吧。其实就是将一个大的迁移任务分为了一个个任务包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // stride 在单核下直接等于 n，多核模式下为 (n&gt;&gt;&gt;3)/NCPU，最小值是 16 // stride 可以理解为”步长“，有 n 个位置是需要进行迁移的， // 将这 n 个任务分为多个任务包，每个任务包有 stride 个任务 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 // 前面我们说了，外围会保证第一个发起迁移的线程调用此方法时，参数 nextTab 为 null // 之后参与迁移的线程调用此方法时，nextTab 不会为 null if (nextTab == null) &#123; try &#123; // 容量翻倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; // nextTable 是 ConcurrentHashMap 中的属性 nextTable = nextTab; // transferIndex 也是 ConcurrentHashMap 的属性，用于控制迁移的位置 transferIndex = n; &#125; int nextn = nextTab.length; // ForwardingNode 翻译过来就是正在被迁移的 Node // 这个构造方法会生成一个Node，key、value 和 next 都为 null，关键是 hash 为 MOVED // 后面我们会看到，原数组中位置 i 处的节点完成迁移工作后， // 就会将位置 i 处设置为这个 ForwardingNode，用来告诉其他线程该位置已经处理过了 // 所以它其实相当于是一个标志。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // advance 指的是做完了一个位置的迁移工作，可以准备做下一个位置的了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab /* * 下面这个 for 循环，最难理解的在前面，而要看懂它们，应该先看懂后面的，然后再倒回来看 * */ // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 下面这个 while 真的是不好理解 // advance 为 true 表示可以进行下一个位置的迁移了 // 简单理解结局：i 指向了 transferIndex，bound 指向了 transferIndex-stride while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 将 transferIndex 值赋给 nextIndex // 这里 transferIndex 一旦小于等于 0，说明原数组的所有位置都有相应的线程去处理了 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; // 看括号中的代码，nextBound 是这次迁移任务的边界，注意，是从后往前 bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; // 所有的迁移操作已经完成 nextTable = null; // 将新的 nextTab 赋值给 table 属性，完成迁移 table = nextTab; // 重新计算 sizeCtl：n 是原数组长度，所以 sizeCtl 得出的值将是新数组长度的 0.75 倍 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // 之前我们说过，sizeCtl 在迁移前会设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 // 然后，每有一个线程参与迁移就会将 sizeCtl 加 1， // 这里使用 CAS 操作对 sizeCtl 进行减 1，代表做完了属于自己的任务 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 到这里，说明 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT， // 也就是说，所有的迁移任务都做完了，也就会进入到上面的 if(finishing)&#123;&#125; 分支了 finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 该位置处是一个 ForwardingNode，代表该位置已经迁移过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 对数组该位置处的结点加锁，开始处理数组该位置处的迁移工作 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // 头结点的 hash 大于 0，说明是链表的 Node 节点 if (fh &gt;= 0) &#123; // 下面这一块和 Java7 中的 ConcurrentHashMap 迁移是差不多的， // 需要将链表一分为二， // 找到原链表中的 lastRun，然后 lastRun 及其之后的节点是一起进行迁移的 // lastRun 之前的节点需要进行克隆，然后分到两个链表中 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 其中的一个链表放在新数组的位置 i setTabAt(nextTab, i, ln); // 另一个链表放在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; else if (f instanceof TreeBin) &#123; // 红黑树的迁移 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 如果一分为二后，节点数少于 8，那么将红黑树转换回链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; // 将 ln 放置在新数组的位置 i setTabAt(nextTab, i, ln); // 将 hn 放置在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 说到底，transfer 这个方法并没有实现所有的迁移任务，每次调用这个方法只实现了 transferIndex 往前 stride 个位置的迁移工作，其他的需要由外围来控制。 这个时候，再回去仔细看 tryPresize 方法可能就会更加清晰一些了。 get 过程分析get 方法从来都是最简单的，这里也不例外： 计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) &amp; h 根据该位置处结点性质进行相应查找 如果该位置为 null，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 判断头结点是否就是我们需要的节点 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // 如果头结点的 hash 小于 0，说明 正在扩容，或者该位置是红黑树 else if (eh &lt; 0) // 参考 ForwardingNode.find(int h, Object k) 和 TreeBin.find(int h, Object k) return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 简单说一句，此方法的大部分内容都很简单，只有正好碰到扩容的情况，ForwardingNode.find(int h, Object k) 稍微复杂一些，不过在了解了数据迁移的过程后，这个也就不难了，所以限于篇幅这里也不展开说了。 总结其实也不是很难嘛，虽然没有像之前的 AQS 和线程池一样一行一行源码进行分析，但还是把所有初学者可能会糊涂的地方都进行了深入的介绍，只要是稍微有点基础的读者，应该是很容易就能看懂 HashMap 和 ConcurrentHashMap 源码了。 看源码不算是目的吧，深入地了解 Doug Lea 的设计思路，我觉得还挺有趣的，大师就是大师，代码写得真的是好啊。 我发现很多人都以为我写博客主要是源码分析，说真的，我对于源码分析没有那么大热情，主要都是为了用源码说事罢了，可能之后的文章还是会有比较多的源码分析成分。 不要脸地自以为本文的质量还是挺高的，信息量比较大，如果你觉得有写得不好的地方，或者说看完本文你还是没看懂它们，那么请提出来~ （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[想了解Java后端学习路线？你只需要这一张图！]]></title>
    <url>%2F2019%2F10%2F13%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2F%E6%83%B3%E4%BA%86%E8%A7%A3Java%E5%90%8E%E7%AB%AF%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%EF%BC%9F%E4%BD%A0%E5%8F%AA%E9%9C%80%E8%A6%81%E8%BF%99%E4%B8%80%E5%BC%A0%E5%9B%BE%EF%BC%81%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言学习路线图往往是学习一样技术的入门指南。网上搜到的Java学习路线图也是一抓一大把。 今天我只选一张图，仅此一图，足以包罗Java后端技术的知识点。所谓不求最好，但求最全，学习Java后端的同学完全可以参考这张图进行学习路线安排。 当然，有一些知识点是可选的，并不是说上面有的你都要会啦。我在复习秋招的过程中就是基于此图进行复习的，感觉效果还是不错的。 闲言少叙，我们还是看看这张图上都包含哪些有价值的信息吧。再次说明，本文只对路线图做一个简单地解读，仅供参考。大家可以根据自身情况来指定合理的学习计划，相信也会大有裨益。 由于图片比较大，如果觉得看不清楚，可以点击原文链接查看原图。 1 计算机基础这部分内容是计算机相关专业同学的课程，但是非科班的小伙伴（譬如在下）就需要花时间恶补了。特别是计算机网络，操作系统，数据结构这三门课程。 至于编译原理，个人大概懂一点就行了，我也只看过简单的概念和状态机相关的内容，并不是特别重要。 2 Java编程这里的Java编程部分包含了很多内容。我们可以分别看看，大概归纳一下就是这几个部分。 Java基础这里的Java基础包括基本语法，集合类框架，以及一些高级特性，比如反射，注解等等。 Java基础的知识点非常多，所以要真正搞懂也没有那么简单，另外，随着时间推移，一些新特性也需要得到我们的重视，比如时下流行的JDK8。 设计模式我一直觉得设计模式可以和Java基础一块学，所以我也把它放在这里。当然，一些真正使用到设计模式的地方，譬如JDK的集合类，IO流等等，也需要你足够重视。 Java Web技术Java Web技术包括J2EE，以及web框架，乃至一系列常用的组件。 1 J2EE主要包括的就是servlet，jsp这些比较复古的web开发组件了。虽然现在直接用它们的情况比较少，但是我们还是需要花一些时间去掌握它们的。 2 web框架常用的就是Spring了，相应的，hibernate和mybatis也需要了解一下。 3 同时，JavaWeb开发时的常用类库，比如jnuit单元测试，log4j日志工具，以及构建工具maven，都属于我们要掌握的范畴。 4 最后，要注意的是，Web相关的一些基本知识，比如HTTP协议，网络安全基础，也是我们要考虑的部分。 Java并发技术Java的并发技术泛指Java的多线程技术，以及JUC包里的并发类，比如线程池，并发工具类，阻塞队列等等。 Java并发技术完全可以独立出来学习，是Java进阶知识的一大重点。 Java网络编程和服务器这一块内容是Java中比较复杂但也很重要的一块内容。比如BIO,NIO,AIO的一些使用和原理，以及tomcat这类web服务器，甚至是netty这种网络编程框架，都是可以去了解和学习的内容。 Jvm基础与调优JVM是提供Java程序运行的一个进程，学习JVM知识，也是我们的必经之路。除了看懂《深入理解jvm虚拟机》以外，我们还要学习的内容就是JVM调优，使用合适的工具诊断问题，最后解决问题。 这部分内容在面试中呈现的不仅仅是GC,内存分区，以及类加载器，也包括了我所说的JVM调优问题。 3 Linux作为后台同学，常年被面试官问linux相关的问题，所以觉得学好linux还是蛮重要的，除了基本命令以外，最好还能了解一些shell脚本，甚至是内核相关的知识，这方面是我的一个弱项。 4 数据相关在这个路线图里，数据部分囊括了非常多的数据源，我们可以来看看都有哪些是我们需要掌握的。 关系数据库Mysql这个不必多说，人手都要会，不管是基础的crud，索引，抑或是进阶的存储引擎，分布式方案等，我们都需要对应掌握。 缓存如Redis，memcache一类的缓存，作为后端开发者的我们也需要对应掌握，当然，它们的高级特性，以及分布式方案，也是必备的知识点。 搜索引擎基于Lucene的solr，elasticsearch这类搜索引擎，本质上也是数据源，但是并不是后端必备的内容，不过学一学也没有坏处啦。 大数据海量数据处理的场景越来越多，大数据技术如hadoop，storm等也越来越火，但是大数据应用一般会由专业的大数据工程师来做，所以我们学一些基本内容也就足够了。 5 算法和数据结构 算法一直是校招同学面前的一座大山，作为后端同学来讲，除了基本的数据结构算法以外，也要会一些高级的算法，譬如dp，搜索，贪心等等。 另外，通过LeetCode等题库来刷题的方式来巩固算法也是公认的好办法了。 6 分布式最后一个部分，也是内容最多，覆盖面最广泛的部分了。分布式相关的技术实在太多了，我们这里也会做一下简单的归纳。 web架构先了解web架构的发展会对分布式的学习有更好的理解，毕竟架构的发展也对应着分布式技术的发展。 分布式理论这部分内容包括分布式的发展演化，base理论和cap理论等等，学习分布式技术之前，最好能对这部分概念有一定了解。 一致性问题强一致性的解决方案：事务和锁，弱一致性的方案：消息队列。 分布式session一个常见的问题，也有多种解决方案 分布式缓存和上面说的缓存一样，只不过这里侧重缓存的分布式方案 分布式数据库这里指的数据库的分布式方案，也包括hbase这种分布式数据库。 负载均衡负载均衡也是一个值得探讨的话题，一般我们讨论的是七层和四层负载均衡。 消息队列消息队列是一个比较复杂的分布式组件，我们可以了解常用消息队列比如amq，kafka等的实现。 服务化服务化的核心包括rpc，服务注册中心等等。分布式服务相关技术也是后端同学必须掌握的内容。 虚拟化虚拟化同样不是后端同学必须掌握的内容，只不过现在越来越多的服务部署方式使用的是docker和云服务的方式。所以了解一下也没有什么不好的。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>学习路线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈12：捋一捋大数据研发的基本概念]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%8812%EF%BC%9A%E6%8D%8B%E4%B8%80%E6%8D%8B%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A0%94%E5%8F%91%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[作者：[木东居士] 个人主页：http://www.mdjs.info也可以关注我：木东居士。 文章可以转载, 但必须以超链接形式标明文章原始出处和作者信息 0x00 前言 你了解你的数据吗？ 前几天突然来了点灵感，想梳理一下自己对数据的理解，因此便有了这篇博客或者说这系列博客来聊聊数据。 数据从业者有很多，比如说数据开发工程师、数据仓库工程师、数据分析师、数据挖掘工程师、数据产品经理等等，不同岗位的童鞋对数据的理解有很大的不一样，而且侧重点也不同。那么，是否有一些数据相关的基础知识是所有数据从业者都值得了解的？不同的岗位对数据的理解又有多大的不同？数据开发工程师是否有必要去了解数据分析师是如何看待数据的？ 本系列博客会尝试去学习、挖掘和总结这些内容，在数据的海洋中一起装x一起飞。 0x01 数据？数据！开篇先上几个问题： 你知道自己的系统数据接入量是多少吗？ 你知道数据的分布情况吗？ 你知道自己常用的数据有什么隐藏的坑吗？ 如果你对前面说的问题有不太了解的，那么我们就可以在以后的内容中一起愉快地交流和探讨。如果前面说的问题你的回答都是 “Yes”，那么我还是会尝试用新的问题来留住你。比如说： 既然你知道系统的数据接入量，那你知道每天的数据量波动吗？波动量在多大范围内是正常情况？ 你知道的数据分布情况是什么样子的？除了性别、年龄和城市的分布，还有什么分布？ 在偌大的数据仓库中，哪些数据被使用最多，哪些数据又无人问津，这些你了解吗？ 在最常用的那批数据中，有哪些核心的维度？有相同维度的两个表之间的数据口径是否也一样？ 假设你对上面的问题有稍许困惑或者感兴趣，我们正式开始对数据的认知之旅。 0x02 概览现在，我们粗略地将数据从业者分为数据集群运维、数据开发工程师、数据仓库工程师、数据分析师、数据挖掘工程师和数据产品经理，这一小节先起一个引子来大致说明不同岗位对数据的了解是不同的，后文会详细地说明细节内容。 首先要说明的是，在工作中数据相关的职位都是有很多重合的，很难一刀切区分不同岗位的职责，比如说数据开发工程师本身就是一个很大的概念，他可以做数据接入、数据清洗、数据仓库开发、数据挖掘算法开发等等，再比如说数据分析师，很多数据分析师既要做数据分析，又要做一些提数的需求，有时候还要自己做各种处理。 公司的数据团队越大，相应的岗位职责就会越细分，反之亦然。在这里我们姑且用数据开发工程师和数据仓库工程师做对比来说明不同职责的同学对数据理解的侧重点有什么不同。我们假设数据开发工程师侧重于数据的接入、存储和基本的数据处理，数据仓库工程师侧重于数据模型的设计和开发（比如维度建模）。 数据开发工程师对数据最基本的了解是需要知道数据的接入状态，比如说每天总共接入多少数据，整体数据量是多大，接入的业务有多少，每个业务的接入量多大，多大波动范围是正常？然后还要对数据的存储周期有一个把握，比如说有多少表的存储周期是30天，有多少是90天？集群每日新增的存储量是多大，多久后集群存储会撑爆？ 数据仓库工程师对上面的内容也要有一定的感知力，但是会有所区别，比如说，数据仓库工程师会更关注自己仓库建模中用到业务的数据状态。然后还需要知道终点业务的数据分布，比如说用户表中的年龄分布、性别分布、地域分布等。除此之外还应关注数据口径问题，比如说有很多份用户资料表，每张表的性别取值是否都是：男、女、未知，还是说会有用数值类型：1男、2女、0未知。 然后数据开发工程师对数据异常的侧重点可能会在今天的数据是否延迟落地，总量是否波动很大，数据可用率是否正常。 数据仓库工程师对数据异常的侧重点则可能是，今天落地的数据中性别为 0 的数据量是否激增（这可能会造成数据倾斜），某一个关键维度取值是否都为空。 上面的例子可能都会在一个数据质量监控系统中一起解决，但是我们在这里不讨论系统的设计，而是先有整体的意识和思路。 0x03 关于内容那么，后续博客的内容会是什么样子的呢？目前来看，我认为会有两个角度： 抛开岗位的区分度，从基本到高级来阐释对数据的了解。比如说数据分布，最基本的程度只需要知道每天的接入量；深一点地话需要了解到其中重点维度的分布，比如说男女各多少；再深一点可能要需要知道重点维度的数据值分布，比如说年龄分布，怎样来合理划分年龄段，不同年龄段的比例。 每个岗位会关注的一些侧重点。这点笔者认为不太好区分，因为很多岗位重合度比较高，但是笔者会尝试去总结，同时希望大家一起来探讨这个问题。 0xFF 总结本篇主要是抛出一些问题，后续会逐步展开地细说数据从业者对数据理解。其实最开始我想用“数据敏感度”、“数据感知力”这类标题，但是感觉这种概念比较难定义，因此用了比较口语化的标题。 笔者认为，在数据从业者的职业生涯中，不应只有编程、算法和系统，还应有一套数据相关的方法论，这套方法论会来解决某一领域的问题，即使你们的系统从Hadoop换到了Spark，数据模型从基本的策略匹配换到了深度学习，这些方法论也依旧会伴你整个职业生涯。因此这系列博客会尝试去学习、挖掘和总结一套这样的方法论，与君共勉。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈10：Docker 核心技术与实现原理]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%8810%EF%BC%9ADocker%20%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[要搞懂docker的核心原理和技术，首先一定要对Linux内核有一定了解。 提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Docker 已经成为了很多公司的标配，也不再是一个只能在开发阶段使用的玩具了。作为在生产环境中广泛应用的产品，Docker 有着非常成熟的社区以及大量的使用者，代码库中的内容也变得非常庞大。 同样，由于项目的发展、功能的拆分以及各种奇怪的改名 PR，让我们再次理解 Docker 的的整体架构变得更加困难。 虽然 Docker 目前的组件较多，并且实现也非常复杂，但是本文不想过多的介绍 Docker 具体的实现细节，我们更想谈一谈 Docker 这种虚拟化技术的出现有哪些核心技术的支撑。 首先，Docker 的出现一定是因为目前的后端在开发和运维阶段确实需要一种虚拟化技术解决开发环境和生产环境环境一致的问题，通过 Docker 我们可以将程序运行的环境也纳入到版本控制中，排除因为环境造成不同运行结果的可能。但是上述需求虽然推动了虚拟化技术的产生，但是如果没有合适的底层技术支撑，那么我们仍然得不到一个完美的产品。本文剩下的内容会介绍几种 Docker 使用的核心技术，如果我们了解它们的使用方法和原理，就能清楚 Docker 的实现原理。 Namespaces命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。 在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 Docker 其实就通过 Linux 的 Namespaces 对不同的容器实现了隔离。 Linux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 进程进程是 Linux 以及现在操作系统中非常重要的概念，它表示一个正在执行的程序，也是在现代分时系统中的一个任务单元。在每一个 *nix 的操作系统上，我们都能够通过 ps 命令打印出当前操作系统中正在执行的进程，比如在 Ubuntu 上，使用该命令就能得到以下的结果： 1$ ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Apr08 ? 00:00:09 /sbin/initroot 2 0 0 Apr08 ? 00:00:00 [kthreadd]root 3 2 0 Apr08 ? 00:00:05 [ksoftirqd/0]root 5 2 0 Apr08 ? 00:00:00 [kworker/0:0H]root 7 2 0 Apr08 ? 00:07:10 [rcu_sched]root 39 2 0 Apr08 ? 00:00:00 [migration/0]root 40 2 0 Apr08 ? 00:01:54 [watchdog/0]... 当前机器上有很多的进程正在执行，在上述进程中有两个非常特殊，一个是 pid 为 1 的 /sbin/init 进程，另一个是 pid 为 2 的 kthreadd 进程，这两个进程都是被 Linux 中的上帝进程 idle 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程，而后者负责管理和调度其他的内核进程。 如果我们在当前的 Linux 操作系统下运行一个新的 Docker 容器，并通过 exec 进入其内部的 bash 并打印其中的全部进程，我们会得到以下的结果： 1root@iZ255w13cy6Z:~# docker run -it -d ubuntub809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79root@iZ255w13cy6Z:~# docker exec -it b809a2eb3630 /bin/bashroot@b809a2eb3630:/# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 15:42 pts/0 00:00:00 /bin/bashroot 9 0 0 15:42 pts/1 00:00:00 /bin/bashroot 17 9 0 15:43 pts/1 00:00:00 ps -ef 在新的容器内部执行 ps 命令打印出了非常干净的进程列表，只有包含当前 ps -ef 在内的三个进程，在宿主机器上的几十个进程都已经消失不见了。 当前的 Docker 容器成功将容器内的进程与宿主机器中的进程隔离，如果我们在宿主机器上打印当前的全部进程时，会得到下面三条与 Docker 相关的结果： 1UID PID PPID C STIME TTY TIME CMDroot 29407 1 0 Nov16 ? 00:08:38 /usr/bin/dockerd --raw-logsroot 1554 29407 0 Nov19 ? 00:03:28 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runcroot 5006 1554 0 08:38 ? 00:00:00 docker-containerd-shim b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 /var/run/docker/libcontainerd/b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 docker-runc 在当前的宿主机器上，可能就存在由上述的不同进程构成的进程树： 这就是在使用 clone(2) 创建新进程时传入 CLONE_NEWPID 实现的，也就是使用 Linux 的命名空间实现进程的隔离，Docker 容器内部的任意进程都对宿主机器的进程一无所知。 1containerRouter.postContainersStart└── daemon.ContainerStart └── daemon.createSpec └── setNamespaces └── setNamespace Docker 的容器就是使用上述技术实现与宿主机器的进程隔离，当我们每次运行 docker run 或者 docker start 时，都会在下面的方法中创建一个用于设置进程间隔离的 Spec： 1func (daemon *Daemon) createSpec(c *container.Container) (*specs.Spec, error) &#123; s := oci.DefaultSpec() // ... if err := setNamespaces(daemon, &amp;s, c); err != nil &#123; return nil, fmt.Errorf(&quot;linux spec namespaces: %v&quot;, err) &#125; return &amp;s, nil&#125; 在 setNamespaces 方法中不仅会设置进程相关的命名空间，还会设置与用户、网络、IPC 以及 UTS 相关的命名空间： 1func setNamespaces(daemon *Daemon, s *specs.Spec, c *container.Container) error &#123; // user // network // ipc // uts // pid if c.HostConfig.PidMode.IsContainer() &#123; ns := specs.LinuxNamespace&#123;Type: &quot;pid&quot;&#125; pc, err := daemon.getPidContainer(c) if err != nil &#123; return err &#125; ns.Path = fmt.Sprintf(&quot;/proc/%d/ns/pid&quot;, pc.State.GetPID()) setNamespace(s, ns) &#125; else if c.HostConfig.PidMode.IsHost() &#123; oci.RemoveNamespace(s, specs.LinuxNamespaceType(&quot;pid&quot;)) &#125; else &#123; ns := specs.LinuxNamespace&#123;Type: &quot;pid&quot;&#125; setNamespace(s, ns) &#125; return nil&#125; 所有命名空间相关的设置 Spec 最后都会作为 Create 函数的入参在创建新的容器时进行设置： 1daemon.containerd.Create(context.Background(), container.ID, spec, createOptions) 所有与命名空间的相关的设置都是在上述的两个函数中完成的，Docker 通过命名空间成功完成了与宿主机进程和网络的隔离。 网络如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。 每一个使用 docker run 启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。 在这一部分，我们将介绍 Docker 默认的网络设置模式：网桥模式。在这种模式下，除了分配隔离的网络命名空间之外，Docker 还会为所有的容器设置 IP 地址。当 Docker 服务器在主机上启动之后会创建新的虚拟网桥 docker0，随后在该主机上启动的全部服务在默认情况下都与该网桥相连。 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口： 1$ brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242a6654980 no veth3e84d4f veth9953b75 docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。 1$ iptables -t nat -LChain PREROUTING (policy ACCEPT)target prot opt source destinationDOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCAL Chain DOCKER (2 references)target prot opt source destinationRETURN all -- anywhere anywhere 我们在当前的机器上使用 docker run -d -p 6379:6379 redis 命令启动了一个新的 Redis 容器，在这之后我们再查看当前 iptables 的 NAT 配置就会看到在 DOCKER 的链中出现了一条新的规则： 1DNAT tcp -- anywhere anywhere tcp dpt:6379 to:192.168.0.4:6379 上述规则会将从任意源发送到当前机器 6379 端口的 TCP 包转发到 192.168.0.4:6379 所在的地址上。 这个地址其实也是 Docker 为 Redis 服务分配的 IP 地址，如果我们在当前机器上直接 ping 这个 IP 地址就会发现它是可以访问到的： 1$ ping 192.168.0.4PING 192.168.0.4 (192.168.0.4) 56(84) bytes of data.64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=0.069 ms64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=0.043 ms^C--- 192.168.0.4 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 999msrtt min/avg/max/mdev = 0.043/0.056/0.069/0.013 ms 从上述的一系列现象，我们就可以推测出 Docker 是如何将容器的内部的端口暴露出来并对数据包进行转发的了；当有 Docker 的容器需要将服务暴露给宿主机器，就会为容器分配一个 IP 地址，同时向 iptables 中追加一条新的规则。 当我们使用 redis-cli 在宿主机器的命令行中访问 127.0.0.1:6379 的地址时，经过 iptables 的 NAT PREROUTING 将 ip 地址定向到了 192.168.0.4，重定向过的数据包就可以通过 iptables 中的 FILTER 配置，最终在 NAT POSTROUTING 阶段将 ip 地址伪装成 127.0.0.1，到这里虽然从外面看起来我们请求的是 127.0.0.1:6379，但是实际上请求的已经是 Docker 容器暴露出的端口了。 1$ redis-cli -h 127.0.0.1 -p 6379 pingPONG Docker 通过 Linux 的命名空间实现了网络的隔离，又通过 iptables 进行数据包转发，让 Docker 容器能够优雅地为宿主机器或者其他容器提供服务。 libnetwork整个网络部分的功能都是通过 Docker 拆分出来的 libnetwork 实现的，它提供了一个连接不同容器的实现，同时也能够为应用给出一个能够提供一致的编程接口和网络层抽象的容器网络模型。 The goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications. libnetwork 中最重要的概念，容器网络模型由以下的几个主要组件组成，分别是 Sandbox、Endpoint 和 Network： 在容器网络模型中，每一个容器内部都包含一个 Sandbox，其中存储着当前容器的网络栈配置，包括容器的接口、路由表和 DNS 设置，Linux 使用网络命名空间实现这个 Sandbox，每一个 Sandbox 中都可能会有一个或多个 Endpoint，在 Linux 上就是一个虚拟的网卡 veth，Sandbox 通过 Endpoint 加入到对应的网络中，这里的网络可能就是我们在上面提到的 Linux 网桥或者 VLAN。 想要获得更多与 libnetwork 或者容器网络模型相关的信息，可以阅读 Design · libnetwork 了解更多信息，当然也可以阅读源代码了解不同 OS 对容器网络模型的不同实现。 挂载点虽然我们已经通过 Linux 的命名空间解决了进程和网络隔离的问题，在 Docker 进程中我们已经没有办法访问宿主机器上的其他进程并且限制了网络的访问，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。 在新的进程中创建隔离的挂载点命名空间需要在 clone 函数中传入 CLONE_NEWNS，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统。 如果一个容器需要启动，那么它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中。 想要正常启动一个容器就需要在 rootfs 中挂载以上的几个特定的目录，除了上述的几个目录需要挂载之外我们还需要建立一些符号链接保证系统 IO 不会出现问题。 为了保证当前的容器进程没有办法访问宿主机器上其他目录，我们在这里还需要通过 libcontainer 提供的 pivot_root 或者 chroot 函数改变进程能够访问个文件目录的根节点。 1// pivor_rootput_old = mkdir(...);pivot_root(rootfs, put_old);chdir(&quot;/&quot;);unmount(put_old, MS_DETACH);rmdir(put_old); // chrootmount(rootfs, &quot;/&quot;, NULL, MS_MOVE, NULL);chroot(&quot;.&quot;);chdir(&quot;/&quot;); 到这里我们就将容器需要的目录挂载到了容器中，同时也禁止当前的容器进程访问宿主机器上的其他目录，保证了不同文件系统的隔离。 这一部分的内容是作者在 libcontainer 中的 SPEC.md 文件中找到的，其中包含了 Docker 使用的文件系统的说明，对于 Docker 是否真的使用 chroot 来确保当前的进程无法访问宿主机器的目录，作者其实也没有确切的答案，一是 Docker 项目的代码太多庞大，不知道该从何入手，作者尝试通过 Google 查找相关的结果，但是既找到了无人回答的 问题，也得到了与 SPEC 中的描述有冲突的 答案 ，如果各位读者有明确的答案可以在博客下面留言，非常感谢。 chroot 在这里不得不简单介绍一下 chroot（change root），在 Linux 系统中，系统默认的目录就都是以 / 也就是根目录开头的，chroot 的使用能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。 与 chroot 的相关内容部分来自 理解 chroot 一文，各位读者可以阅读这篇文章获得更详细的信息。 CGroups我们通过 Linux 的命名空间为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是命名空间并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存，如果在同一台机器上运行了多个对彼此以及宿主机器一无所知的『容器』，这些容器却共同占用了宿主机器的物理资源。 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题，而 Control Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。 每一个 CGroup 都是一组被相同的标准和参数限制的进程，不同的 CGroup 之间是有层级关系的，也就是说它们之间可以从父类继承一些用于限制资源使用的标准和参数。 Linux 的 CGroup 能够为一组进程分配资源，也就是我们在上面提到的 CPU、内存、网络带宽等资源，通过对资源的分配，CGroup 能够提供以下的几种功能： 在 CGroup 中，所有的任务就是一个系统的一个进程，而 CGroup 就是一组按照某种标准划分的进程，在 CGroup 这种机制中，所有的资源控制都是以 CGroup 作为单位实现的，每一个进程都可以随时加入一个 CGroup 也可以随时退出一个 CGroup。 – CGroup 介绍、应用实例及原理描述 Linux 使用文件系统来实现 CGroup，我们可以直接使用下面的命令查看当前的 CGroup 中有哪些子系统： 1$ lssubsys -mcpuset /sys/fs/cgroup/cpusetcpu /sys/fs/cgroup/cpucpuacct /sys/fs/cgroup/cpuacctmemory /sys/fs/cgroup/memorydevices /sys/fs/cgroup/devicesfreezer /sys/fs/cgroup/freezerblkio /sys/fs/cgroup/blkioperf_event /sys/fs/cgroup/perf_eventhugetlb /sys/fs/cgroup/hugetlb 大多数 Linux 的发行版都有着非常相似的子系统，而之所以将上面的 cpuset、cpu 等东西称作子系统，是因为它们能够为对应的控制组分配资源并限制资源的使用。 如果我们想要创建一个新的 cgroup 只需要在想要分配或者限制资源的子系统下面创建一个新的文件夹，然后这个文件夹下就会自动出现很多的内容，如果你在 Linux 上安装了 Docker，你就会发现所有子系统的目录下都有一个名为 docker 的文件夹： 1$ ls cpucgroup.clone_children ...cpu.stat docker notify_on_release release_agent tasks $ ls cpu/docker/9c3057f1291b53fd54a3d12023d2644efe6a7db6ddf330436ae73ac92d401cf1 cgroup.clone_children ...cpu.stat notify_on_release release_agent tasks 9c3057xxx 其实就是我们运行的一个 Docker 容器，启动这个容器时，Docker 会为这个容器创建一个与容器标识符相同的 CGroup，在当前的主机上 CGroup 就会有以下的层级关系： 每一个 CGroup 下面都有一个 tasks 文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，cpu.cfs_quota_us 文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。 如果系统管理员想要控制 Docker 某个容器的资源使用率就可以在 docker这个父控制组下面找到对应的子控制组并且改变它们对应文件的内容，当然我们也可以直接在程序运行时就使用参数，让 Docker 进程去改变相应文件中的内容。 1$ docker run -it -d --cpu-quota=50000 busybox53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274$ cd 53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274/$ lscgroup.clone_children cgroup.event_control cgroup.procs cpu.cfs_period_us cpu.cfs_quota_us cpu.shares cpu.stat notify_on_release tasks$ cat cpu.cfs_quota_us50000 当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除，Docker 在使用 CGroup 时其实也只是做了一些创建文件夹改变文件内容的文件操作，不过 CGroup 的使用也确实解决了我们限制子容器资源占用的问题，系统管理员能够为多个容器合理的分配资源并且不会出现多个容器互相抢占资源的问题。 UnionFSLinux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。 镜像到底是什么，它又是如何组成和组织的是作者使用 Docker 以来的一段时间内一直比较让作者感到困惑的问题，我们可以使用 docker run 非常轻松地从远程下载 Docker 的镜像并在本地运行。 Docker 镜像其实本质就是一个压缩包，我们可以使用下面的命令将一个 Docker 镜像中的文件导出： 1$ docker export $(docker create busybox) | tar -C rootfs -xvf -$ lsbin dev etc home proc root sys tmp usr var 你可以看到这个 busybox 镜像中的目录结构与 Linux 操作系统的根目录中的内容并没有太多的区别，可以说 Docker 镜像就是一个文件。 存储驱动Docker 使用了一系列不同的存储驱动管理镜像内的文件系统并运行容器，这些存储驱动与 Docker 卷（volume）有些不同，存储引擎管理着能够在多个容器之间共享的存储。 想要理解 Docker 使用的存储驱动，我们首先需要理解 Docker 是如何构建并且存储镜像的，也需要明白 Docker 的镜像是如何被每一个容器所使用的；Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层： 1FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py 容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像： 当镜像被 docker run 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。 容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。 AUFSUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统『联合』到同一个挂载点的文件系统服务。而 AUFS 即 Advanced UnionFS 其实就是 UnionFS 的升级版，它能够提供更优秀的性能和效率。 AUFS 作为联合文件系统，它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，这些文件夹在 AUFS 中称作分支，整个『联合』的过程被称为_联合挂载（Union Mount）_： 每一个镜像层或者容器层都是 /var/lib/docker/ 目录下的一个子文件夹；在 Docker 中，所有镜像层和容器层的内容都存储在 /var/lib/docker/aufs/diff/ 目录中： 1$ ls /var/lib/docker/aufs/diff/00adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c 93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d800adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c-init 93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d8-init019a8283e2ff6fca8d0a07884c78b41662979f848190f0658813bb6a9a464a90 93b06191602b7934fafc984fbacae02911b579769d0debd89cf2a032e7f35cfa... 而 /var/lib/docker/aufs/layers/ 中存储着镜像层的元数据，每一个文件都保存着镜像层的元数据，最后的 /var/lib/docker/aufs/mnt/ 包含镜像或者容器层的挂载点，最终会被 Docker 通过联合的方式进行组装。 上面的这张图片非常好的展示了组装的过程，每一个镜像层都是建立在另一个镜像层之上的，同时所有的镜像层都是只读的，只有每个容器最顶层的容器层才可以被用户直接读写，所有的容器都建立在一些底层服务（Kernel）上，包括命名空间、控制组、rootfs 等等，这种容器的组装方式提供了非常大的灵活性，只读的镜像层通过共享也能够减少磁盘的占用。 其他存储驱动AUFS 只是 Docker 使用的存储驱动的一种，除了 AUFS 之外，Docker 还支持了不同的存储驱动，包括 aufs、devicemapper、overlay2、zfs 和 vfs 等等，在最新的 Docker 中，overlay2 取代了 aufs 成为了推荐的存储驱动，但是在没有 overlay2 驱动的机器上仍然会使用 aufs 作为 Docker 的默认驱动。 不同的存储驱动在存储镜像和容器文件时也有着完全不同的实现，有兴趣的读者可以在 Docker 的官方文档 Select a storage driver 中找到相应的内容。 想要查看当前系统的 Docker 上使用了哪种存储驱动只需要使用以下的命令就能得到相对应的信息： 1$ docker info | grep StorageStorage Driver: aufs 作者的这台 Ubuntu 上由于没有 overlay2 存储驱动，所以使用 aufs 作为 Docker 的默认存储驱动。 总结Docker 目前已经成为了非常主流的技术，已经在很多成熟公司的生产环境中使用，但是 Docker 的核心技术其实已经有很多年的历史了，Linux 命名空间、控制组和 UnionFS 三大技术支撑了目前 Docker 的实现，也是 Docker 能够出现的最重要原因。 作者在学习 Docker 实现原理的过程中查阅了非常多的资料，从中也学习到了很多与 Linux 操作系统相关的知识，不过由于 Docker 目前的代码库实在是太过庞大，想要从源代码的角度完全理解 Docker 实现的细节已经是非常困难的了，但是如果各位读者真的对其实现细节感兴趣，可以从 Docker CE 的源代码开始了解 Docker 的原理。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈9：先搞懂Docker核心概念吧]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%889%EF%BC%9A%E5%85%88%E6%90%9E%E6%87%82Docker%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E5%90%A7%2F</url>
    <content type="text"><![CDATA[可能是把Docker的概念讲的最清楚的一篇文章本文只是对Docker的概念做了较为详细的介绍，并不涉及一些像Docker环境的安装以及Docker的一些常见操作和命令。 阅读本文大概需要15分钟，通过阅读本文你将知道一下概念： 容器 什么是Docker？ Docker思想、特点 Docker容器主要解决什么问题 容器 VS 虚拟机 Docker基本概念： 镜像（Image），容器（Container），仓库（Repository） Docker 是世界领先的软件容器平台，所以想要搞懂Docker的概念我们必须先从容器开始说起。 一 先从认识容器开始1.1 什么是容器？先来看看容器较为官方的解释一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 再来看看容器较为通俗的解释如果需要通俗的描述容器的话，我觉得容器就是一个存放东西的地方，就像书包可以装各种文具、衣柜可以放各种衣服、鞋架可以放各种鞋子一样。我们现在所说的容器存放的东西可能更偏向于应用比如网站、程序甚至是系统环境。 1.2 图解物理机、虚拟机与容器关于虚拟机与容器的对比在后面会详细介绍到，这里只是通过网上的图片加深大家对于物理机、虚拟机与容器这三者的理解。 物理机 虚拟机： 容器： 通过上面这三张抽象图，我们可以大概可以通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 相信通过上面的解释大家对于容器这个既陌生又熟悉的概念有了一个初步的认识，下面我们就来谈谈Docker的一些概念。 二 再来谈谈Docker的一些概念 2.1 什么是Docker？说实话关于Docker是什么并太好说，下面我通过四点向你说明Docker到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 的cgroup，namespace，以及AUFS类的UnionFS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进 程，因此也称其为容器。Docke最初实现是基于 LXC. Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 2.2 Docker思想 集装箱 标准化： ①运输方式 ② 存储方式 ③ API接口 隔离 2.3 Docker容器的特点 轻量在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 2.4 为什么要用Docker Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题；——一致的运行环境 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。——更快速的启动时间 避免公用的服务器，资源会容易受到其他用户的影响。——隔离性 善于处理集中爆发的服务器使用压力；——弹性伸缩，快速扩展 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。——迁移方便 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。——持续交付和部署 每当说起容器，我们不得不将其与虚拟机做一个比较。就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 三 容器 VS 虚拟机 简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。 3.1 两者对比图 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便. 3.2 容器与虚拟机 (VM) 总结 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。 通过Docker官网，我们知道了这么多Docker的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 3.3 容器与虚拟机 (VM)两者是可以共存的就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 Docker中非常重要的三个基本概念，理解了这三个概念，就理解了 Docker 的整个生命周期。 四 Docker基本概念Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期 4.1 镜像（Image）——一个特殊的文件系统 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 4.2 容器（Container)——镜像运行时的实体 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 4.3 仓库（Repository）——集中存放镜像文件的地方 镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry 公开服务是官方的 Docker Hub ，这也是默认的 Registry，并拥有大量的高质量的官方镜像，网址为：hub.docker.com/ 。在国内访问Docker Hub 可能会比较慢国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。 除了使用公开服务外，用户还可以在 本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。 Docker的概念基本上已经讲完，最后我们谈谈：Build, Ship, and Run。 五 最后谈谈：Build, Ship, and Run如果你搜索Docker官网，会发现如下的字样：“Docker - Build, Ship, and Run Any App, Anywhere”。那么Build, Ship, and Run到底是在干什么呢？ Build（构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Ship（运输镜像） ：主机和仓库间运输，这里的仓库就像是超级码头一样。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。 Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将Docker称为码头工人或码头装卸工，这和Docker的中文翻译搬运工人如出一辙。 六 总结本文主要把Docker中的一些常见概念做了详细的阐述，但是并不涉及Docker的安装、镜像的使用、容器的操作等内容。这部分东西，希望读者自己可以通过阅读书籍与官方文档的形式掌握。如果觉得官方文档阅读起来很费力的话，这里推荐一本书籍《Docker技术入门与实战第二版》。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈8：OpenStack架构设计]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%888%EF%BC%9AOpenStack%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[OpenStack 是开源云计算平台，支持多种虚拟化环境，并且其服务组件都提供了 API接口 便于二次开发。 OpenStack通过各种补充服务提供基础设施即服务 Infrastructure-as-a-Service (IaaS)的解决方案。每个服务都提供便于集成的应用程序接口Application Programming Interface (API)。 openstack 逻辑架构图 OpenStack 本身是一个分布式系统，不但各个服务可以分布部署，服务中的组件也可以分布部署。 这种分布式特性让 OpenStack 具备极大的灵活性、伸缩性和高可用性。 当然从另一个角度讲，这也使得 OpenStack 比一般系统复杂，学习难度也更大。 后面章节我们会深入学习 Keystone、Glance、Nova、Neutron 和 Cinder 这几个 OpenStack 最重要最核心的服务。 openstack的核心和扩展的主要项目如下： OpenStack Compute (code-name Nova) 计算服务 OpenStack Networking (code-name Neutron) 网络服务 OpenStack Object Storage (code-name Swift) 对象存储服务 OpenStack Block Storage (code-name Cinder) 块设备存储服务 OpenStack Identity (code-name Keystone) 认证服务 OpenStack Image Service (code-name Glance) 镜像文件服务 OpenStack Dashboard (code-name Horizon) 仪表盘服务 OpenStack Telemetry (code-name Ceilometer) 告警服务 OpenStack Orchestration (code-name Heat) 流程服务 OpenStack Database (code-name Trove) 数据库服务 OpenStack的各个服务之间通过统一的REST风格的API调用，实现系统的松耦合。上图是OpenStack各个服务之间API调用的概览，其中实线代表client 的API调用，虚线代表各个组件之间通过rpc调用进行通信。松耦合架构的好处是，各个组件的开发人员可以只关注各自的领域，对各自领域的修改不会影响到其他开发人员。不过从另一方面来讲，这种松耦合的架构也给整个系统的维护带来了一定的困难，运维人员要掌握更多的系统相关的知识去调试出了问题的组件。所以无论对于开发还是维护人员，搞清楚各个组件之间的相互调用关系是怎样的都是非常必要的。 对Linux经验丰富的OpenStack新用户，使用openstack是非常容易的，在后续openstack系列文章中会逐步展开介绍。 OpenStack 项目和组件OpenStack services Dashboard 【Horizon】 提供了一个基于web的自服务门户，与OpenStack底层服务交互，诸如启动一个实例，分配IP地址以及配置访问控制。 Compute 【Nova】 在OpenStack环境中计算实例的生命周期管理。按需响应包括生成、调度、回收虚拟机等操作。 Networking 【Neutron】 确保为其它OpenStack服务提供网络连接即服务，比如OpenStack计算。为用户提供API定义网络和使用。基于插件的架构其支持众多的网络提供商和技术。 Object Storage 【Swift】 通过一个 RESTful,基于HTTP的应用程序接口存储和任意检索的非结构化数据对象。它拥有高容错机制，基于数据复制和可扩展架构。它的实现并像是一个文件服务器需要挂载目录。在此种方式下，它写入对象和文件到多个硬盘中，以确保数据是在集群内跨服务器的多份复制。 Block Storage 【Cinder】 为运行实例而提供的持久性块存储。它的可插拔驱动架构的功能有助于创建和管理块存储设备。 Identity service 【Keystone】 为其他OpenStack服务提供认证和授权服务，为所有的OpenStack服务提供一个端点目录。 Image service 【Glance】 存储和检索虚拟机磁盘镜像，OpenStack计算会在实例部署时使用此服务。 Telemetry服务 【Ceilometer】 为OpenStack云的计费、基准、扩展性以及统计等目的提供监测和计量。 Orchestration服务 【Heat服务】 Orchestration服务支持多样化的综合的云应用，通过调用OpenStack-native REST API和CloudFormation-compatible Query API，支持HOT &lt;Heat Orchestration Template (HOT)&gt;格式模板或者AWS CloudFormation格式模板 通过对这些组件的介绍，可以帮助我们在后续的内容中，了解各个组件的作用，便于排查问题，而在你对基础安装，配置，操作和故障诊断熟悉之后，你应该考虑按照生产架构来进行部署。 生产部署架构建议使用自动化部署工具，例如Ansible, Chef, Puppet, or Salt来自动化部署，管理生产环境。 这个示例架构需要至少2个（主机）节点来启动基础服务virtual machine &lt;virtual machine (VM)&gt;或者实例。像块存储服务，对象存储服务这一类服务还需要额外的节点。 网络代理驻留在控制节点上而不是在一个或者多个专用的网络节点上。 私有网络的覆盖流量通过管理网络而不是专用网络 控制器控制节点上运行身份认证服务，镜像服务，计算服务的管理部分，网络服务的管理部分，多种网络代理以及仪表板。也需要包含一些支持服务，例如：SQL数据库，term:消息队列, and NTP。 可选的，可以在计算节点上运行部分块存储，对象存储，Orchestration 和 Telemetry 服务。 计算节点上需要至少两块网卡。 计算计算节点上运行计算服务中管理实例的管理程序部分。默认情况下，计算服务使用 KVM。 你可以部署超过一个计算节点。每个结算节点至少需要两块网卡。 块设备存储可选的块存储节点上包含了磁盘，块存储服务和共享文件系统会向实例提供这些磁盘。 为了简单起见，计算节点和本节点之间的服务流量使用管理网络。生产环境中应该部署一个单独的存储网络以增强性能和安全。 你可以部署超过一个块存储节点。每个块存储节点要求至少一块网卡。 对象存储可选的对象存储节点包含了磁盘。对象存储服务用这些磁盘来存储账号，容器和对象。 为了简单起见，计算节点和本节点之间的服务流量使用管理网络。生产环境中应该部署一个单独的存储网络以增强性能和安全。 这个服务要求两个节点。每个节点要求最少一块网卡。你可以部署超过两个对象存储节点。 网络openstack网络是非常复杂的，并且也支持多种模式其中支持GRE，VLAN,VXLAN等，在openstack中网络是通过一个组件Neutron提供服务，Neutron 管理的网络资源包括如下。 network 是一个隔离的二层广播域。Neutron 支持多种类型的 network，包括 local, flat, VLAN, VxLAN 和 GRE。 local 网络与其他网络和节点隔离。local 网络中的 instance 只能与位于同一节点上同一网络的 instance 通信，local 网络主要用于单机测试。 flat 网络是无 vlan tagging 的网络。flat 网络中的 instance 能与位于同一网络的 instance 通信，并且可以跨多个节点。 vlan 网络是具有 802.1q tagging 的网络。vlan 是一个二层的广播域，同一 vlan 中的 instance 可以通信，不同 vlan 只能通过 router 通信。vlan 网络可以跨节点，是应用最广泛的网络类型。 vxlan 是基于隧道技术的 overlay 网络。vxlan 网络通过唯一的 segmentation ID（也叫 VNI）与其他 vxlan 网络区分。vxlan 中数据包会通过 VNI 封装成 UPD 包进行传输。因为二层的包通过封装在三层传输，能够克服 vlan 和物理网络基础设施的限制。 gre 是与 vxlan 类似的一种 overlay 网络。主要区别在于使用 IP 包而非 UDP 进行封装。 不同 network 之间在二层上是隔离的。以 vlan 网络为例，network A 和 network B 会分配不同的 VLAN ID，这样就保证了 network A 中的广播包不会跑到 network B 中。当然，这里的隔离是指二层上的隔离，借助路由器不同 network 是可能在三层上通信的。network 必须属于某个 Project（ Tenant 租户），Project 中可以创建多个 network。 network 与 Project 之间是 1对多关系。 subnet 是一个 IPv4 或者 IPv6 地址段。instance 的 IP 从 subnet 中分配。每个 subnet 需要定义 IP 地址的范围和掩码。 port 可以看做虚拟交换机上的一个端口。port 上定义了 MAC 地址和 IP 地址，当 instance 的虚拟网卡 VIF（Virtual Interface） 绑定到 port 时，port 会将 MAC 和 IP 分配给 VIF。port 与 subnet 是 1对多 关系。一个 port 必须属于某个 subnet；一个 subnet 可以有多个 port。 如上图所示，为VLAN模式下，网络节点的通信方式。 在我们后续实施安装的时候，选择使用VXLAN网络模式，下面我们来重点介绍一下VXLAN模式。 VXLAN网络模式，可以隔离广播风暴，不需要交换机配置chunk口，解决了vlan id个数限制，解决了gre点对点隧道个数过多问题，实现了大2层网络，可以让vm在机房之间无缝迁移，便于跨机房部署。缺点是，vxlan增加了ip头部大小，需要降低vm的mtu值，传输效率上会略有下降。 涉及的 Linux 网络技术Neutron 的设计目标是实现“网络即服务”，为了达到这一目标，在设计上遵循了基于“软件定义网络”实现网络虚拟化的原则，在实现上充分利用了 Linux 系统上的各种网络相关的技术。理解了 Linux 系统上的这些概念将有利于快速理解 Neutron 的原理和实现。 bridge：网桥，Linux中用于表示一个能连接不同网络设备的虚拟设备，linux中传统实现的网桥类似一个hub设备，而ovs管理的网桥一般类似交换机。 br-int：bridge-integration，综合网桥，常用于表示实现主要内部网络功能的网桥。 br-ex：bridge-external，外部网桥，通常表示负责跟外部网络通信的网桥。 GRE：General Routing Encapsulation，一种通过封装来实现隧道的方式。在openstack中一般是基于L3的gre，即original pkt/GRE/IP/Ethernet VETH：虚拟ethernet接口，通常以pair的方式出现，一端发出的网包，会被另一端接收，可以形成两个网桥之间的通道。 qvb：neutron veth, Linux Bridge-side qvo：neutron veth, OVS-side TAP设备：模拟一个二层的网络设备，可以接受和发送二层网包。 TUN设备：模拟一个三层的网络设备，可以接受和发送三层网包。 iptables：Linux 上常见的实现安全策略的防火墙软件。 Vlan：虚拟 Lan，同一个物理 Lan 下用标签实现隔离，可用标号为1-4094。 VXLAN：一套利用 UDP 协议作为底层传输协议的 Overlay 实现。一般认为作为 VLan 技术的延伸或替代者。 namespace：用来实现隔离的一套机制，不同 namespace 中的资源之间彼此不可见。 总结openstack是一个非法复杂的分布式软件，涉及到很多底层技术，我自己对一些网络的理解也是非常有限，主要还是应用层面的知识，所以本章内容写的比较浅显一些，有问题请留言？在下一章节我们会进入生产环境如何实施规划openstack集群，至于openstack底层的技术，我也没有很深入研究，如果有任何不恰当的地方可以进行留言，非常感谢！ 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈7：OpenStack的基石KVM]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%887%EF%BC%9AOpenStack%E7%9A%84%E5%9F%BA%E7%9F%B3KVM%2F</url>
    <content type="text"><![CDATA[Qemu，KVM，Virsh傻傻的分不清原创文章，转载请注明： 转载自Itweet的博客 当你安装了一台Linux，想启动一个KVM虚拟机的时候，你会发现需要安装不同的软件，启动虚拟机的时候，有多种方法： virsh start kvm命令 qemu命令 qemu-kvm命令 qemu-system-x86_64命令 这些之间是什么关系呢？请先阅读上一篇《白话虚拟化技术》 有了上一篇的基础，我们就能说清楚来龙去脉。 KVM（Kernel-based Virtual Machine的英文缩写）是内核内建的虚拟机。有点类似于 Xen ，但更追求更简便的运作，比如运行此虚拟机，仅需要加载相应的 kvm 模块即可后台待命。和 Xen 的完整模拟不同的是，KVM 需要芯片支持虚拟化技术（英特尔的 VT 扩展或者 AMD 的 AMD-V 扩展）。 首先看qemu，其中关键字emu，全称emulator，模拟器，所以单纯使用qemu是采用的完全虚拟化的模式。 Qemu向Guest OS模拟CPU，也模拟其他的硬件，GuestOS认为自己和硬件直接打交道，其实是同Qemu模拟出来的硬件打交道，Qemu将这些指令转译给真正的硬件。由于所有的指令都要从Qemu里面过一手，因而性能比较差 按照上一次的理论，完全虚拟化是非常慢的，所以要使用硬件辅助虚拟化技术Intel-VT，AMD-V，所以需要CPU硬件开启这个标志位，一般在BIOS里面设置。查看是否开启 对于Intel CPU 可用命令 grep “vmx” /proc/cpuinfo 判断 对于AMD CPU 可用命令 grep “svm” /proc/cpuinfo 判断 当确认开始了标志位之后，通过KVM，GuestOS的CPU指令不用经过Qemu转译，直接运行，大大提高了速度。 所以KVM在内核里面需要有一个模块，来设置当前CPU是Guest OS在用，还是Host OS在用。 查看内核模块中是否含有kvm, ubuntu默认加载这些模块 KVM内核模块通过/dev/kvm暴露接口，用户态程序可以通过ioctl来访问这个接口，例如书写下面的程序 Qemu将KVM整合进来，通过ioctl调用/dev/kvm接口，将有关CPU指令的部分交由内核模块来做，就是qemu-kvm (qemu-system-XXX) Qemu-kvm对kvm的整合从release_0_5_1开始有branch，在1.3.0正式merge到master qemu和kvm整合之后，CPU的性能问题解决了，另外Qemu还会模拟其他的硬件，如Network, Disk，同样全虚拟化的方式也会影响这些设备的性能。 于是qemu采取半虚拟化或者类虚拟化的方式，让Guest OS加载特殊的驱动来做这件事情。 例如网络需要加载virtio_net，存储需要加载virtio_blk，Guest需要安装这些半虚拟化驱动，GuestOS知道自己是虚拟机，所以数据直接发送给半虚拟化设备，经过特殊处理，例如排队，缓存，批量处理等性能优化方式，最终发送给真正的硬件，一定程度上提高了性能。 至此整个关系如下： qemu-kvm会创建Guest OS，当需要执行CPU指令的时候，通过/dev/kvm调用kvm内核模块，通过硬件辅助虚拟化方式加速。如果需要进行网络和存储访问，则通过类虚拟化或者直通Pass through的方式，通过加载特殊的驱动，加速访问网络和存储资源。 然而直接用qemu或者qemu-kvm或者qemu-system-xxx的少，大多数还是通过virsh启动，virsh属于libvirt工具，libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和API，可不止管理KVM。 Libvirt分服务端和客户端，Libvirtd是一个daemon进程，是服务端，可以被本地的virsh调用，也可以被远程的virsh调用，virsh相当于客户端。 Libvirtd调用qemu-kvm操作虚拟机，有关CPU虚拟化的部分，qemu-kvm调用kvm的内核模块来实现 这下子，整个相互关系才搞清楚了。 虽然使用virsh创建虚拟机相对简单，但是为了探究虚拟机的究竟如何使用，下一次，我们来解析一下如何裸使用qemu-kvm来创建一台虚拟机，并且能上网。 如果搭建使用过vmware桌面版或者virtualbox桌面版，创建一个能上网的虚拟机非常简单，但是其实背后做了很多事情，下一次我们裸用qemu-kvm，全部使用手工配置，看创建虚拟机都做了哪些事情。 本章节我们主要介绍通过VMware技术虚拟出相关的Linux软件环境，在Linux系统中，安装KVM虚拟化软件，实实在在的去实践一下KVM到底是一个什么样的技术？ Kvm虚拟化技术实践VMware虚拟机支持Kvm虚拟化技术？在VMware创建的虚拟机中，默认不支持Kvm虚拟化技术，需要芯片级的扩展支持，幸好VMware提供完整的解决方案，可以通过修改虚拟化引擎。 VMware软件版本信息，VMware® Workstation 11.0.0 build-2305329 首先，你需要启动VMware软件，新建一个CentOS 6.x类型的虚拟机，正常安装完成，这个虚拟机默认的虚拟化引擎，首选模式为”自动”。 如果想让我们的VMware虚拟化出来的CentOS虚拟机支持KVM虚拟化，我们需要修改它支持的虚拟化引擎,打开新建的虚拟机，虚拟机状态必须处于关闭状态，通过双击编辑虚拟机设置 &gt; 硬件 ，选择处理器菜单，右边会出现虚拟化引擎区域，选择首选模式为 Intel Tv-x/EPT或AMD-V/RVI,接下来勾选虚拟化Intel Tv-x/EPT或AMD-V/RVI(v)，点击确定。 KVM需要虚拟机宿主（host）的处理器带有虚拟化支持（对于Intel处理器来说是VT-x，对于AMD处理器来说是AMD-V）。你可以通过以下命令来检查你的处理器是否支持虚拟化： 1grep --color -E &apos;(vmx|svm)&apos; /proc/cpuinfo 如果运行后没有显示，那么你的处理器不支持硬件虚拟化，你不能使用KVM。 注意: 如果是硬件服务器，您可能需要在BIOS中启用虚拟化支持，参考 Private Cloud personal workstation 安装Kvm虚拟化软件安装kvm虚拟化软件，我们需要一个Linux操作系统环境，这里我们选择的Linux版本为CentOS release 6.8 (Final)，在这个VMware虚拟化出来的虚拟机中安装kvm虚拟化软件，具体步骤如下： 首选安装epel源 1sudo rpm -ivh http://mirrors.ustc.edu.cn/fedora/epel/6/x86_64/epel-release-6-8.noarch.rpm 安装kvm虚拟化软件 1sudo yum install qemu-kvm qeum-kvm-tools virt-manager libvirt 启动kvm虚拟化软件 1sudo /etc/init.d/libvirtd start 启动成功之后你可以通过/etc/init.d/libvirtd status查看启动状态，这个时候，kvm会自动生成一个本地网桥 virbr0，可以通过命令查看他的详细信息 1# ifconfig virbr0virbr0 Link encap:Ethernet HWaddr 52:54:00:D7:23:AD inet addr:192.168.122.1 Bcast:192.168.122.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 b) TX bytes:0 (0.0 b) KVM默认使用NAT网络模式。虚拟机获取一个私有 IP（例如 192.168.122.0/24 网段的），并通过本地主机的NAT访问外网。 1# brctl showbridge name bridge id STP enabled interfacesvirbr0 8000.525400d723ad yes virbr0-nic 创建一个本地网桥virbr0，包括两个端口：virbr0-nic 为网桥内部端口，vnet0 为虚拟机网关端口（192.168.122.1）。 虚拟机启动后，配置 192.168.122.1（vnet0）为网关。所有网络操作均由本地主机系统负责。 DNS/DHCP的实现，本地主机系统启动一个 dnsmasq 来负责管理。 1ps aux|grep dnsmasq 注意： 启动libvirtd之后自动启动iptables，并且写上一些默认规则。 1# iptables -nvL -t natChain PREROUTING (policy ACCEPT 304 packets, 38526 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 7 packets, 483 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE tcp -- * * 192.168.122.0/24 !192.168.122.0/24 masq ports: 1024-65535 0 0 MASQUERADE udp -- * * 192.168.122.0/24 !192.168.122.0/24 masq ports: 1024-65535 0 0 MASQUERADE all -- * * 192.168.122.0/24 !192.168.122.0/24 Chain OUTPUT (policy ACCEPT 7 packets, 483 bytes) pkts bytes target prot opt in out source destination kvm创建虚拟机上传一个镜像文件：CentOS-6.6-x86_64-bin-DVD1.iso 通过qemu创建一个raw格式的文件(注：QEMU使用的镜像文件：qcow2与raw，它们都是QEMU(KVM)虚拟机使用的磁盘文件格式)，大小为5G。 1qemu-img create -f raw /data/Centos-6.6-x68_64.raw 5G 查看创建的raw磁盘格式文件信息 1qemu-img info /data/Centos-6.6-x68_64.raw image: /data/Centos-6.6-x68_64.rawfile format: rawvirtual size: 5.0G (5368709120 bytes)disk size: 0 启动，kvm虚拟机，进行操作系统安装 1virt-install --virt-type kvm --name CentOS-6.6-x86_64 --ram 512 --cdrom /data/CentOS-6.6-x86_64-bin-DVD1.iso --disk path=/data/Centos-6.6-x68_64.raw --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole 启动之后，通过命令查看启动状态，默认会在操作系统开一个5900的端口，可以通过虚拟机远程管理软件vnc客户端连接，然后可视化的方式安装操作系统。 1# netstat -ntlp|grep 5900tcp 0 0 0.0.0.0:5900 0.0.0.0:* LISTEN 2504/qemu-kvm 注意：kvm安装的虚拟机，不确定是那一台，在后台就是一个进程，每增加一台端口号+1，第一次创建的为5900！ 虚拟机远程管理软件我们可以使用虚拟机远程管理软件VNC进行操作系统的安装，我使用过的两款不错的虚拟机远程管理终端软件，一个是Windows上使用，一个在Mac上为了方便安装一个Google Chrome插件后即可开始使用，软件信息 Tightvnc 或者 VNC[@Viewer](https://link.juejin.im/?target=https%3A%2F%2Fgithub.com%2FViewer &quot;@Viewer&quot;) for Google Chrome 如果你和我一样使用的是Google Chrome提供的VNC插件，使用方式，在Address输入框中输入，宿主机IP:59000,Picture Quality选择框使用默认选项，点击Connect进入到安装操作系统的界面，你可以安装常规的方式进行安装，等待系统安装完成重启，然后就可以正常使用kvm虚拟化出来的操作系统了。 Tightvnc软件的使用，请参考官方手册。 Tightvnc下载地址：www.tightvnc.com/download.ph… Tightvnc下载地址：www.tightvnc.com/download/2.… Tightvnc下载地址：www.tightvnc.com/download/2.… KVM虚拟机管理kvm虚拟机是通过virsh命令进行管理的，libvirt是Linux上的虚拟化库，是长期稳定的C语言API，支持KVM/QEMU、Xen、LXC等主流虚拟化方案。链接：libvirt.org/virsh是Libvirt对应的shell命令。 查看所有虚拟机状态 1virsh list --all 启动虚拟机 1virsh start [NAME] 列表启动状态的虚拟机 1virsh list 常用命令查看 1virsh --help|more less libvirt虚拟机配置文件虚拟机libvirt配置文件在/etc/libvirt/qemu路径下，生产中我们需要去修改它的网络信息。 1# lltotal 8-rw-------. 1 root root 3047 Oct 19 2016 Centos-6.6-x68_64.xmldrwx------. 3 root root 4096 Oct 17 2016 networks 注意：不能直接修改xml文件，需要通过提供的命令！ 1virsh edit Centos-6.6-x68_64 kvm三种网络类型,桥接、NAT、仅主机模式，默认NAT模式,其他机器无法登陆，生产中一般选择桥接。 监控kvm虚拟机 安装软件监控虚拟机 1yum install virt-top -y 查看虚拟机资源使用情况 1virt-top virt-top 23:46:39 - x86_64 1/1CPU 3392MHz 3816MB1 domains, 1 active, 1 running, 0 sleeping, 0 paused, 0 inactive D:0 O:0 X:0CPU: 5.6% Mem: 2024 MB (2024 MB by guests) ID S RDRQ WRRQ RXBY TXBY %CPU %MEM TIME NAME 1 R 0 1 52 0 5.6 53.0 5:16.15 centos-6.8 KVM修改NAT模式为桥接[案例]在开始案例之前，需要知道的必要信息，宿主机IP是192.168.2.200，操作系统版本Centos-6.6-x68_64。 启动虚拟网卡 1ifup eth0 这里网卡是NAT模式，可以上网，ping通其他机器，但是其他机器无法登陆！ 宿主机查看网卡信息 1brctl show ifconfig virbr0 ifconfig vnet0 实现网桥，在kvm宿主机完成 步骤1，创建一个网桥，新建网桥连接到eth0,删除eth0,让新的网桥拥有eth0的ip 1brctl addbr br0 #创建一个网桥 brctl show #显示网桥信息 brctl addif br0 eth0 &amp;&amp; ip addr del dev eth0 192.168.2.200/24 &amp;&amp; ifconfig br0 192.168.2.200/24 up brctl show #查看结果ifconfig br0 #验证br0是否成功取代了eth0的IP 注意: 这里的IP地址为 宿主机ip 修改虚拟机桥接到br0网卡，在宿主机修改 1virsh list --all ps aux |grep kvm virsh stop Centos-6.6-x68_64 virsh list --all 修改虚拟机桥接到宿主机，修改52行type为bridge，第54行bridge为br0 1# virsh edit Centos-6.6-x68_64 # 命令 52 &lt;interface type=&apos;network&apos;&gt; 53 &lt;mac address=&apos;52:54:00:2a:2d:60&apos;/&gt; 54 &lt;source network=&apos;default&apos;/&gt; 55 56 &lt;/interface&gt; 修改为：52 &lt;interface type=&apos;bridge&apos;&gt; 53 &lt;mac address=&apos;52:54:00:2a:2d:60&apos;/&gt; 54 &lt;source bridge=&apos;br0&apos;/&gt; 55 56 &lt;/interface&gt; 启动虚拟机，看到启动前后，桥接变化，vnet0被桥接到了br0 启动前： 1# brctl showbridge name bridge id STP enabled interfacesbr0 8000.000c29f824c9 no eth0virbr0 8000.525400353d8e yes virbr0-nic 启动后： 1# virsh start CentOS-6.6-x86_64Domain CentOS-6.6-x86_64 started # brctl show bridge name bridge id STP enabled interfacesbr0 8000.000c29f824c9 no eth0 vnet0virbr0 8000.525400353d8e yes virbr0-nic Vnc登陆后，修改ip地址，看到dhcp可以使用，被桥接到现有的ip段，ip是自动获取,而且是和宿主机在同一个IP段. 1# ifup eth0 从宿主机登陆此服务器，可以成功。 1# ssh 192.168.2.108root@192.168.2.108&apos;s password: Last login: Sat Jan 30 12:40:28 2016 从同一网段其他服务器登陆此虚拟机,也可以成功,至此让kvm管理的服务器能够桥接上网就完成了，在生产环境中，桥接上网是非常必要的。 总结通过kvm相关的命令来创建虚拟机，安装和调试是非常必要的，因为现有的很多私有云，公有云产品都使用到了kvm这样的技术，学习基本的kvm使用对维护openstack集群有非常要的作用，其次所有的openstack image制作也得通过kvm这样的底层技术来完成，最后上传到openstack的镜像管理模块，才能开始通过openstack image生成云主机。 到此，各位应该能够体会到，其实kvm是一个非常底层和核心的虚拟化技术，而openstack就是对kvm这样的技术进行了一个上层封装，可以非常方便，可视化的操作和维护kvm虚拟机，这就是现在牛上天的云计算技术最底层技术栈，具体怎么实现请看下图。 如上图，没有openstack我们依然可以通过，libvirt来对虚拟机进行操作，只不过比较繁琐和难以维护。通过openstack就可以非常方便的进行底层虚拟化技术的管理、维护、使用。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈6：白话虚拟化技术]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%886%EF%BC%9A%E7%99%BD%E8%AF%9D%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[内核，是指的操作系统内核。 所有的操作系统都有内核，无论是Windows还是Linux，都管理着三个重要的资源：计算，网络，存储。 计算指CPU和内存，网络即网络设备，存储即硬盘之类的。 内核是个大管家，想象你的机器上跑着很多的程序，有word，有excel，看着视频，听着音乐，每个程序都要使用CPU和内存，都要上网，都要存硬盘，如果没有一个大管家管着，大家随便用，就乱了。所以需要管家来协调调度整个资源，谁先用，谁后用，谁用多少，谁放在这里，谁放在那里，都需要管家操心。 所以在这个计算机大家庭里面，管家有着比普通的程序更高的权限，运行在内核态，而其他的普通程序运行在用户态，用户态的程序一旦要申请公共的资源，就需要向管家申请，管家帮它分配好，它才能用。 为了区分内核态和用户态，CPU专门设置四个特权等级0,1,2,3 来做这个事情。 当时写Linux内核的时候，估计大牛们还不知道将来虚拟机会大放异彩，大牛们想，一共两级特权，一个内核态，一个用户态，却有四个等级，好奢侈，好富裕，就敞开了用，内核态运行在第0等级，用户态运行在第3等级，占了两头，太不会过日子了。 大牛们在写Linux内核的时候，如果用户态程序做事情，就将扳手掰到第3等级，一旦要申请使用更多的资源，就需要申请将扳手掰到第0等级，内核才能在高权限访问这些资源，申请完资源，返回到用户态，扳手再掰回去。 这个程序一直非常顺利的运行着，直到虚拟机的出现。 如果大家用过Vmware桌面版，或者Virtualbox桌面版，你可以用这个虚拟化软件创建虚拟机，在虚拟机里面安装一个Linux或者windows，外面的操作系统也可以是Linux或者Windows。 当你使用虚拟机软件的时候，和你的excel一样，都是在你的任务栏里面并排的放着，是一个普通的应用。 当你进入虚拟机的时候，虚拟机里面的excel也是一个普通的应用。 但是当你设身处地的站在虚拟机里面的内核的角度思考一下人生，你就困惑了，我到底个啥？ 在硬件上的操作系统来看，我是一个普通的应用，只能运行在用户态。可是大牛们生我的时候，我的每一行代码，都告诉我，我是个内核啊，应该运行在内核态，当虚拟机里面的excel要访问网络的时候，向我请求，我的代码就要努力的去操作网络资源，我努力，但是我做不到，我没有权限！ 我分裂了。 虚拟化层，也就是Vmware或者Virtualbox需要帮我解决这个问题。 第一种方式，完全虚拟化，其实就是骗我。虚拟化软件模拟假的CPU，内存，网络，硬盘给我，让我自我感觉良好，终于又像个内核了。 真正的工作模式是这样的。 虚拟机内核：我要在CPU上跑一个指令！ 虚拟化软件：没问题，你是内核嘛，可以跑 虚拟化软件转过头去找物理机内核：报告管家，我管理的虚拟机里面的一个要执行一个CPU指令，帮忙来一小段时间空闲的CPU时间，让我代他跑个指令。 物理机内核：你等着，另一个跑着呢。好嘞，他终于跑完了，该你了。 虚拟化软件：我代他跑，终于跑完了，出来结果了 虚拟化软件转头给虚拟机内核：哥们，跑完了，结果是这个，我说你是内核吧，绝对有权限，没问题，下次跑指令找我啊。 虚拟机内核：看来我真的是内核呢。可是哥，好像这点指令跑的有点慢啊。 虚拟化软件：这就不错啦，好几个排着队跑呢。 内存的申请模式如下。 虚拟机内核：我启动需要4G内存，我好分给我上面的应用。 虚拟化软件：没问题，才4G，你是内核嘛，马上申请好。 虚拟化软件转头给物理机内核：报告，管家，我启动了一个虚拟机，需要4G内存，给我4个房间呗。 物理机内核：怎么又一个虚拟机啊，好吧，给你90,91,92,93四个房间。 虚拟化软件转头给虚拟机内核：哥们，内存有了，0,1,2,3这个四个房间都是你的，你看，你是内核嘛，独占资源，从0编号的就是你的。 虚拟机内核：看来我真的是内核啊，能从头开始用。那好，我就在房间2的第三个柜子里面放个东西吧。 虚拟化软件：要放东西啊，没问题。心里想：我查查看，这个虚拟机是90号房间开头的，他要在房间2放东西，那就相当于在房间92放东西。 虚拟化软件转头给物理机内核：报告，管家，我上面的虚拟机要在92号房间的第三个柜子里面放个东西。 好了，说完了CPU和内存的例子，不细说网络和硬盘了，也是类似，都是虚拟化软件模拟一个给虚拟机内核看的，其实啥事儿都需要虚拟化软件转一遍。 这种方式一个坏处，就是慢，往往慢到不能忍受。 于是虚拟化软件想，我能不能不当传话筒，还是要让虚拟机内核正视自己的身份，别说你是内核，你还真喘上了，你不是物理机，你是虚拟机。 但是怎么解决权限等级的问题呢？于是Intel的VT-x和AMD的AMD-V从硬件层面帮上了忙。当初谁让你们这些写内核的大牛用等级这么奢侈，用完了0，就是3，也不省着点用，没办法，只好另起炉灶弄一个新的标志位，表示当前是在虚拟机状态下，还是真正的物理机内核下。 对于虚拟机内核来讲，只要将标志位设为虚拟机状态，则可以直接在CPU上执行大部分的指令，不需要虚拟化软件在中间转述，除非遇到特别敏感的指令，才需要将标志位设为物理机内核态运行，这样大大提高了效率。 所以安装虚拟机的时候，务必要将物理CPU的这个标志位打开，是否打开对于Intel可以查看grep “vmx” /proc/cpuinfo，对于AMD可以查看grep “svm” /proc/cpuinfo 这叫做硬件辅助虚拟化。 另外就是访问网络或者硬盘的时候，为了取得更高的性能，也需要让虚拟机内核加载特殊的驱动，也是让虚拟机内核从代码层面就重新定位自己的身份，不能像访问物理机一样访问网络或者硬盘，而是用一种特殊的方式：我知道我不是物理机内核，我知道我是虚拟机，我没那么高的权限，我很可能和很多虚拟机共享物理资源，所以我要学会排队，我写硬盘其实写的是一个物理机上的文件，那我的写文件的缓存方式是不是可以变一下，我发送网络包，根本就不是发给真正的网络设备，而是给虚拟的设备，我可不可以直接在内存里面拷贝给他，等等等等。 一旦我知道我不是物理机内核，痛定思痛，只好重新认识自己，反而能找出很多方式来优化我的资源访问。 这叫做类虚拟化或者半虚拟化。 如果您想更技术的了解本文背后的原理，请看书《系统虚拟化——原理与实现》 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈5：云计算的前世今生]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%885%EF%BC%9A%E4%BA%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F%2F</url>
    <content type="text"><![CDATA[作者简介：刘超，网易云解决方案首席架构师。10年云计算领域研发及架构经验，Open DC/OS贡献者。长期专注于kubernetes, OpenStack、Hadoop、Docker、Lucene、Mesos等开源软件的企业级应用及产品化。曾出版《Lucene应用开发揭秘》。 以下为正文： 云计算概述云计算主要解决了四个方面的内容：计算，网络，存储，应用。 计算就是CPU和内存，例如“1+1”这个最简单的算法就是把“1”放在内存里面，然后CPU做加法，返回的结果“2”又保存在内存里面。网络就是你插根网线能上网。存储就是你下个电影有地方放。本次讨论就是围绕这四个部分来讲的。其中，计算、网络、存储三个是IaaS层面，应用是PaaS层面。 云计算发展脉络云计算整个发展过程，用一句话来形容，就是“分久必合，合久必分”。 第一阶段：合，即物理设备物理设备简介在互联网发展初期，大家都爱用物理设备： 服务器用物理机，像戴尔、惠普、IBM、联想等物理服务器，随着硬件设备的进步，物理服务器越来越强大了，64核128G内存都算是普通配置； 网络用的是硬件交换机和路由器，例如思科的，华为的，从1GE到10GE，现在有40GE和100GE，带宽越来越牛； 存储方面有的用普通的磁盘，也有更快的SSD盘。容量从M，到G，连笔记本电脑都能配置到T，更何况磁盘阵列； 物理设备的缺点部署应用直接使用物理机，看起来很爽，有种土豪的感觉，却有大大的缺点： 人工运维。如果你在一台服务器上安装软件，把系统安装坏了，怎么办？只有重装。当你想配置一下交换机的参数，需要串口连上去进行配置；当你想增加一块磁盘，要买一块插进服务器，这些都需要人工来，而且很大可能要求机房。你们公司在北五环，机房在南六环，这酸爽。 浪费资源。其实你只想部署一个小小的网站，却要用128G的内存。混着部署吧，就有隔离性的问题。 隔离性差。你把好多的应用部署在同一台物理机上，他们之间抢内存、抢cpu，一个写满了硬盘，另一个就没法用了，一个弄挂了内核，另一个也跟著挂了，如果部署两个相同的应用，端口还会冲突，动不动就会出错。 第二阶段：分，即虚拟化虚拟化简介因为物理设备的以上缺点，就有了第一次“合久必分”的过程，叫做虚拟化。所谓虚拟化，就是把实的变成虚的： 物理机变为虚拟机。cpu是虚拟的，内存是虚拟的，内核是虚拟的，硬盘是虚拟的； 物理交换机变为虚拟交换机。网卡是虚拟的，交换机是虚拟的，带宽也是虚拟的； 物理存储变成虚拟存储。多块硬盘虚拟成一大块； 虚拟化解决的问题虚拟化很好地解决了在物理设备阶段存在的三个问题： 人工运维。虚拟机的创建和删除都可以远程操作，虚拟机被玩坏了，删了再建一个分钟级别的。虚拟网络的配置也可以远程操作，创建网卡、分配带宽都是调用接口就能搞定的； 资源浪费。虚拟化了以后，资源可以分配地很小很小，比如1个cpu，1G内存，1M带宽，1G硬盘，都可以被虚拟出来； 隔离性差。每个虚拟机都有独立的cpu、 内存、硬盘、网卡，不同虚拟机之间的应用互不干扰； 虚拟化时代的生态在虚拟化阶段，领跑者是Vmware，可以实现基本的计算、网络、存储的虚拟化。如同这个世界有闭源就有开源、有windows就有linux、有Apple就有Android一样，有Vmware，就有Xen和KVM。 在开源虚拟化方面，Xen 的Citrix做的不错，后来Redhat在KVM发力不少；对于网络虚拟化，有Openvswitch，可以通过命令创建网桥、网卡、设置VLAN、设置带宽；对于存储虚拟化，本地盘有LVM，可以将多个硬盘变成一大块盘，然后在里面切出一小块给用户。 虚拟化的缺点但是虚拟化也有缺点。通过虚拟化软件创建虚拟机，需要人工指定放在哪台机器上、硬盘放在哪个存储设备上，网络的VLAN ID、带宽的具体配置等，都需要人工指定。所以仅使用虚拟化的运维工程师往往有一个Excel表格，记录有多少台物理机，每台机器部署了哪些虚拟机。受此限制，一般虚拟化的集群数目都不是特别大。 第三阶段：合，即云计算云计算解决的问题为了解决虚拟化阶段遗留的问题，于是有了分久必合的过程。这个过程我们可以形象地称为池化。虚拟化将资源分得很细，但是如此细分的资源靠Excel去管理，成本太高。池化就是将资源打成一个大的池，当需要资源的时候，帮助用户自动地选择，而非用户指定。这个阶段的关键点：调度器Scheduler。 私有云、公有云的两极分化这样，Vmware有了自己的Vcloud；也有了基于Xen和KVM的私有云平台CloudStack（后来Citrix将其收购后开源）。 当这些私有云平台在用户的数据中心里卖得奇贵无比、赚得盆满钵盈的时候，有其他的公司开始了另外的选择。这就是AWS和Google，他们开始了公有云领域的探索。 AWS最初就是基于Xen技术进行虚拟化的，并且最终形成了公有云平台。也许AWS最初只是不想让自己的电商领域的利润全部交给私有云厂商吧，所以自己的云平台首先支撑起了自己的业务。在这个过程中，AWS严肃地使用了自己的云计算平台，使得公有云平台并不是对资源的配置更加友好，而是对应用的部署更加友好，最终大放异彩。 私有云厂商与公有云厂商的联系与区别如果仔细观察就会发现，私有云和公有云虽然使用的是类似的技术，但在产品设计上却是完全不同的两种生物。 私有云厂商和公有云厂商也拥有类似的技术，但在产品运营上呈现出完全不同的基因。 私有云厂商是卖资源的，所以往往在卖私有云平台的时候伴随着卖计算、网络、存储设备。在产品设计上，私有云厂商往往会对客户强调其几乎不会使用的计算、网络、存储的技术参数，因为这些参数可以在和友商对标的过程中占尽优势。私有云的厂商几乎没有自己的大规模应用，所以私有云厂商的平台做出来是给别人用的，自己不会大规模使用，所以产品往往围绕资源展开，而不会对应用的部署友好。 公有云的厂商往往都是有自己大规模的应用需要部署，所以其产品的设计可以将常见的应用部署需要的模块作为组件提供出来，用户可以像拼积木一样，拼接一个适用于自己应用的架构。公有云厂商不必关心各种技术参数的PK，不必关心是否开源，是否兼容各种虚拟化平台，是否兼容各种服务器设备、网络设备、存储设备。你管我用什么，客户部署应用方便就好。 公有云生态及老二的逆袭公有云的第一名AWS活的自然很爽，作为第二名Rackspace就不那么舒坦了。 没错，互联网行业基本上就是一家独大，那第二名如何逆袭呢？开源是很好的办法，让整个行业一起为这个云平台出力。于是Rackspace与美国航空航天局（NASA）合作创始了开源云平台OpenStack。 OpenStack现在发展的和AWS有点像了，所以从OpenStack的模块组成可以看到云计算池化的方法。 OpenStack的组件 计算池化模块Nova：OpenStack的计算虚拟化主要使用KVM，然而到底在哪个物理机上开虚拟机呢，这要靠nova-scheduler； 网络池化模块Neutron：OpenStack的网络虚拟化主要使用Openvswitch，然而对于每一个Openvswitch的虚拟网络、虚拟网卡、VLAN、带宽的配置，不需要登录到集群上配置，Neutron可以通过SDN的方式进行配置； 存储池化模块Cinder: OpenStack的存储虚拟化，如果使用本地盘，则基于LVM，使用哪个LVM上分配的盘，也是通过scheduler来的。后来就有了将多台机器的硬盘打成一个池的方式Ceph，而调度的过程，则在Ceph层完成。 OpenStack带来私有云市场的红海有了OpenStack，所有的私有云厂商都疯了，原来VMware在私有云市场赚的实在太多了，眼巴巴的看着，没有对应的平台可以和他抗衡。现在有了现成的框架，再加上自己的硬件设备，几乎所有的IT厂商巨头，全部都加入到社区里，将OpenStack开发为自己的产品，连同硬件设备一起，杀入私有云市场。 公有or私有？网易云的选择网易云当然也没有错过这次风口，上线了自己的OpenStack集群，网易云基于OpenStack自主研发了IaaS服务，在计算虚拟化方面，通过裁剪KVM镜像，优化虚拟机启动流程等改进，实现了虚拟机的秒级别启动。在网络虚拟化方面，通过SDN和Openvswitch技术，实现了虚拟机之间的高性能互访。在存储虚拟化方面，通过优化Ceph存储，实现高性能云盘。 但是网易云并没有杀进私有云市场，而是使用OpenStack支撑起了自己的应用，这是互联网的思维。而仅仅是资源层面弹性是不够的，还需要开发出对应用部署友好的组件。例如数据库，负载均衡，缓存等，这些都是应用部署必不可少的，也是网易云在大规模应用实践中，千锤百炼过的。这些组件称为PaaS。 第四阶段：分，即容器现在来谈谈，应用层面，即PaaS层。前面一直在讲IaaS层的故事，也即基础设施即服务，基本上在谈计算、网络、存储的事情。现在应该说说应用层，即PaaS层的事情了。 1. PaaS的定义与作用IaaS的定义比较清楚，PaaS的定义就没那么清楚了。有人把数据库、负载均衡、缓存作为PaaS服务；有人把大数据Hadoop,、Spark平台作为PaaS服务；还有人将应用的安装与管理，例如Puppet、 Chef,、Ansible作为PaaS服务。 其实PaaS主要用于管理应用层。我总结为两部分：一部分是你自己的应用应当自动部署，比如Puppet、Chef、Ansible、 Cloud Foundry等，可以通过脚本帮你部署；另一部分是你觉得复杂的通用应用不用部署，比如数据库、缓存、大数据平台，可以在云平台上一点即得。 要么就是自动部署，要么就是不用部署，总的来说就是应用层你也少操心，就是PaaS的作用。当然最好还是都不用去部署，一键可得，所以公有云平台将通用的服务都做成了PaaS平台。另一些你自己开发的应用，除了你自己其他人不会知道，所以你可以用工具变成自动部署。 2. PaaS的优点PaaS最大的优点，就是可以实现应用层的弹性伸缩。比如在双十一期间，10个节点要变成100个节点，如果使用物理设备，再买90台机器肯定来不及，仅仅有IaaS实现资源的弹性是不够的，再创建90台虚拟机，也是空的，还是需要运维人员一台一台地部署。所以有了PaaS就好了，一台虚拟机启动后，马上运行自动部署脚本，进行应用的安装，90台机器自动安装好了应用，才是真正的弹性伸缩。 3. PaaS部署的问题当然这种部署方式也有一个问题，就是无论Puppet、 Chef、Ansible把安装脚本抽象的再好，说到底也是基于脚本的，然而应用所在的环境千差万别。文件路径的差别，文件权限的差别，依赖包的差别，应用环境的差别，Tomcat、 PHP、 Apache等软件版本的差别，JDK、Python等版本的差别，是否安装了一些系统软件，是否占用了哪些端口，都可能造成脚本执行的不成功。所以看起来是一旦脚本写好，就能够快速复制了，但是环境稍有改变，就需要把脚本进行新一轮的修改、测试、联调。例如在数据中心写好的脚本移到AWS上就不一定直接能用，在AWS上联调好了，迁移到Google Cloud上也可能会再出问题。 容器的诞生1. 容器的定义于是容器便应运而生。容器是Container，Container另一个意思是集装箱，其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是打包，二是标准。设想没有集装箱的时代，如果将货物从A运到B，中间要经过三个码头，换三次船的话，货物每次都要卸下船来，摆的七零八落，然后换船的时候，需要重新摆放整齐，在没有集装箱的时候，船员们都需要在岸上待几天再走。而在有了集装箱后，所有的货物都打包在一起了，并且集装箱的尺寸全部一致，所以每次换船的时候，整体一个箱子搬过去就可以了，小时级别就能完成，船员再也不用长时间上岸等待了。 2.容器在开发中的应用设想A就是程序员，B就是用户，货物就是代码及运行环境，中间的三个码头分别是开发，测试，上线。假设代码的运行环境如下： Ubuntu操作系统 创建用户hadoop 下载解压JDK 1.7在某个目录下 将这个目录加入JAVA_HOME和PATH的环境变量里面 将环境变量的export放在hadoop用户的home目录下的.bashrc文件中 下载并解压tomcat 7 将war放到tomcat的webapp路径下面 修改tomcat的启动参数，将Java的Heap Size设为1024M 看，一个简单的Java网站，就需要考虑这么多零零散散的东西，如果不打包，就需要在开发，测试，生产的每个环境上查看，保证环境的一致，甚至要将这些环境重新搭建一遍，就像每次将货物打散了重装一样麻烦。中间稍有差池，比如开发环境用了JDK 1.8，而线上是JDK 1.7；比如开发环境用了root用户，线上需要使用hadoop用户，都可能导致程序的运行失败。 容器的诞生云计算的前世今生（上）中提到：云计算解决了基础资源层的弹性伸缩，却没有解决PaaS层应用随基础资源层弹性伸缩而带来的批量、快速部署问题。于是容器应运而生。 容器是Container，Container另一个意思是集装箱，其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是打包，二是标准。 在没有集装箱的时代，假设将货物从A运到B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，摆的七零八落，然后搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能走。 有了集装箱以后，所有的货物都打包在一起了，并且集装箱的尺寸全部一致，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不能上岸长时间耽搁了。这是集装箱“打包”、“标准”两大特点在生活中的应用。下面用一个简单的案例来看看容器在开发部署中的实际应用。 假设有一个简单的Java网站需要上线，代码的运行环境如下： 看，一个简单的Java网站，就有这么多零零散散的东西！这就像很多零碎地货物，如果不打包，就需要在开发、测试、生产的每个环境上重新查看以保证环境的一致，有时甚至要将这些环境重新搭建一遍，就像每次将货物卸载、重装一样麻烦。中间稍有差池，比如开发环境用了JDK 1.8，而线上是JDK 1.7；比如开发环境用了root用户，线上需要使用hadoop用户，都可能导致程序的运行失败。 那么容器如何对应用打包呢？还是要学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。好在ubuntu中的lxc技术早就能做到这一点。 封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为namespace，也即每个namespace中的应用看到的是不同的IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为cgroup，也即明明整台机器有很多的CPU、内存，而一个应用只能用其中的一部分。有了这两项技术，集装箱的铁盒子我们是焊好了，接下来是决定往里面放什么。 最简单粗暴的方法，就是将上面列表中所有的都放到集装箱里面。但是这样太大了！因为即使你安装一个干干静静的ubuntu操作系统，什么都不装，就很大了。把操作系统装进容器相当于把船也放到了集装箱里面！传统的虚拟机镜像就是这样的，动辄几十G。答案当然是NO！所以第一项操作系统不能装进容器。 撇下第一项操作系统，剩下的所有的加起来，也就几百M，就轻便多了。因此一台服务器上的容器是共享操作系统内核的，容器在不同机器之间的迁移不带内核，这也是很多人声称容器是轻量级的虚拟机的原因。轻不白轻，自然隔离性就差了，一个容器让操作系统崩溃了，其他容器也就跟着崩溃了，这相当于一个集装箱把船压漏水了，所有的集装箱一起沉。 另一个需要撇下的就是随着应用的运行而产生并保存在本地的数据。这些数据多以文件的形式存在，例如数据库文件、文本文件。这些文件会随着应用的运行，越来越大，如果这些数据也放在容器里面，会让容器变得很大，影响容器在不同环境的迁移。而且这些数据在开发、测试、线上环境之间的迁移是没有意义的，生产环境不可能用测试环境的文件，所以往往这些数据也是保存在容器外面的存储设备上。也是为什么人们称容器是无状态的。 至此集装箱焊好了，货物也装进去了，接下来就是如何将这个集装箱标准化，从而在哪艘船上都能运输。这里的标准一个是镜像，一个是容器的运行环境。 所谓的镜像，就是将你焊好集装箱的那个时刻，将集装箱的状态保存下来，就像孙悟空说定，集装箱里面就定在了那一刻，然后将这一刻的状态保存成一系列文件。这些文件的格式是标准的，谁看到这些文件，都能还原当时定住的那个时刻。将镜像还原成运行时的过程（就是读取镜像文件，还原那个时刻的过程）就是容器的运行的过程。除了大名鼎鼎的Docker，还有其他的容器，例如AppC、Mesos Container，都能运行容器镜像。所以说容器不等于Docker。 总而言之，容器是轻量级的、隔离差的、适用于无状态的，可以基于镜像标准实现跨主机、跨环境的随意迁移。 有了容器，使得PaaS层对于用户自身应用的自动部署变得快速而优雅。容器快，快在了两方面，第一是虚拟机启动的时候要先启动操作系统，容器不用启动操作系统，因为是共享内核的。第二是虚拟机启动后使用脚本安装应用，容器不用安装应用，因为已经打包在镜像里面了。所以最终虚拟机的启动是分钟级别，而容器的启动是秒级。容器咋这么神奇。其实一点都不神奇，第一是偷懒少干活了，第二是提前把活干好了。 因为容器的启动快，人们往往不会创建一个个小的虚拟机来部署应用，因为这样太费时间了，而是创建一个大的虚拟机，然后在大的虚拟机里面再划分容器，而不同的用户不共享大的虚拟机，可以实现操作系统内核的隔离。这又是一次合久必分的过程。由IaaS层的虚拟机池，划分为更细粒度的容器池。 容器管理平台有了容器的管理平台，又是一次分久必合的过程。 容器的粒度更加细，管理起来更难管，甚至是手动操作难以应对的。假设你有100台物理机，其实规模不是太大，用Excel人工管理是没问题的，但是一台上面开10台虚拟机，虚拟机的个数就是1000台，人工管理已经很困难了，但是一台虚拟机里面开10个容器，就是10000个容器，你是不是已经彻底放弃人工运维的想法了。 所以容器层面的管理平台是一个新的挑战，关键字就是自动化： 自发现：容器与容器之间的相互配置还能像虚拟机一样，记住IP地址，然后互相配置吗？这么多容器，你怎么记得住一旦一台虚拟机挂了重启，IP改变，应该改哪些配置，列表长度至少万行级别的啊。所以容器之间的配置通过名称来的，无论容器跑到哪台机器上，名称不变，就能访问到。 自修复：容器挂了，或是进程宕机了，能像虚拟机那样，登陆上去查看一下进程状态，如果不正常重启一下么？你要登陆万台docker了。所以容器的进程挂了，容器就自动挂掉了，然后自动重启。 弹性自伸缩 Auto Scaling：当容器的性能不足的时候，需要手动伸缩，手动部署么？当然也要自动来。 当前火热的容器管理平台有三大流派： 一个是Kubernetes，我们称为段誉型。段誉(Kubernetes)的父亲(Borg)武功高强，出身皇族(Google)，管理过偌大的一个大理国(Borg是Google数据中心的容器管理平台)。作为大理段式后裔，段誉的武功基因良好(Kubernetes的理念设计比较完善)，周围的高手云集，习武环境也好(Kubernetes生态活跃，热度高)，虽然刚刚出道的段誉武功不及其父亲，但是只要跟着周围的高手不断切磋，武功既可以飞速提升。 一个是Mesos，我们称为乔峰型。乔峰(Mesos)的主要功夫降龙十八掌(Mesos的调度功能)独步武林，为其他帮派所无。而且乔峰也管理过人数众多的丐帮(Mesos管理过Tweeter的容器集群)。后来乔峰从丐帮出来，在江湖中特例独行(Mesos的创始人成立了公司Mesosphere)。乔峰的优势在于，乔峰的降龙十八掌(Mesos)就是在丐帮中使用的降龙十八掌，相比与段誉初学其父的武功来说，要成熟很多。但是缺点是，降龙十八掌只掌握在少数的几个丐帮帮主手中(Mesos社区还是以Mesosphere为主导)，其他丐帮兄弟只能远远崇拜乔峰，而无法相互切磋(社区热度不足)。 一个是Swarm，我们称为慕容型。慕容家族(Swarm是Docker家族的集群管理软件)的个人功夫是非常棒的(Docker可以说称为容器的事实标准)，但是看到段誉和乔峰能够管理的组织规模越来越大，有一统江湖的趋势，着实眼红了，于是开始想创建自己的慕容鲜卑帝国(推出Swarm容器集群管理软件)。但是个人功夫好，并不代表着组织能力强(Swarm的集群管理能力)，好在慕容家族可以借鉴段誉和乔峰的组织管理经验，学习各家公司，以彼之道，还施彼身，使得慕容公子的组织能力(Swarm借鉴了很多前面的集群管理思想)也在逐渐的成熟中。 三大容器门派，到底鹿死谁手，谁能一统江湖，尚未可知。 网易之所以选型Kubernetes作为自己的容器管理平台，是因为基于 Borg 成熟的经验打造的 Kubernetes，为容器编排管理提供了完整的开源方案，并且社区活跃，生态完善，积累了大量分布式、服务化系统架构的最佳实践。 容器初体验想不想尝试一下最先进的容器管理平台呢？我们先了解一下Docker的生命周期。如图所示。 图中最中间就是最核心的两个部分，一个是镜像Images，一个是容器Containers。镜像运行起来就是容器。容器运行的过程中，基于原始镜像做了改变，比如安装了程序，添加了文件，也可以提交回去(commit)成为镜像。如果大家安装过系统，镜像有点像GHOST镜像，从GHOST镜像安装一个系统，运行起来，就相当于容器；容器里面自带应用，就像GHOST镜像安装的系统里面不是裸的操作系统，里面可能安装了微信，QQ，视频播放软件等。安装好的系统使用的过程中又安装了其他的软件，或者下载了文件，还可以将这个系统重新GHOST成一个镜像，当其他人通过这个镜像再安装系统的时候，则其他的软件也就自带了。 普通的GHOST镜像就是一个文件，但是管理不方便，比如如果有十个GHOST镜像的话，你可能已经记不清楚哪个镜像里面安装了哪个版本的软件了。所以容器镜像有tag的概念，就是一个标签，比如dev-1.0，dev-1.1，production-1.1等，凡是能够帮助你区分不同镜像的，都可以。为了镜像的统一管理，有一个镜像库的东西，可以通过push将本地的镜像放到统一的镜像库中保存，可以通过pull将镜像库中的镜像拉到本地来。 从镜像运行一个容器可使用下面的命令，如果初步使用Docker，记下下面这一个命令就可以了。 这行命令会启动一个里面安装了mysql的容器。其中docker run就是运行一个容器；–name就是给这个容器起个名字；-v 就是挂数据盘，将外面的一个目录/my/own/datadir挂载到容器里面的一个目录/var/lib/mysql作为数据盘，外面的目录是在容器所运行的主机上的，也可以是远程的一个云盘；-e 是设置容器运行环境的环境变量，环境变量是最常使用的设置参数的方式，例如这里设置mysql的密码。mysql:tag就是镜像的名字和标签。 docker stop可以停止这个容器，start可以再启动这个容器，restart可以重启这个容器。在容器内部做了改变，例如安装了新的软件，产生了新的文件，则调用docker commit变成新的镜像。 镜像生产过程，除了可以通过启动一个docker，手动修改，然后调用docker commit形成新镜像之外，还可以通过书写Dockerfile，通过docker build来编译这个Dockerfile来形成新镜像。为什么要这样做呢？前面的方式太不自动化了，需要手工干预，而且还经常会忘了手工都做了什么。用Dockerfile可以很好的解决这个问题。 Dockerfile的一个简单的例子如下： 这其实是一个镜像的生产说明书，Docker build的过程就是根据这个生产说明书来生产镜像： FROM基础镜像，先下载这个基础镜像，然后从这个镜像启动一个容器，并且登陆到容器里面； RUN运行一个命令，在容器里面运行这个命令； COPY/ADD将一些文件添加到容器里面； 最终给容器设置启动命令 ENTRYPOINT，这个命令不在镜像生成过程中执行，而是在容器运行的时候作为主程序执行； 将所有的修改commit成镜像。 这里需要说明一下的就是主程序，是Docker里面一个重要的概念，虽然镜像里面可以安装很多的程序，但是必须有一个主程序，主程序和容器的生命周期完全一致，主程序在则容器在，主程序亡则容器亡。 就像图中展示的一样，容器是一个资源限制的框，但是这个框没有底，全靠主进程撑着，主进程挂了，衣服架子倒了，衣服也就垮了。 了解了如何运行一个独立的容器，接下来介绍如何使用容器管理平台。 容器管理平台初体验 容器管理平台会对容器做更高的抽象，容器不再是单打独斗，而且组成集团军共同战斗。多个容器组成一个Pod，这几个容器亲如兄弟，干的也是相关性很强的活，能够通过localhost访问彼此，真是兄弟齐心，力可断金。有的任务一帮兄弟还刚不住，就需要多个Pod合力完成，这个由ReplicationController进行控制，可以将一个Pod复制N个副本，同时承载任务，众人拾柴火焰高。 N个Pod如果对外散兵作战，一是无法合力，二是给人很乱的感觉，因而需要有一个老大，作为代言人，将大家团结起来，一致对外，这就是Service。老大对外提供统一的虚拟IP和端口，并将这个IP和服务名关联起来，访问服务名，则自动映射为虚拟IP。老大的意思就是，如果外面要访问我这个团队，喊一声名字就可以，例如”雷锋班，帮敬老院打扫卫生！”，你不用管雷锋班的那个人去打扫卫生，每个人打扫哪一部分，班长会统一分配。 最上层通过namespace分隔完全隔离的环境，例如生产环境，测试环境，开发环境等。就像军队分华北野战军，东北野战军一样。野战军立正，出发，部署一个Tomcat的Java应用。 作者：网易云基础服务链接：https://www.jianshu.com/p/52312b1eb633來源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈4：Elasticsearch与solr入门实践]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%884%EF%BC%9AElasticsearch%E4%B8%8Esolr%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[阮一峰：全文搜索引擎 Elasticsearch 入门教程阅读 1093 收藏 76 2017-08-23 原文链接：www.ruanyifeng.com 9月7日-8日 北京，与 Google Twitch 等团队技术大咖面对面www.bagevent.com 全文搜索属于最常见的需求，开源的 Elasticsearch （以下简称 Elastic）是目前全文搜索引擎的首选。 它可以快速地储存、搜索和分析海量数据。维基百科、Stack Overflow、Github 都采用它。 Elastic 的底层是开源库 Lucene。但是，你没法直接用 Lucene，必须自己写代码去调用它的接口。Elastic 是 Lucene 的封装，提供了 REST API 的操作接口，开箱即用。 本文从零开始，讲解如何使用 Elastic 搭建自己的全文搜索引擎。每一步都有详细的说明，大家跟着做就能学会。 一、安装Elastic 需要 Java 8 环境。如果你的机器还没安装 Java，可以参考这篇文章，注意要保证环境变量JAVA_HOME正确设置。 安装完 Java，就可以跟着官方文档安装 Elastic。直接下载压缩包比较简单。 12&gt; $ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.1.zip$ unzip elasticsearch-5.5.1.zip$ cd elasticsearch-5.5.1/ &gt; 接着，进入解压后的目录，运行下面的命令，启动 Elastic。 12&gt; $ ./bin/elasticsearch&gt; 如果这时报错“max virtual memory areas vm.max_map_count [65530] is too low”，要运行下面的命令。 12&gt; $ sudo sysctl -w vm.max_map_count=262144&gt; 如果一切正常，Elastic 就会在默认的9200端口运行。这时，打开另一个命令行窗口，请求该端口，会得到说明信息。 12&gt; $ curl localhost:9200 &#123; &quot;name&quot; : &quot;atntrTf&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;tf9250XhQ6ee4h7YI11anA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.5.1&quot;, &quot;build_hash&quot; : &quot;19c13d0&quot;, &quot;build_date&quot; : &quot;2017-07-18T20:44:24.823Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;&gt; 上面代码中，请求9200端口，Elastic 返回一个 JSON 对象，包含当前节点、集群、版本等信息。 按下 Ctrl + C，Elastic 就会停止运行。 默认情况下，Elastic 只允许本机访问，如果需要远程访问，可以修改 Elastic 安装目录的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，然后重新启动 Elastic。 12&gt; network.host: 0.0.0.0&gt; 上面代码中，设成0.0.0.0让任何人都可以访问。线上服务不要这样设置，要设成具体的 IP。 二、基本概念2.1 Node 与 ClusterElastic 本质上是一个分布式数据库，允许多台服务器协同工作，每台服务器可以运行多个 Elastic 实例。 单个 Elastic 实例称为一个节点（node）。一组节点构成一个集群（cluster）。 2.2 IndexElastic 会索引所有字段，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。 所以，Elastic 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。 下面的命令可以查看当前节点的所有 Index。 12&gt; $ curl -X GET &apos;http://localhost:9200/_cat/indices?v&apos;&gt; 2.3 DocumentIndex 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。 Document 使用 JSON 格式表示，下面是一个例子。 12&gt; &#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot;&#125;&gt; 同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。 2.4 TypeDocument 可以分组，比如weather这个 Index 里面，可以按城市分组（北京和上海），也可以按气候分组（晴天和雨天）。这种分组就叫做 Type，它是虚拟的逻辑分组，用来过滤 Document。 不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。 下面的命令可以列出每个 Index 所包含的 Type。 12&gt; $ curl &apos;localhost:9200/_mapping?pretty=true&apos;&gt; 根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。 三、新建和删除 Index新建 Index，可以直接向 Elastic 服务器发出 PUT 请求。下面的例子是新建一个名叫weather的 Index。 12&gt; $ curl -X PUT &apos;localhost:9200/weather&apos;&gt; 服务器返回一个 JSON 对象，里面的acknowledged字段表示操作成功。 12&gt; &#123; &quot;acknowledged&quot;:true, &quot;shards_acknowledged&quot;:true&#125;&gt; 然后，我们发出 DELETE 请求，删除这个 Index。 12&gt; $ curl -X DELETE &apos;localhost:9200/weather&apos;&gt; 四、中文分词设置首先，安装中文分词插件。这里使用的是 ik，也可以考虑其他插件（比如 smartcn）。 12&gt; $ ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip&gt; 上面代码安装的是5.5.1版的插件，与 Elastic 5.5.1 配合使用。 接着，重新启动 Elastic，就会自动加载这个新安装的插件。 然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。 12&gt; $ curl -X PUT &apos;localhost:9200/accounts&apos; -d &apos;&#123; &quot;mappings&quot;: &#123; &quot;person&quot;: &#123; &quot;properties&quot;: &#123; &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;desc&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;&apos;&gt; 上面代码中，首先新建一个名称为accounts的 Index，里面有一个名称为person的 Type。person有三个字段。 user title desc 这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。 Elastic 的分词器称为 analyzer。我们对每个字段指定分词器。 12&gt; &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_max_word&quot;&#125;&gt; 上面代码中，analyzer是字段文本的分词器，search_analyzer是搜索词的分词器。ik_max_word分词器是插件ik提供的，可以对文本进行最大数量的分词。 五、数据操作5.1 新增记录向指定的 /Index/Type 发送 PUT 请求，就可以在 Index 里面新增一条记录。比如，向/accounts/person发送请求，就可以新增一条人员记录。 12&gt; $ curl -X PUT &apos;localhost:9200/accounts/person/1&apos; -d &apos;&#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot;&#125;&apos; &gt; 服务器返回的 JSON 对象，会给出 Index、Type、Id、Version 等信息。 12&gt; &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;1&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;, &quot;created&quot;:true&#125;&gt; 如果你仔细看，会发现请求路径是/accounts/person/1，最后的1是该条记录的 Id。它不一定是数字，任意字符串（比如abc）都可以。 新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。 12&gt; $ curl -X POST &apos;localhost:9200/accounts/person&apos; -d &apos;&#123; &quot;user&quot;: &quot;李四&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;系统管理&quot;&#125;&apos;&gt; 上面代码中，向/accounts/person发出一个 POST 请求，添加一个记录。这时，服务器返回的 JSON 对象里面，_id字段就是一个随机字符串。 12&gt; &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;AV3qGfrC6jMbsbXb6k1p&quot;, &quot;_version&quot;:1, &quot;result&quot;:&quot;created&quot;, &quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;, &quot;created&quot;:true&#125;&gt; 注意，如果没有先创建 Index（这个例子是accounts），直接执行上面的命令，Elastic 也不会报错，而是直接生成指定的 Index。所以，打字的时候要小心，不要写错 Index 的名称。 5.2 查看记录向/Index/Type/Id发出 GET 请求，就可以查看这条记录。 12&gt; $ curl &apos;localhost:9200/accounts/person/1?pretty=true&apos;&gt; 上面代码请求查看/accounts/person/1这条记录，URL 的参数pretty=true表示以易读的格式返回。 返回的数据中，found字段表示查询成功，_source字段返回原始记录。 12&gt; &#123; &quot;_index&quot; : &quot;accounts&quot;, &quot;_type&quot; : &quot;person&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;数据库管理&quot; &#125;&#125;&gt; 如果 Id 不正确，就查不到数据，found字段就是false。 12&gt; $ curl &apos;localhost:9200/weather/beijing/abc?pretty=true&apos; &#123; &quot;_index&quot; : &quot;accounts&quot;, &quot;_type&quot; : &quot;person&quot;, &quot;_id&quot; : &quot;abc&quot;, &quot;found&quot; : false&#125;&gt; 5.3 删除记录删除记录就是发出 DELETE 请求。 12&gt; $ curl -X DELETE &apos;localhost:9200/accounts/person/1&apos;&gt; 这里先不要删除这条记录，后面还要用到。 5.4 更新记录更新记录就是使用 PUT 请求，重新发送一次数据。 12&gt; $ curl -X PUT &apos;localhost:9200/accounts/person/1&apos; -d &apos;&#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;数据库管理，软件开发&quot;&#125;&apos; &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;1&quot;, &quot;_version&quot;:2, &quot;result&quot;:&quot;updated&quot;, &quot;_shards&quot;:&#123;&quot;total&quot;:2,&quot;successful&quot;:1,&quot;failed&quot;:0&#125;, &quot;created&quot;:false&#125;&gt; 上面代码中，我们将原始数据从”数据库管理”改成”数据库管理，软件开发”。 返回结果里面，有几个字段发生了变化。 12&gt; &quot;_version&quot; : 2,&quot;result&quot; : &quot;updated&quot;,&quot;created&quot; : false&gt; 可以看到，记录的 Id 没变，但是版本（version）从1变成2，操作类型（result）从created变成updated，created字段变成false，因为这次不是新建记录。 六、数据查询6.1 返回所有记录使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; &#123; &quot;took&quot;:2, &quot;timed_out&quot;:false, &quot;_shards&quot;:&#123;&quot;total&quot;:5,&quot;successful&quot;:5,&quot;failed&quot;:0&#125;, &quot;hits&quot;:&#123; &quot;total&quot;:2, &quot;max_score&quot;:1.0, &quot;hits&quot;:[ &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;AV3qGfrC6jMbsbXb6k1p&quot;, &quot;_score&quot;:1.0, &quot;_source&quot;: &#123; &quot;user&quot;: &quot;李四&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;系统管理&quot; &#125; &#125;, &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;1&quot;, &quot;_score&quot;:1.0, &quot;_source&quot;: &#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;数据库管理，软件开发&quot; &#125; &#125; ] &#125;&#125;&gt; 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。 total：返回记录数，本例是2条。 max_score：最高的匹配程度，本例是1.0。 hits：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 6.2 全文搜索Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;软件&quot; &#125;&#125;&#125;&apos;&gt; 上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含”软件”这个词。返回结果如下。 12&gt; &#123; &quot;took&quot;:3, &quot;timed_out&quot;:false, &quot;_shards&quot;:&#123;&quot;total&quot;:5,&quot;successful&quot;:5,&quot;failed&quot;:0&#125;, &quot;hits&quot;:&#123; &quot;total&quot;:1, &quot;max_score&quot;:0.28582606, &quot;hits&quot;:[ &#123; &quot;_index&quot;:&quot;accounts&quot;, &quot;_type&quot;:&quot;person&quot;, &quot;_id&quot;:&quot;1&quot;, &quot;_score&quot;:0.28582606, &quot;_source&quot;: &#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;数据库管理，软件开发&quot; &#125; &#125; ] &#125;&#125;&gt; Elastic 默认一次返回10条结果，可以通过size字段改变这个设置。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;, &quot;size&quot;: 1&#125;&apos;&gt; 上面代码指定，每次只返回一条结果。 还可以通过from字段，指定位移。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125;&apos;&gt; 上面代码指定，从位置1开始（默认是从位置0开始），只返回一条结果。 6.3 逻辑运算如果有多个搜索关键字， Elastic 认为它们是or关系。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;软件 系统&quot; &#125;&#125;&#125;&apos;&gt; 上面代码搜索的是软件 or 系统。 如果要执行多个关键词的and搜索，必须使用布尔查询。 12&gt; $ curl &apos;localhost:9200/accounts/person/_search&apos; -d &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;软件&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;系统&quot; &#125; &#125; ] &#125; &#125;&#125;&apos;&gt; 七、参考链接 ElasticSearch 官方手册 A Practical Introduction to Elasticsearch （完） 一、前言在开发网站/App项目的时候，通常需要搭建搜索服务。比如，新闻类应用需要检索标题/内容，社区类应用需要检索用户/帖子。 对于简单的需求，可以使用数据库的 LIKE 模糊搜索，示例： SELECT * FROM news WHERE title LIKE ‘%法拉利跑车%’ 可以查询到所有标题含有 “法拉利跑车” 关键词的新闻，但是这种方式有明显的弊端： 1、模糊查询性能极低，当数据量庞大的时候，往往会使数据库服务中断； 2、无法查询相关的数据，只能严格在标题中匹配关键词。 因此，需要搭建专门提供搜索功能的服务，具备分词、全文检索等高级功能。 Solr 就是这样一款搜索引擎，可以让你快速搭建适用于自己业务的搜索服务。 二、安装到官网 http://lucene.apache.org/solr/ 下载安装包，解压并进入 Solr 目录： wget ‘http://apache.website-solution.net/lucene/solr/6.2.0/solr-6.2.0.tgz&#39; tar xvf solr-6.2.0.tgz cd solr-6.2.0 目录结构如下： Solr 6.2 目录结构 启动 Solr 服务之前，确认已经安装 Java 1.8 ： 查看 Java 版本 启动 Solr 服务： ./bin/solr start -m 1g Solr 将默认监听 8983 端口，其中 -m 1g 指定分配给 JVM 的内存为 1 G。 在浏览器中访问 Solr 管理后台： http://127.0.0.1:8983/solr/#/ Solr 管理后台 创建 Solr 应用： ./bin/solr create -c my_news 可以在 solr-6.2.0/server/solr 目录下生成 my_news 文件夹，结构如下： my_news 目录结构 同时，可以在管理后台看到 my_news： 管理后台 三、创建索引我们将从 MySQL 数据库中导入数据到 Solr 并建立索引。 首先，需要了解 Solr 中的两个概念： 字段(field) 和 字段类型(fieldType)，配置示例如下： schema.xml 示例 field 指定一个字段的名称、是否索引/存储和字段类型。 fieldType 指定一个字段类型的名称以及在查询/索引的时候可能用到的分词插件。 将 solr-6.2.0\server\solr\my_news\conf 目录下默认的配置文件 managed-schema 重命名为 schema.xml 并加入新的 fieldType： 分词类型 在 my_news 目录下创建 lib 目录，将用到的分词插件 ik-analyzer-solr5-5.x.jar 加到 lib 目录，结构如下： my_news 目录结构 在 Solr 安装目录下重启服务： ./bin/solr restart 可以在管理后台看到新加的类型： text_ik 类型 接下来创建和我们数据库字段对应的 field：title 和 content，类型选为 text_ik： 新建字段 title 将要导入数据的 MySQL 数据库表结构： 编辑 conf/solrconfig.xml 文件，加入类库和数据库配置： 类库 dataimport config 同时新建数据库连接配置文件 conf/db-mysql-config.xml ，内容如下： 数据库配置文件 将数据库连接组件 mysql-connector-java-5.1.39-bin.jar 放到 lib 目录下，重启 Solr，访问管理后台，执行全量导入数据： 全量导入数据 创建定时更新脚本： 定时更新脚本 加入到定时任务，每5分钟增量更新一次索引： 定时任务 在 Solr 管理后台测试搜索结果： 分词搜索结果 至此，基本的搜索引擎搭建完毕，外部应用只需通过 http 协议提供查询参数，就可以获取搜索结果。 四、搜索干预通常需要对搜索结果进行人工干预，比如编辑推荐、竞价排名或者屏蔽搜索结果。Solr 已经内置了 QueryElevationComponent 插件，可以从配置文件中获取搜索关键词对应的干预列表，并将干预结果排在搜索结果的前面。 在 solrconfig.xml 文件中，可以看到： 干预其请求配置 定义了搜索组件 elevator，应用在 /elevate 的搜索请求中，干预结果的配置文件在 solrconfig.xml 同目录下的 elevate.xml 中，干预配置示例： 重启 Solr ，当搜索 “关键词” 的时候，id 为 1和 4 的文档将出现在前面，同时 id = 3 的文档被排除在结果之外，可以看到，没有干预的时候，搜索结果为： 无干预结果 当有搜索干预的时候： 干预结果 通过配置文件干预搜索结果，虽然简单，但是每次更新都要重启 Solr 才能生效，稍显麻烦，我们可以仿照 QueryElevationComponent 类，开发自己的干预组件，例如:从 Redis 中读取干预配置。 五、中文分词中文的搜索质量，和分词的效果息息相关，可以在 Solr 管理后台测试分词： 分词结果测试 上例可以看到，使用 IKAnalyzer 分词插件，对 “北京科技大学” 分词的测试结果。当用户搜索 “北京”、“科技大学”、“科技大”、“科技”、“大学” 这些关键词的时候，都会搜索到文本内容含 “北京科技大学” 的文档。 常用的中文分词插件有 IKAnalyzer、mmseg4j和 Solr 自带的 smartcn 等，分词效果各有优劣，具体选择哪个，可以根据自己的业务场景，分别测试效果再选择。 分词插件一般都有自己的默认词库和扩展词库，默认词库包含了绝大多数常用的中文词语。如果默认词库无法满足你的需求，比如某些专业领域的词汇，可以在扩展词库中手动添加，这样分词插件就能识别新词语了。 分词插件扩展词库配置示例 分词插件还可以指定停止词库，将某些无意义的词汇剔出分词结果，比如：“的”、“哼” 等，例如： 去除无意义的词 六、总结以上介绍了 Solr 最常用的一些功能，Solr 本身还有很多其他丰富的功能，比如分布式部署。 希望对你有所帮助。 七、附录1、参考资料： https://wiki.apache.org/solr/ http://lucene.apache.org/solr/quickstart.html https://cwiki.apache.org/confluence/display/solr/Apache+Solr+Reference+Guide 2、上述 Demo 中用到的所有配置文件、Jar 包： https://github.com/Ceelog/OpenSchool/blob/master/my_news.zip 3、还有疑问？联系作者微博/微信 @Ceelog 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈3：Lucene基础原理与实践]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%883%EF%BC%9ALucene%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[一、总论 根据lucene.apache.org/java/docs/i…定义： Lucene是一个高效的，基于Java的全文检索库。 所以在了解Lucene之前要费一番工夫了解一下全文检索。 那么什么叫做全文检索呢？这要从我们生活中的数据说起。 我们生活中的数据总体分为两种：结构化数据和非结构化数据。 结构化数据：指具有固定格式或有限长度的数据，如数据库，元数据等。 非结构化数据：指不定长或无固定格式的数据，如邮件，word文档等。 当然有的地方还会提到第三种，半结构化数据，如XML，HTML等，当根据需要可按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。 非结构化数据又一种叫法叫全文数据。 按照数据的分类，搜索也分为两种： 对结构化数据的搜索：如对数据库的搜索，用SQL语句。再如对元数据的搜索，如利用windows搜索对文件名，类型，修改时间进行搜索等。 对非结构化数据的搜索：如利用windows的搜索也可以搜索文件内容，Linux下的grep命令，再如用Google和百度可以搜索大量内容数据。 对非结构化数据也即对全文数据的搜索主要有两种方法： 一种是顺序扫描法(Serial Scanning)：所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，接着看下一个文件，直到扫描完所有的文件。如利用windows的搜索也可以搜索文件内容，只是相当的慢。如果你有一个80G硬盘，如果想在上面找到一个内容包含某字符串的文件，不花他几个小时，怕是做不到。Linux下的grep命令也是这一种方式。大家可能觉得这种方法比较原始，但对于小数据量的文件，这种方法还是最直接，最方便的。但是对于大量的文件，这种方法就很慢了。 有人可能会说，对非结构化数据顺序扫描很慢，对结构化数据的搜索却相对较快（由于结构化数据有一定的结构可以采取一定的搜索算法加快速度），那么把我们的非结构化数据想办法弄得有一定结构不就行了吗？ 这种想法很天然，却构成了全文检索的基本思路，也即将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。 这部分从非结构化数据中提取出的然后重新组织的信息，我们称之索引。 这种说法比较抽象，举几个例子就很容易明白，比如字典，字典的拼音表和部首检字表就相当于字典的索引，对每一个字的解释是非结构化的，如果字典没有音节表和部首检字表，在茫茫辞海中找一个字只能顺序扫描。然而字的某些信息可以提取出来进行结构化处理，比如读音，就比较结构化，分声母和韵母，分别只有几种可以一一列举，于是将读音拿出来按一定的顺序排列，每一项读音都指向此字的详细解释的页数。我们搜索时按结构化的拼音搜到读音，然后按其指向的页数，便可找到我们的非结构化数据——也即对字的解释。 这种先建立索引，再对索引进行搜索的过程就叫全文检索(Full-text Search)。 下面这幅图来自《Lucene in action》，但却不仅仅描述了Lucene的检索过程，而是描述了全文检索的一般过程。 全文检索大体分两个过程，索引创建(Indexing)和搜索索引(Search)。 索引创建：将现实世界中所有的结构化和非结构化数据提取信息，创建索引的过程。 搜索索引：就是得到用户的查询请求，搜索创建的索引，然后返回结果的过程。 于是全文检索就存在三个重要问题： 索引里面究竟存些什么？(Index) 如何创建索引？(Indexing) 如何对索引进行搜索？(Search) 下面我们顺序对每个个问题进行研究。 二、索引里面究竟存些什么索引里面究竟需要存些什么呢？ 首先我们来看为什么顺序扫描的速度慢： 其实是由于我们想要搜索的信息和非结构化数据中所存储的信息不一致造成的。 非结构化数据中所存储的信息是每个文件包含哪些字符串，也即已知文件，欲求字符串相对容易，也即是从文件到字符串的映射。而我们想搜索的信息是哪些文件包含此字符串，也即已知字符串，欲求文件，也即从字符串到文件的映射。两者恰恰相反。于是如果索引总能够保存从字符串到文件的映射，则会大大提高搜索速度。 由于从字符串到文件的映射是文件到字符串映射的反向过程，于是保存这种信息的索引称为反向索引。 反向索引的所保存的信息一般如下： 假设我的文档集合里面有100篇文档，为了方便表示，我们为文档编号从1到100，得到下面的结构 左边保存的是一系列字符串，称为词典。 每个字符串都指向包含此字符串的文档(Document)链表，此文档链表称为倒排表(Posting List)。 有了索引，便使保存的信息和要搜索的信息一致，可以大大加快搜索的速度。 比如说，我们要寻找既包含字符串“lucene”又包含字符串“solr”的文档，我们只需要以下几步： 1. 取出包含字符串“lucene”的文档链表。 2. 取出包含字符串“solr”的文档链表。 3. 通过合并链表，找出既包含“lucene”又包含“solr”的文件。 看到这个地方，有人可能会说，全文检索的确加快了搜索的速度，但是多了索引的过程，两者加起来不一定比顺序扫描快多少。的确，加上索引的过程，全文检索不一定比顺序扫描快，尤其是在数据量小的时候更是如此。而对一个很大量的数据创建索引也是一个很慢的过程。 然而两者还是有区别的，顺序扫描是每次都要扫描，而创建索引的过程仅仅需要一次，以后便是一劳永逸的了，每次搜索，创建索引的过程不必经过，仅仅搜索创建好的索引就可以了。 这也是全文搜索相对于顺序扫描的优势之一：一次索引，多次使用。 三、如何创建索引全文检索的索引创建过程一般有以下几步： 第一步：一些要索引的原文档(Document)。为了方便说明索引创建过程，这里特意用两个文件为例： 文件一：Students should be allowed to go out with their friends, but not allowed to drink beer. 文件二：My friend Jerry went to school to see his students but found them drunk which is not allowed. 第二步：将原文档传给分次组件(Tokenizer)。分词组件(Tokenizer)会做以下几件事情(此过程称为Tokenize)： 将文档分成一个一个单独的单词。 去除标点符号。 去除停词(Stop word)。 所谓停词(Stop word)就是一种语言中最普通的一些单词，由于没有特别的意义，因而大多数情况下不能成为搜索的关键词，因而创建索引时，这种词会被去掉而减少索引的大小。 英语中挺词(Stop word)如：“the”,“a”，“this”等。 对于每一种语言的分词组件(Tokenizer)，都有一个停词(stop word)集合。 经过分词(Tokenizer)后得到的结果称为词元(Token)。 在我们的例子中，便得到以下词元(Token)： “Students”，“allowed”，“go”，“their”，“friends”，“allowed”，“drink”，“beer”，“My”，“friend”，“Jerry”，“went”，“school”，“see”，“his”，“students”，“found”，“them”，“drunk”，“allowed”。 第三步：将得到的词元(Token)传给语言处理组件(Linguistic Processor)。语言处理组件(linguistic processor)主要是对得到的词元(Token)做一些同语言相关的处理。 对于英语，语言处理组件(Linguistic Processor)一般做以下几点： 变为小写(Lowercase)。 将单词缩减为词根形式，如“cars”到“car”等。这种操作称为：stemming。 将单词转变为词根形式，如“drove”到“drive”等。这种操作称为：lemmatization。 Stemming 和 lemmatization的异同： 相同之处：Stemming和lemmatization都要使词汇成为词根形式。 两者的方式不同： Stemming采用的是“缩减”的方式：“cars”到“car”，“driving”到“drive”。 Lemmatization采用的是“转变”的方式：“drove”到“drove”，“driving”到“drive”。 两者的算法不同： Stemming主要是采取某种固定的算法来做这种缩减，如去除“s”，去除“ing”加“e”，将“ational”变为“ate”，将“tional”变为“tion”。 Lemmatization主要是采用保存某种字典的方式做这种转变。比如字典中有“driving”到“drive”，“drove”到“drive”，“am, is, are”到“be”的映射，做转变时，只要查字典就可以了。 Stemming和lemmatization不是互斥关系，是有交集的，有的词利用这两种方式都能达到相同的转换。 语言处理组件(linguistic processor)的结果称为词(Term)。 在我们的例子中，经过语言处理，得到的词(Term)如下： “student”，“allow”，“go”，“their”，“friend”，“allow”，“drink”，“beer”，“my”，“friend”，“jerry”，“go”，“school”，“see”，“his”，“student”，“find”，“them”，“drink”，“allow”。 也正是因为有语言处理的步骤，才能使搜索drove，而drive也能被搜索出来。 第四步：将得到的词(Term)传给索引组件(Indexer)。索引组件(Indexer)主要做以下几件事情： 1. 利用得到的词(Term)创建一个字典。 在我们的例子中字典如下： Term Document ID student 1 allow 1 go 1 their 1 friend 1 allow 1 drink 1 beer 1 my 2 friend 2 jerry 2 go 2 school 2 see 2 his 2 student 2 find 2 them 2 drink 2 allow 2 对字典按字母顺序进行排序。 Term Document ID allow 1 allow 1 allow 2 beer 1 drink 1 drink 2 find 2 friend 1 friend 2 go 1 go 2 his 2 jerry 2 my 2 school 2 see 2 student 1 student 2 their 1 them 2 合并相同的词(Term)成为文档倒排(Posting List)链表。 在此表中，有几个定义： Document Frequency 即文档频次，表示总共有多少文件包含此词(Term)。 Frequency 即词频率，表示此文件中包含了几个此词(Term)。 所以对词(Term) “allow”来讲，总共有两篇文档包含此词(Term)，从而词(Term)后面的文档链表总共有两项，第一项表示包含“allow”的第一篇文档，即1号文档，此文档中，“allow”出现了2次，第二项表示包含“allow”的第二个文档，是2号文档，此文档中，“allow”出现了1次。 到此为止，索引已经创建好了，我们可以通过它很快的找到我们想要的文档。 而且在此过程中，我们惊喜地发现，搜索“drive”，“driving”，“drove”，“driven”也能够被搜到。因为在我们的索引中，“driving”，“drove”，“driven”都会经过语言处理而变成“drive”，在搜索时，如果您输入“driving”，输入的查询语句同样经过我们这里的一到三步，从而变为查询“drive”，从而可以搜索到想要的文档。 三、如何对索引进行搜索？到这里似乎我们可以宣布“我们找到想要的文档了”。 然而事情并没有结束，找到了仅仅是全文检索的一个方面。不是吗？如果仅仅只有一个或十个文档包含我们查询的字符串，我们的确找到了。然而如果结果有一千个，甚至成千上万个呢？那个又是您最想要的文件呢？ 打开Google吧，比如说您想在微软找份工作，于是您输入“Microsoft job”，您却发现总共有22600000个结果返回。好大的数字呀，突然发现找不到是一个问题，找到的太多也是一个问题。在如此多的结果中，如何将最相关的放在最前面呢？ 当然Google做的很不错，您一下就找到了jobs at Microsoft。想象一下，如果前几个全部是“Microsoft does a good job at software industry…”将是多么可怕的事情呀。 如何像Google一样，在成千上万的搜索结果中，找到和查询语句最相关的呢？ 如何判断搜索出的文档和查询语句的相关性呢？ 这要回到我们第三个问题：如何对索引进行搜索？ 搜索主要分为以下几步： 第一步：用户输入查询语句。查询语句同我们普通的语言一样，也是有一定语法的。 不同的查询语句有不同的语法，如SQL语句就有一定的语法。 查询语句的语法根据全文检索系统的实现而不同。最基本的有比如：AND, OR, NOT等。 举个例子，用户输入语句：lucene AND learned NOT hadoop。 说明用户想找一个包含lucene和learned然而不包括hadoop的文档。 第二步：对查询语句进行词法分析，语法分析，及语言处理。由于查询语句有语法，因而也要进行语法分析，语法分析及语言处理。 1. 词法分析主要用来识别单词和关键字。 如上述例子中，经过词法分析，得到单词有lucene，learned，hadoop, 关键字有AND, NOT。 如果在词法分析中发现不合法的关键字，则会出现错误。如lucene AMD learned，其中由于AND拼错，导致AMD作为一个普通的单词参与查询。 2. 语法分析主要是根据查询语句的语法规则来形成一棵语法树。 如果发现查询语句不满足语法规则，则会报错。如lucene NOT AND learned，则会出错。 如上述例子，lucene AND learned NOT hadoop形成的语法树如下： 3. 语言处理同索引过程中的语言处理几乎相同。 如learned变成learn等。 经过第二步，我们得到一棵经过语言处理的语法树。 第三步：搜索索引，得到符合语法树的文档。此步骤有分几小步： 首先，在反向索引表中，分别找出包含lucene，learn，hadoop的文档链表。 其次，对包含lucene，learn的链表进行合并操作，得到既包含lucene又包含learn的文档链表。 然后，将此链表与hadoop的文档链表进行差操作，去除包含hadoop的文档，从而得到既包含lucene又包含learn而且不包含hadoop的文档链表。 此文档链表就是我们要找的文档。 第四步：根据得到的文档和查询语句的相关性，对结果进行排序。虽然在上一步，我们得到了想要的文档，然而对于查询结果应该按照与查询语句的相关性进行排序，越相关者越靠前。 如何计算文档和查询语句的相关性呢？ 不如我们把查询语句看作一片短小的文档，对文档与文档之间的相关性(relevance)进行打分(scoring)，分数高的相关性好，就应该排在前面。 那么又怎么对文档之间的关系进行打分呢？ 这可不是一件容易的事情，首先我们看一看判断人之间的关系吧。 首先看一个人，往往有很多要素，如性格，信仰，爱好，衣着，高矮，胖瘦等等。 其次对于人与人之间的关系，不同的要素重要性不同，性格，信仰，爱好可能重要些，衣着，高矮，胖瘦可能就不那么重要了，所以具有相同或相似性格，信仰，爱好的人比较容易成为好的朋友，然而衣着，高矮，胖瘦不同的人，也可以成为好的朋友。 因而判断人与人之间的关系，首先要找出哪些要素对人与人之间的关系最重要，比如性格，信仰，爱好。其次要判断两个人的这些要素之间的关系，比如一个人性格开朗，另一个人性格外向，一个人信仰佛教，另一个信仰上帝，一个人爱好打篮球，另一个爱好踢足球。我们发现，两个人在性格方面都很积极，信仰方面都很善良，爱好方面都爱运动，因而两个人关系应该会很好。 我们再来看看公司之间的关系吧。 首先看一个公司，有很多人组成，如总经理，经理，首席技术官，普通员工，保安，门卫等。 其次对于公司与公司之间的关系，不同的人重要性不同，总经理，经理，首席技术官可能更重要一些，普通员工，保安，门卫可能较不重要一点。所以如果两个公司总经理，经理，首席技术官之间关系比较好，两个公司容易有比较好的关系。然而一位普通员工就算与另一家公司的一位普通员工有血海深仇，怕也难影响两个公司之间的关系。 因而判断公司与公司之间的关系，首先要找出哪些人对公司与公司之间的关系最重要，比如总经理，经理，首席技术官。其次要判断这些人之间的关系，不如两家公司的总经理曾经是同学，经理是老乡，首席技术官曾是创业伙伴。我们发现，两家公司无论总经理，经理，首席技术官，关系都很好，因而两家公司关系应该会很好。 分析了两种关系，下面看一下如何判断文档之间的关系了。 首先，一个文档有很多词(Term)组成，如search, lucene, full-text, this, a, what等。 其次对于文档之间的关系，不同的Term重要性不同，比如对于本篇文档，search, Lucene, full-text就相对重要一些，this, a , what可能相对不重要一些。所以如果两篇文档都包含search, Lucene，fulltext，这两篇文档的相关性好一些，然而就算一篇文档包含this, a, what，另一篇文档不包含this, a, what，也不能影响两篇文档的相关性。 因而判断文档之间的关系，首先找出哪些词(Term)对文档之间的关系最重要，如search, Lucene, fulltext。然后判断这些词(Term)之间的关系。 找出词(Term)对文档的重要性的过程称为计算词的权重(Term weight)的过程。 计算词的权重(term weight)有两个参数，第一个是词(Term)，第二个是文档(Document)。 词的权重(Term weight)表示此词(Term)在此文档中的重要程度，越重要的词(Term)有越大的权重(Term weight)，因而在计算文档之间的相关性中将发挥更大的作用。 判断词(Term)之间的关系从而得到文档相关性的过程应用一种叫做向量空间模型的算法(Vector Space Model)。 下面仔细分析一下这两个过程： 1. 计算权重(Term weight)的过程。影响一个词(Term)在一篇文档中的重要性主要有两个因素： Term Frequency (tf)：即此Term在此文档中出现了多少次。tf 越大说明越重要。 Document Frequency (df)：即有多少文档包含次Term。df 越大说明越不重要。 容易理解吗？词(Term)在文档中出现的次数越多，说明此词(Term)对该文档越重要，如“搜索”这个词，在本文档中出现的次数很多，说明本文档主要就是讲这方面的事的。然而在一篇英语文档中，this出现的次数更多，就说明越重要吗？不是的，这是由第二个因素进行调整，第二个因素说明，有越多的文档包含此词(Term), 说明此词(Term)太普通，不足以区分这些文档，因而重要性越低。 这也如我们程序员所学的技术，对于程序员本身来说，这项技术掌握越深越好（掌握越深说明花时间看的越多，tf越大），找工作时越有竞争力。然而对于所有程序员来说，这项技术懂得的人越少越好（懂得的人少df小），找工作越有竞争力。人的价值在于不可替代性就是这个道理。 道理明白了，我们来看看公式： 这仅仅只term weight计算公式的简单典型实现。实现全文检索系统的人会有自己的实现，Lucene就与此稍有不同。 2. 判断Term之间的关系从而得到文档相关性的过程，也即向量空间模型的算法(VSM)。我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。 于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。 Document = {term1, term2, …… ,term N} Document Vector = {weight1, weight2, …… ,weight N} 同样我们把查询语句看作一个简单的文档，也用向量来表示。 Query = {term1, term 2, …… , term N} Query Vector = {weight1, weight2, …… , weight N} 我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。 如图： 我们认为两个向量之间的夹角越小，相关性越大。 所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。 有人可能会问，查询语句一般是很短的，包含的词(Term)是很少的，因而查询向量的维数很小，而文档很长，包含词(Term)很多，文档向量维数很大。你的图中两者维数怎么都是N呢？ 在这里，既然要放到相同的向量空间，自然维数是相同的，不同时，取二者的并集，如果不含某个词(Term)时，则权重(Term Weight)为0。 相关性打分公式如下： 举个例子，查询语句有11个Term，共有三篇文档搜索出来。其中各自的权重(Term weight)，如下表格。 t10 t11 .477 .477 .176 .176 .176 .477 .954 .176 .176 .176 .176 .176 .176 .477 .176 于是计算，三篇文档同查询语句的相关性打分分别为： 于是文档二相关性最高，先返回，其次是文档一，最后是文档三。 到此为止，我们可以找到我们最想要的文档了。 说了这么多，其实还没有进入到Lucene，而仅仅是信息检索技术(Information retrieval)中的基本理论，然而当我们看过Lucene后我们会发现，Lucene是对这种基本理论的一种基本的的实践。所以在以后分析Lucene的文章中，会常常看到以上理论在Lucene中的应用。 在进入Lucene之前，对上述索引创建和搜索过程所一个总结，如图： 此图参照www.lucene.com.cn/about.htm中文章《开放源代码的全文检索引擎Lucene》 1. 索引过程： 1) 有一系列被索引文件 2) 被索引文件经过语法分析和语言处理形成一系列词(Term)。 3) 经过索引创建形成词典和反向索引表。 4) 通过索引存储将索引写入硬盘。 2. 搜索过程： a) 用户输入查询语句。 b) 对查询语句经过语法分析和语言分析得到一系列词(Term)。 c) 通过语法分析得到一个查询树。 d) 通过索引存储将索引读入到内存。 e) 利用查询树搜索索引，从而得到每个词(Term)的文档链表，对文档链表进行交，差，并得到结果文档。 f) 将搜索到的结果文档对查询的相关性进行排序。 g) 返回查询结果给用户。 下面我们可以进入Lucene的世界了。 CSDN中此文章链接为blog.csdn.net/forfuture19… Javaeye中此文章链接为forfuture1978.javaeye.com/blog/546771 Spring Boot 中使用 Java API 调用 luceneGithub 代码代码我已放到 Github ，导入spring-boot-lucene-demo 项目 github spring-boot-lucene-demo 添加依赖1&lt;!--对分词索引查询解析--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; lucene-queryparser &lt;version&gt;7.1.0&lt;/version&gt;&lt;/dependency&gt; &lt;!--高亮 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; lucene-highlighter &lt;version&gt;7.1.0&lt;/version&gt;&lt;/dependency&gt; &lt;!--smartcn 中文分词器 SmartChineseAnalyzer smartcn分词器 需要lucene依赖 且和lucene版本同步--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; lucene-analyzers-smartcn &lt;version&gt;7.1.0&lt;/version&gt;&lt;/dependency&gt; &lt;!--ik-analyzer 中文分词器--&gt;&lt;dependency&gt; &lt;groupId&gt;cn.bestwu&lt;/groupId&gt; ik-analyzers &lt;version&gt;5.1.0&lt;/version&gt;&lt;/dependency&gt; &lt;!--MMSeg4j 分词器--&gt;&lt;dependency&gt; &lt;groupId&gt;com.chenlb.mmseg4j&lt;/groupId&gt; mmseg4j-solr &lt;version&gt;2.4.0&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.solr&lt;/groupId&gt; solr-core &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 配置 lucene1private Directory directory; private IndexReader indexReader; private IndexSearcher indexSearcher; @Beforepublic void setUp() throws IOException &#123; //索引存放的位置，设置在当前目录中 directory = FSDirectory.open(Paths.get(&quot;indexDir/&quot;)); //创建索引的读取器 indexReader = DirectoryReader.open(directory); //创建一个索引的查找器，来检索索引库 indexSearcher = new IndexSearcher(indexReader);&#125; @Afterpublic void tearDown() throws Exception &#123; indexReader.close();&#125; ** * 执行查询，并打印查询到的记录数 * * @param query * @throws IOException */public void executeQuery(Query query) throws IOException &#123; TopDocs topDocs = indexSearcher.search(query, 100); //打印查询到的记录数 System.out.println(&quot;总共查询到&quot; + topDocs.totalHits + &quot;个文档&quot;); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; //取得对应的文档对象 Document document = indexSearcher.doc(scoreDoc.doc); System.out.println(&quot;id：&quot; + document.get(&quot;id&quot;)); System.out.println(&quot;title：&quot; + document.get(&quot;title&quot;)); System.out.println(&quot;content：&quot; + document.get(&quot;content&quot;)); &#125;&#125; /** * 分词打印 * * @param analyzer * @param text * @throws IOException */public void printAnalyzerDoc(Analyzer analyzer, String text) throws IOException &#123; TokenStream tokenStream = analyzer.tokenStream(&quot;content&quot;, new StringReader(text)); CharTermAttribute charTermAttribute = tokenStream.addAttribute(CharTermAttribute.class); try &#123; tokenStream.reset(); while (tokenStream.incrementToken()) &#123; System.out.println(charTermAttribute.toString()); &#125; tokenStream.end(); &#125; finally &#123; tokenStream.close(); analyzer.close(); &#125;&#125; 创建索引1@Testpublic void indexWriterTest() throws IOException &#123; long start = System.currentTimeMillis(); //索引存放的位置，设置在当前目录中 Directory directory = FSDirectory.open(Paths.get(&quot;indexDir/&quot;)); //在 6.6 以上版本中 version 不再是必要的，并且，存在无参构造方法，可以直接使用默认的 StandardAnalyzer 分词器。 Version version = Version.LUCENE_7_1_0; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 //创建索引写入配置 IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); //创建索引写入对象 IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); //创建Document对象，存储索引 Document doc = new Document(); int id = 1; //将字段加入到doc中 doc.add(new IntPoint(&quot;id&quot;, id)); doc.add(new StringField(&quot;title&quot;, &quot;Spark&quot;, Field.Store.YES)); doc.add(new TextField(&quot;content&quot;, &quot;Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎&quot;, Field.Store.YES)); doc.add(new StoredField(&quot;id&quot;, id)); //将doc对象保存到索引库中 indexWriter.addDocument(doc); indexWriter.commit(); //关闭流 indexWriter.close(); long end = System.currentTimeMillis(); System.out.println(&quot;索引花费了&quot; + (end - start) + &quot; 毫秒&quot;);&#125; 响应 117:58:14.655 [main] DEBUG org.wltea.analyzer.dic.Dictionary - 加载扩展词典：ext.dic17:58:14.660 [main] DEBUG org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典：stopword.dic索引花费了879 毫秒 删除文档1@Testpublic void deleteDocumentsTest() throws IOException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 //创建索引写入配置 IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); //创建索引写入对象 IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 删除title中含有关键词“Spark”的文档 long count = indexWriter.deleteDocuments(new Term(&quot;title&quot;, &quot;Spark&quot;)); // 除此之外IndexWriter还提供了以下方法： // DeleteDocuments(Query query):根据Query条件来删除单个或多个Document // DeleteDocuments(Query[] queries):根据Query条件来删除单个或多个Document // DeleteDocuments(Term term):根据Term来删除单个或多个Document // DeleteDocuments(Term[] terms):根据Term来删除单个或多个Document // DeleteAll():删除所有的Document //使用IndexWriter进行Document删除操作时，文档并不会立即被删除，而是把这个删除动作缓存起来，当IndexWriter.Commit()或IndexWriter.Close()时，删除操作才会被真正执行。 indexWriter.commit(); indexWriter.close(); System.out.println(&quot;删除完成:&quot; + count);&#125; 响应 1删除完成:1 更新文档1/** * 测试更新 * 实际上就是删除后新增一条 * * @throws IOException */@Testpublic void updateDocumentTest() throws IOException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 //创建索引写入配置 IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); //创建索引写入对象 IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); Document doc = new Document(); int id = 1; doc.add(new IntPoint(&quot;id&quot;, id)); doc.add(new StringField(&quot;title&quot;, &quot;Spark&quot;, Field.Store.YES)); doc.add(new TextField(&quot;content&quot;, &quot;Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎&quot;, Field.Store.YES)); doc.add(new StoredField(&quot;id&quot;, id)); long count = indexWriter.updateDocument(new Term(&quot;id&quot;, &quot;1&quot;), doc); System.out.println(&quot;更新文档:&quot; + count); indexWriter.close();&#125; 响应 1更新文档:1 按词条搜索1/** * 按词条搜索 * &lt;p&gt; * TermQuery是最简单、也是最常用的Query。TermQuery可以理解成为“词条搜索”， * 在搜索引擎中最基本的搜索就是在索引中搜索某一词条，而TermQuery就是用来完成这项工作的。 * 在Lucene中词条是最基本的搜索单位，从本质上来讲一个词条其实就是一个名/值对。 * 只不过这个“名”是字段名，而“值”则表示字段中所包含的某个关键字。 * * @throws IOException */@Testpublic void termQueryTest() throws IOException &#123; String searchField = &quot;title&quot;; //这是一个条件查询的api，用于添加条件 TermQuery query = new TermQuery(new Term(searchField, &quot;Spark&quot;)); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 多条件查询1/** * 多条件查询 * * BooleanQuery也是实际开发过程中经常使用的一种Query。 * 它其实是一个组合的Query，在使用时可以把各种Query对象添加进去并标明它们之间的逻辑关系。 * BooleanQuery本身来讲是一个布尔子句的容器，它提供了专门的API方法往其中添加子句， * 并标明它们之间的关系，以下代码为BooleanQuery提供的用于添加子句的API接口： * * @throws IOException */@Testpublic void BooleanQueryTest() throws IOException &#123; String searchField1 = &quot;title&quot;; String searchField2 = &quot;content&quot;; Query query1 = new TermQuery(new Term(searchField1, &quot;Spark&quot;)); Query query2 = new TermQuery(new Term(searchField2, &quot;Apache&quot;)); BooleanQuery.Builder builder = new BooleanQuery.Builder(); // BooleanClause用于表示布尔查询子句关系的类， // 包 括： // BooleanClause.Occur.MUST， // BooleanClause.Occur.MUST_NOT， // BooleanClause.Occur.SHOULD。 // 必须包含,不能包含,可以包含三种.有以下6种组合： // // 1．MUST和MUST：取得连个查询子句的交集。 // 2．MUST和MUST_NOT：表示查询结果中不能包含MUST_NOT所对应得查询子句的检索结果。 // 3．SHOULD与MUST_NOT：连用时，功能同MUST和MUST_NOT。 // 4．SHOULD与MUST连用时，结果为MUST子句的检索结果,但是SHOULD可影响排序。 // 5．SHOULD与SHOULD：表示“或”关系，最终检索结果为所有检索子句的并集。 // 6．MUST_NOT和MUST_NOT：无意义，检索无结果。 builder.add(query1, BooleanClause.Occur.SHOULD); builder.add(query2, BooleanClause.Occur.SHOULD); BooleanQuery query = builder.build(); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 匹配前缀1/** * 匹配前缀 * &lt;p&gt; * PrefixQuery用于匹配其索引开始以指定的字符串的文档。就是文档中存在xxx% * &lt;p&gt; * * @throws IOException */@Testpublic void prefixQueryTest() throws IOException &#123; String searchField = &quot;title&quot;; Term term = new Term(searchField, &quot;Spar&quot;); Query query = new PrefixQuery(term); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 短语搜索1/** * 短语搜索 * &lt;p&gt; * 所谓PhraseQuery，就是通过短语来检索，比如我想查“big car”这个短语， * 那么如果待匹配的document的指定项里包含了&quot;big car&quot;这个短语， * 这个document就算匹配成功。可如果待匹配的句子里包含的是“big black car”， * 那么就无法匹配成功了，如果也想让这个匹配，就需要设定slop， * 先给出slop的概念：slop是指两个项的位置之间允许的最大间隔距离 * * @throws IOException */@Testpublic void phraseQueryTest() throws IOException &#123; String searchField = &quot;content&quot;; String query1 = &quot;apache&quot;; String query2 = &quot;spark&quot;; PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.add(new Term(searchField, query1)); builder.add(new Term(searchField, query2)); builder.setSlop(0); PhraseQuery phraseQuery = builder.build(); //执行查询，并打印查询到的记录数 executeQuery(phraseQuery);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 相近词语搜索1/** * 相近词语搜索 * &lt;p&gt; * FuzzyQuery是一种模糊查询，它可以简单地识别两个相近的词语。 * * @throws IOException */@Testpublic void fuzzyQueryTest() throws IOException &#123; String searchField = &quot;content&quot;; Term t = new Term(searchField, &quot;大规模&quot;); Query query = new FuzzyQuery(t); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 通配符搜索1/** * 通配符搜索 * &lt;p&gt; * Lucene也提供了通配符的查询，这就是WildcardQuery。 * 通配符“?”代表1个字符，而“*”则代表0至多个字符。 * * @throws IOException */@Testpublic void wildcardQueryTest() throws IOException &#123; String searchField = &quot;content&quot;; Term term = new Term(searchField, &quot;大*规模&quot;); Query query = new WildcardQuery(term); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 分词查询1/** * 分词查询 * * @throws IOException * @throws ParseException */@Testpublic void queryParserTest() throws IOException, ParseException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 String searchField = &quot;content&quot;; //指定搜索字段和分析器 QueryParser parser = new QueryParser(searchField, analyzer); //用户输入内容 Query query = parser.parse(&quot;计算引擎&quot;); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 多个 Field 分词查询1/** * 多个 Field 分词查询 * * @throws IOException * @throws ParseException */@Testpublic void multiFieldQueryParserTest() throws IOException, ParseException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 String[] filedStr = new String[]&#123;&quot;title&quot;, &quot;content&quot;&#125;; //指定搜索字段和分析器 QueryParser queryParser = new MultiFieldQueryParser(filedStr, analyzer); //用户输入内容 Query query = queryParser.parse(&quot;Spark&quot;); //执行查询，并打印查询到的记录数 executeQuery(query);&#125; 响应 1总共查询到1个文档id：1title：Sparkcontent：Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 中文分词器1/** * IKAnalyzer 中文分词器 * SmartChineseAnalyzer smartcn分词器 需要lucene依赖 且和lucene版本同步 * * @throws IOException */@Testpublic void AnalyzerTest() throws IOException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = null; String text = &quot;Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎&quot;; analyzer = new IKAnalyzer();//IKAnalyzer 中文分词 printAnalyzerDoc(analyzer, text); System.out.println(); analyzer = new ComplexAnalyzer();//MMSeg4j 中文分词 printAnalyzerDoc(analyzer, text); System.out.println(); analyzer = new SmartChineseAnalyzer();//Lucene 中文分词器 printAnalyzerDoc(analyzer, text);&#125; 三种分词响应 1apachespark专为大规模规模模数数据处理数据处理而设设计快速通用计算引擎 1apachespark是专为大规模数据处理而设计的快速通用的计算引擎 1apachspark是专为大规模数据处理而设计的快速通用的计算引擎 高亮处理1/** * 高亮处理 * * @throws IOException */@Testpublic void HighlighterTest() throws IOException, ParseException, InvalidTokenOffsetsException &#123; //Analyzer analyzer = new StandardAnalyzer(); // 标准分词器，适用于英文 //Analyzer analyzer = new SmartChineseAnalyzer();//中文分词 //Analyzer analyzer = new ComplexAnalyzer();//中文分词 //Analyzer analyzer = new IKAnalyzer();//中文分词 Analyzer analyzer = new IKAnalyzer();//中文分词 String searchField = &quot;content&quot;; String text = &quot;Apache Spark 大规模数据处理&quot;; //指定搜索字段和分析器 QueryParser parser = new QueryParser(searchField, analyzer); //用户输入内容 Query query = parser.parse(text); TopDocs topDocs = indexSearcher.search(query, 100); // 关键字高亮显示的html标签，需要导入lucene-highlighter-xxx.jar SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(&quot;&quot;, &quot;&quot;); Highlighter highlighter = new Highlighter(simpleHTMLFormatter, new QueryScorer(query)); for (ScoreDoc scoreDoc : topDocs.scoreDocs) &#123; //取得对应的文档对象 Document document = indexSearcher.doc(scoreDoc.doc); // 内容增加高亮显示 TokenStream tokenStream = analyzer.tokenStream(&quot;content&quot;, new StringReader(document.get(&quot;content&quot;))); String content = highlighter.getBestFragment(tokenStream, document.get(&quot;content&quot;)); System.out.println(content); &#125; &#125; 响应 1Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎! 代码我已放到 Github ，导入spring-boot-lucene-demo 项目 github spring-boot-lucene-demo 作者：Peng Lei 出处：Segment Fault PengLei `Blog 专栏 版权归作者所有，转载请注明出处 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈2：搜索引擎工作原理]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%882%EF%BC%9A%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文作者：顿炖链接：https://www.zhihu.com/question/19937854/answer/98791215来源：知乎 写在前面 Max Grigorev最近写了一篇文章，题目是《What every software engineer should know about search》，这篇文章里指出了现在一些软件工程师的问题，他们认为开发一个搜索引擎功能就是搭建一个ElasticSearch集群，而没有深究背后的技术，以及技术发展趋势。Max认为，除了搜索引擎自身的搜索问题解决、人类使用方式等之外，也需要解决索引、分词、权限控制、国际化等等的技术点，看了他的文章，勾起了我多年前的想法。 很多年前，我曾经想过自己实现一个搜索引擎，作为自己的研究生论文课题，后来琢磨半天没有想出新的技术突破点（相较于已发表的文章），所以切换到了大数据相关的技术点。当时没有写出来，心中有点小遗憾，毕竟凭借搜索引擎崛起的谷歌是我内心渴望的公司。今天我就想结合自己的一些积累，聊聊作为一名软件工程师，您需要了解的搜索引擎知识。 搜索引擎发展过程 现代意义上的搜索引擎的祖先，是1990年由蒙特利尔大学学生Alan Emtage发明的Archie。即便没有英特网，网络中文件传输还是相当频繁的，而且由于大量的文件散布在各个分散的FTP主机中，查询起来非常不便，因此Alan Emtage想到了开发一个可以以文件名查找文件的系统，于是便有了Archie。Archie工作原理与现在的搜索引擎已经很接近，它依靠脚本程序自动搜索网上的文件，然后对有关信息进行索引，供使用者以一定的表达式查询。 互联网兴起后，需要能够监控的工具。世界上第一个用于监测互联网发展规模的“机器人”程序是Matthew Gray开发的World wide Web Wanderer，刚开始它只用来统计互联网上的服务器数量，后来则发展为能够检索网站域名。 随着互联网的迅速发展，每天都会新增大量的网站、网页，检索所有新出现的网页变得越来越困难，因此，在Matthew Gray的Wanderer基础上，一些编程者将传统的“蜘蛛”程序工作原理作了些改进。现代搜索引擎都是以此为基础发展的。 搜索引擎分类 全文搜索引擎 当前主流的是全文搜索引擎，较为典型的代表是Google、百度。全文搜索引擎是指通过从互联网上提取的各个网站的信息（以网页文字为主），保存在自己建立的数据库中。用户发起检索请求后，系统检索与用户查询条件匹配的相关记录，然后按一定的排列顺序将结果返回给用户。从搜索结果来源的角度，全文搜索引擎又可细分为两种，一种是拥有自己的检索程序（Indexer），俗称“蜘蛛”（Spider）程序或“机器人”（Robot）程序，并自建网页数据库，搜索结果直接从自身的数据存储层中调用；另一种则是租用其他引擎的数据库，并按自定的格式排列搜索结果，如Lycos引擎。 目录索引类搜索引擎 虽然有搜索功能，但严格意义上不能称为真正的搜索引擎，只是按目录分类的网站链接列表而已。用户完全可以按照分类目录找到所需要的信息，不依靠关键词（Keywords）进行查询。目录索引中最具代表性的莫过于大名鼎鼎的Yahoo、新浪分类目录搜索。 元搜索引擎 元搜索引擎在接受用户查询请求时，同时在其他多个引擎上进行搜索，并将结果返回给用户。著名的元搜索引擎有InfoSpace、Dogpile、Vivisimo等，中文元搜索引擎中具代表性的有搜星搜索引擎。在搜索结果排列方面，有的直接按来源引擎排列搜索结果，如Dogpile，有的则按自定的规则将结果重新排列组合，如Vivisimo。 相关实现技术 搜索引擎产品虽然一般都只有一个输入框，但是对于所提供的服务，背后有很多不同业务引擎支撑，每个业务引擎又有很多不同的策略，每个策略又有很多模块协同处理，及其复杂。 搜索引擎本身包含网页抓取、网页评价、反作弊、建库、倒排索引、索引压缩、在线检索、ranking排序策略等等知识。 网络爬虫技术 网络爬虫技术指的是针对网络数据的抓取。因为在网络中抓取数据是具有关联性的抓取，它就像是一只蜘蛛一样在互联网中爬来爬去，所以我们很形象地将其称为是网络爬虫技术。网络爬虫也被称为是网络机器人或者是网络追逐者。 网络爬虫获取网页信息的方式和我们平时使用浏览器访问网页的工作原理是完全一样的，都是根据HTTP协议来获取，其流程主要包括如下步骤： 1）连接DNS域名服务器，将待抓取的URL进行域名解析（URL——&gt;IP）； 2）根据HTTP协议，发送HTTP请求来获取网页内容。 一个完整的网络爬虫基础框架如下图所示： 整个架构共有如下几个过程： 1）需求方提供需要抓取的种子URL列表，根据提供的URL列表和相应的优先级，建立待抓取URL队列（先来先抓）； 2）根据待抓取URL队列的排序进行网页抓取； 3）将获取的网页内容和信息下载到本地的网页库，并建立已抓取URL列表（用于去重和判断抓取的进程）； 4）将已抓取的网页放入到待抓取的URL队列中，进行循环抓取操作； 索引 从用户的角度来看，搜索的过程是通过关键字在某种资源中寻找特定的内容的过程。而从计算机的角度来看，实现这个过程可以有两种办法。一是对所有资源逐个与关键字匹配，返回所有满足匹配的内容；二是如同字典一样事先建立一个对应表，把关键字与资源的内容对应起来，搜索时直接查找这个表即可。显而易见，第二个办法效率要高得多。建立这个对应表事实上就是建立逆向索引（inverted index）的过程。 Lucene Lucene是一个高性能的java全文检索工具包，它使用的是倒排文件索引结构。 全文检索大体分两个过程，索引创建 (Indexing) 和搜索索引 (Search) 。 索引创建：将现实世界中所有的结构化和非结构化数据提取信息，创建索引的过程。搜索索引：就是得到用户的查询请求，搜索创建的索引，然后返回结果的过程。 非结构化数据中所存储的信息是每个文件包含哪些字符串，也即已知文件，欲求字符串相对容易，也即是从文件到字符串的映射。而我们想搜索的信息是哪些文件包含此字符串，也即已知字符串，欲求文件，也即从字符串到文件的映射。两者恰恰相反。于是如果索引总能够保存从字符串到文件的映射，则会大大提高搜索速度。 由于从字符串到文件的映射是文件到字符串映射的反向过程，于是保存这种信息的索引称为反向索引 。 反向索引的所保存的信息一般如下： 假设我的文档集合里面有100篇文档，为了方便表示，我们为文档编号从1到100，得到下面的结构 每个字符串都指向包含此字符串的文档(Document)链表，此文档链表称为倒排表 (Posting List)。 ElasticSearch Elasticsearch是一个实时的分布式搜索和分析引擎，可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。Elasticsearch是一个建立在全文搜索引擎 Apache Lucene™ 基础上的搜索引擎，但是Lucene只是一个框架，要充分利用它的功能，需要使用JAVA，并且在程序中集成Lucene。Elasticsearch使用Lucene作为内部引擎，但是在使用它做全文搜索时，只需要使用统一开发好的API即可，而不需要了解其背后复杂的Lucene的运行原理。 Solr Solr是一个基于Lucene的搜索引擎服务器。Solr 提供了层面搜索、命中醒目显示并且支持多种输出格式（包括 XML/XSLT 和 JSON 格式）。它易于安装和配置，而且附带了一个基于 HTTP 的管理界面。Solr已经在众多大型的网站中使用，较为成熟和稳定。Solr 包装并扩展了 Lucene，所以Solr的基本上沿用了Lucene的相关术语。更重要的是，Solr 创建的索引与 Lucene 搜索引擎库完全兼容。通过对Solr 进行适当的配置，某些情况下可能需要进行编码，Solr 可以阅读和使用构建到其他 Lucene 应用程序中的索引。此外，很多 Lucene 工具（如Nutch、 Luke）也可以使用Solr 创建的索引。 Hadoop 谷歌公司发布的一系列技术白皮书导致了Hadoop的诞生。Hadoop是一系列大数据处理工具，可以被用在大规模集群里。Hadoop目前已经发展为一个生态体系，包括了很多组件，如图所示。 Cloudera是一家将Hadoop技术用于搜索引擎的公司，用户可以采用全文搜索方式检索存储在HDFS（Hadoop分布式文件系统）和Apache HBase里面的数据，再加上开源的搜索引擎Apache Solr，Cloudera提供了搜索功能，并结合Apache ZooKeeper进行分布式处理的管理、索引切分以及高性能检索。 PageRank 谷歌Pagerank算法基于随机冲浪模型，基本思想是基于网站之间的相互投票，即我们常说的网站之间互相指向。如果判断一个网站是高质量站点时，那么该网站应该是被很多高质量的网站引用又或者是该网站引用了大量的高质量权威的站点。 国际化 坦白说，Google虽然做得非常好，无论是技术还是产品设计，都很好。但是国际化确实是非常难做的，很多时候在细分领域还是会有其他搜索引擎的生存余地。例如在韩国，Naver是用户的首选，它本身基于Yahoo的Overture系统，广告系统则是自己开发的。在捷克，我们则更多会使用Seznam。在瑞典，用户更多选择Eniro，它最初是瑞典的黄页开发公司。 国际化、个性化搜索、匿名搜索，这些都是Google这样的产品所不能完全覆盖到的，事实上，也没有任何一款产品可以适用于所有需求。 自己实现搜索引擎 如果我们想要实现搜索引擎，最重要的是索引模块和搜索模块。索引模块在不同的机器上各自进行对资源的索引，并把索引文件统一传输到同一个地方（可以是在远程服务器上，也可以是在本地）。搜索模块则利用这些从多个索引模块收集到的数据完成用户的搜索请求。因此，我们可以理解两个模块之间相对是独立的，它们之间的关联不是通过代码，而是通过索引和元数据，如下图所示。 对于索引的建立，我们需要注意性能问题。当需要进行索引的资源数目不多时，隔一定的时间进行一次完全索引，不会占用很长时间。但在大型应用中，资源的容量是巨大的，如果每次都进行完整的索引，耗费的时间会很惊人。我们可以通过跳过已经索引的资源内容，删除已不存在的资源内容的索引，并进行增量索引来解决这个问题。这可能会涉及文件校验和索引删除等。另一方面，框架可以提供查询缓存功能，提高查询效率。框架可以在内存中建立一级缓存，并使用如 OSCache或 EHCache缓存框架，实现磁盘上的二级缓存。当索引的内容变化不频繁时，使用查询缓存更会明显地提高查询速度、降低资源消耗。 搜索引擎解决方案 Sphinx 俄罗斯一家公司开源的全文搜索引擎软件Sphinx，单一索引最大可包含1亿条记录，在1千万条记录情况下的查询速度为0.x秒（毫秒级）。Sphinx创建索引的速度很快，根据网上的资料，Sphinx创建100万条记录的索引只需3～4分钟，创建1000万条记录的索引可以在50分钟内完成，而只包含最新10万条记录的增量索引，重建一次只需几十秒。 OmniFind OmniFind 是 IBM 公司推出的企业级搜索解决方案。基于 UIMA (Unstructured Information Management Architecture) 技术，它提供了强大的索引和获取信息功能，支持巨大数量、多种类型的文档资源（无论是结构化还是非结构化），并为 Lotus®Domino®和 WebSphere®Portal 专门进行了优化。下一代搜索引擎 从技术和产品层面来看，接下来的几年，甚至于更长时间，应该没有哪一家搜索引擎可以撼动谷歌的技术领先优势和产品地位。但是我们也可以发现一些现象，例如搜索假期租房的时候，人们更喜欢使用Airbub，而不是Google，这就是针对匿名/个性化搜索需求，这些需求是谷歌所不能完全覆盖到的，毕竟原始数据并不在谷歌。我们可以看一个例子：DuckDuckGo。这是一款有别于大众理解的搜索引擎，DuckDuckGo强调的是最佳答案，而不是更多的结果，所以每个人搜索相同关键词时，返回的结果是不一样的。 另一个方面技术趋势是引入人工智能技术。在搜索体验上，通过大量算法的引入，对用户搜索的内容和访问偏好进行分析，将标题摘要进行一定程度的优化，以更容易理解的方式呈现给用户。谷歌在搜索引擎AI化的步骤领先于其他厂商，2016年，随着Amit Singhal被退休，John Giannandrea上位的交接班过程后，正式开启了自身的革命。Giannandrea是深度神经网络、近似人脑中的神经元网络研究方面的顶级专家，通过分析海量级的数字数据，这些神经网络可以学习排列方式，例如对图片进行分类、识别智能手机的语音控制等等，对应也可以应用在搜索引擎。因此，Singhal向Giannandrea的过渡，也意味着传统人为干预的规则设置的搜索引擎向AI技术的过渡。引入深度学习技术之后的搜索引擎，通过不断的模型训练，它会深层次地理解内容，并为客户提供更贴近实际需求的服务，这才是它的有用，或者可怕之处。 Google搜索引擎的工作流程 贴个图，自己感受下。 详细点的 ： 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈11：十分钟理解Kubernetes核心概念]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%8811%EF%BC%9A%E5%8D%81%E5%88%86%E9%92%9F%E7%90%86%E8%A7%A3Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[十分钟带你理解Kubernetes核心概念本文将会简单介绍Kubernetes的核心概念。因为这些定义可以在Kubernetes的文档中找到，所以文章也会避免用大段的枯燥的文字介绍。相反，我们会使用一些图表（其中一些是动画）和示例来解释这些概念。我们发现一些概念（比如Service）如果没有图表的辅助就很难全面地理解。在合适的地方我们也会提供Kubernetes文档的链接以便读者深入学习。 容器特性、镜像、网络；Kubernetes架构、核心组件、基本功能；Kubernetes设计理念、架构设计、基本功能、常用对象、设计原则；Kubernetes的数据库、运行时、网络、插件已经落地经验；微服务架构、组件、监控方案等。 这就开始吧。 什么是Kubernetes？Kubernetes（k8s）是自动化容器操作的开源平台，这些操作包括部署，调度和节点集群间扩展。如果你曾经用过Docker容器技术部署容器，那么可以将Docker看成Kubernetes内部使用的低级别组件。Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。使用Kubernetes可以： 自动化容器的部署和复制 随时扩展或收缩容器规模 将容器组织成组，并且提供容器间的负载均衡 很容易地升级应用程序容器的新版本 提供容器弹性，如果容器失效就替换它，等等… 实际上，使用Kubernetes只需一个部署文件，使用一条命令就可以部署多层容器（前端，后台等）的完整集群： 1$ kubectl create -f single-config-file.yaml kubectl是和Kubernetes API交互的命令行程序。现在介绍一些核心概念。 集群集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台。下图展示这样的集群。注意该图为了强调核心概念有所简化。这里可以看到一个典型的Kubernetes架构图。 1.png 上图可以看到如下组件，使用特别的图标表示Service和Label： Pod Container（容器） Label（标签） Replication Controller（复制控制器） Service（服务） Node（节点） Kubernetes Master（Kubernetes主节点） PodPod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。Pod是短暂的，不是持续性实体。你可能会有这些问题： 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持卷的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，下文会详细介绍。 Lable正如图所示，一些Pod有Label。一个Label是attach到Pod的一对键/值对，用来传递用户定义的属性。比如，你可能创建了一个”tier”和“app”标签，通过Label（tier=frontend, app=myapp）来标记前端Pod容器，使用Label（tier=backend, app=myapp）标记后台Pod。然后可以使用Selectors选择带有特定Label的Pod，并且将Service或者Replication Controller应用到上面。 Replication Controller是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么，能否将Pods划到逻辑组里？ Replication Controller确保任意时间都有指定数量的Pod“副本”在运行。如果为某个Pod创建了Replication Controller并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3.如下面的动画所示： 2.gif 如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动升级时很有用。 当创建Replication Controller时，需要指定两个东西： Pod模板：用来创建Pod副本的模板 Label：Replication Controller需要监控的Pod的标签。 现在已经创建了Pod的一些副本，那么在这些副本上如何均衡负载呢？我们需要的是Service。 Service如果Pods是短暂的，那么重启时IP地址可能会改变，怎么才能从前端容器正确可靠地指向后台容器呢？ Service是定义一系列Pod以及访问这些Pod的策略的一层抽象。Service通过Label找到Pod组。因为Service是抽象的，所以在图表里通常看不到它们的存在，这也就让这一概念更难以理解。 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，lable选择器为（tier=backend, app=myapp）。backend-service 的Service会完成如下两件重要的事情： 会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。 现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。这里有更多技术细节。 下述动画展示了Service的功能。注意该图作了很多简化。如果不进入网络配置，那么达到透明的负载均衡目标所涉及的底层网络和路由相对先进。如果有兴趣，这里有更深入的介绍。 3.gif 有一个特别类型的Kubernetes Service，称为’LoadBalancer‘，作为外部负载均衡器使用，在一定数量的Pod之间均衡流量。比如，对于负载均衡Web流量很有用。 Node节点（上图橘色方框）是物理或者虚拟机器，作为Kubernetes worker，通常称为Minion。每个节点都运行如下Kubernetes关键组件： Kubelet：是主节点代理。 Kube-proxy：Service使用其将链接路由到Pod，如上文所述。 Docker或Rocket：Kubernetes使用的容器技术来创建容器。 Kubernetes Master集群拥有一个Kubernetes Master（紫色方框）。Kubernetes Master提供集群的独特视角，并且拥有一系列组件，比如Kubernetes API Server。API Server提供可以用来和集群交互的REST端点。master节点包括用来创建和复制Pod的Replication Controller。 下一步现在我们已经了解了Kubernetes核心概念的基本知识，你可以进一步阅读Kubernetes 用户手册。用户手册提供了快速并且完备的学习文档。如果迫不及待想要试试Kubernetes，可以使用Google Container Engine。Google Container Engine是托管的Kubernetes容器环境。简单注册/登录之后就可以在上面尝试示例了。 原文链接：Learn the Kubernetes Key Concepts in 10 Minutes（翻译：崔婧雯）＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝译者介绍崔婧雯，现就职于IBM，高级软件工程师，负责IBM WebSphere业务流程管理软件的系统测试工作。曾就职于VMware从事桌面虚拟化产品的质量保证工作。对虚拟化，中间件技术，业务流程管理有浓厚的兴趣。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈1：搜索引擎基础倒排索引]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%881%EF%BC%9A%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%9F%BA%E7%A1%80%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[什么是倒排索引？ 见其名知其意，有倒排索引，对应肯定，有正向索引。 正向索引（forward index），反向索引（inverted index）更熟悉的名字是倒排索引。 在搜索引擎中每个文件都对应一个文件ID，文件内容被表示为一系列关键词的集合（实际上在搜索引擎索引库中，关键词也已经转换为关键词ID）。例如“文档1”经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。 得到正向索引的结构如下： “文档1”的ID &gt; 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。 “文档2”的ID &gt; 此文档出现的关键词列表。 一般是通过key，去找value。 当用户在主页上搜索关键词“华为手机”时，假设只存在正向索引（forward index），那么就需要扫描索引库中的所有文档，找出所有包含关键词“华为手机”的文档，再根据打分模型进行打分，排出名次后呈现给用户。因为互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。 所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词。 得到倒排索引的结构如下： “关键词1”：“文档1”的ID，“文档2”的ID，…………。 “关键词2”：带有此关键词的文档ID列表。 从词的关键字，去找文档。 1.单词——文档矩阵单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型，图1展示了其含义。图3-1的每列代表一个文档，每行代表一个单词，打对勾的位置代表包含关系。 图1 单词-文档矩阵 从纵向即文档这个维度来看，每列代表文档包含了哪些单词，比如文档1包含了词汇1和词汇4，而不包含其它单词。从横向即单词这个维度来看，每行代表了哪些文档包含了某个单词。比如对于词汇1来说，文档1和文档4中出现过单词1，而其它文档不包含词汇1。矩阵中其它的行列也可作此种解读。 搜索引擎的索引其实就是实现“单词-文档矩阵”的具体[数据结构](http://lib.csdn.net/base/datastructure &quot;算法与数据结构知识库&quot;)。可以有不同的方式来实现上述概念模型，比如“倒排索引”、“签名文件”、“后缀树”等方式。但是各项实验数据表明，“倒排索引”是实现单词到文档映射关系的最佳实现方式，所以本博文主要介绍“倒排索引”的技术细节。2.倒排索引基本概念 文档(Document)：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如Word，PDF，html，XML等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。 文档集合(Document Collection)：由若干文档构成的集合称之为文档集合。比如海量的互联网网页或者说大量的电子邮件都是文档集合的具体例子。 文档编号(Document ID)：在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为“文档编号”，后文有时会用DocID来便捷地代表文档编号。 单词编号(Word ID)：与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。 倒排索引(Inverted Index)：倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。 倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。 倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。 关于这些概念之间的关系，通过图2可以比较清晰的看出来。 3.倒排索引简单实例倒排索引从逻辑结构和基本思路上来讲非常简单。下面我们通过具体实例来进行说明，使得读者能够对倒排索引有一个宏观而直接的感受。 假设文档集合包含五个文档，每个文档内容如图3所示，在图中最左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。 图3 文档集合 中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统将文档自动切分成单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引（参考图3-4）。在图4中，“单词ID”一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表。比如单词“谷歌”，其单词编号为1，倒排列表为{1,2,3,4,5}，说明文档集合中每个文档都包含了这个单词。 图4 简单的倒排索引 之所以说图4所示倒排索引是最简单的，是因为这个索引系统只记载了哪些文档包含某个单词，而事实上，索引系统还可以记录除此之外的更多信息。图5是一个相对复杂些的倒排索引，与图4的基本索引系统比，在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息（TF），即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。在图5的例子里，单词“创始人”的单词编号为7，对应的倒排列表内容为：（3:1），其中的3代表文档编号为3的文档包含这个单词，数字1代表词频信息，即这个单词在3号文档中只出现过1次，其它单词对应的倒排列表所代表含义与此相同。 图 5 带有单词频率信息的倒排索引 实用的倒排索引还可以记载更多的信息，图6所示索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的“文档频率信息”（对应图6的第三栏）以及在倒排列表中记录单词在某个文档出现的位置信息。 图6 带有单词频率、文档频率和出现位置信息的倒排索引 “文档频率信息”代表了在文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子。而单词在某个文档中出现的位置信息并非索引系统一定要记录的，在实际的索引系统里可以包含，也可以选择不包含这个信息，之所以如此，因为这个信息对于搜索系统来说并非必需的，位置信息只有在支持“短语查询”的时候才能够派上用场。 以单词“拉斯”为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;&lt;4&gt;)，(5;1;&lt;4&gt;)},其含义为在文档3和文档5出现过这个单词，单词频率都为1，单词“拉斯”在两个文档中的出现位置都是4，即文档中第四个单词是“拉斯”。 图6所示倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此，区别无非是采取哪些具体的数据结构来实现上述逻辑结构。 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“Facebook”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程，具体实现方案本书第五章会做详细描述。4. 单词词典 单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。 对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。4.1 哈希加链表 图7是这种词典结构的示意图。这种词典结构主要由两个部分构成： 主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。 在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明该单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。 在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以图7为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。4.2 树形结构 B树（或者B+树）是另外一种高效查找结构，图8是一个 B树结构示意图。B树与哈希方式查找不同，需要字典项能够按照大小排序（数字或者字符序），而哈希方式则无须数据满足此项要求。 B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。 图8 B树查找结构 总结 单词ID：记录每个单词的单词编号；单词：对应的单词；文档频率：代表文档集合中有多少个文档包含某个单词倒排列表：包含单词ID及其他必要信息DocId：单词出现的文档idTF：单词在某个文档中出现的次数POS：单词在文档中出现的位置 以单词“加盟”为例，其单词编号为6，文档频率为3，代表整个文档集合中有三个文档包含这个单词，对应的倒排列表为{(2;1;&lt;4&gt;),(3;1;&lt;7&gt;),(5;1;&lt;5&gt;)}，含义是在文档2，3，5出现过这个单词，在每个文档的出现过1次，单词“加盟”在第一个文档的POS是4，即文档的第四个单词是“加盟”，其他的类似。这个倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端技术杂谈开篇：云计算，大数据与AI的故事]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%2F%E5%90%8E%E7%AB%AF%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88%E5%BC%80%E7%AF%87%EF%BC%9A%E4%BA%91%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8EAI%E7%9A%84%E6%95%85%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[快速看懂云计算，大数据，人工智能我今天要讲这三个话题，一个是云计算，一个大数据，一个人工智能，我为什么要讲这三个东西呢？因为这三个东西现在非常非常的火，它们之间好像互相有关系，一般谈云计算的时候也会提到大数据，谈人工智能的时候也会提大数据，谈人工智能的时候也会提云计算。所以说感觉他们又相辅相成不可分割，如果是非技术的人员来讲可能比较难理解说这三个之间的相互关系，所以有必要解释一下。 一、云计算最初是实现资源管理的灵活性 我们首先来说云计算，云计算最初的目标是对资源的管理，管理的主要是计算资源，网络资源，存储资源三个方面。 1.1 管数据中心就像配电脑 什么叫计算，网络，存储资源呢？就说你要买台笔记本电脑吧，你是不是要关心这台电脑什么样的CPU啊？多大的内存啊？这两个我们称为计算资源。 这台电脑要能上网吧，需要有个网口可以插网线，或者有无线网卡可以连接我们家的路由器，您家也需要到运营商比如联通，移动，电信开通一个网络，比如100M的带宽，然后会有师傅弄一根网线到您家来，师傅可能会帮您将您的路由器和他们公司的网络连接配置好，这样您家的所有的电脑，手机，平板就都可以通过您的路由器上网了。这就是网络。 您可能还会问硬盘多大啊？原来硬盘都很小，10G之类的，后来500G，1T，2T的硬盘也不新鲜了。(1T是1000G)，这就是存储。 对于一台电脑是这个样子的，对于一个数据中心也是同样的。想象你有一个非常非常大的机房，里面堆了很多的服务器，这些服务器也是有CPU，内存，硬盘的，也是通过类似路由器的设备上网的。这个时候的一个问题就是，运营数据中心的人是怎么把这些设备统一的管理起来的呢？ 1.2 灵活就是想啥时要都有，想要多少都行 管理的目标就是要达到两个方面的灵活性。哪两个方面呢？比如有个人需要一台很小很小的电脑，只有一个CPU，1G内存，10G的硬盘，一兆的带宽，你能给他吗？像这种这么小规格的电脑，现在随便一个笔记本电脑都比这个配置强了，家里随便拉一个宽带都要100M。然而如果去一个云计算的平台上，他要想要这个资源的时候，只要一点就有了。 所以说它就能达到两个方面灵活性。 第一个方面就是想什么时候要就什么时候要，比如需要的时候一点就出来了，这个叫做时间灵活性。 第二个方面就是想要多少呢就有多少，比如需要一个很小很小的电脑，可以满足，比如需要一个特别大的空间，以云盘为例，似乎云盘给每个人分配的空间动不动就就很大很大，随时上传随时有空间，永远用不完，这个叫做空间灵活性。 空间灵活性和时间灵活性，也即我们常说的云计算的弹性。 为了解决这个弹性的问题，经历了漫长时间的发展。 1.3 物理设备不灵活 首先第一个阶段就是物理机，或者说物理设备时期。这个时期相当于客户需要一台电脑，我们就买一台放在数据中心里。物理设备当然是越来越牛，例如服务器，内存动不动就是百G内存，例如网络设备，一个端口的带宽就能有几十G甚至上百G，例如存储，在数据中心至少是PB级别的(一个P是1000个T，一个T是1000个G)。 然而物理设备不能做到很好的灵活性。首先它不能够达到想什么时候要就什么时候要、比如买台服务器，哪怕买个电脑，都有采购的时间。突然用户告诉某个云厂商，说想要开台电脑，如果使用物理服务器，当时去采购啊就很难，如果说供应商啊关系一般，可能采购一个月，供应商关系好的话也需要一个星期。用户等了一个星期后，这时候电脑才到位，用户还要登录上去开始慢慢部署自己的应用，时间灵活性非常差。第二是空间灵活性也不行，例如上述的用户，要一个很小很小的电脑，现在哪还有这么小型号的电脑啊。不能为了满足用户只要一个G的内存是80G硬盘的，就去买一个这么小的机器。但是如果买一个大的呢，因为电脑大，就向用户多收钱，用户说他只用这么小的一点，如果让用户多付钱就很冤。 1.4 虚拟化灵活多了 有人就想办法了。第一个办法就是虚拟化。用户不是只要一个很小的电脑么？数据中心的物理设备都很强大，我可以从物理的CPU，内存，硬盘中虚拟出一小块来给客户，同时也可以虚拟出一小块来给其他客户，每个客户都只能看到自己虚的那一小块，其实每个客户用的是整个大的设备上其中的一小块。虚拟化的技术能使得不同的客户的电脑看起来是隔离的，我看着好像这块盘就是我的，你看这呢这块盘就是你的，实际情况可能我这个10G和您这个10G是落在同样一个很大很大的这个存储上的。 而且如果事先物理设备都准备好，虚拟化软件虚拟出一个电脑是非常快的，基本上几分钟就能解决。所以在任何一个云上要创建一台电脑，一点几分钟就出来了，就是这个道理。 这个空间灵活性和时间灵活性就基本解决了。 1.5 虚拟世界的赚钱与情怀 在虚拟化阶段，最牛的公司是Vmware，是实现虚拟化技术比较早的一家公司，可以实现计算，网络，存储的虚拟化，这家公司很牛，性能也做得非常好，然后虚拟化软件卖的也非常好，赚了好多的钱，后来让EMC(世界五百强，存储厂商第一品牌)给收购了。 但是这个世界上还是有很多有情怀的人的，尤其是程序员里面，有情怀的人喜欢做一件什么事情呢？开源。这个世界上很多软件都是有闭源就有开源，源就是源代码。就是说某个软件做的好，所有人都爱用，这个软件的代码呢，我封闭起来只有我公司知道，其他人不知道，如果其他人想用这个软件，就要付我钱，这就叫闭源。但是世界上总有一些大牛看不惯钱都让一家赚了去。大牛们觉得，这个技术你会我也会，你能开发出来，我也能，我开发出来就是不收钱，把代码拿出来分享给大家，全世界谁用都可以，所有的人都可以享受到好处，这个叫做开源。 比如最近蒂姆·伯纳斯·李就是个非常有情怀的人，2017年，他因“发明万维网、第一个浏览器和使万维网得以扩展的基本协议和算法”而获得2016年度的图灵奖。图灵奖就是计算机界的诺贝尔奖。然而他最令人敬佩的是，他将万维网，也就是我们常见的www的技术无偿贡献给全世界免费使用。我们现在在网上的所有行为都应该感谢他的功劳，如果他将这个技术拿来收钱，应该和比尔盖茨差不多有钱。 例如在闭源的世界里有windows，大家用windows都得给微软付钱，开源的世界里面就出现了Linux。比尔盖茨靠windows，Office这些闭源的软件赚了很多钱，称为世界首富，就有大牛开发了另外一种操作系统Linux。很多人可能没有听说过Linux，很多后台的服务器上跑的程序都是Linux上的，比如大家享受双十一，支撑双十一抢购的系统，无论是淘宝，京东，考拉，都是跑在Linux上的。 再如有apple就有安卓。apple市值很高，但是苹果系统的代码我们是看不到的。于是就有大牛写了安卓手机操作系统。所以大家可以看到几乎所有的其他手机厂商，里面都装安卓系统，因为苹果系统不开源，而安卓系统大家都可以用。 在虚拟化软件也一样，有了Vmware，这个软件非常非常的贵。那就有大牛写了两个开源的虚拟化软件，一个叫做Xen，一个叫做KVM，如果不做技术的，可以不用管这两个名字，但是后面还是会提到。 1.6 虚拟化的半自动和云计算的全自动 虚拟化软件似乎解决了灵活性问题，其实不全对。因为虚拟化软件一般创建一台虚拟的电脑，是需要人工指定这台虚拟电脑放在哪台物理机上的，可能还需要比较复杂的人工配置，所以使用Vmware的虚拟化软件，需要考一个很牛的证书，能拿到这个证书的人，薪资是相当的高，也可见复杂程度。所以仅仅凭虚拟化软件所能管理的物理机的集群规模都不是特别的大，一般在十几台，几十台，最多百台这么一个规模。这一方面会影响时间灵活性，虽然虚拟出一台电脑的时间很短，但是随着集群规模的扩大，人工配置的过程越来越复杂，越来越耗时。另一方面也影响空间灵活性，当用户数量多的时候，这点集群规模，还远达不到想要多少要多少的程度，很可能这点资源很快就用完了，还得去采购。所以随着集群的规模越来越大，基本都是千台起步，动辄上万台，甚至几十上百万台，如果去查一下BAT，包括网易，包括谷歌，亚马逊，服务器数目都大的吓人。这么多机器要靠人去选一个位置放这台虚拟化的电脑并做相应的配置，几乎是不可能的事情，还是需要机器去做这个事情。 人们发明了各种各样的算法来做这个事情，算法的名字叫做调度(Scheduler)。通俗一点的说，就是有一个调度中心，几千台机器都在一个池子里面，无论用户需要多少CPU，内存，硬盘的虚拟电脑，调度中心会自动在大池子里面找一个能够满足用户需求的地方，把虚拟电脑启动起来做好配置，用户就直接能用了。这个阶段，我们称为池化，或者云化，到了这个阶段，才可以称为云计算，在这之前都只能叫虚拟化。 1.7 云计算的私有与公有 云计算大致分两种，一个是私有云，一个是公有云，还有人把私有云和公有云连接起来称为混合云，我们暂且不说这个。私有云就是把虚拟化和云化的这套软件部署在别人的数据中心里面，使用私有云的用户往往很有钱，自己买地建机房，自己买服务器，然后让云厂商部署在自己这里，Vmware后来除了虚拟化，也推出了云计算的产品，并且在私有云市场赚的盆满钵满。所谓公有云就是虚拟化和云化软件部署在云厂商自己数据中心里面的，用户不需要很大的投入，只要注册一个账号，就能在一个网页上点一下创建一台虚拟电脑，例如AWS也即亚马逊的公有云，例如国内的阿里云，腾讯云，网易云等。 亚马逊呢为什么要做公有云呢？我们知道亚马逊原来是国外比较大的一个电商，它做电商的时候也肯定会遇到类似双11的场景，在某一个时刻大家都冲上来买东西。当大家都冲上买东西的时候，就特别需要云的时间灵活性和空间灵活性。因为它不能时刻准备好所有的资源，那样太浪费了。但也不能什么都不准备，看着双十一这么多用户想买东西登不上去。所以需要双十一的时候，创建一大批虚拟电脑来支撑电商应用，过了双十一再把这些资源都释放掉去干别的。所以亚马逊是需要一个云平台的。 然而商用的虚拟化软件实在是太贵了，亚马逊总不能把自己在电商赚的钱全部给了虚拟化厂商吧。于是亚马逊基于开源的虚拟化技术，如上所述的Xen或者KVM，开发了一套自己的云化软件。没想到亚马逊后来电商越做越牛，云平台也越做越牛。而且由于他的云平台需要支撑自己的电商应用，而传统的云计算厂商多为IT厂商出身，几乎没有自己的应用，因而亚马逊的云平台对应用更加的友好，迅速发展成为云计算的第一品牌，赚了很多钱。在亚马逊公布其云计算平台财报之前，人们都猜测，亚马逊电商赚钱，云也赚钱吗？后来一公布财报，发现不是一般的赚钱，仅仅去年，亚马逊AWS年营收达122亿美元，运营利润31亿美元。 1.8 云计算的赚钱与情怀 公有云的第一名亚马逊过得很爽，第二名Rackspace过的就一般了。没办法，这就是互联网行业的残酷性，多是赢者通吃的模式。所以第二名如果不是云计算行业的，很多人可能都没听过了。第二名就想，我干不过老大怎么办呢？开源吧。如上所述，亚马逊虽然使用了开源的虚拟化技术，但是云化的代码是闭源的，很多想做又做不了云化平台的公司，只能眼巴巴的看着亚马逊挣大钱。Rackspace把源代码一公开，整个行业就可以一起把这个平台越做越好，兄弟们大家一起上，和老大拼了。 于是Rackspace和美国航空航天局合作创办了开源软件OpenStack，如图所示OpenStack的架构图，不是云计算行业的不用弄懂这个图，但是能够看到三个关键字，Compute计算，Networking网络，Storage存储。还是一个计算，网络，存储的云化管理平台。 当然第二名的技术也是非常棒的，有了OpenStack之后，果真像Rackspace想象的一样，所有想做云的大企业都疯了，你能想象到的所有如雷贯耳的大型IT企业，IBM，惠普，戴尔，华为，联想等等，都疯了。原来云平台大家都想做，看着亚马逊和Vmware赚了这么多钱，眼巴巴看着没办法，想自己做一个好像难度还挺大。现在好了，有了这样一个开源的云平台OpenStack，所有的IT厂商都加入到这个社区中来，对这个云平台进行贡献，包装成自己的产品，连同自己的硬件设备一起卖。有的做了私有云，有的做了公有云，OpenStack已经成为开源云平台的事实标准。 1.9 IaaS, 资源层面的灵活性 随着OpenStack的技术越来越成熟，可以管理的规模也越来越大，并且可以有多个OpenStack集群部署多套，比如北京部署一套，杭州部署两套，广州部署一套，然后进行统一的管理。这样整个规模就更大了。在这个规模下，对于普通用户的感知来讲，基本能够做到想什么时候要就什么什么药，想要多少就要多少。还是拿云盘举例子，每个用户云盘都分配了5T甚至更大的空间，如果有1亿人，那加起来空间多大啊。其实背后的机制是这样的，分配你的空间，你可能只用了其中很少一点，比如说它分配给你了5个T，这么大的空间仅仅是你看到的，而不是真的就给你了，你其实只用了50个G，则真实给你的就是50个G，随着你文件的不断上传，分给你的空间会越来越多。当大家都上传，云平台发现快满了的时候(例如用了70%)，会采购更多的服务器，扩充背后的资源，这个对用户是透明的，看不到的，从感觉上来讲，就实现了云计算的弹性。其实有点像银行，给储户的感觉是什么时候取钱都有，只要不同时挤兑，银行就不会垮。 这里做一个简单的总结，到了这个阶段，云计算基本上实现了时间灵活性和空间灵活性，实现了计算，网络，存储资源的弹性。计算，网络，存储我们常称为基础设施Infranstracture, 因而这个阶段的弹性称为资源层面的弹性，管理资源的云平台，我们称为基础设施服务，就是我们常听到的IaaS，Infranstracture As A Service。 二、云计算不光管资源，也要管应用 有了IaaS，实现了资源层面的弹性就够了吗？显然不是。还有应用层面的弹性。这里举个例子，比如说实现一个电商的应用，平时十台机器就够了，双十一需要一百台。你可能觉得很好办啊，有了IaaS，新创建九十台机器就可以了啊。但是90台机器创建出来是空的啊，电商应用并没有放上去啊，只能你公司的运维人员一台一台的弄，还是需要很长时间才能安装好的。虽然资源层面实现了弹性，但是没有应用层的弹性，依然灵活性是不够的。 有没有方法解决这个问题呢？于是人们在IaaS平台之上又加了一层，用于管理资源以上的应用弹性的问题，这一层通常称为PaaS（Platform As A Service）。这一层往往比较难理解，其实大致分两部分，一部分我称为你自己的应用自动安装，一部分我称为通用的应用不用安装。 我们先来说第一部分，自己的应用自动安装。比如电商应用是你自己开发的，除了你自己，其他人是不知道怎么安装的，比如电商应用，安装的时候需要配置支付宝或者微信的账号，才能别人在你的电商上买东西的时候，付的钱是打到你的账户里面的，除了你，谁也不知道，所以安装的过程平台帮不了忙，但是能够帮你做的自动化，你需要做一些工作，将自己的配置信息融入到自动化的安装过程中方可。比如上面的例子，双十一新创建出来的90台机器是空的，如果能够提供一个工具，能够自动在这新的90台机器上将电商应用安装好，就能够实现应用层面的真正弹性。例如Puppet, Chef, Ansible, Cloud Foundary都可以干这件事情，最新的容器技术Docker能更好的干这件事情，不做技术的可以不用管这些词。 第二部分，通用的应用不用安装。所谓通用的应用，一般指一些复杂性比较高，但是大家都在用的，例如数据库。几乎所有的应用都会用数据库，但是数据库软件是标准的，虽然安装和维护比较复杂，但是无论谁安装都是一样。这样的应用可以变成标准的PaaS层的应用放在云平台的界面上。当用户需要一个数据库的时候，一点就出来了，用户就可以直接用了。有人问，既然谁安装都一个样，那我自己来好了，不需要花钱在云平台上买。当然不是，数据库是一个非常难的东西，光Oracle这家公司，靠数据库就能赚这么多钱。买Oracle也是要花很多很多钱的。然而大多数云平台会提供Mysql这样的开源数据库，又是开源，钱不需要花这么多了，但是维护这个数据库，却需要专门招一个很大的团队，如果这个数据库能够优化到能够支撑双十一，也不是一年两年能够搞定的。比如您是一个做单车的，当然没必要招一个非常大的数据库团队来干这件事情，成本太高了，应该交给云平台来做这件事情，专业的事情专业的人来自，云平台专门养了几百人维护这套系统，您只要专注于您的单车应用就可以了。 要么是自动部署，要么是不用部署，总的来说就是应用层你也要少操心，这就是PaaS层的重要作用。 虽说脚本的方式能够解决自己的应用的部署问题，然而不同的环境千差万别，一个脚本往往在一个环境上运行正确，到另一个环境就不正确了。 而容器是能更好的解决这个问题的。 容器是 Container，Container另一个意思是集装箱，其实容器的思想就是要变成软件交付的集装箱。集装箱的特点，一是封装，二是标准。 在没有集装箱的时代，假设将货物从 A运到 B，中间要经过三个码头、换三次船。每次都要将货物卸下船来，摆的七零八落，然后搬上船重新整齐摆好。因此在没有集装箱的时候，每次换船，船员们都要在岸上待几天才能走。 有了集装箱以后，所有的货物都打包在一起了，并且集装箱的尺寸全部一致，所以每次换船的时候，一个箱子整体搬过去就行了，小时级别就能完成，船员再也不用上岸长时间耽搁了。 这是集装箱“封装”、“标准”两大特点在生活中的应用。 那么容器如何对应用打包呢？还是要学习集装箱，首先要有个封闭的环境，将货物封装起来，让货物之间互不干扰，互相隔离，这样装货卸货才方便。好在 Ubuntu中的LXC技术早就能做到这一点。 封闭的环境主要使用了两种技术，一种是看起来是隔离的技术，称为 Namespace，也即每个 Namespace中的应用看到的是不同的 IP地址、用户空间、程号等。另一种是用起来是隔离的技术，称为 Cgroups，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。 所谓的镜像，就是将你焊好集装箱的那一刻，将集装箱的状态保存下来，就像孙悟空说：“定”，集装箱里面就定在了那一刻，然后将这一刻的状态保存成一系列文件。这些文件的格式是标准的，谁看到这些文件都能还原当时定住的那个时刻。将镜像还原成运行时的过程（就是读取镜像文件，还原那个时刻的过程）就是容器运行的过程。 有了容器，使得 PaaS层对于用户自身应用的自动部署变得快速而优雅。 三、大数据拥抱云计算 在PaaS层中一个复杂的通用应用就是大数据平台。大数据是如何一步一步融入云计算的呢？ 3.1 数据不大也包含智慧 一开始这个大数据并不大，你想象原来才有多少数据？现在大家都去看电子书，上网看新闻了，在我们80后小时候，信息量没有那么大，也就看看书，看看报，一个星期的报纸加起来才有多少字啊，如果你不在一个大城市，一个普通的学校的图书馆加起来也没几个书架，是后来随着信息化的到来，信息才会越来越多。 首先我们来看一下大数据里面的数据，就分三种类型，一种叫结构化的数据，一种叫非结构化的数据，还有一种叫半结构化的数据。什么叫结构化的数据呢？叫有固定格式和有限长度的数据。例如填的表格就是结构化的数据，国籍：中华人民共和国，民族：汉，性别：男，这都叫结构化数据。现在越来越多的就是非结构化的数据，就是不定长，无固定格式的数据，例如网页，有时候非常长，有时候几句话就没了，例如语音，视频都是非结构化的数据。半结构化数据是一些xml或者html的格式的，不从事技术的可能不了解，但也没有关系。 数据怎么样才能对人有用呢？其实数据本身不是有用的，必须要经过一定的处理。例如你每天跑步带个手环收集的也是数据，网上这么多网页也是数据，我们称为Data，数据本身没有什么用处，但是数据里面包含一个很重要的东西，叫做信息Information，数据十分杂乱，经过梳理和清洗，才能够称为信息。信息会包含很多规律，我们需要从信息中将规律总结出来，称为知识knowledge，知识改变命运。信息是很多的，但是有人看到了信息相当于白看，但是有人就从信息中看到了电商的未来，有人看到了直播的未来，所以人家就牛了，你如果没有从信息中提取出知识，天天看朋友圈，也只能在互联网滚滚大潮中做个看客。有了知识，然后利用这些知识去应用于实战，有的人会做得非常好，这个东西叫做智慧intelligence。有知识并不一定有智慧，例如好多学者很有知识，已经发生的事情可以从各个角度分析的头头是道，但一到实干就歇菜，并不能转化成为智慧。而很多的创业家之所以伟大，就是通过获得的知识应用于实践，最后做了很大的生意。 所以数据的应用分这四个步骤：数据，信息，知识，智慧。这是很多商家都想要的，你看我收集了这么多的数据，能不能基于这些数据来帮我做下一步的决策，改善我的产品，例如让用户看视频的时候旁边弹出广告，正好是他想买的东西，再如让用户听音乐的时候，另外推荐一些他非常想听的其他音乐。用户在我的应用或者网站上随便点点鼠标，输入文字对我来说都是数据，我就是要将其中某些东西提取出来，指导实践，形成智慧，让用户陷入到我的应用里面不可自拔，上了我的网就不想离开，手不停的点，不停的买，很多人说双十一我都想断网了，我老婆在上面不断的买买买，买了A又推荐B，老婆大人说，“哎呀，B也是我喜欢的啊，老公我要买”。你说这个程序怎么这么牛，这么有智慧，比我还了解我老婆，这件事情是怎么做到的呢？ 3.2 数据如何升华为智慧 数据的处理分几个步骤，完成了才最后会有智慧。 第一个步骤叫数据的收集。首先得有数据，数据的收集有两个方式，第一个方式是拿，专业点的说法叫抓取或者爬取，例如搜索引擎就是这么做的，它把网上的所有的信息都下载到它的数据中心，然后你一搜才能搜出来。比如你去搜索的时候，结果会是一个列表，这个列表为什么会在搜索引擎的公司里面呢，就是因为他把这个数据啊都拿下来了，但是你一点链接，点出来这个网站就不在搜索引擎它们公司了。比如说新浪有个新闻，你拿百度搜出来，你不点的时候，那一页在百度数据中心，一点出来的网页就是在新浪的数据中心了。另外一个方式就是推送，有很多终端可以帮我收集数据，比如说小米手环，可以将你每天跑步的数据，心跳的数据，睡眠的数据都上传到数据中心里面。 第二个步骤是数据的传输。一般会通过队列方式进行，因为数据量实在是太大了，数据必须经过处理才会有用，可是系统处理不过来，只好排好队，慢慢的处理。 第三个步骤是数据的存储。现在数据就是金钱，掌握了数据就相当于掌握了钱。要不然网站怎么知道你想买什么呢？就是因为它有你历史的交易的数据，这个信息可不能给别人，十分宝贵，所以需要存储下来。 第四个步骤是数据的处理和分析。上面存储的数据是原始数据，原始数据多是杂乱无章的，有很多垃圾数据在里面，因而需要清洗和过滤，得到一些高质量的数据。对于高质量的数据，就可以进行分析，从而对数据进行分类，或者发现数据之间的相互关系，得到知识。比如盛传的沃尔玛超市的啤酒和尿布的故事，就是通过对人们的购买数据进行分析，发现了男人一般买尿布的时候，会同时购买啤酒，这样就发现了啤酒和尿布之间的相互关系，获得知识，然后应用到实践中，将啤酒和尿布的柜台弄的很近，就获得了智慧。 第五个步骤就是对于数据的检索和挖掘。检索就是搜索，所谓外事不决问google，内事不决问百度。内外两大搜索引擎都是讲分析后的数据放入搜索引擎，从而人们想寻找信息的时候，一搜就有了。另外就是挖掘，仅仅搜索出来已经不能满足人们的要求了，还需要从信息中挖掘出相互的关系。比如财经搜索，当搜索某个公司股票的时候，该公司的高管是不是也应该被挖掘出来呢？如果仅仅搜索出这个公司的股票发现涨的特别好，于是你就去买了，其实其高管发了一个声明，对股票十分不利，第二天就跌了，这不坑害广大股民么？所以通过各种算法挖掘数据中的关系，形成知识库，十分重要。 3.3 大数据时代，众人拾柴火焰高 当数据量很小的时候，很少的几台机器就能解决。慢慢的当数据量越来越大，最牛的服务器都解决不了问题的时候，就想怎么办呢？要聚合多台机器的力量，大家齐心协力一起把这个事搞定，众人拾柴火焰高。 对于数据的收集，对于IoT来讲，外面部署这成千上万的检测设备，将大量的温度，适度，监控，电力等等数据统统收集上来，对于互联网网页的搜索引擎来讲，需要将整个互联网所有的网页都下载下来，这显然一台机器做不到，需要多台机器组成网络爬虫系统，每台机器下载一部分，同时工作，才能在有限的时间内，将海量的网页下载完毕。 对于数据的传输，一个内存里面的队列肯定会被大量的数据挤爆掉，于是就产生了基于硬盘的分布式队列，这样队列可以多台机器同时传输，随你数据量多大，只要我的队列足够多，管道足够粗，就能够撑得住。 对于数据的存储，一台机器的文件系统肯定是放不下了，所以需要一个很大的分布式文件系统来做这件事情，把多台机器的硬盘打成一块大的文件系统。 再如数据的分析，可能需要对大量的数据做分解，统计，汇总，一台机器肯定搞不定，处理到猴年马月也分析不完，于是就有分布式计算的方法，将大量的数据分成小份，每台机器处理一小份，多台机器并行处理，很快就能算完。例如著名的Terasort对1个TB的数据排序，相当于1000G，如果单机处理，怎么也要几个小时，但是并行处理209秒就完成了。 所以说大数据平台，什么叫做大数据，说白了就是一台机器干不完，大家一起干。随着数据量越来越大，很多不大的公司都需要处理相当多的数据，这些小公司没有这么多机器可怎么办呢？ 3.4 大数据需要云计算，云计算需要大数据 说到这里，大家想起云计算了吧。当想要干这些活的时候，需要好多好多的机器一块做，真的是想什么时候要，想要多少就要多少。例如大数据分析公司的财务情况，可能一周分析一次，如果要把这一百台机器或者一千台机器都在那放着，一周用一次对吧，非常浪费。那能不能需要计算的时候，把这一千台机器拿出来，然后不算的时候，这一千台机器可以去干别的事情。谁能做这个事儿呢？只有云计算，可以为大数据的运算提供资源层的灵活性。而云计算也会部署大数据放到它的PaaS平台上，作为一个非常非常重要的通用应用。因为大数据平台能够使得多台机器一起干一个事儿，这个东西不是一般人能开发出来的，也不是一般人玩得转的，怎么也得雇个几十上百号人才能把这个玩起来，所以说就像数据库一样，其实还是需要有一帮专业的人来玩这个东西。现在公有云上基本上都会有大数据的解决方案了，一个小公司我需要大数据平台的时候，不需要采购一千台机器，只要到公有云上一点，这一千台机器都出来了，并且上面已经部署好了的大数据平台，只要把数据放进去算就可以了。 云计算需要大数据，大数据需要云计算，两个人就这样结合了。 四、人工智能拥抱大数据 4.1 机器什么时候才能懂人心 虽说有了大数据，人的欲望总是这个不能够满足。虽说在大数据平台里面有搜索引擎这个东西，想要什么东西我一搜就出来了。但是也存在这样的情况，我想要的东西不会搜，表达不出来，搜索出来的又不是我想要的。例如音乐软件里面推荐一首歌，这首歌我没听过，当然不知道名字，也没法搜，但是软件推荐给我，我的确喜欢，这就是搜索做不到的事情。当人们使用这种应用的时候，会发现机器知道我想要什么，而不是说当我想要的时候，去机器里面搜索。这个机器真像我的朋友一样懂我，这就有点人工智能的意思了。 人们很早就在想这个事情了。最早的时候，人们想象，如果要是有一堵墙，墙后面是个机器，我给它说话，它就给我回应，我如果感觉不出它那边是人还是机器，那它就真的是一个人工智能的东西了。 4.2 让机器学会推理 怎么才能做到这一点呢？人们就想：我首先要告诉计算机人类的推理的能力。你看人重要的是什么呀，人和动物的区别在什么呀，就是能推理。我要是把我这个推理的能力啊告诉机器，机器就能根据你的提问，推理出相应的回答，真能这样多好。推理其实人们慢慢的让机器能够做到一些了，例如证明数学公式。这是一个非常让人惊喜的一个过程，机器竟然能够证明数学公式。但是慢慢发现其实这个结果，也没有那么令人惊喜，因为大家发现了一个问题，数学公式非常严谨，推理过程也非常严谨，而且数学公式很容易拿机器来进行表达，程序也相对容易表达。然而人类的语言就没这么简单了，比如今天晚上，你和你女朋友约会，你女朋友说：如果你早来，我没来，你等着，如果我早来，你没来，你等着。这个机器就比比较难理解了，但是人都懂，所以你和女朋友约会，你是不敢迟到的。 4.3 教给机器知识 所以仅仅告诉机器严格的推理是不够的，还要告诉机器一些知识。但是知识这个事儿，一般人可能就做不来了，可能专家可以，比如语言领域的专家，或者财经领域的专家。语言领域和财经领域知识能不能表示成像数学公式一样稍微严格点呢？例如语言专家可能会总结出主谓宾定状补这些语法规则，主语后面一定是谓语，谓语后面一定是宾语，将这些总结出来，并严格表达出来不久行了吗？后来发现这个不行，太难总结了，语言表达千变万化。就拿主谓宾的例子，很多时候在口语里面就省略了谓语，别人问：你谁啊？我回答：我刘超。但是你不能规定在语音语义识别的时候，要求对着机器说标准的书面语，这样还是不够智能，就像罗永浩在一次演讲中说的那样，每次对着手机，用书面语说：请帮我呼叫某某某，这是一件很尴尬的事情。 人工智能这个阶段叫做专家系统。专家系统不易成功，一方面是知识比较难总结，另一方面总结出来的知识难以教给计算机。因为你自己还迷迷糊糊，似乎觉得有规律，就是说不出来，就怎么能够通过编程教给计算机呢？ 4.4 算了，教不会你自己学吧 于是人们想到，看来机器是和人完全不一样的物种，干脆让机器自己学习好了。机器怎么学习呢？既然机器的统计能力这么强，基于统计学习，一定能从大量的数字中发现一定的规律。 其实在娱乐圈有很好的一个例子，可见一斑 有一位网友统计了知名歌手在大陆发行的 9 张专辑中 117 首歌曲的歌词，同一词语在一首歌出现只算一次，形容词、名词和动词的前十名如下表所示（词语后面的数字是出现的次数）： | a | 形容词 | b | 名词 | c | 动词 | | 0 | 孤独:34 | 0 | 生命:50 | 0 | 爱:54 | | 1 | 自由:17 | 1 | 路:37 | 1 | 碎:37 | | 2 | 迷惘:16 | 2 | 夜:29 | 2 | 哭:35 | | 3 | 坚强:13 | 3 | 天空:24 | 3 | 死:27 | | 4 | 绝望:8 | 4 | 孩子:23 | 4 | 飞:26 | | 5 | 青春:7 | 5 | 雨:21 | 5 | 梦想:14 | | 6 | 迷茫:6 | 6 | 石头:9 | 6 | 祈祷:10 | | 7 | 光明:6 | 7 | 鸟:9 | 7 | 离去:10 | 如果我们随便写一串数字，然后按照数位依次在形容词、名词和动词中取出一个词，连在一起会怎么样呢？ 例如取圆周率 3.1415926，对应的词语是：坚强，路，飞，自由，雨，埋，迷惘。稍微连接和润色一下： 坚强的孩子， 依然前行在路上， 张开翅膀飞向自由， 让雨水埋葬他的迷惘。 是不是有点感觉了？当然真正基于统计的学习算法比这个简单的统计复杂的多。 然而统计学习比较容易理解简单的相关性，例如一个词和另一个词总是一起出现，两个词应该有关系，而无法表达复杂的相关性，并且统计方法的公式往往非常复杂，为了简化计算，常常做出各种独立性的假设，来降低公式的计算难度，然而现实生活中，具有独立性的事件是相对较少的。 4.5 模拟大脑的工作方式 于是人类开始从机器的世界，反思人类的世界是怎么工作的。 人类的脑子里面不是存储着大量的规则，也不是记录着大量的统计数据，而是通过神经元的触发实现的，每个神经元有从其他神经元的输入，当接收到输入的时候，会产生一个输出来刺激其他的神经元，于是大量的神经元相互反应，最终形成各种输出的结果。例如当人们看到美女瞳孔放大，绝不是大脑根据身材比例进行规则判断，也不是将人生中看过的所有的美女都统计一遍，而是神经元从视网膜触发到大脑再回到瞳孔。在这个过程中，其实很难总结出每个神经元对最终的结果起到了哪些作用，反正就是起作用了。 于是人们开始用一个数学单元模拟神经元 这个神经元有输入，有输出，输入和输出之间通过一个公式来表示，输入根据重要程度不同(权重)，影响着输出。 于是将n个神经元通过像一张神经网络一样连接在一起，n这个数字可以很大很大，所有的神经元可以分成很多列，每一列很多个排列起来，每个神经元的对于输入的权重可以都不相同，从而每个神经元的公式也不相同。当人们从这张网络中输入一个东西的时候，希望输出一个对人类来讲正确的结果。例如上面的例子，输入一个写着2的图片，输出的列表里面第二个数字最大，其实从机器来讲，它既不知道输入的这个图片写的是2，也不知道输出的这一系列数字的意义，没关系，人知道意义就可以了。正如对于神经元来说，他们既不知道视网膜看到的是美女，也不知道瞳孔放大是为了看的清楚，反正看到美女，瞳孔放大了，就可以了。 对于任何一张神经网络，谁也不敢保证输入是2，输出一定是第二个数字最大，要保证这个结果，需要训练和学习。毕竟看到美女而瞳孔放大也是人类很多年进化的结果。学习的过程就是，输入大量的图片，如果结果不是想要的结果，则进行调整。如何调整呢，就是每个神经元的每个权重都向目标进行微调，由于神经元和权重实在是太多了，所以整张网络产生的结果很难表现出非此即彼的结果，而是向着结果微微的进步，最终能够达到目标结果。当然这些调整的策略还是非常有技巧的，需要算法的高手来仔细的调整。正如人类见到美女，瞳孔一开始没有放大到能看清楚，于是美女跟别人跑了，下次学习的结果是瞳孔放大一点点，而不是放大鼻孔。 4.6 没道理但做得到 听起来也没有那么有道理，但是的确能做到，就是这么任性。 神经网络的普遍性定理是这样说的，假设某个人给你某种复杂奇特的函数，f(x)： 不管这个函数是什么样的，总会确保有个神经网络能够对任何可能的输入x，其值f(x)（或者某个能够准确的近似）是神经网络的输出。 如果在函数代表着规律，也意味着这个规律无论多么奇妙，多么不能理解，都是能通过大量的神经元，通过大量权重的调整，表示出来的。 4.7 人工智能的经济学解释 这让我想到了经济学，于是比较容易理解了。 我们把每个神经元当成社会中从事经济活动的个体。于是神经网络相当于整个经济社会，每个神经元对于社会的输入，都有权重的调整，做出相应的输出，比如工资涨了，菜价也涨了，股票跌了，我应该怎么办，怎么花自己的钱。这里面没有规律么？肯定有，但是具体什么规律呢？却很难说清楚。 基于专家系统的经济属于计划经济，整个经济规律的表示不希望通过每个经济个体的独立决策表现出来，而是希望通过专家的高屋建瓴和远见卓识总结出来。专家永远不可能知道哪个城市的哪个街道缺少一个卖甜豆腐脑的。于是专家说应该产多少钢铁，产多少馒头，往往距离人民生活的真正需求有较大的差距，就算整个计划书写个几百页，也无法表达隐藏在人民生活中的小规律。 基于统计的宏观调控就靠谱的多了，每年统计局都会统计整个社会的就业率，通胀率，GDP等等指标，这些指标往往代表着很多的内在规律，虽然不能够精确表达，但是相对靠谱。然而基于统计的规律总结表达相对比较粗糙，比如经济学家看到这些统计数据可以总结出长期来看房价是涨还是跌，股票长期来看是涨还是跌，如果经济总体上扬，房价和股票应该都是涨的。但是基于统计数据，无法总结出股票，物价的微小波动规律。 基于神经网络的微观经济学才是对整个经济规律最最准确的表达，每个人对于从社会中的输入，进行各自的调整，并且调整同样会作为输入反馈到社会中。想象一下股市行情细微的波动曲线，正是每个独立的个体各自不断交易的结果，没有统一的规律可循。而每个人根据整个社会的输入进行独立决策，当某些因素经过多次训练，也会形成宏观上的统计性的规律，这也就是宏观经济学所能看到的。例如每次货币大量发行，最后房价都会上涨，多次训练后，人们也就都学会了。 4.8 人工智能需要大数据 然而神经网络包含这么多的节点，每个节点包含非常多的参数，整个参数量实在是太大了，需要的计算量实在太大，但是没有关系啊，我们有大数据平台，可以汇聚多台机器的力量一起来计算，才能在有限的时间内得到想要的结果。 人工智能可以做的事情非常多，例如可以鉴别垃圾邮件，鉴别黄色暴力文字和图片等。这也是经历了三个阶段的。第一个阶段依赖于关键词黑白名单和过滤技术，包含哪些词就是黄色或者暴力的文字。随着这个网络语言越来越多，词也不断的变化，不断的更新这个词库就有点顾不过来。第二个阶段时，基于一些新的算法，比如说贝叶斯过滤等，你不用管贝叶斯算法是什么，但是这个名字你应该听过，这个一个基于概率的算法。第三个阶段就是基于大数据和人工智能，进行更加精准的用户画像和文本理解和图像理解。 由于人工智能算法多是依赖于大量的数据的，这些数据往往需要面向某个特定的领域(例如电商，邮箱)进行长期的积累，如果没有数据，就算有人工智能算法也白搭，所以人工智能程序很少像前面的IaaS和PaaS一样，将人工智能程序给某个客户安装一套让客户去用，因为给某个客户单独安装一套，客户没有相关的数据做训练，结果往往是很差的。但是云计算厂商往往是积累了大量数据的，于是就在云计算厂商里面安装一套，暴露一个服务接口，比如您想鉴别一个文本是不是涉及黄色和暴力，直接用这个在线服务就可以了。这种形势的服务，在云计算里面称为软件即服务，SaaS (Software AS A Service) 于是工智能程序作为SaaS平台进入了云计算。 五、云计算，大数据，人工智能过上了美好的生活 终于云计算的三兄弟凑齐了，分别是IaaS，PaaS和SaaS，所以一般在一个云计算平台上，云，大数据，人工智能都能找得到。对一个大数据公司，积累了大量的数据，也会使用一些人工智能的算法提供一些服务。对于一个人工智能公司，也不可能没有大数据平台支撑。所以云计算，大数据，人工智能就这样整合起来，完成了相遇，相识，相知。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端技术</category>
      </categories>
      <tags>
        <tag>大后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南12：深度解读 java 线程池设计思想及源码实现]]></title>
    <url>%2F2019%2F10%2F12%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9712%EF%BC%9A%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BB%20java%20%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E5%8F%8A%E6%BA%90%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/ 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言我相信大家都看过很多的关于线程池的文章，基本上也是面试的时候必问的，如果你在看过很多文章以后，还是一知半解的，那希望这篇文章能让你真正的掌握好 Java 线程池。 本文一大重点是源码解析，同时会有少量篇幅介绍线程池设计思想以及作者 Doug Lea 实现过程中的一些巧妙用法。本文还是会一行行关键代码进行分析，目的是为了让那些自己看源码不是很理解的同学可以得到参考。 线程池是非常重要的工具，如果你要成为一个好的工程师，还是得比较好地掌握这个知识，很多线上问题都是因为没有用好线程池导致的。即使你为了谋生，也要知道，这基本上是面试必问的题目，而且面试官很容易从被面试者的回答中捕捉到被面试者的技术水平。 本文略长，建议在 pc 上阅读，边看文章边翻源码（Java7 和 Java8 都一样），建议想好好看的读者抽出至少 30 分钟的整块时间来阅读。当然，如果读者仅为面试准备，可以直接滑到最后的总结部分。 总览开篇来一些废话。下图是 java 线程池几个相关类的继承结构： 先简单说说这个继承结构，Executor 位于最顶层，也是最简单的，就一个 execute(Runnable runnable) 接口方法定义。 ExecutorService 也是接口，在 Executor 接口的基础上添加了很多的接口方法，所以一般来说我们会使用这个接口。 然后再下来一层是 AbstractExecutorService，从名字我们就知道，这是抽象类，这里实现了非常有用的一些方法供子类直接使用，之后我们再细说。 然后才到我们的重点部分 ThreadPoolExecutor 类，这个类提供了关于线程池所需的非常丰富的功能。 另外，我们还涉及到下图中的这些类： 同在并发包中的 Executors 类，类名中带字母 s，我们猜到这个是工具类，里面的方法都是静态方法，如以下我们最常用的用于生成 ThreadPoolExecutor 的实例的一些方法： 12345678910public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 另外，由于线程池支持获取线程执行的结果，所以，引入了 Future 接口，RunnableFuture 继承自此接口，然后我们最需要关心的就是它的实现类 FutureTask。到这里，记住这个概念，在线程池的使用过程中，我们是往线程池提交任务（task），使用过线程池的都知道，我们提交的每个任务是实现了 Runnable 接口的，其实就是先将 Runnable 的任务包装成 FutureTask，然后再提交到线程池。这样，读者才能比较容易记住 FutureTask 这个类名：它首先是一个任务（Task），然后具有 Future 接口的语义，即可以在将来（Future）得到执行的结果。 当然，线程池中的 BlockingQueue 也是非常重要的概念，如果线程数达到 corePoolSize，我们的每个任务会提交到等待队列中，等待线程池中的线程来取任务并执行。这里的 BlockingQueue 通常我们使用其实现类 LinkedBlockingQueue、ArrayBlockingQueue 和 SynchronousQueue，每个实现类都有不同的特征，使用场景之后会慢慢分析。想要详细了解各个 BlockingQueue 的读者，可以参考我的前面的一篇对 BlockingQueue 的各个实现类进行详细分析的文章。 把事情说完整：除了上面说的这些类外，还有一个很重要的类，就是定时任务实现类 ScheduledThreadPoolExecutor，它继承自本文要重点讲解的 ThreadPoolExecutor，用于实现定时执行。不过本文不会介绍它的实现，我相信读者看完本文后可以比较容易地看懂它的源码。 以上就是本文要介绍的知识，废话不多说，开始进入正文。 Executor 接口1234567/* * @since 1.5 * @author Doug Lea */public interface Executor &#123; void execute(Runnable command);&#125; 我们可以看到 Executor 接口非常简单，就一个 void execute(Runnable command) 方法，代表提交一个任务。为了让大家理解 java 线程池的整个设计方案，我会按照 Doug Lea 的设计思路来多说一些相关的东西。 我们经常这样启动一个线程： 123new Thread(new Runnable()&#123; // do something&#125;).start(); 用了线程池 Executor 后就可以像下面这么使用： 123Executor executor = anExecutor;executor.execute(new RunnableTask1());executor.execute(new RunnableTask2()); 如果我们希望线程池同步执行每一个任务，我们可以这么实现这个接口： 12345class DirectExecutor implements Executor &#123; public void execute(Runnable r) &#123; r.run();// 这里不是用的new Thread(r).start()，也就是说没有启动任何一个新的线程。 &#125;&#125; 我们希望每个任务提交进来后，直接启动一个新的线程来执行这个任务，我们可以这么实现： 12345class ThreadPerTaskExecutor implements Executor &#123; public void execute(Runnable r) &#123; new Thread(r).start(); // 每个任务都用一个新的线程来执行 &#125;&#125; 我们再来看下怎么组合两个 Executor 来使用，下面这个实现是将所有的任务都加到一个 queue 中，然后从 queue 中取任务，交给真正的执行器执行，这里采用 synchronized 进行并发控制： 123456789101112131415161718192021222324252627282930313233343536class SerialExecutor implements Executor &#123; // 任务队列 final Queue&lt;Runnable&gt; tasks = new ArrayDeque&lt;Runnable&gt;(); // 这个才是真正的执行器 final Executor executor; // 当前正在执行的任务 Runnable active; // 初始化的时候，指定执行器 SerialExecutor(Executor executor) &#123; this.executor = executor; &#125; // 添加任务到线程池: 将任务添加到任务队列，scheduleNext 触发执行器去任务队列取任务 public synchronized void execute(final Runnable r) &#123; tasks.offer(new Runnable() &#123; public void run() &#123; try &#123; r.run(); &#125; finally &#123; scheduleNext(); &#125; &#125; &#125;); if (active == null) &#123; scheduleNext(); &#125; &#125; protected synchronized void scheduleNext() &#123; if ((active = tasks.poll()) != null) &#123; // 具体的执行转给真正的执行器 executor executor.execute(active); &#125; &#125;&#125; 当然了，Executor 这个接口只有提交任务的功能，太简单了，我们想要更丰富的功能，比如我们想知道执行结果、我们想知道当前线程池有多少个线程活着、已经完成了多少任务等等，这些都是这个接口的不足的地方。接下来我们要介绍的是继承自 Executor 接口的 ExecutorService 接口，这个接口提供了比较丰富的功能，也是我们最常使用到的接口。 ExecutorService一般我们定义一个线程池的时候，往往都是使用这个接口： 12ExecutorService executor = Executors.newFixedThreadPool(args...);ExecutorService executor = Executors.newCachedThreadPool(args...); 因为这个接口中定义的一系列方法大部分情况下已经可以满足我们的需要了。 那么我们简单初略地来看一下这个接口中都有哪些方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public interface ExecutorService extends Executor &#123; // 关闭线程池，已提交的任务继续执行，不接受继续提交新任务 void shutdown(); // 关闭线程池，尝试停止正在执行的所有任务，不接受继续提交新任务 // 它和前面的方法相比，加了一个单词“now”，区别在于它会去停止当前正在进行的任务 List&lt;Runnable&gt; shutdownNow(); // 线程池是否已关闭 boolean isShutdown(); // 如果调用了 shutdown() 或 shutdownNow() 方法后，所有任务结束了，那么返回true // 这个方法必须在调用shutdown或shutdownNow方法之后调用才会返回true boolean isTerminated(); // 等待所有任务完成，并设置超时时间 // 我们这么理解，实际应用中是，先调用 shutdown 或 shutdownNow， // 然后再调这个方法等待所有的线程真正地完成，返回值意味着有没有超时 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // 提交一个 Callable 任务 &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task); // 提交一个 Runnable 任务，第二个参数将会放到 Future 中，作为返回值， // 因为 Runnable 的 run 方法本身并不返回任何东西 &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); // 提交一个 Runnable 任务 Future&lt;?&gt; submit(Runnable task); // 执行所有任务，返回 Future 类型的一个 list &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException; // 也是执行所有任务，但是这里设置了超时时间 &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException; // 只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException; // 同上一个方法，只有其中的一个任务结束了，就可以返回，返回执行完的那个任务的结果， // 不过这个带超时，超过指定的时间，抛出 TimeoutException 异常 &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 这些方法都很好理解，一个简单的线程池主要就是这些功能，能提交任务，能获取结果，能关闭线程池，这也是为什么我们经常用这个接口的原因。 FutureTask在继续往下层介绍 ExecutorService 的实现类之前，我们先来说说相关的类 FutureTask。 1234567891011Future Runnable \ / \ / RunnableFuture | | FutureTaskFutureTask 通过 RunnableFuture 间接实现了 Runnable 接口，所以每个 Runnable 通常都先包装成 FutureTask，然后调用 executor.execute(Runnable command) 将其提交给线程池 我们知道，Runnable 的 void run() 方法是没有返回值的，所以，通常，如果我们需要的话，会在 submit 中指定第二个参数作为返回值： 1&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); 其实到时候会通过这两个参数，将其包装成 Callable。它和 Runnable 的区别在于 run() 没有返回值，而 Callable 的 call() 方法有返回值，同时，如果运行出现异常，call() 方法会抛出异常。 1234public interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 在这里，就不展开说 FutureTask 类了，因为本文篇幅本来就够大了，这里我们需要知道怎么用就行了。 下面，我们来看看 ExecutorService 的抽象实现 AbstractExecutorService 。 AbstractExecutorServiceAbstractExecutorService 抽象类派生自 ExecutorService 接口，然后在其基础上实现了几个实用的方法，这些方法提供给子类进行调用。 这个抽象类实现了 invokeAny 方法和 invokeAll 方法，这里的两个 newTaskFor 方法也比较有用，用于将任务包装成 FutureTask。定义于最上层接口 Executor中的 void execute(Runnable command) 由于不需要获取结果，不会进行 FutureTask 的包装。 需要获取结果（FutureTask），用 submit 方法，不需要获取结果，可以用 execute 方法。 下面，我将一行一行源码地来分析这个类，跟着源码来看看其实现吧： Tips: invokeAny 和 invokeAll 方法占了这整个类的绝大多数篇幅，读者可以选择适当跳过，因为它们可能在你的实践中使用的频次比较低，而且它们不带有承前启后的作用，不用担心会漏掉什么导致看不懂后面的代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261public abstract class AbstractExecutorService implements ExecutorService &#123; // RunnableFuture 是用于获取执行结果的，我们常用它的子类 FutureTask // 下面两个 newTaskFor 方法用于将我们的任务包装成 FutureTask 提交到线程池中执行 protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Runnable runnable, T value) &#123; return new FutureTask&lt;T&gt;(runnable, value); &#125; protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) &#123; return new FutureTask&lt;T&gt;(callable); &#125; // 提交任务 public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); // 1\. 将任务包装成 FutureTask RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); // 2\. 交给执行器执行，execute 方法由具体的子类来实现 // 前面也说了，FutureTask 间接实现了Runnable 接口。 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); // 1\. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); // 2\. 交给执行器执行 execute(ftask); return ftask; &#125; public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); // 1\. 将任务包装成 FutureTask RunnableFuture&lt;T&gt; ftask = newTaskFor(task); // 2\. 交给执行器执行 execute(ftask); return ftask; &#125; // 此方法目的：将 tasks 集合中的任务提交到线程池执行，任意一个线程执行完后就可以结束了 // 第二个参数 timed 代表是否设置超时机制，超时时间为第三个参数， // 如果 timed 为 true，同时超时了还没有一个线程返回结果，那么抛出 TimeoutException 异常 private &lt;T&gt; T doInvokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, boolean timed, long nanos) throws InterruptedException, ExecutionException, TimeoutException &#123; if (tasks == null) throw new NullPointerException(); // 任务数 int ntasks = tasks.size(); if (ntasks == 0) throw new IllegalArgumentException(); // List&lt;Future&lt;T&gt;&gt; futures= new ArrayList&lt;Future&lt;T&gt;&gt;(ntasks); // ExecutorCompletionService 不是一个真正的执行器，参数 this 才是真正的执行器 // 它对执行器进行了包装，每个任务结束后，将结果保存到内部的一个 completionQueue 队列中 // 这也是为什么这个类的名字里面有个 Completion 的原因吧。 ExecutorCompletionService&lt;T&gt; ecs = new ExecutorCompletionService&lt;T&gt;(this); try &#123; // 用于保存异常信息，此方法如果没有得到任何有效的结果，那么我们可以抛出最后得到的一个异常 ExecutionException ee = null; long lastTime = timed ? System.nanoTime() : 0; Iterator&lt;? extends Callable&lt;T&gt;&gt; it = tasks.iterator(); // 首先先提交一个任务，后面的任务到下面的 for 循环一个个提交 futures.add(ecs.submit(it.next())); // 提交了一个任务，所以任务数量减 1 --ntasks; // 正在执行的任务数(提交的时候 +1，任务结束的时候 -1) int active = 1; for (;;) &#123; // ecs 上面说了，其内部有一个 completionQueue 用于保存执行完成的结果 // BlockingQueue 的 poll 方法不阻塞，返回 null 代表队列为空 Future&lt;T&gt; f = ecs.poll(); // 为 null，说明刚刚提交的第一个线程还没有执行完成 // 在前面先提交一个任务，加上这里做一次检查，也是为了提高性能 if (f == null) &#123; if (ntasks &gt; 0) &#123; --ntasks; futures.add(ecs.submit(it.next())); ++active; &#125; // 这里是 else if，不是 if。这里说明，没有任务了，同时 active 为 0 说明 // 任务都执行完成了。其实我也没理解为什么这里做一次 break？ // 因为我认为 active 为 0 的情况，必然从下面的 f.get() 返回了 // 2018-02-23 感谢读者 newmicro 的 comment， // 这里的 active == 0，说明所有的任务都执行失败，那么这里是 for 循环出口 else if (active == 0) break; // 这里也是 else if。这里说的是，没有任务了，但是设置了超时时间，这里检测是否超时 else if (timed) &#123; // 带等待的 poll 方法 f = ecs.poll(nanos, TimeUnit.NANOSECONDS); // 如果已经超时，抛出 TimeoutException 异常，这整个方法就结束了 if (f == null) throw new TimeoutException(); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; &#125; // 这里是 else。说明，没有任务需要提交，但是池中的任务没有完成，还没有超时(如果设置了超时) // take() 方法会阻塞，直到有元素返回，说明有任务结束了 else f = ecs.take(); &#125; /* * 我感觉上面这一段并不是很好理解，这里简单说下。 * 1\. 首先，这在一个 for 循环中，我们设想每一个任务都没那么快结束， * 那么，每一次都会进到第一个分支，进行提交任务，直到将所有的任务都提交了 * 2\. 任务都提交完成后，如果设置了超时，那么 for 循环其实进入了“一直检测是否超时” 这件事情上 * 3\. 如果没有设置超时机制，那么不必要检测超时，那就会阻塞在 ecs.take() 方法上， 等待获取第一个执行结果 * 4\. 如果所有的任务都执行失败，也就是说 future 都返回了， 但是 f.get() 抛出异常，那么从 active == 0 分支出去(感谢 newmicro 提出) // 当然，这个需要看下面的 if 分支。 */ // 有任务结束了 if (f != null) &#123; --active; try &#123; // 返回执行结果，如果有异常，都包装成 ExecutionException return f.get(); &#125; catch (ExecutionException eex) &#123; ee = eex; &#125; catch (RuntimeException rex) &#123; ee = new ExecutionException(rex); &#125; &#125; &#125;// 注意看 for 循环的范围，一直到这里 if (ee == null) ee = new ExecutionException(); throw ee; &#125; finally &#123; // 方法退出之前，取消其他的任务 for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException, ExecutionException &#123; try &#123; return doInvokeAny(tasks, false, 0); &#125; catch (TimeoutException cannotHappen) &#123; assert false; return null; &#125; &#125; public &lt;T&gt; T invokeAny(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; return doInvokeAny(tasks, true, unit.toNanos(timeout)); &#125; // 执行所有的任务，返回任务结果。 // 先不要看这个方法，我们先想想，其实我们自己提交任务到线程池，也是想要线程池执行所有的任务 // 只不过，我们是每次 submit 一个任务，这里以一个集合作为参数提交 public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks) throws InterruptedException &#123; if (tasks == null) throw new NullPointerException(); List&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; // 这个很简单 for (Callable&lt;T&gt; t : tasks) &#123; // 包装成 FutureTask RunnableFuture&lt;T&gt; f = newTaskFor(t); futures.add(f); // 提交任务 execute(f); &#125; for (Future&lt;T&gt; f : futures) &#123; if (!f.isDone()) &#123; try &#123; // 这是一个阻塞方法，直到获取到值，或抛出了异常 // 这里有个小细节，其实 get 方法签名上是会抛出 InterruptedException 的 // 可是这里没有进行处理，而是抛给外层去了。此异常发生于还没执行完的任务被取消了 f.get(); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; &#125; &#125; done = true; // 这个方法返回，不像其他的场景，返回 List&lt;Future&gt;，其实执行结果还没出来 // 这个方法返回是真正的返回，任务都结束了 return futures; &#125; finally &#123; // 为什么要这个？就是上面说的有异常的情况 if (!done) for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125; // 带超时的 invokeAll，我们找不同吧 public &lt;T&gt; List&lt;Future&lt;T&gt;&gt; invokeAll(Collection&lt;? extends Callable&lt;T&gt;&gt; tasks, long timeout, TimeUnit unit) throws InterruptedException &#123; if (tasks == null || unit == null) throw new NullPointerException(); long nanos = unit.toNanos(timeout); List&lt;Future&lt;T&gt;&gt; futures = new ArrayList&lt;Future&lt;T&gt;&gt;(tasks.size()); boolean done = false; try &#123; for (Callable&lt;T&gt; t : tasks) futures.add(newTaskFor(t)); long lastTime = System.nanoTime(); Iterator&lt;Future&lt;T&gt;&gt; it = futures.iterator(); // 每提交一个任务，检测一次是否超时 while (it.hasNext()) &#123; execute((Runnable)(it.next())); long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; // 超时 if (nanos &lt;= 0) return futures; &#125; for (Future&lt;T&gt; f : futures) &#123; if (!f.isDone()) &#123; if (nanos &lt;= 0) return futures; try &#123; // 调用带超时的 get 方法，这里的参数 nanos 是剩余的时间， // 因为上面其实已经用掉了一些时间了 f.get(nanos, TimeUnit.NANOSECONDS); &#125; catch (CancellationException ignore) &#123; &#125; catch (ExecutionException ignore) &#123; &#125; catch (TimeoutException toe) &#123; return futures; &#125; long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; &#125; &#125; done = true; return futures; &#125; finally &#123; if (!done) for (Future&lt;T&gt; f : futures) f.cancel(true); &#125; &#125;&#125; 到这里，我们发现，这个抽象类包装了一些基本的方法，可是像 submit、invokeAny、invokeAll 等方法，它们都没有真正开启线程来执行任务，它们都只是在方法内部调用了 execute 方法，所以最重要的 execute(Runnable runnable) 方法还没出现，需要等具体执行器来实现这个最重要的部分，这里我们要说的就是 ThreadPoolExecutor 类了。 鉴于本文的篇幅，我觉得看到这里的读者应该已经不多了，大家都习惯了快餐文化。我写的每篇文章都力求让读者可以通过我的一篇文章而对相关内容有全面的了解，所以篇幅不免长了些。 ThreadPoolExecutorThreadPoolExecutor 是 JDK 中的线程池实现，这个类实现了一个线程池需要的各个方法，它实现了任务提交、线程管理、监控等等方法。 我们可以基于它来进行业务上的扩展，以实现我们需要的其他功能，比如实现定时任务的类 ScheduledThreadPoolExecutor 就继承自 ThreadPoolExecutor。当然，这不是本文关注的重点，下面，还是赶紧进行源码分析吧。 首先，我们来看看线程池实现中的几个概念和处理流程。 我们先回顾下提交任务的几个方法： 123456789101112131415161718public Future&lt;?&gt; submit(Runnable task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;Void&gt; ftask = newTaskFor(task, null); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task, result); execute(ftask); return ftask;&#125;public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); RunnableFuture&lt;T&gt; ftask = newTaskFor(task); execute(ftask); return ftask;&#125; 一个最基本的概念是，submit 方法中，参数是 Runnable 类型（也有Callable 类型），这个参数不是用于 new Thread(runnable).start() 中的，此处的这个参数不是用于启动线程的，这里指的是任务，任务要做的事情是 run() 方法里面定义的或 Callable 中的 call() 方法里面定义的。 初学者往往会搞混这个，因为 Runnable 总是在各个地方出现，经常把一个 Runnable 包到另一个 Runnable 中。请把它想象成有个 Task 接口，这个接口里面有一个 run() 方法。 我们回过神来继续往下看，我画了一个简单的示意图来描述线程池中的一些主要的构件： 当然，上图没有考虑队列是否有界，提交任务时队列满了怎么办？什么情况下会创建新的线程？提交任务时线程池满了怎么办？空闲线程怎么关掉？这些问题下面我们会一一解决。 我们经常会使用 Executors 这个工具类来快速构造一个线程池，对于初学者而言，这种工具类是很有用的，开发者不需要关注太多的细节，只要知道自己需要一个线程池，仅仅提供必需的参数就可以了，其他参数都采用作者提供的默认值。 12345678910public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 这里先不说有什么区别，它们最终都会导向这个构造方法： 1234567891011121314151617181920212223public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); // 这几个参数都是必须要有的 if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 基本上，上面的构造方法中列出了我们最需要关心的几个属性了，下面逐个介绍下构造方法中出现的这几个属性： corePoolSize 核心线程数，不要抠字眼，反正先记着有这么个属性就可以了。 maximumPoolSize 最大线程数，线程池允许创建的最大线程数。 workQueue 任务队列，BlockingQueue 接口的某个实现（常使用 ArrayBlockingQueue 和 LinkedBlockingQueue）。 keepAliveTime 空闲线程的保活时间，如果某线程的空闲时间超过这个值都没有任务给它做，那么可以被关闭了。注意这个值并不会对所有线程起作用，如果线程池中的线程数少于等于核心线程数 corePoolSize，那么这些线程不会因为空闲太长时间而被关闭，当然，也可以通过调用 allowCoreThreadTimeOut(true)使核心线程数内的线程也可以被回收。 threadFactory 用于生成线程，一般我们可以用默认的就可以了。通常，我们可以通过它将我们的线程的名字设置得比较可读一些，如 Message-Thread-1， Message-Thread-2 类似这样。 handler： 当线程池已经满了，但是又有新的任务提交的时候，该采取什么策略由这个来指定。有几种方式可供选择，像抛出异常、直接拒绝然后返回等，也可以自己实现相应的接口实现自己的逻辑，这个之后再说。 除了上面几个属性外，我们再看看其他重要的属性。 Doug Lea 采用一个 32 位的整数来存放线程池的状态和当前池中的线程数，其中高 3 位用于存放线程池状态，低 29 位表示线程数（即使只有 29 位，也已经不小了，大概 5 亿多，现在还没有哪个机器能起这么多线程的吧）。我们知道，java 语言在整数编码上是统一的，都是采用补码的形式，下面是简单的移位操作和布尔操作，都是挺简单的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));// 这里 COUNT_BITS 设置为 29(32-3)，意味着前三位用于存放线程状态，后29位用于存放线程数// 很多初学者很喜欢在自己的代码中写很多 29 这种数字，或者某个特殊的字符串，然后分布在各个地方，这是非常糟糕的private static final int COUNT_BITS = Integer.SIZE - 3;// 000 11111111111111111111111111111// 这里得到的是 29 个 1，也就是说线程池的最大线程数是 2^29-1=536870911// 以我们现在计算机的实际情况，这个数量还是够用的private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// 我们说了，线程池的状态存放在高 3 位中// 运算结果为 111跟29个0：111 00000000000000000000000000000private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;// 000 00000000000000000000000000000private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;// 001 00000000000000000000000000000private static final int STOP = 1 &lt;&lt; COUNT_BITS;// 010 00000000000000000000000000000private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;// 011 00000000000000000000000000000private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;// 将整数 c 的低 29 位修改为 0，就得到了线程池的状态private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;// 将整数 c 的高 3 为修改为 0，就得到了线程池中的线程数private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125;/* * Bit field accessors that don&apos;t require unpacking ctl. * These depend on the bit layout and on workerCount being never negative. */private static boolean runStateLessThan(int c, int s) &#123; return c &lt; s;&#125;private static boolean runStateAtLeast(int c, int s) &#123; return c &gt;= s;&#125;private static boolean isRunning(int c) &#123; return c &lt; SHUTDOWN;&#125; 上面就是对一个整数的简单的位操作，几个操作方法将会在后面的源码中一直出现，所以读者最好把方法名字和其代表的功能记住，看源码的时候也就不需要来来回回翻了。 在这里，介绍下线程池中的各个状态和状态变化的转换过程： RUNNING：这个没什么好说的，这是最正常的状态：接受新的任务，处理等待队列中的任务 SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务 STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程 TIDYING：所有的任务都销毁了，workCount 为 0。线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated() TERMINATED：terminated() 方法结束后，线程池的状态就会变成这个 RUNNING 定义为 -1，SHUTDOWN 定义为 0，其他的都比 0 大，所以等于 0 的时候不能提交任务，大于 0 的话，连正在执行的任务也需要中断。 看了这几种状态的介绍，读者大体也可以猜到十之八九的状态转换了，各个状态的转换过程有以下几种： RUNNING -&gt; SHUTDOWN：当调用了 shutdown() 后，会发生这个状态转换，这也是最重要的 (RUNNING or SHUTDOWN) -&gt; STOP：当调用 shutdownNow() 后，会发生这个状态转换，这下要清楚 shutDown() 和 shutDownNow() 的区别了 SHUTDOWN -&gt; TIDYING：当任务队列和线程池都清空后，会由 SHUTDOWN 转换为 TIDYING STOP -&gt; TIDYING：当任务队列清空后，发生这个转换 TIDYING -&gt; TERMINATED：这个前面说了，当 terminated() 方法结束后 上面的几个记住核心的就可以了，尤其第一个和第二个。 另外，我们还要看看一个内部类 Worker，因为 Doug Lea 把线程池中的线程包装成了一个个 Worker，翻译成工人，就是线程池中做任务的线程。所以到这里，我们知道任务是 Runnable（内部变量名叫 task 或 command），线程是 Worker。 Worker 这里又用到了抽象类 AbstractQueuedSynchronizer。题外话，AQS 在并发中真的是到处出现，而且非常容易使用，写少量的代码就能实现自己需要的同步方式（对 AQS 源码感兴趣的读者请参看我之前写的几篇文章）。 1234567891011121314151617181920212223242526272829303132private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; private static final long serialVersionUID = 6138294804551838833L; // 这个是真正的线程，任务靠你啦 final Thread thread; // 前面说了，这里的 Runnable 是任务。为什么叫 firstTask？因为在创建线程的时候，如果同时指定了 // 这个线程起来以后需要执行的第一个任务，那么第一个任务就是存放在这里的(线程可不止执行这一个任务) // 当然了，也可以为 null，这样线程起来了，自己到任务队列（BlockingQueue）中取任务（getTask 方法）就行了 Runnable firstTask; // 用于存放此线程完成的任务数，注意了，这里用了 volatile，保证可见性 volatile long completedTasks; // Worker 只有这一个构造方法，传入 firstTask，也可以传 null Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; // 调用 ThreadFactory 来创建一个新的线程 this.thread = getThreadFactory().newThread(this); &#125; // 这里调用了外部类的 runWorker 方法 public void run() &#123; runWorker(this); &#125; ...// 其他几个方法没什么好看的，就是用 AQS 操作，来获取这个线程的执行权，用了独占锁&#125; 前面虽然啰嗦，但是简单。有了上面的这些基础后，我们终于可以看看 ThreadPoolExecutor 的 execute 方法了，前面源码分析的时候也说了，各种方法都最终依赖于 execute 方法： 12345678910111213141516171819202122232425262728293031323334353637383940public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // 前面说的那个表示 “线程池状态” 和 “线程数” 的整数 int c = ctl.get(); // 如果当前线程数少于核心线程数，那么直接添加一个 worker 来执行任务， // 创建一个新的线程，并把当前任务 command 作为这个线程的第一个任务(firstTask) if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务成功，那么就结束了。提交任务嘛，线程池已经接受了这个任务，这个方法也就可以返回了 // 至于执行的结果，到时候会包装到 FutureTask 中。 // 返回 false 代表线程池不允许提交任务 if (addWorker(command, true)) return; c = ctl.get(); &#125; // 到这里说明，要么当前线程数大于等于核心线程数，要么刚刚 addWorker 失败了 // 如果线程池处于 RUNNING 状态，把这个任务添加到任务队列 workQueue 中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; /* 这里面说的是，如果任务进入了 workQueue，我们是否需要开启新的线程 * 因为线程数在 [0, corePoolSize) 是无条件开启新的线程 * 如果线程数已经大于等于 corePoolSize，那么将任务添加到队列中，然后进到这里 */ int recheck = ctl.get(); // 如果线程池已不处于 RUNNING 状态，那么移除已经入队的这个任务，并且执行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果线程池还是 RUNNING 的，并且线程数为 0，那么开启新的线程 // 到这里，我们知道了，这块代码的真正意图是：担心任务提交到队列中了，但是线程都关闭了 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 如果 workQueue 队列满了，那么进入到这个分支 // 以 maximumPoolSize 为界创建新的 worker， // 如果失败，说明当前线程数已经达到 maximumPoolSize，执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 对创建线程的错误理解：如果线程数少于 corePoolSize，创建一个线程，如果线程数在 [corePoolSize, maximumPoolSize] 之间那么可以创建线程或复用空闲线程，keepAliveTime 对这个区间的线程有效。 从上面的几个分支，我们就可以看出，上面的这段话是错误的。 上面这些一时半会也不可能全部消化搞定，我们先继续往下吧，到时候再回头看几遍。 这个方法非常重要 addWorker(Runnable firstTask, boolean core) 方法，我们看看它是怎么创建新的线程的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105// 第一个参数是准备提交给这个线程执行的任务，之前说了，可以为 null// 第二个参数为 true 代表使用核心线程数 corePoolSize 作为创建线程的界限，也就说创建这个线程的时候，// 如果线程池中的线程总数已经达到 corePoolSize，那么不能响应这次创建线程的请求// 如果是 false，代表使用最大线程数 maximumPoolSize 作为界限private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 这个非常不好理解 // 如果线程池已关闭，并满足以下条件之一，那么不创建新的 worker： // 1\. 线程池状态大于 SHUTDOWN，其实也就是 STOP, TIDYING, 或 TERMINATED // 2\. firstTask != null // 3\. workQueue.isEmpty() // 简单分析下： // 还是状态控制的问题，当线程池处于 SHUTDOWN 的时候，不允许提交任务，但是已有的任务继续执行 // 当状态大于 SHUTDOWN 时，不允许提交任务，且中断正在执行的任务 // 多说一句：如果线程池处于 SHUTDOWN，但是 firstTask 为 null，且 workQueue 非空，那么是允许创建 worker 的 // 这是因为 SHUTDOWN 的语义：不允许提交新的任务，但是要把已经进入到 workQueue 的任务执行完，所以在满足条件的基础上，是允许创建新的 Worker 的 if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 如果成功，那么就是所有创建线程前的条件校验都满足了，准备创建线程执行任务了 // 这里失败的话，说明有其他线程也在尝试往线程池中创建线程 if (compareAndIncrementWorkerCount(c)) break retry; // 由于有并发，重新再读取一下 ctl c = ctl.get(); // 正常如果是 CAS 失败的话，进到下一个里层的for循环就可以了 // 可是如果是因为其他线程的操作，导致线程池的状态发生了变更，如有其他线程关闭了这个线程池 // 那么需要回到外层的for循环 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; /* * 到这里，我们认为在当前这个时刻，可以开始创建线程来执行任务了， * 因为该校验的都校验了，至于以后会发生什么，那是以后的事，至少当前是满足条件的 */ // worker 是否已经启动 boolean workerStarted = false; // 是否已将这个 worker 添加到 workers 这个 HashSet 中 boolean workerAdded = false; Worker w = null; try &#123; final ReentrantLock mainLock = this.mainLock; // 把 firstTask 传给 worker 的构造方法 w = new Worker(firstTask); // 取 worker 中的线程对象，之前说了，Worker的构造方法会调用 ThreadFactory 来创建一个新的线程 final Thread t = w.thread; if (t != null) &#123; // 这个是整个线程池的全局锁，持有这个锁才能让下面的操作“顺理成章”， // 因为关闭一个线程池需要这个锁，至少我持有锁的期间，线程池不会被关闭 mainLock.lock(); try &#123; int c = ctl.get(); int rs = runStateOf(c); // 小于 SHUTTDOWN 那就是 RUNNING，这个自不必说，是最正常的情况 // 如果等于 SHUTDOWN，前面说了，不接受新的任务，但是会继续执行等待队列中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; // worker 里面的 thread 可不能是已经启动的 if (t.isAlive()) throw new IllegalThreadStateException(); // 加到 workers 这个 HashSet 中 workers.add(w); int s = workers.size(); // largestPoolSize 用于记录 workers 中的个数的最大值 // 因为 workers 是不断增加减少的，通过这个值可以知道线程池的大小曾经达到的最大值 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // 添加成功的话，启动这个线程 if (workerAdded) &#123; // 启动线程 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; // 如果线程没有启动，需要做一些清理工作，如前面 workCount 加了 1，将其减掉 if (! workerStarted) addWorkerFailed(w); &#125; // 返回线程是否启动成功 return workerStarted;&#125; 简单看下 addWorkFailed 的处理： 123456789101112131415// workers 中删除掉相应的 worker// workCount 减 1private void addWorkerFailed(Worker w) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; if (w != null) workers.remove(w); decrementWorkerCount(); // rechecks for termination, in case the existence of this worker was holding up termination tryTerminate(); &#125; finally &#123; mainLock.unlock(); &#125;&#125; 回过头来，继续往下走。我们知道，worker 中的线程 start 后，其 run 方法会调用 runWorker 方法： 1234// Worker 类的 run() 方法public void run() &#123; runWorker(this);&#125; 继续往下看 runWorker 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 此方法由 worker 线程启动后调用，这里用一个 while 循环来不断地从等待队列中获取任务并执行// 前面说了，worker 在初始化的时候，可以指定 firstTask，那么第一个任务也就可以不需要从队列中获取final void runWorker(Worker w) &#123; // Thread wt = Thread.currentThread(); // 该线程的第一个任务(如果有的话) Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try &#123; // 循环调用 getTask 获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // 如果线程池状态大于等于 STOP，那么意味着该线程也要中断 if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; // 这是一个钩子方法，留给需要的子类实现 beforeExecute(wt, task); Throwable thrown = null; try &#123; // 到这里终于可以执行任务了 task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; // 这里不允许抛出 Throwable，所以转换为 Error thrown = x; throw new Error(x); &#125; finally &#123; // 也是一个钩子方法，将 task 和异常作为参数，留给需要的子类实现 afterExecute(task, thrown); &#125; &#125; finally &#123; // 置空 task，准备 getTask 获取下一个任务 task = null; // 累加完成的任务数 w.completedTasks++; // 释放掉 worker 的独占锁 w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; // 如果到这里，需要执行线程关闭： // 1\. 说明 getTask 返回 null，也就是说，队列中已经没有任务需要执行了，执行关闭 // 2\. 任务执行过程中发生了异常 // 第一种情况，已经在代码处理了将 workCount 减 1，这个在 getTask 方法分析中会说 // 第二种情况，workCount 没有进行处理，所以需要在 processWorkerExit 中处理 // 限于篇幅，我不准备分析这个方法了，感兴趣的读者请自行分析源码 processWorkerExit(w, completedAbruptly); &#125;&#125; 我们看看 getTask() 是怎么获取任务的，这个方法写得真的很好，每一行都很简单，组合起来却所有的情况都想好了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// 此方法有三种可能：// 1\. 阻塞直到获取到任务返回。我们知道，默认 corePoolSize 之内的线程是不会被回收的，// 它们会一直等待任务// 2\. 超时退出。keepAliveTime 起作用的时候，也就是如果这么多时间内都没有任务，那么应该执行关闭// 3\. 如果发生了以下条件，此方法必须返回 null:// - 池中有大于 maximumPoolSize 个 workers 存在(通过调用 setMaximumPoolSize 进行设置)// - 线程池处于 SHUTDOWN，而且 workQueue 是空的，前面说了，这种不再接受新的任务// - 线程池处于 STOP，不仅不接受新的线程，连 workQueue 中的线程也不再执行private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 两种可能 // 1\. rs == SHUTDOWN &amp;&amp; workQueue.isEmpty() // 2\. rs &gt;= STOP if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; // CAS 操作，减少工作线程数 decrementWorkerCount(); return null; &#125; boolean timed; // Are workers subject to culling? for (;;) &#123; int wc = workerCountOf(c); // 允许核心线程数内的线程回收，或当前线程数超过了核心线程数，那么有可能发生超时关闭 timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; // 这里 break，是为了不往下执行后一个 if (compareAndDecrementWorkerCount(c)) // 两个 if 一起看：如果当前线程数 wc &gt; maximumPoolSize，或者超时，都返回 null // 那这里的问题来了，wc &gt; maximumPoolSize 的情况，为什么要返回 null？ // 换句话说，返回 null 意味着关闭线程。 // 那是因为有可能开发者调用了 setMaximumPoolSize() 将线程池的 maximumPoolSize 调小了，那么多余的 Worker 就需要被关闭 if (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) break; if (compareAndDecrementWorkerCount(c)) return null; c = ctl.get(); // Re-read ctl // compareAndDecrementWorkerCount(c) 失败，线程池中的线程数发生了改变 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; // wc &lt;= maximumPoolSize 同时没有超时 try &#123; // 到 workQueue 中获取任务 Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果此 worker 发生了中断，采取的方案是重试 // 解释下为什么会发生中断，这个读者要去看 setMaximumPoolSize 方法。 // 如果开发者将 maximumPoolSize 调小了，导致其小于当前的 workers 数量， // 那么意味着超出的部分线程要被关闭。重新进入 for 循环，自然会有部分线程会返回 null timedOut = false; &#125; &#125;&#125; 到这里，基本上也说完了整个流程，读者这个时候应该回到 execute(Runnable command) 方法，看看各个分支，我把代码贴过来一下： 12345678910111213141516171819202122232425262728293031323334353637383940public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); // 前面说的那个表示 “线程池状态” 和 “线程数” 的整数 int c = ctl.get(); // 如果当前线程数少于核心线程数，那么直接添加一个 worker 来执行任务， // 创建一个新的线程，并把当前任务 command 作为这个线程的第一个任务(firstTask) if (workerCountOf(c) &lt; corePoolSize) &#123; // 添加任务成功，那么就结束了。提交任务嘛，线程池已经接受了这个任务，这个方法也就可以返回了 // 至于执行的结果，到时候会包装到 FutureTask 中。 // 返回 false 代表线程池不允许提交任务 if (addWorker(command, true)) return; c = ctl.get(); &#125; // 到这里说明，要么当前线程数大于等于核心线程数，要么刚刚 addWorker 失败了 // 如果线程池处于 RUNNING 状态，把这个任务添加到任务队列 workQueue 中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; /* 这里面说的是，如果任务进入了 workQueue，我们是否需要开启新的线程 * 因为线程数在 [0, corePoolSize) 是无条件开启新的线程 * 如果线程数已经大于等于 corePoolSize，那么将任务添加到队列中，然后进到这里 */ int recheck = ctl.get(); // 如果线程池已不处于 RUNNING 状态，那么移除已经入队的这个任务，并且执行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); // 如果线程池还是 RUNNING 的，并且线程数为 0，那么开启新的线程 // 到这里，我们知道了，这块代码的真正意图是：担心任务提交到队列中了，但是线程都关闭了 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 如果 workQueue 队列满了，那么进入到这个分支 // 以 maximumPoolSize 为界创建新的 worker， // 如果失败，说明当前线程数已经达到 maximumPoolSize，执行拒绝策略 else if (!addWorker(command, false)) reject(command);&#125; 上面各个分支中，有两种情况会调用 reject(command) 来处理任务，因为按照正常的流程，线程池此时不能接受这个任务，所以需要执行我们的拒绝策略。接下来，我们说一说 ThreadPoolExecutor 中的拒绝策略。 1234final void reject(Runnable command) &#123; // 执行拒绝策略 handler.rejectedExecution(command, this);&#125; 此处的 handler 我们需要在构造线程池的时候就传入这个参数，它是 RejectedExecutionHandler 的实例。 RejectedExecutionHandler 在 ThreadPoolExecutor 中有四个已经定义好的实现类可供我们直接使用，当然，我们也可以实现自己的策略，不过一般也没有必要。 123456789101112131415161718192021222324252627282930313233343536373839// 只要线程池没有被关闭，那么由提交任务的线程自己来执行这个任务。public static class CallerRunsPolicy implements RejectedExecutionHandler &#123; public CallerRunsPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125;&#125;// 不管怎样，直接抛出 RejectedExecutionException 异常// 这个是默认的策略，如果我们构造线程池的时候不传相应的 handler 的话，那就会指定使用这个public static class AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); &#125;&#125;// 不做任何处理，直接忽略掉这个任务public static class DiscardPolicy implements RejectedExecutionHandler &#123; public DiscardPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125;&#125;// 这个相对霸道一点，如果线程池没有被关闭的话，// 把队列队头的任务(也就是等待了最长时间的)直接扔掉，然后提交这个任务到等待队列中public static class DiscardOldestPolicy implements RejectedExecutionHandler &#123; public DiscardOldestPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; e.getQueue().poll(); e.execute(r); &#125; &#125;&#125; 到这里，ThreadPoolExecutor 的源码算是分析结束了。单纯从源码的难易程度来说，ThreadPoolExecutor 的源码还算是比较简单的，只是需要我们静下心来好好看看罢了。 Executors这节其实也不是分析 Executors 这个类，因为它仅仅是工具类，它的所有方法都是 static 的。 生成一个固定大小的线程池： 12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 最大线程数设置为与核心线程数相等，此时 keepAliveTime 设置为 0（因为这里它是没用的，即使不为 0，线程池默认也不会回收 corePoolSize 内的线程），任务队列采用 LinkedBlockingQueue，无界队列。 过程分析：刚开始，每提交一个任务都创建一个 worker，当 worker 的数量达到 nThreads 后，不再创建新的线程，而是把任务提交到 LinkedBlockingQueue 中，而且之后线程数始终为 nThreads。 生成只有一个线程的固定线程池，这个更简单，和上面的一样，只要设置线程数为 1 就可以了： 123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 生成一个需要的时候就创建新的线程，同时可以复用之前创建的线程（如果这个线程当前没有任务）的线程池： 12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 核心线程数为 0，最大线程数为 Integer.MAX_VALUE，keepAliveTime 为 60 秒，任务队列采用 SynchronousQueue。 这种线程池对于任务可以比较快速地完成的情况有比较好的性能。如果线程空闲了 60 秒都没有任务，那么将关闭此线程并从线程池中移除。所以如果线程池空闲了很长时间也不会有问题，因为随着所有的线程都会被关闭，整个线程池不会占用任何的系统资源。 过程分析：我把 execute 方法的主体黏贴过来，让大家看得明白些。鉴于 corePoolSize 是 0，那么提交任务的时候，直接将任务提交到队列中，由于采用了 SynchronousQueue，所以如果是第一个任务提交的时候，offer 方法肯定会返回 false，因为此时没有任何 worker 对这个任务进行接收，那么将进入到最后一个分支来创建第一个 worker。之后再提交任务的话，取决于是否有空闲下来的线程对任务进行接收，如果有，会进入到第二个 if 语句块中，否则就是和第一个任务一样，进到最后的 else if 分支创建新线程。 1234567891011121314151617int c = ctl.get();// corePoolSize 为 0，所以不会进到这个 if 分支if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get();&#125;// offer 如果有空闲线程刚好可以接收此任务，那么返回 true，否则返回 falseif (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false);&#125;else if (!addWorker(command, false)) reject(command); SynchronousQueue 是一个比较特殊的 BlockingQueue，其本身不储存任何元素，它有一个虚拟队列（或虚拟栈），不管读操作还是写操作，如果当前队列中存储的是与当前操作相同模式的线程，那么当前操作也进入队列中等待；如果是相反模式，则配对成功，从当前队列中取队头节点。具体的信息，可以看我的另一篇关于 BlockingQueue 的文章。 总结我一向不喜欢写总结，因为我把所有需要表达的都写在正文中了，写小篇幅的总结并不能真正将话说清楚，本文的总结部分为准备面试的读者而写，希望能帮到面试者或者没有足够的时间看完全文的读者。 java 线程池有哪些关键属性？ corePoolSize，maximumPoolSize，workQueue，keepAliveTime，rejectedExecutionHandler corePoolSize 到 maximumPoolSize 之间的线程会被回收，当然 corePoolSize 的线程也可以通过设置而得到回收（allowCoreThreadTimeOut(true)）。 workQueue 用于存放任务，添加任务的时候，如果当前线程数超过了 corePoolSize，那么往该队列中插入任务，线程池中的线程会负责到队列中拉取任务。 keepAliveTime 用于设置空闲时间，如果线程数超出了 corePoolSize，并且有些线程的空闲时间超过了这个值，会执行关闭这些线程的操作 rejectedExecutionHandler 用于处理当线程池不能执行此任务时的情况，默认有抛出 RejectedExecutionException 异常、忽略任务、使用提交任务的线程来执行此任务和将队列中等待最久的任务删除，然后提交此任务这四种策略，默认为抛出异常。 说说线程池中的线程创建时机？ 如果当前线程数少于 corePoolSize，那么提交任务的时候创建一个新的线程，并由这个线程执行这个任务； 如果当前线程数已经达到 corePoolSize，那么将提交的任务添加到队列中，等待线程池中的线程去队列中取任务； 如果队列已满，那么创建新的线程来执行任务，需要保证池中的线程数不会超过 maximumPoolSize，如果此时线程数超过了 maximumPoolSize，那么执行拒绝策略。 注意：如果将队列设置为无界队列，那么线程数达到 corePoolSize 后，其实线程数就不会再增长了。因为后面的任务直接往队列塞就行了，此时 maximumPoolSize 参数就没有什么意义。 Executors.newFixedThreadPool(…) 和 Executors.newCachedThreadPool() 构造出来的线程池有什么差别？ 细说太长，往上滑一点点，在 Executors 的小节进行了详尽的描述。 任务执行过程中发生异常怎么处理？ 如果某个任务执行出现异常，那么执行任务的线程会被关闭，而不是继续接收其他任务。然后会启动一个新的线程来代替它。 什么时候会执行拒绝策略？ workers 的数量达到了 corePoolSize（任务此时需要进入任务队列），任务入队成功，与此同时线程池被关闭了，而且关闭线程池并没有将这个任务出队，那么执行拒绝策略。这里说的是非常边界的问题，入队和关闭线程池并发执行，读者仔细看看 execute 方法是怎么进到第一个 reject(command) 里面的。 workers 的数量大于等于 corePoolSize，将任务加入到任务队列，可是队列满了，任务入队失败，那么准备开启新的线程，可是线程数已经达到 maximumPoolSize，那么执行拒绝策略。 因为本文实在太长了，所以我没有说执行结果是怎么获取的，也没有说关闭线程池相关的部分，这个就留给读者吧。 本文篇幅是有点长，如果读者发现什么不对的地方，或者有需要补充的地方，请不吝提出，谢谢。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[你不可错过的 Java 学习资源清单]]></title>
    <url>%2F2019%2F10%2F12%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2F%E4%BD%A0%E4%B8%8D%E5%8F%AF%E9%94%99%E8%BF%87%E7%9A%84Java%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 学习Java和其他技术的资源其实非常多，但是我们需要取其精华去其糟粕，选择那些最好的，最适合我们的，同时也要由浅入深，先易后难。基于这样的一个标准，我在这里为大家提供一份Java的学习资源清单。 Java入门学习资源这里主要推荐的是几个技术学习网站，基本上都是视频学习资源。 1 慕课网慕课网是做得比较好的程序员学习网站了。里面主要提供的是视频学习资源，主要适用于入门，当然其中也有一些进阶的内容，不过一般都是收费的。 2 极客学院极客学院是我最早用于视频学习的网站，当时主要是听室友推荐，看了一些之后发现确实还不错。不过比起慕课网，极客学院的内容可能少一点。 3 w3cSchool这个想必不说大家也知道，最适合入门的学习网站之一，有很多的学习资源，但是也只适合入门，你可以在一天内看完一门语言或技术，大概了解怎么使用。 4 中国MOOC以前我以为这个和慕课网一样，后来发现这个网站主要是做教育资源的，更像是在学校里上课，里面的很多资源都是高校老师提供的，所以想学习一些计算机基础理论知识可以看看这个网站。 5 网易云课堂&amp;腾讯课堂这两个网站大家也都知道，只不过他们不是专门做IT学习资源的，相对来说这方面的学习资源也会比较少一点。 Java后端技术专栏对于校园招聘来说，最重要的还是基础知识。下面的博客专栏出自我的技术博客： https://blog.csdn.net/a724888 这些专栏中有一些文章是我自己原创的，也有一些文章是转载自技术大牛的，基本都是是我在学习Java后端的两年时间内陆续完成的。 总的来说算是比较全面了，做后端方向的同学可以参考一下。 深入浅出Java核心技术 https://blog.csdn.net/column/details/21930.html 本专栏主要介绍Java基础，并且会结合实现原理以及具体实例来讲解。同时还介绍了Java集合类，设计模式以及Java8的相关知识。 深入理解JVM虚拟机 https://blog.csdn.net/column/details/21960.html 带你走进JVM的世界，整合高质量文章以阐述虚拟机的原理及相关技术，让开发者更好地了解Java的底层运行原理以及相应的调优方法。 Java并发指南 https://blog.csdn.net/column/details/21961.html 本专栏主要介绍Java并发编程相关的基本原理以及进阶知识。主要包括Java多线程基础，Java并发编程基本原理以及JUC并发包的使用和源码解析。 Java网络编程与NIO https://blog.csdn.net/column/details/21963.html Java网络编程一直是很重要的一部分内容，其中涉及了socket的使用，以及Java网络编程的IO模型，譬如BIO,NIO,AIO，当然也包括Linux的网络编程模型。 了解这部分知识对于理解网络编程有很多帮助。另外还补充了两个涉及NIO的重要技术：Tomcat和Netty。 JavaWeb技术世界 https://blog.csdn.net/column/details/21850.html 从这里开始打开去往JavaWeb世界的大门。什么是J2EE，什么是JavaWeb，以及这个生态中常用的一些技术：Maven，Spring，Tomcat，Junit，log4j等等。 我们不仅要了解怎么使用它们，更要去了解它们为什么出现，其中一些技术的实现原理是什么。 Spring与SpringMVC源码解析 https://blog.csdn.net/column/details/21851.html 本专栏主要讲解Spring和SpringMVC的实现原理。 Spring是最流行的Java框架之一。 本专栏文章主要包括IOC的实现原理分析，AOP的实现原理分析，事务的实现源码分析等，当然也有SpringMVC的源码解析文章。 重新学习MySQL与Redis https://blog.csdn.net/column/details/21877.html 本专栏介绍MySQL的基本知识，比如基本架构，存储引擎，索引原理，主从复制，事务等内容。当然也会讲解一些和sql语句优化有关的知识。 同时本专栏里也介绍了Redis的基本实现原理，包括数据结构，主从复制，集群方案，分布式锁等实现。 分布式系统理论与实践 https://blog.csdn.net/column/details/24090.html 本专栏介绍分布式的基本理论和相关技术，比如CAP和BASE理论，一致性算法，以及ZooKeeper这类的分布式协调服务。 在分布式实践方面，我们会讲到负载均衡，缓存，分布式事务，分布式锁，以及Dubbo这样的微服务，也包括消息队列，数据库中间件等等。 后端技术杂谈 https://blog.csdn.net/column/details/25481.html 本专栏涵盖了大后端的众多技术文章，当你在Java后端方面有一定基础以后，再多了解一些相关技术总是有好处的。 除了Java后端的文章以外，还会涉及Hadoop生态，云计算技术，搜索引擎，甚至包括一些数据挖掘和AI的文章。 总的来说选取了一些不错的基础类文章，能让你对大后端有一个更直观的认识。 Java工程师书单我之前专门写了一篇文章介绍了Java工程师的书单，可以这里重点列举一些好书，推荐给大家。 完整内容可以参考这篇文章： Java工程师必备书单 《计算机网络：自顶向下》这本从应用层讲到物理层，感觉这种方式学起来更轻松。 《图解算法》《啊哈算法》 这两部书籍非常适合学习算法的入门，前者主要用图解的形式覆盖了大部分常用算法，包括dp，贪心等等，可以作为入门书，后者则把很多常用算法都进行了实现，包括搜索，图，树等一些比较高级的常用算法。 《剑指offer》这本书还是要强烈推荐的，毕竟是面试题经常参考的书籍，当然最好有前面基本的铺垫再看，可能收获更大，这本书在面试之前一般都要嚼烂。如果想看Java版本的代码，可以到我的Github仓库中查看。 《Java编程思想》这本书也是被誉为Java神书的存在了，但是对新手不友好，适合有些基础再看，当然要选择性地看。我当时大概只看了1/3 《Java核心技术卷一》 这本书还是比较适合入门的，当然，这种厚皮书要看完还是很有难度的，不过比起上面那本要简单一些 《深入理解JVM虚拟机》 这本书是Java开发者必须看的书，很多jvm的文章都是提取这本书的内容。JVM是Java虚拟机，赋予了Java程序生命，所以好好看看把，我自己就已经看了三遍了。 《Java并发编程艺术》 这本书是国内作者写的Java并发书籍，比上面那一本更简单易懂，适合作为并发编程的入门书籍，当然，学习并发原理之前，还是先把Java的多线程搞懂吧。 《深入JavaWeb技术内幕》 这本书是Java Web的集大成之作，涵盖了大部分Java Web开发的知识点，不过一本书显然无法把所有细节都讲完，但是作为Java Web的入门或者进阶书籍来看的话还是很不错的。 《Redis设计与实现》 该书全面而完整地讲解了 Redis 的内部运行机制,对 Redis 的大多数单机功能以及所有多机功能的实现原理进行了介绍。这本书把Redis的基本原理讲的一清二楚，包括数据结构，持久化，集群等内容，有空应该看看。 《大型网站技术架构》 这本淘宝系技术指南还是非常值得推崇的，可以说是把大型网站的现代架构进行了一次简单的总结，内容涵盖了各方面，主要讲的是概念，很适合没接触过架构的同学入门。看完以后你会觉得后端技术原来这么博大精深。 《分布式服务框架原理与实践》 上面那本书讲的是分布式架构的实践，而这本书更专注于分布式服务的原理讲解和对应实践，很好地讲述了分布式服务的基本概念，相关技术，以及解决方案等，对于想要学习分布式服务框架的同学来说是本好书。 《从Paxos到Zookeeper分布式一致性原理与实践》 说起分布式系统，我们需要了解它的原理，相关理论及技术，这本书也是从这个角度出发，讲解了分布式系统的一些常用概念，并且带出了分布式一哥zookeeper，可以说是想学分布式技术的同学必看的书籍。 《大数据技术原理与应用》 作为大数据方面的一本教材，厦大教授写的这本书还是非常赞的，从最基础的原理方面讲解了Hadoop的生态系统，并且把每个组件的原理都讲得比较清楚，另外也加入了spark，storm等内容，可以说是大数据入门非常好的一本书了。 技术社区推荐学习Java后端两年的时间里，接触过很多的资料，网站和课程，也走了不少弯路，所以这里也总结一些比较好的资源推荐给大家。 0 CSDN和博客园主流的技术交流平台，虽然广告越打越多了，但是还是有很多不错的博文的。 1 importnew 专注Java学习资源分享，适合Java初学者。 2 并发编程网主要分享Java相关进阶内容，适合Java提高。 3 推酷 一个不错的技术分享社区。 4 segmentfault有点像国内的Stack Overflow，适合交流代码问题的地方。 5 掘金一个很有极客范的技术社区，强推，有很多技术大牛分享优质文章。 6 开发者头条一个整合优质技术博客的社区，里面基本上都是精选的高质量博文，适合技术学习提升。 7 v2ex一个极客社区，除了交流技术以外还会有很多和程序员生活相关的话题分享。 8 知乎这个就不必多说了。我在知乎上也有Java技术和校招的专栏，有兴趣的同学可以看看： https://www.zhihu.com/people/h2pl 9 简书简书上有些技术文章也很不错，有空大家也可以去看看。 10 Github 有一些GitHub的项目还是非常不错的，其中也有仓库会分享技术文章。 我的GitHub：https://github.com/h2pl 技术大牛推荐1 江南白衣这位大大绝对是我的Java启蒙导师，他推荐的Java后端书架让我受益匪浅。 2 码农翻身刘欣，一位工作15年的IBM架构师，用最浅显易懂的文章讲解技术的那些事，力荐，他的文章帮我解决了很多困惑。 3 CoolShell陈皓老师的博客相信大家都看过，干货很多，酷壳应该算是国内最有影响力的个人博客了。 4 廖雪峰学习Git和Python，看它的博客就够了。 5 HollisChuang阿里一位研发大佬的博客，主要分享Java技术文章，内容还不错。 6 梁桂钊阿里另一位研发大佬，博客里的后端技术文章非常丰富。 7 chenssy这位大佬分享的Java技术文章也很多，并且有很多基础方面的文章，新手可以多看看。 8 Java Doop一位魔都Java开发者的技术博客，里面有一些不错的讲解源码的文章，数量不是很多，但是质量都挺不错的。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南11：解读 Java 阻塞队列 BlockingQueue]]></title>
    <url>%2F2019%2F10%2F11%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9711%EF%BC%9A%E8%A7%A3%E8%AF%BB%20Java%20%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%20BlockingQueue%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/ 本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言最近得空，想写篇文章好好说说 java 线程池问题，我相信很多人都一知半解的，包括我自己在仔仔细细看源码之前，也有许多的不解，甚至有些地方我一直都没有理解到位。 说到线程池实现，那么就不得不涉及到各种 BlockingQueue 的实现，那么我想就 BlockingQueue 的问题和大家分享分享我了解的一些知识。 本文没有像之前分析 AQS 那样一行一行源码分析了，不过还是把其中最重要和最难理解的代码说了一遍，所以不免篇幅略长。本文涉及到比较多的 Doug Lea 对 BlockingQueue 的设计思想，希望有心的读者真的可以有一些收获，我觉得自己还是写了一些干货的。 本文直接参考 Doug Lea 写的 Java doc 和注释，这也是我们在学习 java 并发包时最好的材料了。希望大家能有所思、有所悟，学习 Doug Lea 的代码风格，并将其优雅、严谨的作风应用到我们写的每一行代码中。 目录： BlockingQueue 开篇先介绍下 BlockingQueue 这个接口的规则，后面再看其实现。 首先，最基本的来说， BlockingQueue 是一个先进先出的队列（Queue），为什么说是阻塞（Blocking）的呢？是因为 BlockingQueue 支持当获取队列元素但是队列为空时，会阻塞等待队列中有元素再返回；也支持添加元素时，如果队列已满，那么等到队列可以放入新元素时再放入。 BlockingQueue 是一个接口，继承自 Queue，所以其实现类也可以作为 Queue 的实现来使用，而 Queue 又继承自 Collection 接口。 BlockingQueue 对插入操作、移除操作、获取元素操作提供了四种不同的方法用于不同的场景中使用：1、抛出异常；2、返回特殊值（null 或 true/false，取决于具体的操作）；3、阻塞等待此操作，直到这个操作成功；4、阻塞等待此操作，直到成功或者超时指定时间。总结如下： Throws exception Special value Blocks Times out Insert add(e) offer(e) put(e) offer(e, time, unit) Remove remove() poll() take() poll(time, unit) Examine element() peek() not applicable not applicable BlockingQueue 的各个实现都遵循了这些规则，当然我们也不用死记这个表格，知道有这么回事，然后写代码的时候根据自己的需要去看方法的注释来选取合适的方法即可。 对于 BlockingQueue，我们的关注点应该在 put(e) 和 take() 这两个方法，因为这两个方法是带阻塞的。 BlockingQueue 不接受 null 值的插入，相应的方法在碰到 null 的插入时会抛出 NullPointerException 异常。null 值在这里通常用于作为特殊值返回（表格中的第三列），代表 poll 失败。所以，如果允许插入 null 值的话，那获取的时候，就不能很好地用 null 来判断到底是代表失败，还是获取的值就是 null 值。 一个 BlockingQueue 可能是有界的，如果在插入的时候，发现队列满了，那么 put 操作将会阻塞。通常，在这里我们说的无界队列也不是说真正的无界，而是它的容量是 Integer.MAX_VALUE（21亿多）。 BlockingQueue 是设计用来实现生产者-消费者队列的，当然，你也可以将它当做普通的 Collection 来用，前面说了，它实现了 java.util.Collection 接口。例如，我们可以用 remove(x) 来删除任意一个元素，但是，这类操作通常并不高效，所以尽量只在少数的场合使用，比如一条消息已经入队，但是需要做取消操作的时候。 BlockingQueue 的实现都是线程安全的，但是批量的集合操作如 addAll, containsAll, retainAll 和 removeAll 不一定是原子操作。如 addAll(c) 有可能在添加了一些元素后中途抛出异常，此时 BlockingQueue 中已经添加了部分元素，这个是允许的，取决于具体的实现。 BlockingQueue 不支持 close 或 shutdown 等关闭操作，因为开发者可能希望不会有新的元素添加进去，此特性取决于具体的实现，不做强制约束。 最后，BlockingQueue 在生产者-消费者的场景中，是支持多消费者和多生产者的，说的其实就是线程安全问题。 相信上面说的每一句都很清楚了，BlockingQueue 是一个比较简单的线程安全容器，下面我会分析其具体的在 JDK 中的实现，这里又到了 Doug Lea 表演时间了。 BlockingQueue 实现之 ArrayBlockingQueueArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。 其并发控制采用可重入锁来控制，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。 如果读者看过我之前写的《一行一行源码分析清楚 AbstractQueuedSynchronizer（二）》 的关于 Condition 的文章的话，那么你一定能很容易看懂 ArrayBlockingQueue 的源码，它采用一个 ReentrantLock 和相应的两个 Condition 来实现。 ArrayBlockingQueue 共有以下几个属性： 12345678910111213// 用于存放元素的数组final Object[] items;// 下一次读取操作的位置int takeIndex;// 下一次写入操作的位置int putIndex;// 队列中的元素数量int count;// 以下几个就是控制并发用的同步器final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; 我们用个示意图来描述其同步机制： ArrayBlockingQueue 实现并发同步的原理就是，读操作和写操作都需要获取到 AQS 独占锁才能进行操作。如果队列为空，这个时候读操作的线程进入到读线程队列排队，等待写线程写入新的元素，然后唤醒读线程队列的第一个等待线程。如果队列已满，这个时候写操作的线程进入到写线程队列排队，等待读线程将队列元素移除腾出空间，然后唤醒写线程队列的第一个等待线程。 对于 ArrayBlockingQueue，我们可以在构造的时候指定以下三个参数： 队列容量，其限制了队列中最多允许的元素个数； 指定独占锁是公平锁还是非公平锁。非公平锁的吞吐量比较高，公平锁可以保证每次都是等待最久的线程获取到锁； 可以指定用一个集合来初始化，将此集合中的元素在构造方法期间就先添加到队列中。 更具体的源码我就不进行分析了，因为它就是 AbstractQueuedSynchronizer 中 Condition 的使用，感兴趣的读者请看我写的《一行一行源码分析清楚 AbstractQueuedSynchronizer（二）》，因为只要看懂了那篇文章，ArrayBlockingQueue 的代码就没有分析的必要了，当然，如果你完全不懂 Condition，那么基本上也就可以说看不懂 ArrayBlockingQueue 的源码了。 BlockingQueue 实现之 LinkedBlockingQueue底层基于单向链表实现的阻塞队列，可以当做无界队列也可以当做有界队列来使用。看构造方法： 1234// 传说中的无界队列public LinkedBlockingQueue() &#123; this(Integer.MAX_VALUE);&#125; 123456// 传说中的有界队列public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 我们看看这个类有哪些属性： 1234567891011121314151617181920212223// 队列容量private final int capacity;// 队列中的元素数量private final AtomicInteger count = new AtomicInteger(0);// 队头private transient Node&lt;E&gt; head;// 队尾private transient Node&lt;E&gt; last;// take, poll, peek 等读操作的方法需要获取到这个锁private final ReentrantLock takeLock = new ReentrantLock();// 如果读操作的时候队列是空的，那么等待 notEmpty 条件private final Condition notEmpty = takeLock.newCondition();// put, offer 等写操作的方法需要获取到这个锁private final ReentrantLock putLock = new ReentrantLock();// 如果写操作的时候队列是满的，那么等待 notFull 条件private final Condition notFull = putLock.newCondition(); 这里用了两个锁，两个 Condition，简单介绍如下： takeLock 和 notEmpty 怎么搭配：如果要获取（take）一个元素，需要获取 takeLock 锁，但是获取了锁还不够，如果队列此时为空，还需要队列不为空（notEmpty）这个条件（Condition）。 putLock 需要和 notFull 搭配：如果要插入（put）一个元素，需要获取 putLock 锁，但是获取了锁还不够，如果队列此时已满，还需要队列不是满的（notFull）这个条件（Condition）。 首先，这里用一个示意图来看看 LinkedBlockingQueue 的并发读写控制，然后再开始分析源码： 看懂这个示意图，源码也就简单了，读操作是排好队的，写操作也是排好队的，唯一的并发问题在于一个写操作和一个读操作同时进行，只要控制好这个就可以了。 先上构造方法： 12345public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 注意，这里会初始化一个空的头结点，那么第一个元素入队的时候，队列中就会有两个元素。读取元素时，也总是获取头节点后面的一个节点。count 的计数值不包括这个头节点。 我们来看下 put 方法是怎么将元素插入到队尾的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public void put(E e) throws InterruptedException &#123; if (e == null) throw new NullPointerException(); // 如果你纠结这里为什么是 -1，可以看看 offer 方法。这就是个标识成功、失败的标志而已。 int c = -1; Node&lt;E&gt; node = new Node(e); final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; // 必须要获取到 putLock 才可以进行插入操作 putLock.lockInterruptibly(); try &#123; // 如果队列满，等待 notFull 的条件满足。 while (count.get() == capacity) &#123; notFull.await(); &#125; // 入队 enqueue(node); // count 原子加 1，c 还是加 1 前的值 c = count.getAndIncrement(); // 如果这个元素入队后，还有至少一个槽可以使用，调用 notFull.signal() 唤醒等待线程。 // 哪些线程会等待在 notFull 这个 Condition 上呢？ if (c + 1 &lt; capacity) notFull.signal(); &#125; finally &#123; // 入队后，释放掉 putLock putLock.unlock(); &#125; // 如果 c == 0，那么代表队列在这个元素入队前是空的（不包括head空节点）， // 那么所有的读线程都在等待 notEmpty 这个条件，等待唤醒，这里做一次唤醒操作 if (c == 0) signalNotEmpty();&#125;// 入队的代码非常简单，就是将 last 属性指向这个新元素，并且让原队尾的 next 指向这个元素// 这里入队没有并发问题，因为只有获取到 putLock 独占锁以后，才可以进行此操作private void enqueue(Node&lt;E&gt; node) &#123; // assert putLock.isHeldByCurrentThread(); // assert last.next == null; last = last.next = node;&#125;// 元素入队后，如果需要，调用这个方法唤醒读线程来读private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125;&#125; 我们再看看 take 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; // 首先，需要获取到 takeLock 才能进行出队操作 takeLock.lockInterruptibly(); try &#123; // 如果队列为空，等待 notEmpty 这个条件满足再继续执行 while (count.get() == 0) &#123; notEmpty.await(); &#125; // 出队 x = dequeue(); // count 进行原子减 1 c = count.getAndDecrement(); // 如果这次出队后，队列中至少还有一个元素，那么调用 notEmpty.signal() 唤醒其他的读线程 if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; // 出队后释放掉 takeLock takeLock.unlock(); &#125; // 如果 c == capacity，那么说明在这个 take 方法发生的时候，队列是满的 // 既然出队了一个，那么意味着队列不满了，唤醒写线程去写 if (c == capacity) signalNotFull(); return x;&#125;// 取队头，出队private E dequeue() &#123; // assert takeLock.isHeldByCurrentThread(); // assert head.item == null; // 之前说了，头结点是空的 Node&lt;E&gt; h = head; Node&lt;E&gt; first = h.next; h.next = h; // help GC // 设置这个为新的头结点 head = first; E x = first.item; first.item = null; return x;&#125;// 元素出队后，如果需要，调用这个方法唤醒写线程来写private void signalNotFull() &#123; final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125;&#125; 源码分析就到这里结束了吧，毕竟还是比较简单的源码，基本上只要读者认真点都看得懂。 BlockingQueue 实现之 SynchronousQueue它是一个特殊的队列，它的名字其实就蕴含了它的特征 - - 同步的队列。为什么说是同步的呢？这里说的并不是多线程的并发问题，而是因为当一个线程往队列中写入一个元素时，写入操作不会立即返回，需要等待另一个线程来将这个元素拿走；同理，当一个读线程做读操作的时候，同样需要一个相匹配的写线程的写操作。这里的 Synchronous 指的就是读线程和写线程需要同步，一个读线程匹配一个写线程。 我们比较少使用到 SynchronousQueue 这个类，不过它在线程池的实现类 ThreadPoolExecutor 中得到了应用，感兴趣的读者可以在看完这个后去看看相应的使用。 虽然上面我说了队列，但是 SynchronousQueue 的队列其实是虚的，其不提供任何空间（一个都没有）来存储元素。数据必须从某个写线程交给某个读线程，而不是写到某个队列中等待被消费。 你不能在 SynchronousQueue 中使用 peek 方法（在这里这个方法直接返回 null），peek 方法的语义是只读取不移除，显然，这个方法的语义是不符合 SynchronousQueue 的特征的。SynchronousQueue 也不能被迭代，因为根本就没有元素可以拿来迭代的。虽然 SynchronousQueue 间接地实现了 Collection 接口，但是如果你将其当做 Collection 来用的话，那么集合是空的。当然，这个类也是不允许传递 null 值的（并发包中的容器类好像都不支持插入 null 值，因为 null 值往往用作其他用途，比如用于方法的返回值代表操作失败）。 接下来，我们来看看具体的源码实现吧，它的源码不是很简单的那种，我们需要先搞清楚它的设计思想。 源码加注释大概有 1200 行，我们先看大框架： 12345678910111213// 构造时，我们可以指定公平模式还是非公平模式，区别之后再说public SynchronousQueue(boolean fair) &#123; transferer = fair ? new TransferQueue() : new TransferStack();&#125;abstract static class Transferer &#123; // 从方法名上大概就知道，这个方法用于转移元素，从生产者手上转到消费者手上 // 也可以被动地，消费者调用这个方法来从生产者手上取元素 // 第一个参数 e 如果不是 null，代表场景为：将元素从生产者转移给消费者 // 如果是 null，代表消费者等待生产者提供元素，然后返回值就是相应的生产者提供的元素 // 第二个参数代表是否设置超时，如果设置超时，超时时间是第三个参数的值 // 返回值如果是 null，代表超时，或者中断。具体是哪个，可以通过检测中断状态得到。 abstract Object transfer(Object e, boolean timed, long nanos);&#125; Transferer 有两个内部实现类，是因为构造 SynchronousQueue 的时候，我们可以指定公平策略。公平模式意味着，所有的读写线程都遵守先来后到，FIFO 嘛，对应 TransferQueue。而非公平模式则对应 TransferStack。 我们先采用公平模式分析源码，然后再说说公平模式和非公平模式的区别。 接下来，我们看看 put 方法和 take 方法： 12345678910111213141516// 写入值public void put(E o) throws InterruptedException &#123; if (o == null) throw new NullPointerException(); if (transferer.transfer(o, false, 0) == null) &#123; // 1 Thread.interrupted(); throw new InterruptedException(); &#125;&#125;// 读取值并移除public E take() throws InterruptedException &#123; Object e = transferer.transfer(null, false, 0); // 2 if (e != null) return (E)e; Thread.interrupted(); throw new InterruptedException();&#125; 我们看到，写操作 put(E o) 和读操作 take() 都是调用 Transferer.transfer(…) 方法，区别在于第一个参数是否为 null 值。 我们来看看 transfer 的设计思路，其基本算法如下： 当调用这个方法时，如果队列是空的，或者队列中的节点和当前的线程操作类型一致（如当前操作是 put 操作，而队列中的元素也都是写线程）。这种情况下，将当前线程加入到等待队列即可。 如果队列中有等待节点，而且与当前操作可以匹配（如队列中都是读操作线程，当前线程是写操作线程，反之亦然）。这种情况下，匹配等待队列的队头，出队，返回相应数据。 其实这里有个隐含的条件被满足了，队列如果不为空，肯定都是同种类型的节点，要么都是读操作，要么都是写操作。这个就要看到底是读线程积压了，还是写线程积压了。 我们可以假设出一个男女配对的场景：一个男的过来，如果一个人都没有，那么他需要等待；如果发现有一堆男的在等待，那么他需要排到队列后面；如果发现是一堆女的在排队，那么他直接牵走队头的那个女的。 既然这里说到了等待队列，我们先看看其实现，也就是 QNode: 1234567891011static final class QNode &#123; volatile QNode next; // 可以看出来，等待队列是单向链表 volatile Object item; // CAS&apos;ed to or from null volatile Thread waiter; // 将线程对象保存在这里，用于挂起和唤醒 final boolean isData; // 用于判断是写线程节点(isData == true)，还是读线程节点 QNode(Object item, boolean isData) &#123; this.item = item; this.isData = isData; &#125; ...... 相信说了这么多以后，我们再来看 transfer 方法的代码就轻松多了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * Puts or takes an item. */Object transfer(Object e, boolean timed, long nanos) &#123; QNode s = null; // constructed/reused as needed boolean isData = (e != null); for (;;) &#123; QNode t = tail; QNode h = head; if (t == null || h == null) // saw uninitialized value continue; // spin // 队列空，或队列中节点类型和当前节点一致， // 即我们说的第一种情况，将节点入队即可。读者要想着这块 if 里面方法其实就是入队 if (h == t || t.isData == isData) &#123; // empty or same-mode QNode tn = t.next; // t != tail 说明刚刚有节点入队，continue 即可 if (t != tail) // inconsistent read continue; // 有其他节点入队，但是 tail 还是指向原来的，此时设置 tail 即可 if (tn != null) &#123; // lagging tail // 这个方法就是：如果 tail 此时为 t 的话，设置为 tn advanceTail(t, tn); continue; &#125; // if (timed &amp;&amp; nanos &lt;= 0) // can&apos;t wait return null; if (s == null) s = new QNode(e, isData); // 将当前节点，插入到 tail 的后面 if (!t.casNext(null, s)) // failed to link in continue; // 将当前节点设置为新的 tail advanceTail(t, s); // swing tail and wait // 看到这里，请读者先往下滑到这个方法，看完了以后再回来这里，思路也就不会断了 Object x = awaitFulfill(s, e, timed, nanos); // 到这里，说明之前入队的线程被唤醒了，准备往下执行 if (x == s) &#123; // wait was cancelled clean(t, s); return null; &#125; if (!s.isOffList()) &#123; // not already unlinked advanceHead(t, s); // unlink if head if (x != null) // and forget fields s.item = s; s.waiter = null; &#125; return (x != null) ? x : e; // 这里的 else 分支就是上面说的第二种情况，有相应的读或写相匹配的情况 &#125; else &#123; // complementary-mode QNode m = h.next; // node to fulfill if (t != tail || m == null || h != head) continue; // inconsistent read Object x = m.item; if (isData == (x != null) || // m already fulfilled x == m || // m cancelled !m.casItem(x, e)) &#123; // lost CAS advanceHead(h, m); // dequeue and retry continue; &#125; advanceHead(h, m); // successfully fulfilled LockSupport.unpark(m.waiter); return (x != null) ? x : e; &#125; &#125;&#125;void advanceTail(QNode t, QNode nt) &#123; if (tail == t) UNSAFE.compareAndSwapObject(this, tailOffset, t, nt);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041// 自旋或阻塞，直到满足条件，这个方法返回Object awaitFulfill(QNode s, Object e, boolean timed, long nanos) &#123; long lastTime = timed ? System.nanoTime() : 0; Thread w = Thread.currentThread(); // 判断需要自旋的次数， int spins = ((head.next == s) ? (timed ? maxTimedSpins : maxUntimedSpins) : 0); for (;;) &#123; // 如果被中断了，那么取消这个节点 if (w.isInterrupted()) // 就是将当前节点 s 中的 item 属性设置为 this s.tryCancel(e); Object x = s.item; // 这里是这个方法的唯一的出口 if (x != e) return x; // 如果需要，检测是否超时 if (timed) &#123; long now = System.nanoTime(); nanos -= now - lastTime; lastTime = now; if (nanos &lt;= 0) &#123; s.tryCancel(e); continue; &#125; &#125; if (spins &gt; 0) --spins; // 如果自旋达到了最大的次数，那么检测 else if (s.waiter == null) s.waiter = w; // 如果自旋到了最大的次数，那么线程挂起，等待唤醒 else if (!timed) LockSupport.park(this); // spinForTimeoutThreshold 这个之前讲 AQS 的时候其实也说过，剩余时间小于这个阈值的时候，就 // 不要进行挂起了，自旋的性能会比较好 else if (nanos &gt; spinForTimeoutThreshold) LockSupport.parkNanos(this, nanos); &#125;&#125; Doug Lea 的巧妙之处在于，将各个代码凑在了一起，使得代码非常简洁，当然也同时增加了我们的阅读负担，看代码的时候，还是得仔细想想各种可能的情况。 下面，再说说前面说的公平模式和非公平模式的区别。 相信大家心里面已经有了公平模式的工作流程的概念了，我就简单说说 TransferStack 的算法，就不分析源码了。 当调用这个方法时，如果队列是空的，或者队列中的节点和当前的线程操作类型一致（如当前操作是 put 操作，而栈中的元素也都是写线程）。这种情况下，将当前线程加入到等待栈中，等待配对。然后返回相应的元素，或者如果被取消了的话，返回 null。 如果栈中有等待节点，而且与当前操作可以匹配（如栈里面都是读操作线程，当前线程是写操作线程，反之亦然）。将当前节点压入栈顶，和栈中的节点进行匹配，然后将这两个节点出栈。配对和出栈的动作其实也不是必须的，因为下面的一条会执行同样的事情。 如果栈顶是进行匹配而入栈的节点，帮助其进行匹配并出栈，然后再继续操作。 应该说，TransferStack 的源码要比 TransferQueue 的复杂一些，如果读者感兴趣，请自行进行源码阅读。 BlockingQueue 实现之 PriorityBlockingQueue带排序的 BlockingQueue 实现，其并发控制采用的是 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容）。 简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。 它的源码相对比较简单，本节将介绍其核心源码部分。 我们来看看它有哪些属性： 1234567891011121314151617181920212223242526// 构造方法中，如果不指定大小的话，默认大小为 11private static final int DEFAULT_INITIAL_CAPACITY = 11;// 数组的最大容量private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;// 这个就是存放数据的数组private transient Object[] queue;// 队列当前大小private transient int size;// 大小比较器，如果按照自然序排序，那么此属性可设置为 nullprivate transient Comparator&lt;? super E&gt; comparator;// 并发控制所用的锁，所有的 public 且涉及到线程安全的方法，都必须先获取到这个锁private final ReentrantLock lock;// 这个很好理解，其实例由上面的 lock 属性创建private final Condition notEmpty;// 这个也是用于锁，用于数组扩容的时候，需要先获取到这个锁，才能进行扩容操作// 其使用 CAS 操作private transient volatile int allocationSpinLock;// 用于序列化和反序列化的时候用，对于 PriorityBlockingQueue 我们应该比较少使用到序列化private PriorityQueue q; 此类实现了 Collection 和 Iterator 接口中的所有接口方法，对其对象进行迭代并遍历时，不能保证有序性。如果你想要实现有序遍历，建议采用 Arrays.sort(queue.toArray()) 进行处理。PriorityBlockingQueue 提供了 drainTo 方法用于将部分或全部元素有序地填充（准确说是转移，会删除原队列中的元素）到另一个集合中。还有一个需要说明的是，如果两个对象的优先级相同（compare 方法返回 0），此队列并不保证它们之间的顺序。 PriorityBlockingQueue 使用了基于数组的二叉堆来存放元素，所有的 public 方法采用同一个 lock 进行并发控制。 二叉堆：一颗完全二叉树，它非常适合用数组进行存储，对于数组中的元素 a[i]，其左子节点为 a[2*i+1]，其右子节点为 a[2*i + 2]，其父节点为 a[(i-1)/2]，其堆序性质为，每个节点的值都小于其左右子节点的值。二叉堆中最小的值就是根节点，但是删除根节点是比较麻烦的，因为需要调整树。 简单用个图解释一下二叉堆，我就不说太多专业的严谨的术语了，这种数据结构的优点是一目了然的，最小的元素一定是根元素，它是一棵满的树，除了最后一层，最后一层的节点从左到右紧密排列。 下面开始 PriorityBlockingQueue 的源码分析，首先我们来看看构造方法: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 默认构造方法，采用默认值(11)来进行初始化public PriorityBlockingQueue() &#123; this(DEFAULT_INITIAL_CAPACITY, null);&#125;// 指定数组的初始大小public PriorityBlockingQueue(int initialCapacity) &#123; this(initialCapacity, null);&#125;// 指定比较器public PriorityBlockingQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator) &#123; if (initialCapacity &lt; 1) throw new IllegalArgumentException(); this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); this.comparator = comparator; this.queue = new Object[initialCapacity];&#125;// 在构造方法中就先填充指定的集合中的元素public PriorityBlockingQueue(Collection&lt;? extends E&gt; c) &#123; this.lock = new ReentrantLock(); this.notEmpty = lock.newCondition(); // boolean heapify = true; // true if not known to be in heap order boolean screen = true; // true if must screen for nulls if (c instanceof SortedSet&lt;?&gt;) &#123; SortedSet&lt;? extends E&gt; ss = (SortedSet&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) ss.comparator(); heapify = false; &#125; else if (c instanceof PriorityBlockingQueue&lt;?&gt;) &#123; PriorityBlockingQueue&lt;? extends E&gt; pq = (PriorityBlockingQueue&lt;? extends E&gt;) c; this.comparator = (Comparator&lt;? super E&gt;) pq.comparator(); screen = false; if (pq.getClass() == PriorityBlockingQueue.class) // exact match heapify = false; &#125; Object[] a = c.toArray(); int n = a.length; // If c.toArray incorrectly doesn&apos;t return Object[], copy it. if (a.getClass() != Object[].class) a = Arrays.copyOf(a, n, Object[].class); if (screen &amp;&amp; (n == 1 || this.comparator != null)) &#123; for (int i = 0; i &lt; n; ++i) if (a[i] == null) throw new NullPointerException(); &#125; this.queue = a; this.size = n; if (heapify) heapify();&#125; 接下来，我们来看看其内部的自动扩容实现： 123456789101112131415161718192021222324252627282930313233343536373839404142private void tryGrow(Object[] array, int oldCap) &#123; // 这边做了释放锁的操作 lock.unlock(); // must release and then re-acquire main lock Object[] newArray = null; // 用 CAS 操作将 allocationSpinLock 由 0 变为 1，也算是获取锁 if (allocationSpinLock == 0 &amp;&amp; UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset, 0, 1)) &#123; try &#123; // 如果节点个数小于 64，那么增加的 oldCap + 2 的容量 // 如果节点数大于等于 64，那么增加 oldCap 的一半 // 所以节点数较小时，增长得快一些 int newCap = oldCap + ((oldCap &lt; 64) ? (oldCap + 2) : (oldCap &gt;&gt; 1)); // 这里有可能溢出 if (newCap - MAX_ARRAY_SIZE &gt; 0) &#123; // possible overflow int minCap = oldCap + 1; if (minCap &lt; 0 || minCap &gt; MAX_ARRAY_SIZE) throw new OutOfMemoryError(); newCap = MAX_ARRAY_SIZE; &#125; // 如果 queue != array，那么说明有其他线程给 queue 分配了其他的空间 if (newCap &gt; oldCap &amp;&amp; queue == array) // 分配一个新的大数组 newArray = new Object[newCap]; &#125; finally &#123; // 重置，也就是释放锁 allocationSpinLock = 0; &#125; &#125; // 如果有其他的线程也在做扩容的操作 if (newArray == null) // back off if another thread is allocating Thread.yield(); // 重新获取锁 lock.lock(); // 将原来数组中的元素复制到新分配的大数组中 if (newArray != null &amp;&amp; queue == array) &#123; queue = newArray; System.arraycopy(array, 0, newArray, 0, oldCap); &#125;&#125; 扩容方法对并发的控制也非常的巧妙，释放了原来的独占锁 lock，这样的话，扩容操作和读操作可以同时进行，提高吞吐量。 下面，我们来分析下写操作 put 方法和读操作 take 方法。 12345678910111213141516171819202122232425262728293031public void put(E e) &#123; // 直接调用 offer 方法，因为前面我们也说了，在这里，put 方法不会阻塞 offer(e); &#125;public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); final ReentrantLock lock = this.lock; // 首先获取到独占锁 lock.lock(); int n, cap; Object[] array; // 如果当前队列中的元素个数 &gt;= 数组的大小，那么需要扩容了 while ((n = size) &gt;= (cap = (array = queue).length)) tryGrow(array, cap); try &#123; Comparator&lt;? super E&gt; cmp = comparator; // 节点添加到二叉堆中 if (cmp == null) siftUpComparable(n, e, array); else siftUpUsingComparator(n, e, array, cmp); // 更新 size size = n + 1; // 唤醒等待的读线程 notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; return true;&#125; 对于二叉堆而言，插入一个节点是简单的，插入的节点如果比父节点小，交换它们，然后继续和父节点比较。 1234567891011121314// 这个方法就是将数据 x 插入到数组 array 的位置 k 处，然后再调整树private static &lt;T&gt; void siftUpComparable(int k, T x, Object[] array) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) x; while (k &gt; 0) &#123; // 二叉堆中 a[k] 节点的父节点位置 int parent = (k - 1) &gt;&gt;&gt; 1; Object e = array[parent]; if (key.compareTo((T) e) &gt;= 0) break; array[k] = e; k = parent; &#125; array[k] = key;&#125; 我们用图来示意一下，我们接下来要将 11 插入到队列中，看看 siftUp 是怎么操作的。 我们再看看 take 方法： 1234567891011121314public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; // 独占锁 lock.lockInterruptibly(); E result; try &#123; // dequeue 出队 while ( (result = dequeue()) == null) notEmpty.await(); &#125; finally &#123; lock.unlock(); &#125; return result;&#125; 123456789101112131415161718192021private E dequeue() &#123; int n = size - 1; if (n &lt; 0) return null; else &#123; Object[] array = queue; // 队头，用于返回 E result = (E) array[0]; // 队尾元素先取出 E x = (E) array[n]; // 队尾置空 array[n] = null; Comparator&lt;? super E&gt; cmp = comparator; if (cmp == null) siftDownComparable(0, x, array, n); else siftDownUsingComparator(0, x, array, n, cmp); size = n; return result; &#125;&#125; dequeue 方法返回队头，并调整二叉堆的树，调用这个方法必须先获取独占锁。 废话不多说，出队是非常简单的，因为队头就是最小的元素，对应的是数组的第一个元素。难点是队头出队后，需要调整树。 12345678910111213141516171819202122232425262728293031private static &lt;T&gt; void siftDownComparable(int k, T x, Object[] array, int n) &#123; if (n &gt; 0) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;)x; // 这里得到的 half 肯定是非叶节点 // a[n] 是最后一个元素，其父节点是 a[(n-1)/2]。所以 n &gt;&gt;&gt; 1 代表的节点肯定不是叶子节点 // 下面，我们结合图来一行行分析，这样比较直观简单 // 此时 k 为 0, x 为 17，n 为 9 int half = n &gt;&gt;&gt; 1; // 得到 half = 4 while (k &lt; half) &#123; // 先取左子节点 int child = (k &lt;&lt; 1) + 1; // 得到 child = 1 Object c = array[child]; // c = 12 int right = child + 1; // right = 2 // 如果右子节点存在，而且比左子节点小 // 此时 array[right] = 20，所以条件不满足 if (right &lt; n &amp;&amp; ((Comparable&lt;? super T&gt;) c).compareTo((T) array[right]) &gt; 0) c = array[child = right]; // key = 17, c = 12，所以条件不满足 if (key.compareTo((T) c) &lt;= 0) break; // 把 12 填充到根节点 array[k] = c; // k 赋值后为 1 k = child; // 一轮过后，我们发现，12 左边的子树和刚刚的差不多，都是缺少根节点，接下来处理就简单了 &#125; array[k] = key; &#125;&#125; 记住二叉堆是一棵完全二叉树，那么根节点 10 拿掉后，最后面的元素 17 必须找到合适的地方放置。首先，17 和 10 不能直接交换，那么先将根节点 10 的左右子节点中较小的节点往上滑，即 12 往上滑，然后原来 12 留下了一个空节点，然后再把这个空节点的较小的子节点往上滑，即 13 往上滑，最后，留出了位子，17 补上即可。 我稍微调整下这个树，以便读者能更明白： 好了， PriorityBlockingQueue 我们也说完了。 总结我知道本文过长，相信一字不漏看完的读者肯定是少数。 ArrayBlockingQueue 底层是数组，有界队列，如果我们要使用生产者-消费者模式，这是非常好的选择。 LinkedBlockingQueue 底层是链表，可以当做无界和有界队列来使用，所以大家不要以为它就是无界队列。 SynchronousQueue 本身不带有空间来存储任何元素，使用上可以选择公平模式和非公平模式。 PriorityBlockingQueue 是无界队列，基于数组，数据结构为二叉堆，数组第一个也是树的根节点总是最小值。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为什么我会选择走 Java 这条路]]></title>
    <url>%2F2019%2F10%2F11%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BC%9A%E9%80%89%E6%8B%A9%E8%B5%B0%20Java%20%E8%BF%99%E6%9D%A1%E8%B7%AF%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 ​ 阅读本文大概需要 2.8 分钟。 最近有一些小伙伴问我，为什么当初选择走Java这条路，为什么不做C++、前端之类的方向呢，另外还有一些声音：研究生不是应该去做算法方向么，本科生不就可以做开发了吗，为什么还要读研呢。其实吧，这个问题搁在不同人身上，回答可能也是完全不一样的。我可能还是主要以我的角度出发，结合一些读者的问题，来说一说为什么要选择Java这条路。 谈谈我的技术方向选择我最早接触的语言应该是c，再后来又接触了前端、php、C#等语言，对这些语言的了解也仅限于懂得基本语法，写过一些小demo而已，那时候觉得掌握一门语言就是掌握它的语法就行了，于是会在简历上写，了解各种语言，现在想想实在是太可笑了。 不过真的很多初学者都会这么认为，觉得自己不管选哪个方向都可以，这是因为他们不知道自己的技术方向到底是什么，因为他们不管哪个方向都不精。 后来慢慢接触了Android开发，能自己写一些app，觉得这个方向还挺有趣的，于是想着以后干脆做这个吧。不过我那时候也明白自己离进大厂还有很远的距离，于是打算考研。巧的是，读研前的那个假期，去了一家公司实习，做的是Java Web，那时候才觉得Android比Java Web简单多了呀，完全不需要学那么多框架、技术啊，于是更坚定了做Android的决心，只不过那时候也同样发现了，Java web是更加热门的方向，岗位需求量也更大。 读研的时候，一开始也是打算做Android方向的，买了各类Android书籍开始啃，那时候刚好遇到了个实践课程，让我们选方向，有Java、Android、C++等等。当时妹子做前端，为了带上妹子一起做项目，干脆就选了Java Web这个方向，想着应该不影响我做Android啊，于是接下来的时间里就开始学Java Web了。 时间一长，发现这个方向也没有想象的那么难，相反还挺有意思的，毕竟能用到各种各样的框架，技术栈的内容也很丰富，看起来好像比Android的技术含量高很多，加上自己之前学过Java，也有Java Web的学习经历，简历上也能写的好看一点，于是一不做二不休，就开始做Java方向了。就这样，我找到了第一份Java实习，看了一遍Java后端书单，慢慢地在这条路上越走越远，后来我才发现，Java Web远没有想象中那么简单，Java后端技术栈也远不止Java Web这点内容，特别是对于大厂来说，要掌握的东西实在太多了，比如分布式、网络编程、中间件等等。 所以，选择方向这件事，有时候就是看兴趣，看机遇，看你能坚持多久，如果你对一个方向感兴趣，并且愿意持续学习，不断深挖，这个方向可能就适合你，当你在这个方向投入了一定时间之后，有了一定积累和经验，就不太容易再改变方向了。 谈谈各个技术方向的前景之前也有很多读者问过这个问题，做哪个方向更有前景，更有钱景。虽然我只做过一个方向，但是对其他方向也有一些了解和涉猎，不敢说了解得非常多，但是结合自己身边的同学、朋友的情况，还是可以给出一些比较中肯的建议。 其实我最早打算做的是游戏开发的，所以我们先聊聊游戏开发这个方向。 网易游戏在前几年对游戏开发的招聘要求是985硕士以上学历，当时我就是冲着这个要求考了研，后来却没有去做游戏开发，主要原因是游戏开发主要用的是C++，并且主要的岗位要求是客户端方向的技术，比如图形学、引擎技术，以及对C++的掌握程度。 当时自认为有一些Java基础，不愿意转C++，同时也感觉游戏行业大厂太少了，除了腾讯网易就没有什么大公司了，职业发展的空间可能也不大。自己虽然爱玩游戏，但是做开发和玩游戏毕竟是两码事。当然，近两年游戏开发的岗位需求其实还是很大的，因为现在做游戏开发的人太少了，导致网易游 戏放宽了研发工程师的标准，只要求211以上即可，所以，想要从事游戏开发的朋友，其实现在进大厂的机会可能比之前更多了。 说完游戏开发，说一说C++，C++方向和Java一样主要是做后端的，虽然游戏开发大部分也用的是C++，但是C++服务端的需求量确实没有Java大，加上C++的学习难度稍微搞一点，所以我没有选择这个方向。 当然，现在做CV等算法方向的同学都会用到C++，所以相对Java来说，C++方向选择岗位的范围可能也更多一些。不过，正如Java也能做大数据开发一样，选择方向并不是选择语言，比如你做游戏开发或者算法方向，要学的远不止C++，做大数据方向，Java也只是很小的一部分而已。 除此之外，前端、测试、移动端等方向也有很多机会，这些方向的学习难度可能要稍微简单那么一点，所以有很多女生会选择这些方向，如果你想进大厂却对自己不是很有信心，那么这些方向也是很不错的选择。 研究生就应该做算法么？再聊聊现在很火的人工智能、机器学习方向，这个方向说实话最难的地方在于理论知识，也就是机器学习理论、算法模型、统计学知识等内容。很多人对这个方向趋之若鹜的原因，很大程度上是因为这个方向的薪资高，并且相对工程方向来说，工作强度要低一些。 但是，这么热门的方向，竞争有多激烈就不用多说了，大厂的算法岗简历多到数不胜数，你没有论文、实习经历或者比赛为你背书，基本上连简历筛选都过不去，就算你的简历很漂亮，但是很多时候由于岗位需求量不多，只要你不是特别优秀，就可能被安排到研发岗位，这也是我身边很多同学亲身经历的。再有一点，就是有很多算法方向的博士毕业生也会和你竞争，这就有点吓人了，总之，算法方向还是比研发方向更加有难度的，不管是学习难度、面试难度，还竞争激烈程度，都更加明显。 很多人觉得读研就应该做算法，本科生才做研发，我对此不敢苟同，因为主要还是还看个人实际情况，如果你想进大厂，那么至少本科的时候就要有很扎实的基础实力，这对大部分同学来说都是比较困难的，如果你不是名校出身，我觉得进大厂的难度还是比较大的。 很多名校背景的本科生确实可以拿到大厂的研发offer，于是他们会觉得没必要读研，但是对于我这种跨专业的人来说，研究生才是我开始的第一步，跟他们没有什么可比性，所以对于从零开始的我来说，做研发比算法要靠谱的多，对于很多要转行做程序员的人来说，也是一样的道理，应该选择更加符合自己实力的岗位方向，不要好高骛远。 以上内容纯属个人观点！ 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南10：Java 读写锁 ReentrantReadWriteLock 源码分析]]></title>
    <url>%2F2019%2F10%2F10%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%9710%EF%BC%9AJava%20%E8%AF%BB%E5%86%99%E9%94%81%20ReentrantReadWriteLock%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/ 本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文内容：读写锁 ReentrantReadWriteLock 的源码分析，基于 Java7/Java8。 阅读建议：虽然我这里会介绍一些 AQS 的知识，不过如果你完全不了解 AQS，看本文就有点吃力了。 使用示例下面这个例子非常实用，我是 javadoc 的搬运工： 123456789101112131415161718192021222324252627282930313233343536// 这是一个关于缓存操作的故事class CachedData &#123; Object data; volatile boolean cacheValid; // 读写锁实例 final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; // 获取读锁 rwl.readLock().lock(); if (!cacheValid) &#123; // 如果缓存过期了，或者为 null // 释放掉读锁，然后获取写锁 (后面会看到，没释放掉读锁就获取写锁，会发生死锁情况) rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; if (!cacheValid) &#123; // 重新判断，因为在等待写锁的过程中，可能前面有其他写线程执行过了 data = ... cacheValid = true; &#125; // 获取读锁 (持有写锁的情况下，是允许获取读锁的，称为 “锁降级”，反之不行。) rwl.readLock().lock(); &#125; finally &#123; // 释放写锁，此时还剩一个读锁 rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; // 释放读锁 rwl.readLock().unlock(); &#125; &#125;&#125; ReentrantReadWriteLock 分为读锁和写锁两个实例，读锁是共享锁，可被多个线程同时使用，写锁是独占锁。持有写锁的线程可以继续获取读锁，反之不行。 ReentrantReadWriteLock 总览这一节比较重要，我们要先看清楚 ReentrantReadWriteLock 的大框架，然后再到源码细节。 首先，我们来看下 ReentrantReadWriteLock 的结构，它有好些嵌套类： 大家先仔细看看这张图中的信息。然后我们把 ReadLock 和 WriteLock 的代码提出来一起看，清晰一些： 很清楚了，ReadLock 和 WriteLock 中的方法都是通过 Sync 这个类来实现的。Sync 是 AQS 的子类，然后再派生了公平模式和不公平模式。 从它们调用的 Sync 方法，我们可以看到： ReadLock 使用了共享模式，WriteLock 使用了独占模式。 等等，同一个 AQS 实例怎么可以同时使用共享模式和独占模式？？？ 这里给大家回顾下 AQS，我们横向对比下 AQS 的共享模式和独占模式： AQS 的精髓在于内部的属性 state： 对于独占模式来说，通常就是 0 代表可获取锁，1 代表锁被别人获取了，重入例外 而共享模式下，每个线程都可以对 state 进行加减操作 也就是说，独占模式和共享模式对于 state 的操作完全不一样，那读写锁 ReentrantReadWriteLock 中是怎么使用 state 的呢？答案是将 state 这个 32 位的 int 值分为高 16 位和低 16位，分别用于共享模式和独占模式。 源码分析有了前面的概念，大家心里应该都有数了吧，下面就不再那么啰嗦了，直接代码分析。 源代码加注释 1500 行，并不算难，我们要看的代码量不大。如果你前面一节都理解了，那么直接从头开始一行一行往下看就是了，还是比较简单的。 ReentrantReadWriteLock 的前面几行很简单，我们往下滑到 Sync 类，先来看下它的所有的属性： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849abstract static class Sync extends AbstractQueuedSynchronizer &#123; // 下面这块说的就是将 state 一分为二，高 16 位用于共享模式，低16位用于独占模式 static final int SHARED_SHIFT = 16; static final int SHARED_UNIT = (1 &lt;&lt; SHARED_SHIFT); static final int MAX_COUNT = (1 &lt;&lt; SHARED_SHIFT) - 1; static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; // 取 c 的高 16 位值，代表读锁的获取次数(包括重入) static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; // 取 c 的低 16 位值，代表写锁的重入次数，因为写锁是独占模式 static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; // 这个嵌套类的实例用来记录每个线程持有的读锁数量(读锁重入) static final class HoldCounter &#123; // 持有的读锁数 int count = 0; // 线程 id final long tid = getThreadId(Thread.currentThread()); &#125; // ThreadLocal 的子类 static final class ThreadLocalHoldCounter extends ThreadLocal&lt;HoldCounter&gt; &#123; public HoldCounter initialValue() &#123; return new HoldCounter(); &#125; &#125; /** * 组合使用上面两个类，用一个 ThreadLocal 来记录当前线程持有的读锁数量 */ private transient ThreadLocalHoldCounter readHolds; // 用于缓存，记录&quot;最后一个获取读锁的线程&quot;的读锁重入次数， // 所以不管哪个线程获取到读锁后，就把这个值占为已用，这样就不用到 ThreadLocal 中查询 map 了 // 算不上理论的依据：通常读锁的获取很快就会伴随着释放， // 显然，在 获取-&gt;释放 读锁这段时间，如果没有其他线程获取读锁的话，此缓存就能帮助提高性能 private transient HoldCounter cachedHoldCounter; // 第一个获取读锁的线程(并且其未释放读锁)，以及它持有的读锁数量 private transient Thread firstReader = null; private transient int firstReaderHoldCount; Sync() &#123; // 初始化 readHolds 这个 ThreadLocal 属性 readHolds = new ThreadLocalHoldCounter(); // 为了保证 readHolds 的内存可见性 setState(getState()); // ensures visibility of readHolds &#125; ...&#125; state 的高 16 位代表读锁的获取次数，包括重入次数，获取到读锁一次加 1，释放掉读锁一次减 1 state 的低 16 位代表写锁的获取次数，因为写锁是独占锁，同时只能被一个线程获得，所以它代表重入次数 每个线程都需要维护自己的 HoldCounter，记录该线程获取的读锁次数，这样才能知道到底是不是读锁重入，用 ThreadLocal 属性 readHolds 维护 cachedHoldCounter 有什么用？其实没什么用，但能提示性能。将最后一次获取读锁的线程的 HoldCounter 缓存到这里，这样比使用 ThreadLocal 性能要好一些，因为 ThreadLocal 内部是基于 map 来查询的。但是 cachedHoldCounter 这一个属性毕竟只能缓存一个线程，所以它要起提升性能作用的依据就是：通常读锁的获取紧随着就是该读锁的释放。我这里可能表达不太好，但是大家应该是懂的吧。 firstReader 和 firstReaderHoldCount 有什么用？其实也没什么用，但是它也能提示性能。将”第一个”获取读锁的线程记录在 firstReader 属性中，这里的第一个不是全局的概念，等这个 firstReader 当前代表的线程释放掉读锁以后，会有后来的线程占用这个属性的。firstReader 和 firstReaderHoldCount 使得在读锁不产生竞争的情况下，记录读锁重入次数非常方便快速 如果一个线程使用了 firstReader，那么它就不需要占用 cachedHoldCounter 个人认为，读写锁源码中最让初学者头疼的就是这几个用于提升性能的属性了，使得大家看得云里雾里的。主要是因为 ThreadLocal 内部是通过一个 ThreadLocalMap 来操作的，会增加检索时间。而很多场景下，执行 unlock 的线程往往就是刚刚最后一次执行 lock 的线程，中间可能没有其他线程进行 lock。还有就是很多不怎么会发生读锁竞争的场景。 上面说了这么多，是希望能帮大家降低后面阅读源码的压力，大家也可以先看看后面的，然后再慢慢体会。 前面我们好像都只说读锁，完全没提到写锁，主要是因为写锁真的是简单很多，我也特地将写锁的源码放到了后面，我们先啃下最难的读锁先。 读锁获取下面我就不一行一行按源码顺序说了，我们按照使用来说。 我们来看下读锁 ReadLock 的 lock 流程： 123456789// ReadLockpublic void lock() &#123; sync.acquireShared(1);&#125;// AQSpublic final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 然后我们就会进到 Sync 类的 tryAcquireShared 方法： 在 AQS 中，如果 tryAcquireShared(arg) 方法返回值小于 0 代表没有获取到共享锁(读锁)，大于 0 代表获取到 回顾 AQS 共享模式：tryAcquireShared 方法不仅仅在 acquireShared 的最开始被使用，这里是 try，也就可能会失败，如果失败的话，执行后面的 doAcquireShared，进入到阻塞队列，然后等待前驱节点唤醒。唤醒以后，还是会调用 tryAcquireShared 进行获取共享锁的。当然，唤醒以后再 try 是很容易获得锁的，因为这个节点已经排了很久的队了，组织是会照顾它的。 所以，你在看下面这段代码的时候，要想象到两种获取读锁的场景，一种是新来的，一种是排队排到它的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // exclusiveCount(c) 不等于 0，说明有线程持有写锁， // 而且不是当前线程持有写锁，那么当前线程获取读锁失败 // （另，如果持有写锁的是当前线程，是可以继续获取读锁的） if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; // 读锁的获取次数 int r = sharedCount(c); // 读锁获取是否需要被阻塞，稍后细说。为了进去下面的分支，假设这里不阻塞就好了 if (!readerShouldBlock() &amp;&amp; // 判断是否会溢出 (2^16-1，没那么容易溢出的) r &lt; MAX_COUNT &amp;&amp; // 下面这行 CAS 是将 state 属性的高 16 位加 1，低 16 位不变，如果成功就代表获取到了读锁 compareAndSetState(c, c + SHARED_UNIT)) &#123; // ======================= // 进到这里就是获取到了读锁 // ======================= if (r == 0) &#123; // r == 0 说明此线程是第一个获取读锁的，或者说在它前面获取读锁的都走光光了，它也算是第一个吧 // 记录 firstReader 为当前线程，及其持有的读锁数量：1 firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; // 进来这里，说明是 firstReader 重入获取读锁（这非常简单，count 加 1 结束） firstReaderHoldCount++; &#125; else &#123; // 前面我们说了 cachedHoldCounter 用于缓存最后一个获取读锁的线程 // 如果 cachedHoldCounter 缓存的不是当前线程，设置为缓存当前线程的 HoldCounter HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) // 到这里，那么就是 cachedHoldCounter 缓存的是当前线程，但是 count 为 0， // 大家可以思考一下：这里为什么要 set ThreadLocal 呢？(当然，答案肯定不在这块代码中) // 既然 cachedHoldCounter 缓存的是当前线程， // 当前线程肯定调用过 readHolds.get() 进行初始化 ThreadLocal readHolds.set(rh); // count 加 1 rh.count++; &#125; // return 大于 0 的数，代表获取到了共享锁 return 1; &#125; // 往下看 return fullTryAcquireShared(current);&#125; 上面的代码中，要进入 if 分支，需要满足：readerShouldBlock() 返回 false，并且 CAS 要成功（我们先不要纠结 MAX_COUNT 溢出）。 那我们反向推，怎么样进入到最后的 fullTryAcquireShared： readerShouldBlock() 返回 true，2 种情况： 在 FairSync 中说的是 hasQueuedPredecessors()，即阻塞队列中有其他元素在等待锁。 也就是说，公平模式下，有人在排队呢，你新来的不能直接获取锁 在 NonFairSync 中说的是 apparentlyFirstQueuedIsExclusive()，即判断阻塞队列中 head 的第一个后继节点是否是来获取写锁的，如果是的话，让这个写锁先来，避免写锁饥饿。 作者给写锁定义了更高的优先级，所以如果碰上获取写锁的线程马上就要获取到锁了，获取读锁的线程不应该和它抢。 如果 head.next 不是来获取写锁的，那么可以随便抢，因为是非公平模式，大家比比 CAS 速度 compareAndSetState(c, c + SHARED_UNIT) 这里 CAS 失败，存在竞争。可能是和另一个读锁获取竞争，当然也可能是和另一个写锁获取操作竞争。 然后就会来到 fullTryAcquireShared 中再次尝试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/** * 1\. 刚刚我们说了可能是因为 CAS 失败，如果就此返回，那么就要进入到阻塞队列了， * 想想有点不甘心，因为都已经满足了 !readerShouldBlock()，也就是说本来可以不用到阻塞队列的， * 所以进到这个方法其实是增加 CAS 成功的机会 * 2\. 在 NonFairSync 情况下，虽然 head.next 是获取写锁的，我知道它等待很久了，我没想和它抢， * 可是如果我是来重入读锁的，那么只能表示对不起了 */final int fullTryAcquireShared(Thread current) &#123; HoldCounter rh = null; // 别忘了这外层有个 for 循环 for (;;) &#123; int c = getState(); // 如果其他线程持有了写锁，自然这次是获取不到读锁了，乖乖到阻塞队列排队吧 if (exclusiveCount(c) != 0) &#123; if (getExclusiveOwnerThread() != current) return -1; // else we hold the exclusive lock; blocking here // would cause deadlock. &#125; else if (readerShouldBlock()) &#123; /** * 进来这里，说明： * 1\. exclusiveCount(c) == 0：写锁没有被占用 * 2\. readerShouldBlock() 为 true，说明阻塞队列中有其他线程在等待 * * 既然 should block，那进来这里是干什么的呢？ * 答案：是进来处理读锁重入的！ * */ // firstReader 线程重入读锁，直接到下面的 CAS if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) &#123; // cachedHoldCounter 缓存的不是当前线程 // 那么到 ThreadLocal 中获取当前线程的 HoldCounter // 如果当前线程从来没有初始化过 ThreadLocal 中的值，get() 会执行初始化 rh = readHolds.get(); // 如果发现 count == 0，也就是说，纯属上一行代码初始化的，那么执行 remove // 然后往下两三行，乖乖排队去 if (rh.count == 0) readHolds.remove(); &#125; &#125; if (rh.count == 0) // 排队去。 return -1; &#125; /** * 这块代码我看了蛮久才把握好它是干嘛的，原来只需要知道，它是处理重入的就可以了。 * 就是为了确保读锁重入操作能成功，而不是被塞到阻塞队列中等待 * * 另一个信息就是，这里对于 ThreadLocal 变量 readHolds 的处理： * 如果 get() 后发现 count == 0，居然会做 remove() 操作， * 这行代码对于理解其他代码是有帮助的 */ &#125; if (sharedCount(c) == MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // 这里 CAS 成功，那么就意味着成功获取读锁了 // 下面需要做的是设置 firstReader 或 cachedHoldCounter if (sharedCount(c) == 0) &#123; // 如果发现 sharedCount(c) 等于 0，就将当前线程设置为 firstReader firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; // 下面这几行，就是将 cachedHoldCounter 设置为当前线程 if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; cachedHoldCounter = rh; &#125; // 返回大于 0 的数，代表获取到了读锁 return 1; &#125; &#125;&#125; firstReader 是每次将读锁获取次数从 0 变为 1 的那个线程。 能缓存到 firstReader 中就不要缓存到 cachedHoldCounter 中。 上面的源码分析应该说得非常详细了，如果到这里你不太能看懂上面的有些地方的注释，那么可以先往后看，然后再多看几遍。 读锁释放下面我们看看读锁释放的流程： 1234// ReadLockpublic void unlock() &#123; sync.releaseShared(1);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// Syncpublic final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); // 这句代码其实唤醒 获取写锁的线程，往下看就知道了 return true; &#125; return false;&#125;// Syncprotected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); if (firstReader == current) &#123; if (firstReaderHoldCount == 1) // 如果等于 1，那么这次解锁后就不再持有锁了，把 firstReader 置为 null，给后来的线程用 // 为什么不顺便设置 firstReaderHoldCount = 0？因为没必要，其他线程使用的时候自己会设值 firstReader = null; else firstReaderHoldCount--; &#125; else &#123; // 判断 cachedHoldCounter 是否缓存的是当前线程，不是的话要到 ThreadLocal 中取 HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; // 这一步将 ThreadLocal remove 掉，防止内存泄漏。因为已经不再持有读锁了 readHolds.remove(); if (count &lt;= 0) // 就是那种，lock() 一次，unlock() 好几次的逗比 throw unmatchedUnlockException(); &#125; // count 减 1 --rh.count; &#125; for (;;) &#123; int c = getState(); // nextc 是 state 高 16 位减 1 后的值 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // 如果 nextc == 0，那就是 state 全部 32 位都为 0，也就是读锁和写锁都空了 // 此时这里返回 true 的话，其实是帮助唤醒后继节点中的获取写锁的线程 return nextc == 0; &#125;&#125; 读锁释放的过程还是比较简单的，主要就是将 hold count 减 1，如果减到 0 的话，还要将 ThreadLocal 中的 remove 掉。 然后是在 for 循环中将 state 的高 16 位减 1，如果发现读锁和写锁都释放光了，那么唤醒后继的获取写锁的线程。 写锁获取 写锁是独占锁。 如果有读锁被占用，写锁获取是要进入到阻塞队列中等待的。 123456789101112131415161718192021222324252627282930313233343536373839404142// WriteLockpublic void lock() &#123; sync.acquire(1);&#125;// AQSpublic final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; // 如果 tryAcquire 失败，那么进入到阻塞队列等待 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;// Syncprotected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; // 看下这里返回 false 的情况： // c != 0 &amp;&amp; w == 0: 写锁可用，但是有线程持有读锁(也可能是自己持有) // c != 0 &amp;&amp; w !=0 &amp;&amp; current != getExclusiveOwnerThread(): 其他线程持有写锁 // 也就是说，只要有读锁或写锁被占用，这次就不能获取到写锁 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // 这里不需要 CAS，仔细看就知道了，能到这里的，只可能是写锁重入，不然在上面的 if 就拦截了 setState(c + acquires); return true; &#125; // 如果写锁获取不需要 block，那么进行 CAS，成功就代表获取到了写锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; 下面看一眼 writerShouldBlock() 的判定，然后你再回去看一篇写锁获取过程。 1234567891011121314static final class NonfairSync extends Sync &#123; // 如果是非公平模式，那么 lock 的时候就可以直接用 CAS 去抢锁，抢不到再排队 final boolean writerShouldBlock() &#123; return false; // writers can always barge &#125; ...&#125;static final class FairSync extends Sync &#123; final boolean writerShouldBlock() &#123; // 如果是公平模式，那么如果阻塞队列有线程等待的话，就乖乖去排队 return hasQueuedPredecessors(); &#125; ...&#125; 写锁释放123456789101112131415161718192021222324252627282930313233// WriteLockpublic void unlock() &#123; sync.release(1);&#125;// AQSpublic final boolean release(int arg) &#123; // 1\. 释放锁 if (tryRelease(arg)) &#123; // 2\. 如果独占锁释放&quot;完全&quot;，唤醒后继节点 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;// Sync // 释放锁，是线程安全的，因为写锁是独占锁，具有排他性// 实现很简单，state 减 1 就是了protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); setState(nextc); // 如果 exclusiveCount(nextc) == 0，也就是说包括重入的，所有的写锁都释放了， // 那么返回 true，这样会进行唤醒后继节点的操作。 return free;&#125; 看到这里，是不是发现写锁相对于读锁来说要简单很多。 锁降级Doug Lea 没有说写锁更高级，如果有线程持有读锁，那么写锁获取也需要等待。 不过从源码中也可以看出，确实会给写锁一些特殊照顾，如非公平模式下，为了提高吞吐量，lock 的时候会先 CAS 竞争一下，能成功就代表读锁获取成功了，但是如果发现 head.next 是获取写锁的线程，就不会去做 CAS 操作。 Doug Lea 将持有写锁的线程，去获取读锁的过程称为锁降级（Lock downgrading）。这样，此线程就既持有写锁又持有读锁。 但是，锁升级是不可以的。线程持有读锁的话，在没释放的情况下不能去获取写锁，因为会发生死锁。 回去看下写锁获取的源码： 12345678910111213141516protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; // 看下这里返回 false 的情况： // c != 0 &amp;&amp; w == 0: 写锁可用，但是有线程持有读锁(也可能是自己持有) // c != 0 &amp;&amp; w !=0 &amp;&amp; current != getExclusiveOwnerThread(): 其他线程持有写锁 // 也就是说，只要有读锁或写锁被占用，这次就不能获取到写锁 if (w == 0 || current != getExclusiveOwnerThread()) return false; ... &#125; ...&#125; 仔细想想，如果线程 a 先获取了读锁，然后获取写锁，那么线程 a 就到阻塞队列休眠了，自己把自己弄休眠了，而且可能之后就没人去唤醒它了。 总结 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java后端工程师必备书单（从Java基础到分布式）]]></title>
    <url>%2F2019%2F10%2F10%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2FJava%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BF%85%E5%A4%87%E4%B9%A6%E5%8D%95%EF%BC%88%E4%BB%8EJava%E5%9F%BA%E7%A1%80%E5%88%B0%E5%88%86%E5%B8%83%E5%BC%8F%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java开发工程师一般负责后端开发，当然也有专门做Java Web的工程师，但是随着前后端的分离，越来越多的Java工程师需要往大后端方向发展。 今天我们就来介绍一下Java后端开发者的书单。 首先要感谢一下江南白衣大大的后端书架，让我在初学阶段读到了很多好书，直到现在都印象深刻。 我在两年的学习历程中看了很多的书，其中不乏XXX入门到精通，XXX王者归来，XXX指南什么的。 虽然这类书确实毛病很多，但是作为非科班的我来说，当时还是看的津津有味。直到后来我看到一些优秀的书籍，以及白衣哥的书架，我才逐渐认识到看一些精品书籍的好处。 所以我们今天就从这些精品书籍中挑选一些优秀书籍来推荐给大家，当然其中有一些书我自己也没有时间看完。 接下来我们按照后端技术栈各个部分的内容来推荐书籍。 网络1 TCP/IP卷一 这本其实我刚开始没看太懂，可能是当时太水的原因，但是一般是大牛力荐的书。 2 计算机网络：自顶向下 这本从应用层讲到物理层，感觉这种方式学起来更轻松，我比较推荐小白看这本。 3 图解HTTP和图解TCP/IP 相较于前两本大厚书，这两本书更加亲民，小白可以买来看看，还是很适合入门的。 4 计算机网络 没错，就是这本教材，作为非科班选手自己看这本书，那叫一个欲仙欲死啊，看完就忘记了。 操作系统1 深入理解计算机系统 这本书不是严格意义上的操作系统书籍，而是对计算机基础和操作系统做了一个串联，可以解决你很多对于计算机的疑惑，并且对操作系统有一定理解。 其实这本书还是很厚的，有一定难度，建议有一些基础后再看。 2 现代操作系统 这本书其实我没怎么看，比较详细地讲解了操作系，但是也是大家都比较推崇的书，还是那句话，很厚，慎看。 3 Linux内核设计与实现 当你对操作系统有一定理解以后，这本书将为你打开学习Linux内核的大门，当然在此之前你得有一定的c语言开发能力，以及对Linux的了解。反正，我现在还没开始准备好看这本书。 4 Unix网络编程 这两本书的厚度绝对让你感到绝望，其实就是讲解了Unix内核是如何实现网络通信的，其中涉及到很多网络，操作系统的知识，并且你要熟悉c语言。总之，这是一本奉为网络编程神书的存在，不过我等新手还是拿他压压泡面就好了。 网上有很多博客会参照该书的内容，可以去看看它们。 数据结构与算法不瞒你说，由于我非科班，所以算法基础比较差，数据结构是考研时跟着天勤考研指南学的，学习算法也是一路坎坷，举步维艰。还是分享几本比较靠谱的书吧。 1 算法导论 你问我是不是认真的，我就是说说而已，这本书我买来没看过。 2 数据结构与算法（Java版） 这本书对于初学者来说还是比较友好的，当然学Java的看这本。 3 算法图解，啊哈算法 这两部书籍非常适合学习算法的入门，前者主要用图解的形式覆盖了大部分常用算法，包括dp，贪心等等，可以作为入门书，后者则把很多常用算法都进行了实现，包括搜索，图，树等一些比较高级的常用算法。 4 剑指offer 这本书还是要强烈推荐的，毕竟是面试题经常参考的书籍，当然最好有前面基本的铺垫再看，可能收获更大，这本书在面试之前一般都要嚼烂。 5 LeetCode这不是一本书，是一个题库，算法么，终究还是要靠刷题来提升熟练度的。 Java基础1 Java编程思想 这本书也是被誉为Java神书的存在了，但是对新手不友好，适合有些基础再看，当然要选择性地看。我当时大概只看了1/3 2 Java核心技术卷一 这本书还是比较适合入门的，当然，这种厚皮书要看完还是很有难度的，不过比起上面那本要简单一些 Java进阶1 深入理解JVM虚拟机 这本书是Java开发者必须看的书，很多jvm的文章都是提取这本书的内容。JVM是Java虚拟机，赋予了Java程序生命，所以好好看看把，我自己就已经看了三遍了。 2 Java并发编程实战 这本书是Java 并发包作者写的书，所以非常权威，但是比较晦涩难懂，我看的云里雾里的，大家可以按需选择。 3 Java并发编程艺术 这本书是国内作者写的Java并发书籍，比上面那一本更简单易懂，适合作为并发编程的入门书籍，当然，学习并发原理之前，还是先把Java的多线程搞懂吧。 4 Effective Java 这本书和Java编程思想一样被称为神书，主要讲的是Java的一些优化技巧和规范，没有一定开发经验的人看这本书会觉得索然无味，不知所云，所以，先搁着吧。 5 Java性能调优指南 说到JVM调优，可能会有很多的面试题浮现在你的脑海里，这本书比较权威地讲解了Java的性能调优方法，不过我还没怎么看，有空好好看看。 6 Netty权威指南 Netty是基于NIO开发的网络编程框架，使用Java代码编程，其实这本书也可以放在网络或者Java Web部分。不过NIO属于JDK自带的一部分，是必须要掌握的，而对于Netty，大家如果学有余力的话也可以看看。 JavaWeb1 深入JavaWeb技术内幕 这本书是Java Web的集大成之作，涵盖了大部分Java Web开发的知识点，不过一本书显然无法把所有细节都讲完，但是作为Java Web的入门或者进阶书籍来看的话还是很不错的。 2 How Tomcat Works Java Web很重要的一部分内容就是Tomcat，作为应用服务器，Tomcat使用Java开发，其源代码和架构设计都是经典之作。 这是一本讲解Tomcat基本原理的书籍，很好地通过剖析源码来讲解Tomcat的内部结构和运行机制，但是需要一定的基础才能够看懂，我还没看这本书，日后再拜读。 3 Tomcat架构解析 和上面这本书类似，主要讲解Tomcat原理和架构，，要看懂这本书的话，前提是你要对Java基础，NIO以及设计模式有所了解。这本书我也还没看。 4 Spring实战 这本书适合作为Spring的入门书籍，把Spring的概念，使用方式等内容都讲的比较清楚。并且也介绍了Spring MVC的部分内容，Spring框架还是更注重实践的，所以跟着书上的内容去做吧。 5 Spring源码深度解析 学会Spring基础后，可以花点时间看看这本讲源码的书了，这本书对于新手来说不太友好，主要也是因为Spring的代码结构比较复杂，大家也可以看一些博客来完成对源码的学习。 6 Spring MVC学习指南 本书是一本Spring MVC的教程，内容细致、讲解清晰，非常适合Web开发者和想要使用Spring MVC开发基于Java的Web应用的读者阅读。但是由于出的比较早，所以不太适合现在版本。 6 Maven实战 Maven是Java Web开发中不可缺少的一部分，如果想要全面了解其实现原理的话，可以看看这本书。 数据库1 数据库原理数据库原理应该是教材吧，这本书作为数据库入门来说还是可以的，毕竟不是专门做DB的，看大厚书用处不大，这本书把数据库的基本概念都讲完了。 1 sql必知必会 这本书主要是讲解sql语句怎么写，毕竟数据库最重要的一点就是要熟练地使用sql语句，当然这本书也可以当做工具书来使用。 2 深入浅出MySQL 这本书适合作为MySQL的学习书籍，当你有了一定的MySQL使用经验后，可以看看它，该书从数据库的基础、开发、优化、管理维护和架构5个方面对MySQL进行了详细的介绍，讲的不算特别深，但是足够我们使用了。这本书我也只看了一部分。 3 MySQL技术内幕：innodb存储引擎 看完上面那本书以后，对MySQL算是比较熟悉了，不过对于面试中常考的innodb引擎，还是推荐一下这本书把，专门讲解了innodb存储引擎的相关内容。我还没有细看，但是内容足够你学好innodb了。 4 高性能Mysql 这本书可以说是很厚了，更适合DBA拜读，讲的太详细了，打扰了。 5 Redis实战 和MySQL一样，学习Redis的第一步最好也是先实战一下，通过这本书就可以较好地掌握Redis的使用方法，以及相关数据结构了。 6 Redis设计与实现 该书全面而完整地讲解了 Redis 的内部运行机制,对 Redis 的大多数单机功能以及所有多机功能的实现原理进行了介绍。这本书把Redis的基本原理讲的一清二楚，包括数据结构，持久化，集群等内容，有空应该看看。 分布式1 分布式Java应用 这本书是淘宝大牛写的书，主要讲的就是使用rpc来构建分布式的Java应用，讲了很多基础的东西，可以作为入门书籍，不过这本书我之前没有遇到，所以没看过。 2 大型网站技术架构 这本淘宝系技术指南还是非常值得推崇的，可以说是把大型网站的现代架构进行了一次简单的总结，内容涵盖了各方面，主要讲的是概念，很适合没接触过架构的同学入门。看完以后你会觉得后端技术原来这么博大精深。 3 大型分布式网站架构设计与实践 这本书与上面一书相比更倾向于实践，主要讲的是分布式架构的一些解决方案，但是如果你没有接触过相关的场景，可能会看的云里雾里。 4 分布式服务框架原理与实践 上面那本书讲的是分布式架构的实践，而这本书更专注于分布式服务的原理讲解和对应实践，很好地讲述了分布式服务的基本概念，相关技术，以及解决方案等，对于想要学习分布式服务框架的同学来说是本好书。 5 大型网站系统与Java中间件开发实践 话说这些书的名字真实够长的。这本书也是阿里系出品，主要讲的是大型网站系统以及使用的相关中间件，毕竟阿里是中间件大户，所以很多中间件对应用再网站系统中，对于想学习这方面技术的同学来说可以一看。 6 从Paxos到Zookeeper分布式一致性原理与实践 说起分布式系统，我们需要了解它的原理，相关理论及技术，这本书也是从这个角度出发，讲解了分布式系统的一些常用概念，并且带出了分布式一哥zookeeper，可以说是想学分布式技术的同学必看的书籍。 7 大规模分布式存储系统 这本书是阿里巴巴oceanbase核心开发大佬写的书，讲的是分布式存储相关的原理和解决方案，该书不是很厚，如果想做存储方向的同学可以看看。 云计算云计算方面的内容主要是我在实习阶段接触的，如果只是应用开发方向的话这块不懂也罢。主要还是看个人兴趣。 1 OpenStack设计与实现 OpenStack是基于KVM技术的一套私有云生态。这本书很好地讲解了OpenStack的一些基本原理，包括各个组件的设计与实现，比起另一本《OpenStack王者归来》简单易懂的多。当然，前提最好是你对Linux内核和网络有所了解。 2 docker入门与实践 docker是现在应用部署的主流方案了，所以了解一下还是挺有必要的，这本书作为入门书籍足够让你会使用docker了。 3 kubenetes权威指南 kubenetes是docker的集群解决方案，也是一个微服务的解决方案，所以这本书涉及的内容非常多，需要有网络，操作系统以及docker相关的基础。我看这本书的时候可以说是非常晕的。 大数据和云计算一样，大数据方面的内容也不算是Java后端技术栈所需要的，但是这也能为你加分，并且让你跟大数据开发的岗位沾点边，何乐而不为。 1 大数据技术原理与应用 作为大数据方面的一本教材，厦大教授写的这本书还是非常赞的，从最基础的原理方面讲解了Hadoop的生态系统，并且把每个组件的原理都讲得比较清楚，另外也加入了spark，storm等内容，可以说是大数据入门非常好的一本书了。 2 Hadoop实战 这本书很厚，我买的时候大概看了一遍，一头雾水。所以建议先看上面那本书，再来看更加进阶的书籍，否则可能就是浪费时间了。 3 Hadoop权威指南 这本书主要对Hadoop生态中组件进行详细讲解，有点太详细了，如果不是做大数据方向的话，可以不看。 其他：1 Git权威指南 Git是现在大公司主流的代码协同工具，如果你想要了解其底层原理，可以看看这本书。 2 重构 这本书主要介绍的是代码重构的一些指导思想和最佳实践。有重构需求的同学可以看看。 3 - n其他方面的书籍就太多了，比如软件工程方面的，测试方面，Linux方面，以及讲一些程序员自我提升的书籍，就不一一列举了，因为这部分的内容可以不归入Java后端的技术栈。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>书单</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java工程师修炼之路（校招总结）]]></title>
    <url>%2F2019%2F10%2F10%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2FJava%E5%B7%A5%E7%A8%8B%E5%B8%88%E4%BF%AE%E7%82%BC%E4%B9%8B%E8%B7%AF%EF%BC%88%E6%A0%A1%E6%8B%9B%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言在下本是跨专业渣考研的985渣硕一枚，经历研究生两年的学习积累，有幸于2019秋季招聘中拿到几个公司的研发岗offer，包括百度，阿里，腾讯，今日头条，网易，华为等。 一路走来也遇到很多困难，也踩了很多坑，同时我自己也探索了很多的学习方法，总结了很多心得体会，并且，我对校园招聘也做了一些研究和相应的准备。 在今年的秋季招聘结束以后，我也决定把这些东西全部都写成文字，做成专题，以便分享给更多未来将要参加校招的同学。 更多内容后续都发布于微信公众号：黄小斜 大学时期的迷茫与坚定我的本科专业是电子信息工程，基本没有接触过计算机专业的课程，只学过c语言，然后在大三的时候接触过java，Android，以及前端开发。 那时候我还不知道软件开发的水有多深，抱着试一试的态度去应聘了很多公司。结果可想而知。 当年我对游戏开发很有兴趣，特别是对网易游戏情有独钟，但是当我看到网易游戏研发工程师的招聘要求时，我只能望而却步，因为它要求学历至少是985的硕士。 也因为这个契机，我在大三的暑假开始准备考研，花了一个月的时间深思熟虑之后，选择了我大华科。 毕竟是跨专业，在复习过程还是有点吃力的，但是就靠着一股毅力和执着，半年之后，顺利地考上了，成绩还意外地非常好。 研究生时期的探索和规划对于即将读研的同学来说，一般有两件事很重要，一件事是选择导师，一件事是选择方向。 我当时本着想要找实习的想法选择了我现在的导师，事实证明我的选择还是很正确的。 而选择方向这件事，我倒是折腾了好久。研一期间我做的最多的事情就是看书了，当时自己的方向还不明确，所以找了很多书来看。当别人都在专研数据挖掘和机器学习时，我还在各种方向之间摇摆不定。 我在读研之前想做游戏开发和Android开发，但我以前也学过Java Web开发。于是我在网上了解对应方向的资讯，发现游戏研发的就业面比较窄，并且基于我之前的学习经历，java开发可能更加适合我。最终在学校的实训项目中我选择了Java Web项目，从此也真正意义上地踏上了Java的学习之路。 我的Java入门之路之前说过，在研一期间看了很多计算机专业的书籍，比如计算机网络，操作系统，数据库等等，虽然吸收得都不太好，但也算是看过了。 于是我开始踏上学习Java的道路。最开始我找了一些Java的书单，然后买了一些比较基础的书籍，先啃为敬。那时候我看过《Java从入门到精通》这种烂大街的书，也看过《Java编程思想》这种很难懂的书。 一段时间后我感觉吸收效果不好，于是开始把目光转向视频课程了。那时候听舍友力神的建议，到极客学院上看一些视频课程，我当时就觉得这个讲的比书上要好懂一些。后来我又接触到了慕课网，中国MOOC等网站，逐渐地把相关的技术课程都看完了。 那时候正好我们的项目实训还在进行，于是我就把趁热打铁把这些东西用在了项目当中，并且第一次用博客记录下我的实践经验。 现在回头想想，此时的我也只不过是刚刚入门了Java以及web开发。然而那时候不知道天高地厚的我，就开始xjb投各大公司的Java实习岗位了。结果可想而知，那叫一个惨啊。 我的Java进阶之路上文说到我刚刚开始投递实习岗位，是在研一的下学期。当时整天躲在实验室，一边看书一边看视频，接到面试时赶紧刷面经，忙的不亦乐乎。那段时间感觉自己的复习状态和考研差不多。 然而，由于水平确实不咋地，当时我被各大公司的面试官吊打。比如我第一家面的就是百度，三个很简单的问题一个都不会，人家面试官都不好意思打击我了。后来我又面了一些大大小小的互联网公司，虽然情况有所好转，但是总的来说，我要学习的东西还很多。 在准备面试的过程中，我看了很多面经，也看了很多技术博客，发现自己的基础很薄弱，需要系统性的学习。并且这些东西是视频和入门书籍给不了我的。于是我又踏上了找书的道路。 那时候Java书单泛滥，有的书单质量低下，买来的书看两眼就看不下去了。直到我看到了“江南白衣的后端书架”这一文章，才发现Java后端书架原来应该是这样的。于是我照葫芦画瓢把相关书籍都买了，这个阶段，也算是刚刚踏上Java进阶之路吧。 这里面不得不提几本书，对学习Java的同学非常重要，一本是《深入理解JVM虚拟机》，一本是《深入分析Java技术内幕》，以及《Java并发编程艺术》。 再后来，凭着一股不到黄河心不死的精神，终于拿到了网易游戏的实习offer。于是，第一次在大厂实习的机会终于来了，我怀着即期待又忧虑的心情来到了杭州。 我的Java实习之路在猪场实习的时间并不长，也就持续了三个月不到，当时我们部门在做数据仓库，于是我这边主要负责Java Web应用的开发，其实也就是写一些简单的后台接口。 在熟悉了工作流程以后，我很快就适应了工作的节奏，毕竟做的东西也不难，导师也会经常指导，就这样我完成了一个又一个需求，直到后来家里有事，我才临时选择辞职回家。 由于在网易实习的时间比较短，我也留下了一些遗憾，比如对整个项目的架构不够熟悉，并且很多相关技术栈也来不及学习。后来我去熊厂实习的时候，尽量避免了这些问题。 熊厂实习的时间长达半年，部门当时做的是私有云，emmm完全是全新的技术栈啊，于是我基本上又是从零开始学习云计算，但是由于之前的操作系统和网络基础不扎实，在学习相关技术时，基本是两眼一抹黑，学啥啥不会。 这也导致我在上班期间看了很多计算机基础方面的书籍，包括《计算机网络：自顶向下》，《深入理解计算机系统》等等。当然，这也是因为我的工作内容很简单。CRUD你懂的。 于是花时间自学成为了我那时候的主旋律，看书，看内网资源，参加技术分享，倒也非常充实。当然，有空我也会看看项目代码，了解一下技术架构，以便让自己对整个项目有一个更好的理解。 再后来，2018年来了。 抉择时刻：实习转正还是秋招我是2018年1月份离开北京的。当时面临几个问题，是否续租房子，是否继续实习。还有一个小插曲就是当时养的一只猫也得带回去。再三思考后我决定回学校准备秋招。 过年后我就回到学校了，当时我本不打算参加春招，想要潜心修炼准备秋招，但是架不住春招宣传力度大，并且几个大厂都标榜着“转正容易，hc多多”等口号。于是我没忍住，上牛客投了几次简历，打算面几家大厂，心想万一过了就去吧。 简历都投出去了，那也只好复习了啊，当时我们宿舍跟打了鸡血一样，一整天都在刷题，从早到晚泡着刷LeetCode，一个月后终于刷到100多题，也算是能应付一下笔试了吧。 春招我投的公司不多，也就at，网易游戏和京东。最后阿里和京东都给了offer。但是当时阿里的流程走得特别慢，直到内推截止前一天才给我发offer，并且自己也感觉之前面试表现一般，心想我要是去了也是B+，很有可能成为拥抱变化的牺牲品，于是我咬咬牙放弃了，大不了秋招再来。 塞翁失马，焉知非福，春招的颗粒无收反而让我可以安心准备秋招，于是我有大把的时间做我想做的事，制定自己的学习计划，安排自己的生活，不需要去考虑转正这种麻烦事了。 至此，四月终了，春招告一段落。 Java修仙之路平时经常逛牛客，我也经常发些面经啥的，于是很多牛油喜欢调侃说“看神仙”。这时候我只能尴尬又不失礼貌的微笑了0。0 在下不才，成不了神仙，最多就是打游戏的时候能修修仙罢了。 不过你还真别说，网上还真有“Java成神之路”这样的文章，真的打扰了哈哈。 科普一下修仙梗： 修仙梗的意思是喜欢熬夜的人不睡觉不猝死反而要修仙，然后就被广大的网友们互相调侃玩坏了，现在熬夜都不叫熬夜了，新潮的说法就是修仙，熬夜不会猝死啊，会增强法力。 不逗你们了，咱们还是进入正题吧。我在五月份的时候做了一个计划，打算在七月底之前完成复习工作，原因是七月底有阿里的提前批招聘，是我最最重视的一个招聘。这个计划简称三个月计划，我主要做了三个方面的学习规划。 一：首先，便是对Java后端技术栈的复习，这也是最重要的一部分，因为面试永远都是考基础考得最多。 这部分内容又可以细分为多个方面： 1 Java知识体系：包括了Java基础，集合类，设计模式，Java并发技术，Java网络编程，JVM，JavaWeb，Spring框架等等。 2 计算机基础：包括了操作系统，计算机网络，数据结构，数据库，缓存等内容。 3 后端进阶：包括了分布式理论，以及常见分布式技术比如负载均衡，zookeeper，消息队列，分布式缓存等等。当然，这里面也包括系统架构方面的知识，以及RPC，微服务等内容。 4 额外内容：这部分内容因人而异，我主要是因为实习的时候项目涉及了hadoop以及私有云技术栈，所以自己看了很多这方面的东西，譬如Hadoop生态，OpenStack生态，以及docker生态。 我在复习这部分内容的时候，一般先看优质博客，然后自己整理总结对应写一些博客，最后把能够实现的东西实现一下，这样一来一个知识点就搞定了。剩下的事情就是重复这个步骤。 下面放上我的博客：https://blog.csdn.net/a724888 二：其次，便是对算法的学习了。我也把算法的学习分为了几个部分。 1 基础数据结构与算法：主要是复习之前学过的数据结构和算法，额外再看一些算法书籍，譬如《图解算法》，以了解常见算法。 2 剑指offer：剑指offer基本上是面试必考的，所以把它刷个两三遍是很有必要的。 3 LeetCode：搞定前面两项之后，刷LeetCode也会有些底气了，我当时就刷了150题左右，主要也是刷经典的题目。 4 笔试真题：这个就不用多说了，真题是一定要刷的。毕竟各个公司出题的路子都花里胡哨。 刷题多了，就会遇到很多原题和类似题目，所以，尽早开刷，做好准备吧。 三、最后一部分，则是做项目。大概说下我做项目的几个要点吧 1 为什么这时候我还要做项目呢：一来是我觉得实习过程自己接触的东西太细碎，没有对全局做把控，二来是因为想给GitHub加点东西，顺便学点新的技术。于是我选择了当时牛客网上的两个项目来自己做做看。 2 关于项目选择：叶神这两个项目还是讲的非常棒棒的，用的东西也很新，代码也有提供，避免了自己要写前端的尴尬，另外，这两个项目模仿了知乎和头条，更加接地气。 3 把项目做到GitHub上：之前对git也比较熟了，所以想把这个项目按照正常开发的流程走一遍，于是我每天都会做一个模块，然后发布几个版本，记录一下版本更新的细节，写这些东西的时候，自己其实就已经做了思考和总结，感觉还是不错的。 下面放上我的GitHub：https://github.com/h2pl 就这么过了三个月，提前批拉开序幕。 秋招回忆录从七月初第一次投递简历，到九月初，整整两个月的时间，大大小小投了几十家公司，其中很多都是提前批，内推，也经历了许多的笔试，面试。 期间也拿了几个offer，包括百度，腾讯，蚂蚁金服，头条，华为，网易（网易云音乐没给offer，调到了其他部门）。有几家直接收到拒信的，包括拼多多，深信服。还有几家在等待结果。包括快手，斗鱼等。 当然也有一些还没面试完的公司以及待安排面试的公司，这里就不展开说了。 八月底基本上提前批就已经结束了，所以一般这段时间正式校招也开始了，各种大规模的笔试也很多，所以大家即使没有拿到offer也不要灰心，毕竟校招是一场持久战，基本上要到九月十月才能下结论。我之前分享了很多公司的面经，其实大部分都是提前批的，很多都是直接免笔试的，因为我对算法并不是很在行，所以感觉还是比较幸运的。 从七月底第一次面试到9月基本佛系，中间经历了大大小小的面试，这里只进行简单的记录哈，要看面经的话请到我的公众号：程序员江湖。 具体的面经都比较长，这里大概介绍一下面试的情况，然后我会放上面经的链接供大家查阅。 1 阿里面经 阿里中间件研发面经 蚂蚁金服研发面经 岗位是研发工程师，直接找蚂蚁金服的大佬进行内推。 我参与了阿里巴巴中间件部门的提前批面试，一共经历了四次面试，拿到了口头offer。 然后我也参加了蚂蚁金服中间件部门的面试，经历了三次面试，但是没有走流程，所以面试中止了。 最后我走的是蚂蚁金服财富事业群的流程，经历了四次面试，包括一次交叉面，最终拿到了蚂蚁金服的意向书，评级为A。 阿里的面试体验还是比较好的，至少不要求手写算法，但是非常注重Java基础，中间件部门还会特别安排Java基础笔试。 2 腾讯面经 腾讯研发面经 岗位是后台开发工程师，我没有选择意向事业群。 SNG的部门捞了我的简历，开始了面试，他们的技术栈主要是Java，所以比较有的聊。 一共经历了四次技术面试和一次HR面试，目前正在等待结果。 腾讯的面试一如既往地注重考查网络和操作系统，并且喜欢问Linux底层的一些知识，在这方面我还是有很多不足的。 3 百度面经 百度研发面经 百度研发面经整合版 岗位是研发工程师岗位，部门包括百度智能云的三个分部门以及大搜索部门。 百度的提前批面试不走流程，所以可以同时面试好多个部门，所以我参加百度面试的次数大概有12次左右，最终应该是拿了两个部门的offer。 百度的面试风格非常统一，每次面试基本都要到电脑上写算法，所以那段时间写算法写的头皮发麻。 4 网易面经 网易研发面经 面试部门是网易云音乐，岗位是Java开发工程师。 网易是唯一一家我去外地面试的公司，也是我最早去实习的老东家。 一共三轮面试，耗时一个下午。 网易的面试比我想象中的要难，面试官会问的问题都比较深，并且会让你写一些结合实践的代码。 5 头条面经 今日头条研发面经 岗位是后台研发工程师，地点选择了上海。 我参加的是字节跳动的内推面试，当时找了一个牛友要到了白金码，再次感谢这位头条大佬。 然后就开始了一下午的视频面试，一共三轮技术面试，每一轮都要写代码，问问题的风格有点像腾讯，也喜欢问一些底层知识，让我有点懵逼。 如果想看更多公司的面经，也请移步微信公众号：程序员江湖。 另外，我上周还面试了一次亚马逊，因为很多知名外企到十月才开始招人，所以闲了很久之后我又重操旧业了，可能在面完大摩和微软之后，秋招才能正式结束吧 结束也是开始中秋节刚过，国庆节又要到来了。正如每一年的秋招一样，年复一年，在时间面前我们也是渺小的尘埃。 秋招结束不代表着结局，而是新的旅程开始，马上，毕业论文，offer选择，入职准备，毕业旅行等事项也要提上日程了。 不知道明年我们看待学弟学妹的秋招时，会是怎样的一种心境呢。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>Java</tag>
        <tag>校招</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的Java秋招面经大合集]]></title>
    <url>%2F2019%2F10%2F10%2F%E9%BB%84%E5%B0%8F%E6%96%9C%2F%E6%88%91%E7%9A%84Java%E7%A7%8B%E6%8B%9B%E9%9D%A2%E7%BB%8F%E5%A4%A7%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的微信公众号【黄小斜】，也会同步到我的个人博客： www.how2playlife.com 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 阿里面经阿里中间件研发面经 蚂蚁金服研发面经 岗位是研发工程师，直接找蚂蚁金服的大佬进行内推。 我参与了阿里巴巴中间件部门的提前批面试，一共经历了四次面试，拿到了口头offer。 然后我也参加了蚂蚁金服中间件部门的面试，经历了三次面试，但是没有走流程，所以面试中止了。 最后我走的是蚂蚁金服财富事业群的流程，经历了四次面试，包括一次交叉面，最终拿到了蚂蚁金服的意向书，评级为A。 阿里的面试体验还是比较好的，至少不要求手写算法，但是非常注重Java基础，中间件部门还会特别安排Java基础笔试。 腾讯面经腾讯研发面经 岗位是后台开发工程师，我没有选择意向事业群。 SNG的部门捞了我的简历，开始了面试，他们的技术栈主要是Java，所以比较有的聊。 一共经历了四次技术面试和一次HR面试，目前正在等待结果。 腾讯的面试一如既往地注重考查网络和操作系统，并且喜欢问Linux底层的一些知识，在这方面我还是有很多不足的。 百度面经百度研发面经 百度研发面经整合版 岗位是研发工程师岗位，部门包括百度智能云的三个分部门以及大搜索部门。 百度的提前批面试不走流程，所以可以同时面试好多个部门，所以我参加百度面试的次数大概有12次左右，最终应该是拿了两个部门的offer。 百度的面试风格非常统一，每次面试基本都要到电脑上写算法，所以那段时间写算法写的头皮发麻。 网易面经网易研发面经 面试部门是网易云音乐，岗位是Java开发工程师。 网易是唯一一家我去外地面试的公司，也是我最早去实习的老东家。 一共三轮面试，耗时一个下午。 网易的面试比我想象中的要难，面试官会问的问题都比较深，并且会让你写一些结合实践的代码。 头条面经今日头条研发面经 岗位是后台研发工程师，地点选择了上海。 我参加的是字节跳动的内推面试，当时找了一个牛友要到了白金码，再次感谢这位头条大佬。 然后就开始了一下午的视频面试，一共三轮技术面试，每一轮都要写代码，问问题的风格有点像腾讯，也喜欢问一些底层知识，让我有点懵逼。 快手&amp;拼多多面经拼多多&amp;快手研发面经 岗位是Java开发工程师，面试我的部门好像是基础架构部门。 快手是两轮视频面试加上一轮hr面试。然后没下文了，ben 拼多多的岗位是业务平台研发工程师。 当时在学校里参加了面试，过程是比较顺利的，问的问题也都比较有难度。 自我感觉良好，但是最后却收到了拒信，还是挺可惜的。 京东&amp;美团面经京东&amp;美团研发面经 岗位是Java开发工程师 京东和美团都是电话面试，京东是提前批，聊了两次，问我能不能去实习，我说不能，然后就没有下文了。 美团也是提前批的电话面试，直接一面问了一个多小时，有几个问题没答好，直接挂了。后来正式批也没让我参加，可以说是非常真实了。 斗鱼面经斗鱼研发面经 岗位是Java开发工程师（大数据方向） 刚好我人在武汉，于是斗鱼让我去想场面。 大概花了一下午的时间结束所有流程，首先做了一个笔试，还算简单，然后是三轮面试，前两轮主要是技术，最后一轮总监面。 总体来说面的还是不错的，但是没有回应，不太清楚啥原因。 有赞面经有赞研发面经（Java细节） 岗位依然是Java开发工程师 当时是电话面试。 有赞的面试出人意料地很有挑战性，问的都是Java细节，死抠知识点，没有一定准备要回答好还真是很有难度的。 断断续续大概面了三面，后来我不想去现场面，就没了下文。 华为&amp;深信服等面经华为 深信服等研发面经 除了华为和深信服，里面还包含了美图，迅雷，猿辅导等小公司的面经。 华为和深信服是大数据研发岗。其他是后端工程师的岗位。 华为和深信服差不多，技术面试都比较水，所以放一起说。 另外三家小公司的面试难度也差不多，不过最后都没有下文了，感觉也是挺玄学的哈哈。 海康&amp;商汤等面经海康，商汤，顺丰等研发面经 岗位都是后台开发工程师。 海康只经历了简历面，现场面没有去。 商汤也只是稍微聊了一下，就没有后续了。 顺丰经过两面直接给offer。 其中还包括亚马逊的实习生面经。 另外最近面了亚马逊的秋招，感觉难度和实习的面试差不多。面经就不贴了，有兴趣的同学可以和我聊聊。 携程&amp;拼多多面经携程&amp;拼多多研发面经 岗位是Java开发工程师。 携程是武汉现场面，很搞笑的是，携程的面试题是不换的，我同学第二天去面题目一模一样。 并且，携程总共只有一轮面试，真是勤俭节约。 之前拼多多提前批折戟，这次又来参加正式批了。 没想到这次面的更差了2333 有个算法题想半天了不会。于是面完三面草草收场。不得不吐槽一下，负责我们学校面试接待的hr，脾气真是有点大，搞得我都不敢问她问题了。 以上就是本次秋招我整理的面经合集啦，喜欢的朋友可以点赞收藏哈。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>黄小斜原创系列</category>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>干货资源</tag>
        <tag>秋招</tag>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南9：AQS共享模式与并发工具类的实现]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%979%EF%BC%9AAQS%E5%85%B1%E4%BA%AB%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E7%B1%BB%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/ 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言这篇文章是 AQS 系列的最后一篇，第一篇，我们通过 ReentrantLock 公平锁分析了 AQS 的核心，第二篇的重点是把 Condition 说明白，同时也说清楚了对于线程中断的使用。 这篇，我们的关注点是 AQS 最后的部分，AQS 共享模式的使用。有前两篇文章的铺垫，剩下的源码分析将会简单很多。 本文先用 CountDownLatch 将共享模式说清楚，然后顺着把其他 AQS 相关的类 CyclicBarrier、Semaphore 的源码一起过一下。 相对来说，如果读者有前面两篇文章的基础，这篇文章是简单很多，不过对于初学者来说，1 小时估计也是免不了的。 CountDownLatchCountDownLatch 这个类是比较典型的 AQS 的共享模式的使用，这是一个高频使用的类。latch 的中文意思是门栓、栅栏，具体怎么解释我就不废话了，大家随意，看两个例子就知道在哪里用、怎么用了。 使用例子我们看下 Doug Lea 在 java doc 中给出的例子，这个例子非常实用，我经常会写到这个代码。 假设我们有 N ( N &gt; 0 ) 个任务，那么我们会用 N 来初始化一个 CountDownLatch，然后将这个 latch 的引用传递到各个线程中，在每个线程完成了任务后，调用 latch.countDown() 代表完成了一个任务。 调用 latch.await() 的方法的线程会阻塞，直到所有的任务完成。 12345678910111213141516171819202122232425262728293031323334class Driver2 &#123; // ... void main() throws InterruptedException &#123; CountDownLatch doneSignal = new CountDownLatch(N); Executor e = Executors.newFixedThreadPool(8); // 创建 N 个任务，提交给线程池来执行 for (int i = 0; i &lt; N; ++i) // create and start threads e.execute(new WorkerRunnable(doneSignal, i)); // 等待所有的任务完成，这个方法才会返回 doneSignal.await(); // wait for all to finish &#125;&#125;class WorkerRunnable implements Runnable &#123; private final CountDownLatch doneSignal; private final int i; WorkerRunnable(CountDownLatch doneSignal, int i) &#123; this.doneSignal = doneSignal; this.i = i; &#125; public void run() &#123; try &#123; doWork(i); // 这个线程的任务完成了，调用 countDown 方法 doneSignal.countDown(); &#125; catch (InterruptedException ex) &#123; &#125; // return; &#125; void doWork() &#123; ...&#125;&#125; 所以说 CountDownLatch 非常实用，我们常常会将一个比较大的任务进行拆分，然后开启多个线程来执行，等所有线程都执行完了以后，再往下执行其他操作。这里例子中，只有 main 线程调用了 await 方法。 我们再来看另一个例子，这个例子很典型，用了两个 CountDownLatch： 12345678910111213141516171819202122232425262728293031323334353637383940class Driver &#123; // ... void main() throws InterruptedException &#123; CountDownLatch startSignal = new CountDownLatch(1); CountDownLatch doneSignal = new CountDownLatch(N); for (int i = 0; i &lt; N; ++i) // create and start threads new Thread(new Worker(startSignal, doneSignal)).start(); // 这边插入一些代码，确保上面的每个线程先启动起来，才执行下面的代码。 doSomethingElse(); // don&apos;t let run yet // 因为这里 N == 1，所以，只要调用一次，那么所有的 await 方法都可以通过 startSignal.countDown(); // let all threads proceed doSomethingElse(); // 等待所有任务结束 doneSignal.await(); // wait for all to finish &#125;&#125;class Worker implements Runnable &#123; private final CountDownLatch startSignal; private final CountDownLatch doneSignal; Worker(CountDownLatch startSignal, CountDownLatch doneSignal) &#123; this.startSignal = startSignal; this.doneSignal = doneSignal; &#125; public void run() &#123; try &#123; // 为了让所有线程同时开始任务，我们让所有线程先阻塞在这里 // 等大家都准备好了，再打开这个门栓 startSignal.await(); doWork(); doneSignal.countDown(); &#125; catch (InterruptedException ex) &#123; &#125; // return; &#125; void doWork() &#123; ...&#125;&#125; 这个例子中，doneSignal 同第一个例子的使用，我们说说这里的 startSignal。N 个新开启的线程都调用了startSignal.await() 进行阻塞等待，它们阻塞在栅栏上，只有当条件满足的时候（startSignal.countDown()），它们才能同时通过这个栅栏，目的是让所有的线程站在一个起跑线上。 如果始终只有一个线程调用 await 方法等待任务完成，那么 CountDownLatch 就会简单很多，所以之后的源码分析读者一定要在脑海中构建出这么一个场景：有 m 个线程是做任务的，有 n 个线程在某个栅栏上等待这 m 个线程做完任务，直到所有 m 个任务完成后，n 个线程同时通过栅栏。 源码分析Talk is cheap, show me the code. 构造方法，需要传入一个不小于 0 的整数： 123456789101112public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException(&quot;count &lt; 0&quot;); this.sync = new Sync(count);&#125;// 老套路了，内部封装一个 Sync 类继承自 AQSprivate static final class Sync extends AbstractQueuedSynchronizer &#123; Sync(int count) &#123; // 这样就 state == count 了 setState(count); &#125; ...&#125; 代码都是套路，先分析套路：AQS 里面的 state 是一个整数值，这边用一个 int count 参数其实初始化就是设置了这个值，所有调用了 await 方法的等待线程会挂起，然后有其他一些线程会做 state = state - 1 操作，当 state 减到 0 的同时，那个将 state 减为 0 的线程会负责唤醒 所有调用了 await 方法的线程。都是套路啊，只是 Doug Lea 的套路很深，代码很巧妙，不然我们也没有要分析源码的必要。 对于 CountDownLatch，我们仅仅需要关心两个方法，一个是 countDown() 方法，另一个是 await() 方法。 countDown() 方法每次调用都会将 state 减 1，直到 state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，读者这个时候脑子里要有个图：所有调用了 await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来。 我们用以下程序来分析源码，t1 和 t2 负责调用 countDown() 方法，t3 和 t4 调用 await 方法阻塞： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class CountDownLatchDemo &#123; public static void main(String[] args) &#123; CountDownLatch latch = new CountDownLatch(2); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(5000); &#125; catch (InterruptedException ignore) &#123; &#125; // 休息 5 秒后(模拟线程工作了 5 秒)，调用 countDown() latch.countDown(); &#125; &#125;, &quot;t1&quot;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(10000); &#125; catch (InterruptedException ignore) &#123; &#125; // 休息 10 秒后(模拟线程工作了 10 秒)，调用 countDown() latch.countDown(); &#125; &#125;, &quot;t2&quot;); t1.start(); t2.start(); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 阻塞，等待 state 减为 0 latch.await(); System.out.println(&quot;线程 t3 从 await 中返回了&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;线程 t3 await 被中断&quot;); Thread.currentThread().interrupt(); &#125; &#125; &#125;, &quot;t3&quot;); Thread t4 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 阻塞，等待 state 减为 0 latch.await(); System.out.println(&quot;线程 t4 从 await 中返回了&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;线程 t4 await 被中断&quot;); Thread.currentThread().interrupt(); &#125; &#125; &#125;, &quot;t4&quot;); t3.start(); t4.start(); &#125;&#125; 上述程序，大概在过了 10 秒左右的时候，会输出： 12线程 t3 从 await 中返回了线程 t4 从 await 中返回了 这两条输出，顺序不是绝对的 后面的分析，我们假设 t3 先进入阻塞队列 接下来，我们按照流程一步一步走：先 await 等待，然后被唤醒，await 方法返回。 首先，我们来看 await() 方法，它代表线程阻塞，等待 state 的值减为 0。 123456789101112131415161718public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public final void acquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 这也是老套路了，我在第二篇的中断那一节说过了 if (Thread.interrupted()) throw new InterruptedException(); // t3 和 t4 调用 await 的时候，state 都大于 0（state 此时为 2）。 // 也就是说，这个 if 返回 true，然后往里看 if (tryAcquireShared(arg) &lt; 0) doAcquireSharedInterruptibly(arg);&#125;// 只有当 state == 0 的时候，这个方法才会返回 1protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1;&#125; 从方法名我们就可以看出，这个方法是获取共享锁，并且此方法是可中断的（中断的时候抛出 InterruptedException 退出这个方法）。 12345678910111213141516171819202122232425262728private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; // 1\. 入队 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 同上，只要 state 不等于 0，那么这个方法返回 -1 int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; &#125; &#125; // 2 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 我们来仔细分析这个方法，线程 t3 经过第 1 步 addWaiter 入队以后，我们应该可以得到这个： 由于 tryAcquireShared 这个方法会返回 -1，所以 if (r &gt;= 0) 这个分支不会进去。到 shouldParkAfterFailedAcquire 的时候，t3 将 head 的 waitStatus 值设置为 -1，如下： 然后进入到 parkAndCheckInterrupt 的时候，t3 挂起。 我们再分析 t4 入队，t4 会将前驱节点 t3 所在节点的 waitStatus 设置为 -1，t4 入队后，应该是这样的： 然后，t4 也挂起。接下来，t3 和 t4 就等待唤醒了。 接下来，我们来看唤醒的流程。为了让下面的示意图更丰富些，我们假设用 10 初始化 CountDownLatch。 当然，我们的例子中，其实没有 10 个线程，只有 2 个线程 t1 和 t2，只是为了让图好看些罢了。 我们再一步步看具体的流程。首先，我们看 countDown() 方法: 12345678910111213141516171819202122232425public void countDown() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; // 只有当 state 减为 0 的时候，tryReleaseShared 才返回 true // 否则只是简单的 state = state - 1 那么 countDown() 方法就结束了 // 将 state 减到 0 的那个操作才是最复杂的，继续往下吧 if (tryReleaseShared(arg)) &#123; // 唤醒 await 的线程 doReleaseShared(); return true; &#125; return false;&#125;// 这个方法很简单，用自旋的方法实现 state 减 1protected boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125;&#125; countDown 方法就是每次调用都将 state 值减 1，如果 state 减到 0 了，那么就调用下面的方法进行唤醒阻塞队列中的线程： 123456789101112131415161718192021222324// 调用这个方法的时候，state == 0// 这个方法先不要看所有的代码，按照思路往下到我写注释的地方，我们先跑通一个流程，其他的之后还会仔细分析private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // t3 入队的时候，已经将头节点的 waitStatus 设置为 Node.SIGNAL（-1） 了 if (ws == Node.SIGNAL) &#123; // 将 head 的 waitStatue 设置为 0 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 就是这里，唤醒 head 的后继节点，也就是阻塞队列中的第一个节点 // 在这里，也就是唤醒 t3 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) // todo continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; 一旦 t3 被唤醒后，我们继续回到 await 的这段代码，parkAndCheckInterrupt 返回，我们先不考虑中断的情况： 1234567891011121314151617181920212223242526private void doAcquireSharedInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); // 2\. 这里是下一步 p.next = null; // help GC failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // 1\. 唤醒后这个方法返回 parkAndCheckInterrupt()) throw new InterruptedException(); &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 接下来，t3 会进到 setHeadAndPropagate(node, r) 这个方法，先把 head 给占了，然后唤醒队列中其他的线程： 1234567891011121314private void setHeadAndPropagate(Node node, int propagate) &#123; Node h = head; // Record old head for check below setHead(node); // 下面说的是，唤醒当前 node 之后的节点，即 t3 已经醒了，马上唤醒 t4 // 类似的，如果 t4 后面还有 t5，那么 t4 醒了以后，马上将 t5 给唤醒了 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; Node s = node.next; if (s == null || s.isShared()) // 又是这个方法，只是现在的 head 已经不是原来的空节点了，是 t3 的节点了 doReleaseShared(); &#125;&#125; 又回到这个方法了，那么接下来，我们好好分析 doReleaseShared 这个方法，我们根据流程，头节点 head 此时是 t3 节点了： 12345678910111213141516171819202122232425262728293031// 调用这个方法的时候，state == 0private void doReleaseShared() &#123; for (;;) &#123; Node h = head; // 1\. h == null: 说明阻塞队列为空 // 2\. h == tail: 说明头结点可能是刚刚初始化的头节点， // 或者是普通线程节点，但是此节点既然是头节点了，那么代表已经被唤醒了，阻塞队列没有其他节点了 // 所以这两种情况不需要进行唤醒后继节点 if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; // t4 将头节点(此时是 t3)的 waitStatus 设置为 Node.SIGNAL（-1） 了 if (ws == Node.SIGNAL) &#123; // 这里 CAS 失败的场景请看下面的解读 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases // 就是这里，唤醒 head 的后继节点，也就是阻塞队列中的第一个节点 // 在这里，也就是唤醒 t4 unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; // 这个 CAS 失败的场景是：执行到这里的时候，刚好有一个节点入队，入队会将这个 ws 设置为 -1 !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; // 如果到这里的时候，前面唤醒的线程已经占领了 head，那么再循环 // 否则，就是 head 没变，那么退出循环， // 退出循环是不是意味着阻塞队列中的其他节点就不唤醒了？当然不是，唤醒的线程之后还是会调用这个方法的 if (h == head) // loop if head changed break; &#125;&#125; 我们分析下最后一个 if 语句，然后才能解释第一个 CAS 为什么可能会失败： h == head：说明头节点还没有被刚刚用 unparkSuccessor 唤醒的线程（这里可以理解为 t4）占有，此时 break 退出循环。 h != head：头节点被刚刚唤醒的线程（这里可以理解为 t4）占有，那么这里重新进入下一轮循环，唤醒下一个节点（这里是 t4 ）。我们知道，等到 t4 被唤醒后，其实是会主动唤醒 t5、t6、t7…，那为什么这里要进行下一个循环来唤醒 t5 呢？我觉得是出于吞吐量的考虑。 满足上面的 2 的场景，那么我们就能知道为什么上面的 CAS 操作 compareAndSetWaitStatus(h, Node.SIGNAL, 0) 会失败了？ 因为当前进行 for 循环的线程到这里的时候，可能刚刚唤醒的线程 t4 也刚刚好到这里了，那么就有可能 CAS 失败了。 for 循环第一轮的时候会唤醒 t4，t4 醒后会将自己设置为头节点，如果在 t4 设置头节点后，for 循环才跑到 if (h == head)，那么此时会返回 false，for 循环会进入下一轮。t4 唤醒后也会进入到这个方法里面，那么 for 循环第二轮和 t4 就有可能在这个 CAS 相遇，那么就只会有一个成功了。 CyclicBarrier字面意思是“可重复使用的栅栏”或“周期性的栅栏”，总之不是用了一次就没用了的，CyclicBarrier 相比 CountDownLatch 来说，要简单很多，其源码没有什么高深的地方，它是 ReentrantLock 和 Condition 的组合使用。看如下示意图，CyclicBarrier 和 CountDownLatch 是不是很像，只是 CyclicBarrier 可以有不止一个栅栏，因为它的栅栏（Barrier）可以重复使用（Cyclic）。 首先，CyclicBarrier 的源码实现和 CountDownLatch 大相径庭，CountDownLatch 基于 AQS 的共享模式的使用，而 CyclicBarrier 基于 Condition 来实现。 因为 CyclicBarrier 的源码相对来说简单许多，读者只要熟悉了前面关于 Condition 的分析，那么这里的源码是毫无压力的，就是几个特殊概念罢了。 先用一张图来描绘下 CyclicBarrier 里面的一些概念，和它的基本使用流程： 看图我们也知道了，CyclicBarrier 的源码最重要的就是 await() 方法了。 大家先把图看完，然后我们开始源码分析： 123456789101112131415161718192021222324252627282930313233343536public class CyclicBarrier &#123; // 我们说了，CyclicBarrier 是可以重复使用的，我们把每次从开始使用到穿过栅栏当做&quot;一代&quot;，或者&quot;一个周期&quot; private static class Generation &#123; boolean broken = false; &#125; /** The lock for guarding barrier entry */ private final ReentrantLock lock = new ReentrantLock(); // CyclicBarrier 是基于 Condition 的 // Condition 是“条件”的意思，CyclicBarrier 的等待线程通过 barrier 的“条件”是大家都到了栅栏上 private final Condition trip = lock.newCondition(); // 参与的线程数 private final int parties; // 如果设置了这个，代表越过栅栏之前，要执行相应的操作 private final Runnable barrierCommand; // 当前所处的“代” private Generation generation = new Generation(); // 还没有到栅栏的线程数，这个值初始为 parties，然后递减 // 还没有到栅栏的线程数 = parties - 已经到栅栏的数量 private int count; public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; &#125; public CyclicBarrier(int parties) &#123; this(parties, null); &#125; 首先，先看怎么开启新的一代： 123456789// 开启新的一代，当最后一个线程到达栅栏上的时候，调用这个方法来唤醒其他线程，同时初始化“下一代”private void nextGeneration() &#123; // 首先，需要唤醒所有的在栅栏上等待的线程 trip.signalAll(); // 更新 count 的值 count = parties; // 重新生成“新一代” generation = new Generation();&#125; 开启新的一代，类似于重新实例化一个 CyclicBarrier 实例 看看怎么打破一个栅栏： 12345678private void breakBarrier() &#123; // 设置状态 broken 为 true generation.broken = true; // 重置 count 为初始值 parties count = parties; // 唤醒所有已经在等待的线程 trip.signalAll();&#125; 这两个方法之后用得到，现在开始分析最重要的等待通过栅栏方法 await 方法： 123456789101112131415// 不带超时机制public int await() throws InterruptedException, BrokenBarrierException &#123; try &#123; return dowait(false, 0L); &#125; catch (TimeoutException toe) &#123; throw new Error(toe); // cannot happen &#125;&#125;// 带超时机制，如果超时抛出 TimeoutException 异常public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; return dowait(true, unit.toNanos(timeout));&#125; 继续往里看： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException &#123; final ReentrantLock lock = this.lock; // 先要获取到锁，然后在 finally 中要记得释放锁 // 如果记得 Condition 部分的话，我们知道 condition 的 await() 会释放锁，被 signal() 唤醒的时候需要重新获取锁 lock.lock(); try &#123; final Generation g = generation; // 检查栅栏是否被打破，如果被打破，抛出 BrokenBarrierException 异常 if (g.broken) throw new BrokenBarrierException(); // 检查中断状态，如果中断了，抛出 InterruptedException 异常 if (Thread.interrupted()) &#123; breakBarrier(); throw new InterruptedException(); &#125; // index 是这个 await 方法的返回值 // 注意到这里，这个是从 count 递减后得到的值 int index = --count; // 如果等于 0，说明所有的线程都到栅栏上了，准备通过 if (index == 0) &#123; // tripped boolean ranAction = false; try &#123; // 如果在初始化的时候，指定了通过栅栏前需要执行的操作，在这里会得到执行 final Runnable command = barrierCommand; if (command != null) command.run(); // 如果 ranAction 为 true，说明执行 command.run() 的时候，没有发生异常退出的情况 ranAction = true; // 唤醒等待的线程，然后开启新的一代 nextGeneration(); return 0; &#125; finally &#123; if (!ranAction) // 进到这里，说明执行指定操作的时候，发生了异常，那么需要打破栅栏 // 之前我们说了，打破栅栏意味着唤醒所有等待的线程，设置 broken 为 true，重置 count 为 parties breakBarrier(); &#125; &#125; // loop until tripped, broken, interrupted, or timed out // 如果是最后一个线程调用 await，那么上面就返回了 // 下面的操作是给那些不是最后一个到达栅栏的线程执行的 for (;;) &#123; try &#123; // 如果带有超时机制，调用带超时的 Condition 的 await 方法等待，直到最后一个线程调用 await if (!timed) trip.await(); else if (nanos &gt; 0L) nanos = trip.awaitNanos(nanos); &#125; catch (InterruptedException ie) &#123; // 如果到这里，说明等待的线程在 await（是 Condition 的 await）的时候被中断 if (g == generation &amp;&amp; ! g.broken) &#123; // 打破栅栏 breakBarrier(); // 打破栅栏后，重新抛出这个 InterruptedException 异常给外层调用的方法 throw ie; &#125; else &#123; // 到这里，说明 g != generation, 说明新的一代已经产生，即最后一个线程 await 执行完成， // 那么此时没有必要再抛出 InterruptedException 异常，记录下来这个中断信息即可 // 或者是栅栏已经被打破了，那么也不应该抛出 InterruptedException 异常， // 而是之后抛出 BrokenBarrierException 异常 Thread.currentThread().interrupt(); &#125; &#125; // 唤醒后，检查栅栏是否是“破的” if (g.broken) throw new BrokenBarrierException(); // 这个 for 循环除了异常，就是要从这里退出了 // 我们要清楚，最后一个线程在执行完指定任务(如果有的话)，会调用 nextGeneration 来开启一个新的代 // 然后释放掉锁，其他线程从 Condition 的 await 方法中得到锁并返回，然后到这里的时候，其实就会满足 g != generation 的 // 那什么时候不满足呢？barrierCommand 执行过程中抛出了异常，那么会执行打破栅栏操作， // 设置 broken 为true，然后唤醒这些线程。这些线程会从上面的 if (g.broken) 这个分支抛 BrokenBarrierException 异常返回 // 当然，还有最后一种可能，那就是 await 超时，此种情况不会从上面的 if 分支异常返回，也不会从这里返回，会执行后面的代码 if (g != generation) return index; // 如果醒来发现超时了，打破栅栏，抛出异常 if (timed &amp;&amp; nanos &lt;= 0L) &#123; breakBarrier(); throw new TimeoutException(); &#125; &#125; &#125; finally &#123; lock.unlock(); &#125;&#125; 好了，我想我应该讲清楚了吧，我好像几乎没有漏掉任何一行代码吧？ 下面开始收尾工作。 首先，我们看看怎么得到有多少个线程到了栅栏上，处于等待状态： 123456789public int getNumberWaiting() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return parties - count; &#125; finally &#123; lock.unlock(); &#125;&#125; 判断一个栅栏是否被打破了，这个很简单，直接看 broken 的值即可： 123456789public boolean isBroken() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; return generation.broken; &#125; finally &#123; lock.unlock(); &#125;&#125; 前面我们在说 await 的时候也几乎说清楚了，什么时候栅栏会被打破，总结如下： 中断，我们说了，如果某个等待的线程发生了中断，那么会打破栅栏，同时抛出 InterruptedException 异常； 超时，打破栅栏，同时抛出 TimeoutException 异常； 指定执行的操作抛出了异常，这个我们前面也说过。 最后，我们来看看怎么重置一个栅栏： 12345678910public void reset() &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; breakBarrier(); // break the current generation nextGeneration(); // start a new generation &#125; finally &#123; lock.unlock(); &#125;&#125; 我们设想一下，如果初始化时，指定了线程 parties = 4，前面有 3 个线程调用了 await 等待，在第 4 个线程调用 await 之前，我们调用 reset 方法，那么会发生什么？ 首先，打破栅栏，那意味着所有等待的线程（3个等待的线程）会唤醒，await 方法会通过抛出 BrokenBarrierException 异常返回。然后开启新的一代，重置了 count 和 generation，相当于一切归零了。 怎么样，CyclicBarrier 源码很简单吧。 Semaphore有了 CountDownLatch 的基础后，分析 Semaphore 会简单很多。Semaphore 是什么呢？它类似一个资源池（读者可以类比线程池），每个线程需要调用 acquire() 方法获取资源，然后才能执行，执行完后，需要 release 资源，让给其他的线程用。 大概大家也可以猜到，Semaphore 其实也是 AQS 中共享锁的使用，因为每个线程共享一个池嘛。 套路解读：创建 Semaphore 实例的时候，需要一个参数 permits，这个基本上可以确定是设置给 AQS 的 state 的，然后每个线程调用 acquire 的时候，执行 state = state - 1，release 的时候执行 state = state + 1，当然，acquire 的时候，如果 state = 0，说明没有资源了，需要等待其他线程 release。 构造方法： 1234567public Semaphore(int permits) &#123; sync = new NonfairSync(permits);&#125;public Semaphore(int permits, boolean fair) &#123; sync = fair ? new FairSync(permits) : new NonfairSync(permits);&#125; 这里和 ReentrantLock 类似，用了公平策略和非公平策略。 看 acquire 方法： 1234567891011121314public void acquire() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1);&#125;public void acquireUninterruptibly() &#123; sync.acquireShared(1);&#125;public void acquire(int permits) throws InterruptedException &#123; if (permits &lt; 0) throw new IllegalArgumentException(); sync.acquireSharedInterruptibly(permits);&#125;public void acquireUninterruptibly(int permits) &#123; if (permits &lt; 0) throw new IllegalArgumentException(); sync.acquireShared(permits);&#125; 这几个方法也是老套路了，大家基本都懂了吧，这边多了两个可以传参的 acquire 方法，不过大家也都懂的吧，如果我们需要一次获取超过一个的资源，会用得着这个的。 我们接下来看不抛出 InterruptedException 异常的 acquireUninterruptibly() 方法吧： 1234567public void acquireUninterruptibly() &#123; sync.acquireShared(1);&#125;public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 前面说了，Semaphore 分公平策略和非公平策略，我们对比一下两个 tryAcquireShared 方法： 1234567891011121314151617181920212223242526// 公平策略：protected int tryAcquireShared(int acquires) &#123; for (;;) &#123; // 区别就在于是不是会先判断是否有线程在排队，然后才进行 CAS 减操作 if (hasQueuedPredecessors()) return -1; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125;// 非公平策略：protected int tryAcquireShared(int acquires) &#123; return nonfairTryAcquireShared(acquires);&#125;final int nonfairTryAcquireShared(int acquires) &#123; for (;;) &#123; int available = getState(); int remaining = available - acquires; if (remaining &lt; 0 || compareAndSetState(available, remaining)) return remaining; &#125;&#125; 也是老套路了，所以从源码分析角度的话，我们其实不太需要关心是不是公平策略还是非公平策略，它们的区别往往就那么一两行。 我们再回到 acquireShared 方法， 1234public final void acquireShared(int arg) &#123; if (tryAcquireShared(arg) &lt; 0) doAcquireShared(arg);&#125; 由于 tryAcquireShared(arg) 返回小于 0 的时候，说明 state 已经小于 0 了（没资源了），此时 acquire 不能立马拿到资源，需要进入到阻塞队列等待，虽然贴了很多代码，不在乎多这点了： 123456789101112131415161718192021222324252627private void doAcquireShared(int arg) &#123; final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; int r = tryAcquireShared(arg); if (r &gt;= 0) &#123; setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 这个方法我就不介绍了，线程挂起后等待有资源被 release 出来。接下来，我们就要看 release 的方法了： 1234567891011121314151617181920212223// 任务介绍，释放一个资源public void release() &#123; sync.releaseShared(1);&#125;public final boolean releaseShared(int arg) &#123; if (tryReleaseShared(arg)) &#123; doReleaseShared(); return true; &#125; return false;&#125;protected final boolean tryReleaseShared(int releases) &#123; for (;;) &#123; int current = getState(); int next = current + releases; // 溢出，当然，我们一般也不会用这么大的数 if (next &lt; current) // overflow throw new Error(&quot;Maximum permit count exceeded&quot;); if (compareAndSetState(current, next)) return true; &#125;&#125; tryReleaseShared 方法总是会返回 true，然后是 doReleaseShared，这个也是我们熟悉的方法了，我就贴下代码，不分析了，这个方法用于唤醒所有的等待线程： 123456789101112131415161718private void doReleaseShared() &#123; for (;;) &#123; Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; if (ws == Node.SIGNAL) &#123; if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) continue; // loop to recheck cases unparkSuccessor(h); &#125; else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; if (h == head) // loop if head changed break; &#125;&#125; Semphore 的源码确实很简单，基本上都是分析过的老代码的组合使用了。 总结写到这里，终于把 AbstractQueuedSynchronizer 基本上说完了，对于 Java 并发，Doug Lea 真的是神一样的存在。日后我们还会接触到很多 Doug Lea 的代码，希望我们大家都可以朝着大神的方向不断打磨自己的技术，少一些高大上的架构，多一些实实在在的优秀代码吧。 （全文完） 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南8：AQS中的公平锁与非公平锁，Condtion]]></title>
    <url>%2F2019%2F10%2F08%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%978%EF%BC%9AAQS%E4%B8%AD%E7%9A%84%E5%85%AC%E5%B9%B3%E9%94%81%E4%B8%8E%E9%9D%9E%E5%85%AC%E5%B9%B3%E9%94%81%EF%BC%8CCondtion%2F</url>
    <content type="text"><![CDATA[本文转自：http://hongjiev.github.io/2017/06/16/AbstractQueuedSynchronizer 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文章比较长，信息量比较大，建议在 pc 上阅读。文章标题是为了呼应前文，其实可以单独成文的，主要是希望读者看文章能系统看。 本文关注以下几点内容： 深入理解 ReentrantLock 公平锁和非公平锁的区别 深入分析 AbstractQueuedSynchronizer 中的 ConditionObject 深入理解 Java 线程中断和 InterruptedException 异常 基本上本文把以上几点都说清楚了，我假设读者看过上一篇文章中对 AbstractQueuedSynchronizer 的介绍 ，当然如果你已经熟悉 AQS 中的独占锁了，那也可以直接看这篇。各小节之间基本上没什么关系，大家可以只关注自己感兴趣的部分。 其实这篇文章的信息量很大，初学者估计至少要 1 小时才能看完，希望本文对得起大家的时间。 公平锁和非公平锁ReentrantLock 默认采用非公平锁，除非你在构造方法中传入参数 true 。 1234567public ReentrantLock() &#123; // 默认非公平锁 sync = new NonfairSync();&#125;public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 公平锁的 lock 方法： 12345678910111213141516171819202122232425262728293031static final class FairSync extends Sync &#123; final void lock() &#123; acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 1\. 和非公平锁相比，这里多了一个判断：是否有线程在等待 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125;&#125; 非公平锁的 lock 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041static final class NonfairSync extends Sync &#123; final void lock() &#123; // 2\. 和公平锁相比，这里会直接先进行一次CAS，成功就返回了 if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread()); else acquire(1); &#125; // AbstractQueuedSynchronizer.acquire(int arg) public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); &#125; protected final boolean tryAcquire(int acquires) &#123; return nonfairTryAcquire(acquires); &#125;&#125;/** * Performs non-fair tryLock. tryAcquire is implemented in * subclasses, but both need nonfair try for trylock method. */final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; // 这里没有对阻塞队列进行判断 if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false;&#125; 总结：公平锁和非公平锁只有两处不同： 非公平锁在调用 lock 后，首先就会调用 CAS 进行一次抢锁，如果这个时候恰巧锁没有被占用，那么直接就获取到锁返回了。 非公平锁在 CAS 失败后，和公平锁一样都会进入到 tryAcquire 方法，在 tryAcquire 方法中，如果发现锁这个时候被释放了（state == 0），非公平锁会直接 CAS 抢锁，但是公平锁会判断等待队列是否有线程处于等待状态，如果有则不去抢锁，乖乖排到后面。 公平锁和非公平锁就这两点区别，如果这两次 CAS 都不成功，那么后面非公平锁和公平锁是一样的，都要进入到阻塞队列等待唤醒。 相对来说，非公平锁会有更好的性能，因为它的吞吐量比较大。当然，非公平锁让获取锁的时间变得更加不确定，可能会导致在阻塞队列中的线程长期处于饥饿状态。 ConditionTips: 这里重申一下，要看懂这个，必须要先看懂上一篇关于 AbstractQueuedSynchronizer 的介绍，或者你已经有相关的知识了，否则这节肯定是看不懂的。 我们先来看看 Condition 的使用场景，Condition 经常可以用在生产者-消费者的场景中，请看 Doug Lea 给出的这个例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); // condition 依赖于 lock 来产生 final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; // 生产 public void put(Object x) throws InterruptedException &#123; lock.lock(); try &#123; while (count == items.length) notFull.await(); // 队列已满，等待，直到 not full 才能继续生产 items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); // 生产成功，队列已经 not empty 了，发个通知出去 &#125; finally &#123; lock.unlock(); &#125; &#125; // 消费 public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) notEmpty.await(); // 队列为空，等待，直到队列 not empty，才能继续消费 Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); // 被我消费掉一个，队列 not full 了，发个通知出去 return x; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 1、我们可以看到，在使用 condition 时，必须先持有相应的锁。这个和 Object 类中的方法有相似的语义，需要先持有某个对象的监视器锁才可以执行 wait(), notify() 或 notifyAll() 方法。 2、ArrayBlockingQueue 采用这种方式实现了生产者-消费者，所以请只把这个例子当做学习例子，实际生产中可以直接使用 ArrayBlockingQueue 我们常用 obj.wait()，obj.notify() 或 obj.notifyAll() 来实现相似的功能，但是，它们是基于对象的监视器锁的。需要深入了解这几个方法的读者，可以参考我的另一篇文章《深入分析 java 8 编程语言规范：Threads and Locks》。而这里说的 Condition 是基于 ReentrantLock 实现的，而 ReentrantLock 是依赖于 AbstractQueuedSynchronizer 实现的。 在往下看之前，读者心里要有一个整体的概念。condition 是依赖于 ReentrantLock 的，不管是调用 await 进入等待还是 signal 唤醒，都必须获取到锁才能进行操作。 每个 ReentrantLock 实例可以通过调用多次 newCondition 产生多个 ConditionObject 的实例： 1234final ConditionObject newCondition() &#123; // 实例化一个 ConditionObject return new ConditionObject();&#125; 我们首先来看下我们关注的 Condition 的实现类 AbstractQueuedSynchronizer 类中的 ConditionObject。 12345678public class ConditionObject implements Condition, java.io.Serializable &#123; private static final long serialVersionUID = 1173984872572414699L; // 条件队列的第一个节点 // 不要管这里的关键字 transient，是不参与序列化的意思 private transient Node firstWaiter; // 条件队列的最后一个节点 private transient Node lastWaiter; ...... 在上一篇介绍 AQS 的时候，我们有一个阻塞队列，用于保存等待获取锁的线程的队列。这里我们引入另一个概念，叫条件队列（condition queue），我画了一张简单的图用来说明这个。 这里的阻塞队列如果叫做同步队列（sync queue）其实比较贴切，不过为了和前篇呼应，我就继续使用阻塞队列了。记住这里的两个概念，阻塞队列和条件队列。 这里，我们简单回顾下 Node 的属性： 123456&gt; volatile int waitStatus; // 可取值 0、CANCELLED(1)、SIGNAL(-1)、CONDITION(-2)、PROPAGATE(-3)&gt; volatile Node prev;&gt; volatile Node next;&gt; volatile Thread thread;&gt; Node nextWaiter;&gt; prev 和 next 用于实现阻塞队列的双向链表，这里的 nextWaiter 用于实现条件队列的单向链表 基本上，把这张图看懂，你也就知道 condition 的处理流程了。所以，我先简单解释下这图，然后再具体地解释代码实现。 条件队列和阻塞队列的节点，都是 Node 的实例，因为条件队列的节点是需要转移到阻塞队列中去的； 我们知道一个 ReentrantLock 实例可以通过多次调用 newCondition() 来产生多个 Condition 实例，这里对应 condition1 和 condition2。注意，ConditionObject 只有两个属性 firstWaiter 和 lastWaiter； 每个 condition 有一个关联的条件队列，如线程 1 调用 condition1.await() 方法即可将当前线程 1 包装成 Node 后加入到条件队列中，然后阻塞在这里，不继续往下执行，条件队列是一个单向链表； 调用condition1.signal() 触发一次唤醒，此时唤醒的是队头，会将condition1 对应的条件队列的 firstWaiter（队头） 移到阻塞队列的队尾，等待获取锁，获取锁后 await 方法才能返回，继续往下执行。 上面的 2-&gt;3-&gt;4 描述了一个最简单的流程，没有考虑中断、signalAll、还有带有超时参数的 await 方法等，不过把这里弄懂是这节的主要目的。 同时，从图中也可以很直观地看出，哪些操作是线程安全的，哪些操作是线程不安全的。 这个图看懂后，下面的代码分析就简单了。 接下来，我们一步步按照流程来走代码分析，我们先来看看 wait 方法： 12345678910111213141516171819202122232425262728293031// 首先，这个方法是可被中断的，不可被中断的是另一个方法 awaitUninterruptibly()// 这个方法会阻塞，直到调用 signal 方法（指 signal() 和 signalAll()，下同），或被中断public final void await() throws InterruptedException &#123; // 老规矩，既然该方法要响应中断，那么在最开始就判断中断状态 if (Thread.interrupted()) throw new InterruptedException(); // 添加到 condition 的条件队列中 Node node = addConditionWaiter(); // 释放锁，返回值是释放锁之前的 state 值 // await() 之前，当前线程是必须持有锁的，这里肯定要释放掉 int savedState = fullyRelease(node); int interruptMode = 0; // 这里退出循环有两种情况，之后再仔细分析 // 1\. isOnSyncQueue(node) 返回 true，即当前 node 已经转移到阻塞队列了 // 2\. checkInterruptWhileWaiting(node) != 0 会到 break，然后退出循环，代表的是线程中断 while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; // 被唤醒后，将进入阻塞队列，等待获取锁 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 其实，我大体上也把整个 await 过程说得十之八九了，下面我们分步把上面的几个点用源码说清楚。 1. 将节点加入到条件队列addConditionWaiter() 是将当前节点加入到条件队列，看图我们知道，这种条件队列内的操作是线程安全的。 12345678910111213141516171819202122// 将当前线程对应的节点入队，插入队尾private Node addConditionWaiter() &#123; Node t = lastWaiter; // 如果条件队列的最后一个节点取消了，将其清除出去 // 为什么这里把 waitStatus 不等于 Node.CONDITION，就判定为该节点发生了取消排队？ if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; // 这个方法会遍历整个条件队列，然后会将已取消的所有节点清除出队列 unlinkCancelledWaiters(); t = lastWaiter; &#125; // node 在初始化的时候，指定 waitStatus 为 Node.CONDITION Node node = new Node(Thread.currentThread(), Node.CONDITION); // t 此时是 lastWaiter，队尾 // 如果队列为空 if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 上面的这块代码很简单，就是将当前线程进入到条件队列的队尾。 在addWaiter 方法中，有一个 unlinkCancelledWaiters() 方法，该方法用于清除队列中已经取消等待的节点。 当 await 的时候如果发生了取消操作（这点之后会说），或者是在节点入队的时候，发现最后一个节点是被取消的，会调用一次这个方法。 12345678910111213141516171819202122// 等待队列是一个单向链表，遍历链表将已经取消等待的节点清除出去// 纯属链表操作，很好理解，看不懂多看几遍就可以了private void unlinkCancelledWaiters() &#123; Node t = firstWaiter; Node trail = null; while (t != null) &#123; Node next = t.nextWaiter; // 如果节点的状态不是 Node.CONDITION 的话，这个节点就是被取消的 if (t.waitStatus != Node.CONDITION) &#123; t.nextWaiter = null; if (trail == null) firstWaiter = next; else trail.nextWaiter = next; if (next == null) lastWaiter = trail; &#125; else trail = t; t = next; &#125;&#125; 2. 完全释放独占锁回到 wait 方法，节点入队了以后，会调用 int savedState = fullyRelease(node); 方法释放锁，注意，这里是完全释放独占锁（fully release），因为 ReentrantLock 是可以重入的。 考虑一下这里的 savedState。如果在 condition1.await() 之前，假设线程先执行了 2 次 lock() 操作，那么 state 为 2，我们理解为该线程持有 2 把锁，这里 await() 方法必须将 state 设置为 0，然后再进入挂起状态，这样其他线程才能持有锁。当它被唤醒的时候，它需要重新持有 2 把锁，才能继续下去。 123456789101112131415161718192021// 首先，我们要先观察到返回值 savedState 代表 release 之前的 state 值// 对于最简单的操作：先 lock.lock()，然后 condition1.await()。// 那么 state 经过这个方法由 1 变为 0，锁释放，此方法返回 1// 相应的，如果 lock 重入了 n 次，savedState == n// 如果这个方法失败，会将节点设置为&quot;取消&quot;状态，并抛出异常 IllegalMonitorStateExceptionfinal int fullyRelease(Node node) &#123; boolean failed = true; try &#123; int savedState = getState(); // 这里使用了当前的 state 作为 release 的参数，也就是完全释放掉锁，将 state 置为 0 if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; 考虑一下，如果一个线程在不持有 lock 的基础上，就去调用 condition1.await() 方法，它能进入条件队列，但是在上面的这个方法中，由于它不持有锁，release(savedState) 这个方法肯定要返回 false，进入到异常分支，然后进入 finally 块设置 node.waitStatus = Node.CANCELLED，这个已经入队的节点之后会被后继的节点”请出去“。 3. 等待进入阻塞队列释放掉锁以后，接下来是这段，这边会自旋，如果发现自己还没到阻塞队列，那么挂起，等待被转移到阻塞队列。 12345678910int interruptMode = 0;// 如果不在阻塞队列中，注意了，是阻塞队列while (!isOnSyncQueue(node)) &#123; // 线程挂起 LockSupport.park(this); // 这里可以先不用看了，等看到它什么时候被 unpark 再说 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break;&#125; isOnSyncQueue(Node node) 用于判断节点是否已经转移到阻塞队列了： 12345678910111213141516171819202122232425262728293031323334// 在节点入条件队列的时候，初始化时设置了 waitStatus = Node.CONDITION// 前面我提到，signal 的时候需要将节点从条件队列移到阻塞队列，// 这个方法就是判断 node 是否已经移动到阻塞队列了final boolean isOnSyncQueue(Node node) &#123; // 移动过去的时候，node 的 waitStatus 会置为 0，这个之后在说 signal 方法的时候会说到 // 如果 waitStatus 还是 Node.CONDITION，也就是 -2，那肯定就是还在条件队列中 // 如果 node 的前驱 prev 指向还是 null，说明肯定没有在 阻塞队列(prev是阻塞队列链表中使用的) if (node.waitStatus == Node.CONDITION || node.prev == null) return false; // 如果 node 已经有后继节点 next 的时候，那肯定是在阻塞队列了 if (node.next != null) return true; // 下面这个方法从阻塞队列的队尾开始从后往前遍历找，如果找到相等的，说明在阻塞队列，否则就是不在阻塞队列 // 可以通过判断 node.prev() != null 来推断出 node 在阻塞队列吗？答案是：不能。 // 这个可以看上篇 AQS 的入队方法，首先设置的是 node.prev 指向 tail， // 然后是 CAS 操作将自己设置为新的 tail，可是这次的 CAS 是可能失败的。 return findNodeFromTail(node);&#125;// 从阻塞队列的队尾往前遍历，如果找到，返回 trueprivate boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125;&#125; 回到前面的循环，isOnSyncQueue(node) 返回 false 的话，那么进到 LockSupport.park(this); 这里线程挂起。 4. signal 唤醒线程，转移到阻塞队列为了大家理解，这里我们先看唤醒操作，因为刚刚到 LockSupport.park(this); 把线程挂起了，等待唤醒。 唤醒操作通常由另一个线程来操作，就像生产者-消费者模式中，如果线程因为等待消费而挂起，那么当生产者生产了一个东西后，会调用 signal 唤醒正在等待的线程来消费。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 唤醒等待了最久的线程// 其实就是，将这个线程对应的 node 从条件队列转移到阻塞队列public final void signal() &#123; // 调用 signal 方法的线程必须持有当前的独占锁 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first);&#125;// 从条件队列队头往后遍历，找出第一个需要转移的 node// 因为前面我们说过，有些线程会取消排队，但是可能还在队列中private void doSignal(Node first) &#123; do &#123; // 将 firstWaiter 指向 first 节点后面的第一个，因为 first 节点马上要离开了 // 如果将 first 移除后，后面没有节点在等待了，那么需要将 lastWaiter 置为 null if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; // 因为 first 马上要被移到阻塞队列了，和条件队列的链接关系在这里断掉 first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null); // 这里 while 循环，如果 first 转移不成功，那么选择 first 后面的第一个节点进行转移，依此类推&#125;// 将节点从条件队列转移到阻塞队列// true 代表成功转移// false 代表在 signal 之前，节点已经取消了final boolean transferForSignal(Node node) &#123; // CAS 如果失败，说明此 node 的 waitStatus 已不是 Node.CONDITION，说明节点已经取消， // 既然已经取消，也就不需要转移了，方法返回，转移后面一个节点 // 否则，将 waitStatus 置为 0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; // enq(node): 自旋进入阻塞队列的队尾 // 注意，这里的返回值 p 是 node 在阻塞队列的前驱节点 Node p = enq(node); int ws = p.waitStatus; // ws &gt; 0 说明 node 在阻塞队列中的前驱节点取消了等待锁，直接唤醒 node 对应的线程。唤醒之后会怎么样，后面再解释 // 如果 ws &lt;= 0, 那么 compareAndSetWaitStatus 将会被调用，上篇介绍的时候说过，节点入队后，需要把前驱节点的状态设为 Node.SIGNAL(-1) if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 如果前驱节点取消或者 CAS 失败，会进到这里唤醒线程，之后的操作看下一节 LockSupport.unpark(node.thread); return true;&#125; 正常情况下，ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL) 这句中，ws &lt;= 0，而且 compareAndSetWaitStatus(p, ws, Node.SIGNAL) 会返回 true，所以一般也不会进去 if 语句块中唤醒 node 对应的线程。然后这个方法返回 true，也就意味着 signal 方法结束了，节点进入了阻塞队列。 假设发生了阻塞队列中的前驱节点取消等待，或者 CAS 失败，只要唤醒线程，让其进到下一步即可。 5. 唤醒后检查中断状态上一步 signal 之后，我们的线程由条件队列转移到了阻塞队列，之后就准备获取锁了。只要重新获取到锁了以后，继续往下执行。 等线程从挂起中恢复过来，继续往下看 12345678int interruptMode = 0;while (!isOnSyncQueue(node)) &#123; // 线程挂起 LockSupport.park(this); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break;&#125; 先解释下 interruptMode。interruptMode 可以取值为 REINTERRUPT（1），THROW_IE（-1），0 REINTERRUPT： 代表 await 返回的时候，需要重新设置中断状态 THROW_IE： 代表 await 返回的时候，需要抛出 InterruptedException 异常 0 ：说明在 await 期间，没有发生中断 有以下三种情况会让 LockSupport.park(this); 这句返回继续往下执行： 常规路径。signal -&gt; 转移节点到阻塞队列 -&gt; 获取了锁（unpark） 线程中断。在 park 的时候，另外一个线程对这个线程进行了中断 signal 的时候我们说过，转移以后的前驱节点取消了，或者对前驱节点的CAS操作失败了 假唤醒。这个也是存在的，和 Object.wait() 类似，都有这个问题 线程唤醒后第一步是调用 checkInterruptWhileWaiting(node) 这个方法，此方法用于判断是否在线程挂起期间发生了中断，如果发生了中断，是 signal 调用之前中断的，还是 signal 之后发生的中断。 12345678// 1\. 如果在 signal 之前已经中断，返回 THROW_IE// 2\. 如果是 signal 之后中断，返回 REINTERRUPT// 3\. 没有发生中断，返回 0private int checkInterruptWhileWaiting(Node node) &#123; return Thread.interrupted() ? (transferAfterCancelledWait(node) ? THROW_IE : REINTERRUPT) : 0;&#125; Thread.interrupted()：如果当前线程已经处于中断状态，那么该方法返回 true，同时将中断状态重置为 false，所以，才有后续的 重新中断（REINTERRUPT） 的使用。 看看怎么判断是 signal 之前还是之后发生的中断： 1234567891011121314151617181920// 只有线程处于中断状态，才会调用此方法// 如果需要的话，将这个已经取消等待的节点转移到阻塞队列// 返回 true：如果此线程在 signal 之前被取消，final boolean transferAfterCancelledWait(Node node) &#123; // 用 CAS 将节点状态设置为 0 // 如果这步 CAS 成功，说明是 signal 方法之前发生的中断，因为如果 signal 先发生的话，signal 中会将 waitStatus 设置为 0 if (compareAndSetWaitStatus(node, Node.CONDITION, 0)) &#123; // 将节点放入阻塞队列 // 这里我们看到，即使中断了，依然会转移到阻塞队列 enq(node); return true; &#125; // 到这里是因为 CAS 失败，肯定是因为 signal 方法已经将 waitStatus 设置为了 0 // signal 方法会将节点转移到阻塞队列，但是可能还没完成，这边自旋等待其完成 // 当然，这种事情还是比较少的吧：signal 调用之后，没完成转移之前，发生了中断 while (!isOnSyncQueue(node)) Thread.yield(); return false;&#125; 这里再说一遍，即使发生了中断，节点依然会转移到阻塞队列。 到这里，大家应该都知道这个 while 循环怎么退出了吧。要么中断，要么转移成功。 这里描绘了一个场景，本来有个线程，它是排在条件队列的后面的，但是因为它被中断了，那么它会被唤醒，然后它发现自己不是被 signal 的那个，但是它会自己主动去进入到阻塞队列。 6. 获取独占锁while 循环出来以后，下面是这段代码： 12if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; 由于 while 出来后，我们确定节点已经进入了阻塞队列，准备获取锁。 这里的 acquireQueued(node, savedState) 的第一个参数 node 之前已经经过 enq(node) 进入了队列，参数 savedState 是之前释放锁前的 state，这个方法返回的时候，代表当前线程获取了锁，而且 state == savedState了。 注意，前面我们说过，不管有没有发生中断，都会进入到阻塞队列，而 acquireQueued(node, savedState) 的返回值就是代表线程是否被中断。如果返回 true，说明被中断了，而且 interruptMode != THROW_IE，说明在 signal 之前就发生中断了，这里将 interruptMode 设置为 REINTERRUPT，用于待会重新中断。 继续往下： 1234if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters();if (interruptMode != 0) reportInterruptAfterWait(interruptMode); 本着一丝不苟的精神，这边说说 node.nextWaiter != null 怎么满足。我前面也说了 signal 的时候会将节点转移到阻塞队列，有一步是 node.nextWaiter = null，将断开节点和条件队列的联系。 可是，在判断发生中断的情况下，是 signal 之前还是之后发生的？ 这部分的时候，我也介绍了，如果 signal 之前就中断了，也需要将节点进行转移到阻塞队列，这部分转移的时候，是没有设置 node.nextWaiter = null 的。 之前我们说过，如果有节点取消，也会调用 unlinkCancelledWaiters 这个方法，就是这里了。 7. 处理中断状态到这里，我们终于可以好好说下这个 interruptMode 干嘛用了。 0：什么都不做，没有被中断过； THROW_IE：await 方法抛出 InterruptedException 异常，因为它代表在 await() 期间发生了中断； REINTERRUPT：重新中断当前线程，因为它代表 await() 期间没有被中断，而是 signal() 以后发生的中断 1234567private void reportInterruptAfterWait(int interruptMode) throws InterruptedException &#123; if (interruptMode == THROW_IE) throw new InterruptedException(); else if (interruptMode == REINTERRUPT) selfInterrupt();&#125; 这个中断状态这部分内容，大家应该都理解了吧，不理解的话，多看几遍就是了。 * 带超时机制的 await经过前面的 7 步，整个 ConditionObject 类基本上都分析完了，接下来简单分析下带超时机制的 await 方法。 123456public final long awaitNanos(long nanosTimeout) throws InterruptedExceptionpublic final boolean awaitUntil(Date deadline) throws InterruptedExceptionpublic final boolean await(long time, TimeUnit unit) throws InterruptedException 这三个方法都差不多，我们就挑一个出来看看吧： 123456789101112131415161718192021222324252627282930313233343536373839public final boolean await(long time, TimeUnit unit) throws InterruptedException &#123; // 等待这么多纳秒 long nanosTimeout = unit.toNanos(time); if (Thread.interrupted()) throw new InterruptedException(); Node node = addConditionWaiter(); int savedState = fullyRelease(node); // 当前时间 + 等待时长 = 过期时间 final long deadline = System.nanoTime() + nanosTimeout; // 用于返回 await 是否超时 boolean timedout = false; int interruptMode = 0; while (!isOnSyncQueue(node)) &#123; // 时间到啦 if (nanosTimeout &lt;= 0L) &#123; // 这里因为要 break 取消等待了。取消等待的话一定要调用 transferAfterCancelledWait(node) 这个方法 // 如果这个方法返回 true，在这个方法内，将节点转移到阻塞队列成功 // 返回 false 的话，说明 signal 已经发生，signal 方法将节点转移了。也就是说没有超时嘛 timedout = transferAfterCancelledWait(node); break; &#125; // spinForTimeoutThreshold 的值是 1000 纳秒，也就是 1 毫秒 // 也就是说，如果不到 1 毫秒了，那就不要选择 parkNanos 了，自旋的性能反而更好 if (nanosTimeout &gt;= spinForTimeoutThreshold) LockSupport.parkNanos(this, nanosTimeout); if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; // 得到剩余时间 nanosTimeout = deadline - System.nanoTime(); &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode); return !timedout;&#125; 超时的思路还是很简单的，不带超时参数的 await 是 park，然后等待别人唤醒。而现在就是调用 parkNanos 方法来休眠指定的时间，醒来后判断是否 signal 调用了，调用了就是没有超时，否则就是超时了。超时的话，自己来进行转移到阻塞队列，然后抢锁。 * 不抛出 InterruptedException 的 await关于 Condition 最后一小节了。 123456789101112public final void awaitUninterruptibly() &#123; Node node = addConditionWaiter(); int savedState = fullyRelease(node); boolean interrupted = false; while (!isOnSyncQueue(node)) &#123; LockSupport.park(this); if (Thread.interrupted()) interrupted = true; &#125; if (acquireQueued(node, savedState) || interrupted) selfInterrupt();&#125; 很简单，贴一下代码大家就都懂了，我就不废话了。 AbstractQueuedSynchronizer 独占锁的取消排队这篇文章说的是 AbstractQueuedSynchronizer，只不过好像 Condition 说太多了，赶紧把思路拉回来。 接下来，我想说说怎么取消对锁的竞争？ 上篇文章提到过，最重要的方法是这个，我们要在这里面找答案： 123456789101112131415161718192021final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 首先，到这个方法的时候，节点一定是入队成功的。 我把 parkAndCheckInterrupt() 代码贴过来： 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; 这两段代码联系起来看，是不是就清楚了。 如果我们要取消一个线程的排队，我们需要在另外一个线程中对其进行中断。比如某线程调用 lock() 老久不返回，我想中断它。一旦对其进行中断，此线程会从 LockSupport.park(this); 中唤醒，然后 Thread.interrupted(); 返回 true。 我们发现一个问题，即使是中断唤醒了这个线程，也就只是设置了 interrupted = true 然后继续下一次循环。而且，由于 Thread.interrupted(); 会清除中断状态，第二次进 parkAndCheckInterrupt 的时候，返回会是 false。 所以，我们要看到，在这个方法中，interrupted 只是用来记录是否发生了中断，然后用于方法返回值，其他没有做任何相关事情。 所以，我们看外层方法怎么处理 acquireQueued 返回 false 的情况。 12345678public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125;static void selfInterrupt() &#123; Thread.currentThread().interrupt();&#125; 所以说，lock() 方法处理中断的方法就是，你中断归中断，我抢锁还是照样抢锁，几乎没关系，只是我抢到锁了以后，设置线程的中断状态而已，也不抛出任何异常出来。调用者获取锁后，可以去检查是否发生过中断，也可以不理会。 来条分割线。有没有被骗的感觉，我说了一大堆，可是和取消没有任何关系啊。 我们来看 ReentrantLock 的另一个 lock 方法： 123public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125; 方法上多了个 throws InterruptedException ，经过前面那么多知识的铺垫，这里我就不再啰里啰嗦了。 1234567public final void acquireInterruptibly(int arg) throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); if (!tryAcquire(arg)) doAcquireInterruptibly(arg);&#125; 继续往里： 12345678910111213141516171819202122232425private void doAcquireInterruptibly(int arg) throws InterruptedException &#123; final Node node = addWaiter(Node.EXCLUSIVE); boolean failed = true; try &#123; for (;;) &#123; final Node p = node.predecessor(); if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return; &#125; if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) // 就是这里了，一旦异常，马上结束这个方法，抛出异常。 // 这里不再只是标记这个方法的返回值代表中断状态 // 而是直接抛出异常，而且外层也不捕获，一直往外抛到 lockInterruptibly throw new InterruptedException(); &#125; &#125; finally &#123; // 如果通过 InterruptedException 异常出去，那么 failed 就是 true 了 if (failed) cancelAcquire(node); &#125;&#125; 既然到这里了，顺便说说 cancelAcquire 这个方法吧： 1234567891011121314151617181920212223242526272829303132333435363738private void cancelAcquire(Node node) &#123; // Ignore if node doesn&apos;t exist if (node == null) return; node.thread = null; // Skip cancelled predecessors // 找一个合适的前驱。其实就是将它前面的队列中已经取消的节点都”请出去“ Node pred = node.prev; while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // predNext is the apparent node to unsplice. CASes below will // fail if not, in which case, we lost race vs another cancel // or signal, so no further action is necessary. Node predNext = pred.next; // Can use unconditional write instead of CAS here. // After this atomic step, other Nodes can skip past us. // Before, we are free of interference from other threads. node.waitStatus = Node.CANCELLED; // If we are the tail, remove ourselves. if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; // If successor needs signal, try to set pred&apos;s next-link // so it will get one. Otherwise wake it up to propagate. int ws; if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 其实这个方法没什么好说的，一行行看下去就是了，节点取消，只要把 waitStatus 设置为 Node.CANCELLED，会有非常多的情况被从阻塞队列中请出去，主动或被动。 再说 java 线程中断和 InterruptedException 异常在之前的文章中，我们接触了大量的中断，这边算是个总结吧。如果你完全熟悉中断了，没有必要再看这节，本节为新手而写。 线程中断首先，我们要明白，中断不是类似 linux 里面的命令 kill -9 pid，不是说我们中断某个线程，这个线程就停止运行了。中断代表线程状态，每个线程都关联了一个中断状态，是一个 true 或 false 的 boolean 值，初始值为 false。 Java 中的中断和操作系统的中断还不一样，这里就按照状态来理解吧，不要和操作系统的中断联系在一起 关于中断状态，我们需要重点关注 Thread 类中的以下几个方法： 12345678910// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态public boolean isInterrupted() &#123;&#125;// Thread 中的静态方法，检测调用这个方法的线程是否已经中断// 注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false// 所以，如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了public static boolean interrupted() &#123;&#125;// Thread 类中的实例方法，用于设置一个线程的中断状态为 truepublic void interrupt() &#123;&#125; 我们说中断一个线程，其实就是设置了线程的 interrupted status 为 true，至于说被中断的线程怎么处理这个状态，那是那个线程自己的事。如以下代码： 1234while (!Thread.interrupted()) &#123; doWork(); System.out.println(&quot;我做完一件事了，准备做下一件，如果没有其他线程中断我的话&quot;);&#125; 这种代码就是会响应中断的，它会在干活的时候先判断下中断状态，不过，除了 JDK 源码外，其他用中断的场景还是比较少的，毕竟 JDK 源码非常讲究。 当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。 如果线程处于以下三种情况，那么当线程被中断的时候，能自动感知到： 来自 Object 类的 wait()、wait(long)、wait(long, int)， 来自 Thread 类的 join()、join(long)、join(long, int)、sleep(long)、sleep(long, int) 这几个方法的相同之处是，方法上都有: throws InterruptedException 如果线程阻塞在这些方法上（我们知道，这些方法会让当前线程阻塞），这个时候如果其他线程对这个线程进行了中断，那么这个线程会从这些方法中立即返回，抛出 InterruptedException 异常，同时重置中断状态为 false。 实现了 InterruptibleChannel 接口的类中的一些 I/O 阻塞操作，如 DatagramChannel 中的 connect 方法和 receive 方法等 如果线程阻塞在这里，中断线程会导致这些方法抛出 ClosedByInterruptException 并重置中断状态。 Selector 中的 select 方法，参考下我写的 NIO 的文章 一旦中断，方法立即返回 对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。 那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。 InterruptedException 概述它是一个特殊的异常，不是说 JVM 对其有特殊的处理，而是它的使用场景比较特殊。通常，我们可以看到，像 Object 中的 wait() 方法，ReentrantLock 中的 lockInterruptibly() 方法，Thread 中的 sleep() 方法等等，这些方法都带有 throws InterruptedException，我们通常称这些方法为阻塞方法（blocking method）。 阻塞方法一个很明显的特征是，它们需要花费比较长的时间（不是绝对的，只是说明时间不可控），还有它们的方法结束返回往往依赖于外部条件，如 wait 方法依赖于其他线程的 notify，lock 方法依赖于其他线程的 unlock等等。 当我们看到方法上带有 throws InterruptedException 时，我们就要知道，这个方法应该是阻塞方法，我们如果希望它能早点返回的话，我们往往可以通过中断来实现。 除了几个特殊类（如 Object，Thread等）外，感知中断并提前返回是通过轮询中断状态来实现的。我们自己需要写可中断的方法的时候，就是通过在合适的时机（通常在循环的开始处）去判断线程的中断状态，然后做相应的操作（通常是方法直接返回或者抛出异常）。当然，我们也要看到，如果我们一次循环花的时间比较长的话，那么就需要比较长的时间才能感知到线程中断了。 处理中断一旦中断发生，我们接收到了这个信息，然后怎么去处理中断呢？本小节将简单分析这个问题。 我们经常会这么写代码： 123456try &#123; Thread.sleep(10000);&#125; catch (InterruptedException e) &#123; // ignore&#125;// go on 当 sleep 结束继续往下执行的时候，我们往往都不知道这块代码是真的 sleep 了 10 秒，还是只休眠了 1 秒就被中断了。这个代码的问题在于，我们将这个异常信息吞掉了。（对于 sleep 方法，我相信大部分情况下，我们都不在意是否是中断了，这里是举例） AQS 的做法很值得我们借鉴，我们知道 ReentrantLock 有两种 lock 方法： 1234567public void lock() &#123; sync.lock();&#125;public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1);&#125; 前面我们提到过，lock() 方法不响应中断。如果 thread1 调用了 lock() 方法，过了很久还没抢到锁，这个时候 thread2 对其进行了中断，thread1 是不响应这个请求的，它会继续抢锁，当然它不会把“被中断”这个信息扔掉。我们可以看以下代码： 1234567public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // 我们看到，这里也没做任何特殊处理，就是记录下来中断状态。 // 这样，如果外层方法需要去检测的时候，至少我们没有把这个信息丢了 selfInterrupt();// Thread.currentThread().interrupt();&#125; 而对于 lockInterruptibly() 方法，因为其方法上面有 throws InterruptedException ，这个信号告诉我们，如果我们要取消线程抢锁，直接中断这个线程即可，它会立即返回，抛出 InterruptedException 异常。 在并发包中，有非常多的这种处理中断的例子，提供两个方法，分别为响应中断和不响应中断，对于不响应中断的方法，记录中断而不是丢失这个信息。如 Condition 中的两个方法就是这样的： 12void await() throws InterruptedException;void awaitUninterruptibly(); 通常，如果方法会抛出 InterruptedException 异常，往往方法体的第一句就是： 123456&gt; public final void await() throws InterruptedException &#123;&gt; if (Thread.interrupted())&gt; throw new InterruptedException();&gt; ...... &gt; &#125;&gt; 熟练使用中断，对于我们写出优雅的代码是有帮助的，也有助于我们分析别人的源码。 总结这篇文章的信息量真的很大，如果你花了时间，还是没有看懂，那是我的错了。 欢迎大家向我提问，我不一定能每次都及时出现，我出现也不一定能解决大家的问题，欢迎探讨。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南7：JUC的核心类AQS详解]]></title>
    <url>%2F2019%2F10%2F07%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%977%EF%BC%9AJUC%E7%9A%84%E6%A0%B8%E5%BF%83%E7%B1%BBAQS%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文转自：https://www.javadoop.com/post/AbstractQueuedSynchronizer#toc4 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 简介在分析 Java 并发包 java.util.concurrent 源码的时候，少不了需要了解 AbstractQueuedSynchronizer（以下简写AQS）这个抽象类，因为它是 Java 并发包的基础工具类，是实现 ReentrantLock、CountDownLatch、Semaphore、FutureTask 等类的基础。 Google 一下 AbstractQueuedSynchronizer，我们可以找到很多关于 AQS 的介绍，但是很多都没有介绍清楚，因为大部分文章没有把其中的一些关键的细节说清楚。 本文将从 ReentrantLock 的公平锁源码出发，分析下 AbstractQueuedSynchronizer 这个类是怎么工作的，希望能给大家提供一些简单的帮助。 申明以下几点： 本文有点长，但还是挺简单，主要面向读者对象为并发编程的初学者，或者想要阅读 Java 并发包源码的开发者。对于新手来说，可能需要花好几个小时才能完全看懂，但是这时间肯定是值得的。 源码环境 JDK1.7（1.8没啥变化），看到不懂或有疑惑的部分，最好能自己打开源码看看。Doug Lea 大神的代码写得真心不错。 本文不分析共享模式，这样可以给读者减少很多负担，第三篇文章对共享模式进行了分析。而且也不分析 condition 部分，所以应该说很容易就可以看懂了。 本文大量使用我们平时用得最多的 ReentrantLock 的概念，本质上来说是不正确的，读者应该清楚，AQS 不仅仅用来实现可重入锁，只是希望读者可以用锁来联想 AQS 的使用场景，降低阅读压力。 ReentrantLock 的公平锁和非公平锁只有一点点区别，第二篇文章做了介绍。 评论区有读者反馈本文直接用代码说不友好，应该多配点流程图，这篇文章确实有这个问题。但是作为过来人，我想告诉大家，对于 AQS 来说，形式真的不重要，重要的是把细节说清楚。 AQS 结构先来看看 AQS 有哪些属性，搞清楚这些基本就知道 AQS 是什么套路了，毕竟可以猜嘛！ 1234567891011121314// 头结点，你直接把它当做 当前持有锁的线程 可能是最好理解的private transient volatile Node head;// 阻塞的尾节点，每个新的节点进来，都插入到最后，也就形成了一个链表private transient volatile Node tail;// 这个是最重要的，代表当前锁的状态，0代表没有被占用，大于 0 代表有线程持有当前锁// 这个值可以大于 1，是因为锁可以重入，每次重入都加上 1private volatile int state;// 代表当前持有独占锁的线程，举个最重要的使用例子，因为锁可以重入// reentrantLock.lock()可以嵌套调用多次，所以每次用这个来判断当前线程是否已经拥有了锁// if (currentThread == getExclusiveOwnerThread()) &#123;state++&#125;private transient Thread exclusiveOwnerThread; //继承自AbstractOwnableSynchronizer 怎么样，看样子应该是很简单的吧，毕竟也就四个属性啊。 AbstractQueuedSynchronizer 的等待队列示意如下所示，注意了，之后分析过程中所说的 queue，也就是阻塞队列不包含 head，不包含 head，不包含 head。 等待队列中每个线程被包装成一个 Node 实例，数据结构是链表，一起看看源码吧： 123456789101112131415161718192021222324252627282930313233343536static final class Node &#123; // 标识节点当前在共享模式下 static final Node SHARED = new Node(); // 标识节点当前在独占模式下 static final Node EXCLUSIVE = null; // ======== 下面的几个int常量是给waitStatus用的 =========== /** waitStatus value to indicate thread has cancelled */ // 代码此线程取消了争抢这个锁 static final int CANCELLED = 1; /** waitStatus value to indicate successor&apos;s thread needs unparking */ // 官方的描述是，其表示当前node的后继节点对应的线程需要被唤醒 static final int SIGNAL = -1; /** waitStatus value to indicate thread is waiting on condition */ // 本文不分析condition，所以略过吧，下一篇文章会介绍这个 static final int CONDITION = -2; /** * waitStatus value to indicate the next acquireShared should * unconditionally propagate */ // 同样的不分析，略过吧 static final int PROPAGATE = -3; // ===================================================== // 取值为上面的1、-1、-2、-3，或者0(以后会讲到) // 这么理解，暂时只需要知道如果这个值 大于0 代表此线程取消了等待， // ps: 半天抢不到锁，不抢了，ReentrantLock是可以指定timeouot的。。。 volatile int waitStatus; // 前驱节点的引用 volatile Node prev; // 后继节点的引用 volatile Node next; // 这个就是线程本尊 volatile Thread thread;&#125; Node 的数据结构其实也挺简单的，就是 thread + waitStatus + pre + next 四个属性而已，大家先要有这个概念在心里。 上面的是基础知识，后面会多次用到，心里要时刻记着它们，心里想着这个结构图就可以了。下面，我们开始说 ReentrantLock 的公平锁。再次强调，我说的阻塞队列不包含 head 节点。 首先，我们先看下 ReentrantLock 的使用方式。 123456789101112131415161718192021// 我用个web开发中的service概念吧public class OrderService &#123; // 使用static，这样每个线程拿到的是同一把锁，当然，spring mvc中service默认就是单例，别纠结这个 private static ReentrantLock reentrantLock = new ReentrantLock(true); public void createOrder() &#123; // 比如我们同一时间，只允许一个线程创建订单 reentrantLock.lock(); // 通常，lock 之后紧跟着 try 语句 try &#123; // 这块代码同一时间只能有一个线程进来(获取到锁的线程)， // 其他的线程在lock()方法上阻塞，等待获取到锁，再进来 // 执行代码... // 执行代码... // 执行代码... &#125; finally &#123; // 释放锁 reentrantLock.unlock(); &#125; &#125;&#125; ReentrantLock 在内部用了内部类 Sync 来管理锁，所以真正的获取锁和释放锁是由 Sync 的实现类来控制的。 12abstract static class Sync extends AbstractQueuedSynchronizer &#123;&#125; Sync 有两个实现，分别为 NonfairSync（非公平锁）和 FairSync（公平锁），我们看 FairSync 部分。 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 线程抢锁很多人肯定开始嫌弃上面废话太多了，下面跟着代码走，我就不废话了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259static final class FairSync extends Sync &#123; private static final long serialVersionUID = -3000897897090466540L; // 争锁 final void lock() &#123; acquire(1); &#125; // 来自父类AQS，我直接贴过来这边，下面分析的时候同样会这样做，不会给读者带来阅读压力 // 我们看到，这个方法，如果tryAcquire(arg) 返回true, 也就结束了。 // 否则，acquireQueued方法会将线程压到队列中 public final void acquire(int arg) &#123; // 此时 arg == 1 // 首先调用tryAcquire(1)一下，名字上就知道，这个只是试一试 // 因为有可能直接就成功了呢，也就不需要进队列排队了， // 对于公平锁的语义就是：本来就没人持有锁，根本没必要进队列等待(又是挂起，又是等待被唤醒的) if (!tryAcquire(arg) &amp;&amp; // tryAcquire(arg)没有成功，这个时候需要把当前线程挂起，放到阻塞队列中。 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) &#123; selfInterrupt(); &#125; &#125; /** * Fair version of tryAcquire. Don&apos;t grant access unless * recursive call or no waiters or is first. */ // 尝试直接获取锁，返回值是boolean，代表是否获取到锁 // 返回true：1.没有线程在等待锁；2.重入锁，线程本来就持有锁，也就可以理所当然可以直接获取 protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); // state == 0 此时此刻没有线程持有锁 if (c == 0) &#123; // 虽然此时此刻锁是可以用的，但是这是公平锁，既然是公平，就得讲究先来后到， // 看看有没有别人在队列中等了半天了 if (!hasQueuedPredecessors() &amp;&amp; // 如果没有线程在等待，那就用CAS尝试一下，成功了就获取到锁了， // 不成功的话，只能说明一个问题，就在刚刚几乎同一时刻有个线程抢先了 =_= // 因为刚刚还没人的，我判断过了 compareAndSetState(0, acquires)) &#123; // 到这里就是获取到锁了，标记一下，告诉大家，现在是我占用了锁 setExclusiveOwnerThread(current); return true; &#125; &#125; // 会进入这个else if分支，说明是重入了，需要操作：state=state+1 // 这里不存在并发问题 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; // 如果到这里，说明前面的if和else if都没有返回true，说明没有获取到锁 // 回到上面一个外层调用方法继续看: // if (!tryAcquire(arg) // &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // selfInterrupt(); return false; &#125; // 假设tryAcquire(arg) 返回false，那么代码将执行： // acquireQueued(addWaiter(Node.EXCLUSIVE), arg)， // 这个方法，首先需要执行：addWaiter(Node.EXCLUSIVE) /** * Creates and enqueues node for current thread and given mode. * * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared * @return the new node */ // 此方法的作用是把线程包装成node，同时进入到队列中 // 参数mode此时是Node.EXCLUSIVE，代表独占模式 private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure // 以下几行代码想把当前node加到链表的最后面去，也就是进到阻塞队列的最后 Node pred = tail; // tail!=null =&gt; 队列不为空(tail==head的时候，其实队列是空的，不过不管这个吧) if (pred != null) &#123; // 将当前的队尾节点，设置为自己的前驱 node.prev = pred; // 用CAS把自己设置为队尾, 如果成功后，tail == node 了，这个节点成为阻塞队列新的尾巴 if (compareAndSetTail(pred, node)) &#123; // 进到这里说明设置成功，当前node==tail, 将自己与之前的队尾相连， // 上面已经有 node.prev = pred，加上下面这句，也就实现了和之前的尾节点双向连接了 pred.next = node; // 线程入队了，可以返回了 return node; &#125; &#125; // 仔细看看上面的代码，如果会到这里， // 说明 pred==null(队列是空的) 或者 CAS失败(有线程在竞争入队) // 读者一定要跟上思路，如果没有跟上，建议先不要往下读了，往回仔细看，否则会浪费时间的 enq(node); return node; &#125; /** * Inserts node into queue, initializing if necessary. See picture above. * @param node the node to insert * @return node&apos;s predecessor */ // 采用自旋的方式入队 // 之前说过，到这个方法只有两种可能：等待队列为空，或者有线程竞争入队， // 自旋在这边的语义是：CAS设置tail过程中，竞争一次竞争不到，我就多次竞争，总会排到的 private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; // 之前说过，队列为空也会进来这里 if (t == null) &#123; // Must initialize // 初始化head节点 // 细心的读者会知道原来 head 和 tail 初始化的时候都是 null 的 // 还是一步CAS，你懂的，现在可能是很多线程同时进来呢 if (compareAndSetHead(new Node())) // 给后面用：这个时候head节点的waitStatus==0, 看new Node()构造方法就知道了 // 这个时候有了head，但是tail还是null，设置一下， // 把tail指向head，放心，马上就有线程要来了，到时候tail就要被抢了 // 注意：这里只是设置了tail=head，这里可没return哦，没有return，没有return // 所以，设置完了以后，继续for循环，下次就到下面的else分支了 tail = head; &#125; else &#123; // 下面几行，和上一个方法 addWaiter 是一样的， // 只是这个套在无限循环里，反正就是将当前线程排到队尾，有线程竞争的话排不上重复排 node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125; &#125; // 现在，又回到这段代码了 // if (!tryAcquire(arg) // &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) // selfInterrupt(); // 下面这个方法，参数node，经过addWaiter(Node.EXCLUSIVE)，此时已经进入阻塞队列 // 注意一下：如果acquireQueued(addWaiter(Node.EXCLUSIVE), arg))返回true的话， // 意味着上面这段代码将进入selfInterrupt()，所以正常情况下，下面应该返回false // 这个方法非常重要，应该说真正的线程挂起，然后被唤醒后去获取锁，都在这个方法里了 final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); // p == head 说明当前节点虽然进到了阻塞队列，但是是阻塞队列的第一个，因为它的前驱是head // 注意，阻塞队列不包含head节点，head一般指的是占有锁的线程，head后面的才称为阻塞队列 // 所以当前节点可以去试抢一下锁 // 这里我们说一下，为什么可以去试试： // 首先，它是队头，这个是第一个条件，其次，当前的head有可能是刚刚初始化的node， // enq(node) 方法里面有提到，head是延时初始化的，而且new Node()的时候没有设置任何线程 // 也就是说，当前的head不属于任何一个线程，所以作为队头，可以去试一试， // tryAcquire已经分析过了, 忘记了请往前看一下，就是简单用CAS试操作一下state if (p == head &amp;&amp; tryAcquire(arg)) &#123; setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; // 到这里，说明上面的if分支没有成功，要么当前node本来就不是队头， // 要么就是tryAcquire(arg)没有抢赢别人，继续往下看 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; // 什么时候 failed 会为 true??? // tryAcquire() 方法抛异常的情况 if (failed) cancelAcquire(node); &#125; &#125; /** * Checks and updates status for a node that failed to acquire. * Returns true if thread should block. This is the main signal * control in all acquire loops. Requires that pred == node.prev * * @param pred node&apos;s predecessor holding status * @param node the node * @return &#123;@code true&#125; if thread should block */ // 刚刚说过，会到这里就是没有抢到锁呗，这个方法说的是：&quot;当前线程没有抢到锁，是否需要挂起当前线程？&quot; // 第一个参数是前驱节点，第二个参数才是代表当前线程的节点 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; // 前驱节点的 waitStatus == -1 ，说明前驱节点状态正常，当前线程需要挂起，直接可以返回true if (ws == Node.SIGNAL) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true; // 前驱节点 waitStatus大于0 ，之前说过，大于0 说明前驱节点取消了排队。 // 这里需要知道这点：进入阻塞队列排队的线程会被挂起，而唤醒的操作是由前驱节点完成的。 // 所以下面这块代码说的是将当前节点的prev指向waitStatus&lt;=0的节点， // 简单说，就是为了找个好爹，因为你还得依赖它来唤醒呢，如果前驱节点取消了排队， // 找前驱节点的前驱节点做爹，往前遍历总能找到一个好爹的 if (ws &gt; 0) &#123; /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don&apos;t park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ // 仔细想想，如果进入到这个分支意味着什么 // 前驱节点的waitStatus不等于-1和1，那也就是只可能是0，-2，-3 // 在我们前面的源码中，都没有看到有设置waitStatus的，所以每个新的node入队时，waitStatu都是0 // 正常情况下，前驱节点是之前的 tail，那么它的 waitStatus 应该是 0 // 用CAS将前驱节点的waitStatus设置为Node.SIGNAL(也就是-1) compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; // 这个方法返回 false，那么会再走一次 for 循序， // 然后再次进来此方法，此时会从第一个分支返回 true return false; &#125; // private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) // 这个方法结束根据返回值我们简单分析下： // 如果返回true, 说明前驱节点的waitStatus==-1，是正常情况，那么当前线程需要被挂起，等待以后被唤醒 // 我们也说过，以后是被前驱节点唤醒，就等着前驱节点拿到锁，然后释放锁的时候叫你好了 // 如果返回false, 说明当前不需要被挂起，为什么呢？往后看 // 跳回到前面是这个方法 // if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; // parkAndCheckInterrupt()) // interrupted = true; // 1\. 如果shouldParkAfterFailedAcquire(p, node)返回true， // 那么需要执行parkAndCheckInterrupt(): // 这个方法很简单，因为前面返回true，所以需要挂起线程，这个方法就是负责挂起线程的 // 这里用了LockSupport.park(this)来挂起线程，然后就停在这里了，等待被唤醒======= private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted(); &#125; // 2\. 接下来说说如果shouldParkAfterFailedAcquire(p, node)返回false的情况 // 仔细看shouldParkAfterFailedAcquire(p, node)，我们可以发现，其实第一次进来的时候，一般都不会返回true的，原因很简单，前驱节点的waitStatus=-1是依赖于后继节点设置的。也就是说，我都还没给前驱设置-1呢，怎么可能是true呢，但是要看到，这个方法是套在循环里的，所以第二次进来的时候状态就是-1了。 // 解释下为什么shouldParkAfterFailedAcquire(p, node)返回false的时候不直接挂起线程： // =&gt; 是为了应对在经过这个方法后，node已经是head的直接后继节点了。剩下的读者自己想想吧。&#125; 说到这里，也就明白了，多看几遍 final boolean acquireQueued(final Node node, int arg) 这个方法吧。自己推演下各个分支怎么走，哪种情况下会发生什么，走到哪里。 解锁操作最后，就是还需要介绍下唤醒的动作了。我们知道，正常情况下，如果线程没获取到锁，线程会被 LockSupport.park(this); 挂起停止，等待被唤醒。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 唤醒的代码还是比较简单的，你如果上面加锁的都看懂了，下面都不需要看就知道怎么回事了public void unlock() &#123; sync.release(1);&#125;public final boolean release(int arg) &#123; // 往后看吧 if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125;// 回到ReentrantLock看tryRelease方法protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); // 是否完全释放锁 boolean free = false; // 其实就是重入的问题，如果c==0，也就是说没有嵌套锁了，可以释放了，否则还不能释放掉 if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free;&#125;/** * Wakes up node&apos;s successor, if one exists. * * @param node the node */// 唤醒后继节点// 从上面调用处知道，参数node是head头结点private void unparkSuccessor(Node node) &#123; /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node.waitStatus; // 如果head节点当前waitStatus&lt;0, 将其修改为0 if (ws &lt; 0) compareAndSetWaitStatus(node, ws, 0); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ // 下面的代码就是唤醒后继节点，但是有可能后继节点取消了等待（waitStatus==1） // 从队尾往前找，找到waitStatus&lt;=0的所有节点中排在最前面的 Node s = node.next; if (s == null || s.waitStatus &gt; 0) &#123; s = null; // 从后往前找，仔细看代码，不必担心中间有节点取消(waitStatus==1)的情况 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 唤醒线程 LockSupport.unpark(s.thread);&#125; 唤醒线程以后，被唤醒的线程将从以下代码中继续往前走： 12345private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); // 刚刚线程被挂起在这里了 return Thread.interrupted();&#125;// 又回到这个方法了：acquireQueued(final Node node, int arg)，这个时候，node的前驱是head了 好了，后面就不分析源码了，剩下的还有问题自己去仔细看看代码吧。 总结总结一下吧。 在并发环境下，加锁和解锁需要以下三个部件的协调： 锁状态。我们要知道锁是不是被别的线程占有了，这个就是 state 的作用，它为 0 的时候代表没有线程占有锁，可以去争抢这个锁，用 CAS 将 state 设为 1，如果 CAS 成功，说明抢到了锁，这样其他线程就抢不到了，如果锁重入的话，state进行 +1 就可以，解锁就是减 1，直到 state 又变为 0，代表释放锁，所以 lock() 和 unlock() 必须要配对啊。然后唤醒等待队列中的第一个线程，让其来占有锁。 线程的阻塞和解除阻塞。AQS 中采用了 LockSupport.park(thread) 来挂起线程，用 unpark 来唤醒线程。 阻塞队列。因为争抢锁的线程可能很多，但是只能有一个线程拿到锁，其他的线程都必须等待，这个时候就需要一个 queue 来管理这些线程，AQS 用的是一个 FIFO 的队列，就是一个链表，每个 node 都持有后继节点的引用。AQS 采用了 CLH 锁的变体来实现，感兴趣的读者可以参考这篇文章关于CLH的介绍，写得简单明了。 示例图解析下面属于回顾环节，用简单的示例来说一遍，如果上面的有些东西没看懂，这里还有一次帮助你理解的机会。 首先，第一个线程调用 reentrantLock.lock()，翻到最前面可以发现，tryAcquire(1) 直接就返回 true 了，结束。只是设置了 state=1，连 head 都没有初始化，更谈不上什么阻塞队列了。要是线程 1 调用 unlock() 了，才有线程 2 来，那世界就太太太平了，完全没有交集嘛，那我还要 AQS 干嘛。 如果线程 1 没有调用 unlock() 之前，线程 2 调用了 lock(), 想想会发生什么？ 线程 2 会初始化 head【new Node()】，同时线程 2 也会插入到阻塞队列并挂起 (注意看这里是一个 for 循环，而且设置 head 和 tail 的部分是不 return 的，只有入队成功才会跳出循环) 123456789101112131415private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; 首先，是线程 2 初始化 head 节点，此时 head==tail, waitStatus==0 然后线程 2 入队： 同时我们也要看此时节点的 waitStatus，我们知道 head 节点是线程 2 初始化的，此时的 waitStatus 没有设置， java 默认会设置为 0，但是到 shouldParkAfterFailedAcquire 这个方法的时候，线程 2 会把前驱节点，也就是 head 的waitStatus设置为 -1。 那线程 2 节点此时的 waitStatus 是多少呢，由于没有设置，所以是 0； 如果线程 3 此时再进来，直接插到线程 2 的后面就可以了，此时线程 3 的 waitStatus 是 0，到 shouldParkAfterFailedAcquire 方法的时候把前驱节点线程 2 的 waitStatus 设置为 -1。 这里可以简单说下 waitStatus 中 SIGNAL(-1) 状态的意思，Doug Lea 注释的是：代表后继节点需要被唤醒。也就是说这个 waitStatus 其实代表的不是自己的状态，而是后继节点的状态，我们知道，每个 node 在入队的时候，都会把前驱节点的状态改为 SIGNAL，然后阻塞，等待被前驱唤醒。这里涉及的是两个问题：有线程取消了排队、唤醒操作。其实本质是一样的，读者也可以顺着 “waitStatus代表后继节点的状态” 这种思路去看一遍源码。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南6：Java内存模型JMM总结]]></title>
    <url>%2F2019%2F10%2F06%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%976%EF%BC%9AJava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8BJMM%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文转自 https://www.cnblogs.com/kukri/p/9109639.html 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 简介 首先介绍两个名词：1）可见性:一个线程对共享变量值的修改,能够及时地被其他线程看到。2）共享变量:如果一个变量在多个线程的工作内存中都存在副本,那么这个变量就是这几个线程的共享变量 Java线程之间的通信对程序员完全透明，在并发编程中，需要处理两个关键问题：线程之间如何通信及线程之间如何同步。 通信：通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。在共享内存的并发模型里，线程之间共享程序的公共状态，通过写-读内存中的公共状态来进行隐式通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过发送消息来进行显示通信。 同步：同步是指程序中用于控制不同线程间操作发生相对顺序的机制。在共享内存并发模型里，同步是显示进行的，程序员必须显示指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java并发采用的是共享内存模型。 一、Java内存区域（JVM内存区域） Java虚拟机在运行程序时会把其自动管理的内存划分为以上几个区域，每个区域都有的用途以及创建销毁的时机，其中蓝色部分代表的是所有线程共享的数据区域，而绿色部分代表的是每个线程的私有数据区域。 方法区（Method Area）： 方法区属于线程共享的内存区域，又称Non-Heap（非堆），主要用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，根据Java 虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError 异常。值得注意的是在方法区中存在一个叫运行时常量池(Runtime Constant Pool）的区域，它主要用于存放编译器生成的各种字面量和符号引用，这些内容将在类加载后存放到运行时常量池中，以便后续使用。 JVM堆（Java Heap）： Java 堆也是属于线程共享的内存区域，它在虚拟机启动时创建，是Java 虚拟机所管理的内存中最大的一块，主要用于存放对象实例，几乎所有的对象实例都在这里分配内存，注意Java 堆是垃圾收集器管理的主要区域，因此很多时候也被称做GC 堆，如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。 程序计数器(Program Counter Register)： 属于线程私有的数据区域，是一小块内存空间，主要代表当前线程所执行的字节码行号指示器。字节码解释器工作时，通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 虚拟机栈(Java Virtual Machine Stacks)： 属于线程私有的数据区域，与线程同时创建，总数与线程关联，代表Java方法执行的内存模型。栈中只保存基础数据类型和自定义对象的引用(不是对象)，对象都存放在堆区中。每个方法执行时都会创建一个栈桢来存储方法的的变量表、操作数栈、动态链接方法、返回值、返回地址等信息。每个方法从调用直结束就对于一个栈桢在虚拟机栈中的入栈和出栈过程，如下（图有误，应该为栈桢）： 本地方法栈(Native Method Stacks)： 本地方法栈属于线程私有的数据区域，这部分主要与虚拟机用到的 Native 方法相关，一般情况下，我们无需关心此区域。 二、Java内存模型Java内存模型(即Java Memory Model，简称JMM)本身是一种抽象的概念，并不真实存在。Java线程之间的通信由JMM控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系。 由于JVM运行程序的实体是线程，而每个线程创建时JVM都会为其创建一个工作内存(有些地方称为栈空间)，用于存储线程私有的数据，而Java内存模型中规定所有变量都存储在主内存，主内存是共享内存区域，所有线程都可以访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行。 首先要将变量从主内存拷贝的自己的工作内存空间，然后对变量进行操作，操作完成后再将变量写回主内存，不能直接操作主内存中的变量，工作内存中存储着主内存中的变量副本拷贝，前面说过，工作内存是每个线程的私有数据区域，因此不同的线程间无法访问对方的工作内存，线程间的通信(传值)必须通过主内存来完成，其简要访问过程如下图 图3 需要注意的是，JMM与Java内存区域的划分是不同的概念层次，更恰当说JMM描述的是一组规则，通过这组规则控制程序中各个变量在共享数据区域和私有数据区域的访问方式，JMM是围绕原子性，有序性、可见性展开的(稍后会分析)。 JMM与Java内存区域唯一相似点，都存在共享数据区域和私有数据区域，在JMM中主内存属于共享数据区域，从某个程度上讲应该包括了堆和方法区，而工作内存数据线程私有数据区域，从某个程度上讲则应该包括程序计数器、虚拟机栈以及本地方法栈。或许在某些地方，我们可能会看见主内存被描述为堆内存，工作内存被称为线程栈，实际上他们表达的都是同一个含义。关于JMM中的主内存和工作内存说明如下 主内存 主要存储的是Java实例对象以及线程之间的共享变量，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发现线程安全问题。 工作内存 有的书籍中也称为本地内存，主要存储当前方法的所有本地变量信息(工作内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的工作内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息。 注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。注意，工作内存是JMM的一个抽象概念，并不真实存在。 弄清楚主内存和工作内存后，接了解一下主内存与工作内存的数据存储类型以及操作方式，根据虚拟机规范，对于一个实例对象中的成员方法而言，如果方法中包含本地变量是基本数据类型（boolean,byte,short,char,int,long,float,double），将直接存储在工作内存的帧栈结构中，但倘若本地变量是引用类型，那么该变量的引用会存储在功能内存的帧栈中，而对象实例将存储在主内存(共享数据区域，堆)中。 但对于实例对象的成员变量，不管它是基本数据类型或者包装类型(Integer、Double等)还是引用类型，都会被存储到堆区。至于static变量以及类本身相关信息将会存储在主内存中。需要注意的是，在主内存中的实例对象可以被多线程共享，倘若两个线程同时调用了同一个对象的同一个方法，那么两条线程会将要操作的数据拷贝一份到自己的工作内存中，执行完成操作后才刷新到主内存，简单示意图如下所示： 图4 从图3来看，如果线程A与线程B之间要通信的话，必须经历下面两个步骤： 1）线程A把本地内存A中更新过的共享变量刷新到主内存中去 2）线程B到主内存中去读取线程A之前已更新过的共享变量 从以上两个步骤来看，共享内存模型完成了“隐式通信”的过程。 JMM也主要是通过控制主内存与每个线程的工作内存之间的交互，来为Java程序员提供内存可见性的保证。 三、as-if-serial语义、happens-before原则3.1 as-if-serial语义重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。 但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 3.2 happens-before原则 happens-before是JMM最核心的概念。对应Java程序来说，理解happens-before是理解JMM的关键。 设计JMM时，需要考虑两个关键因素： 程序员对内存模型的使用。程序员希望内存模型易于理解、易于编程。程序员希望基于一个强内存模型来编写代码。 编译器和处理器对内存模型的实现。编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现弱内存模型。 但以上两点相互矛盾，所以JSR-133专家组在设计JMM时的核心膜表就是找到一个好的平衡点：一方面，为程序员提高足够强的内存可见性保证；另一方面，对编译器和处理器的限制尽可能地放松。 另外还要一个特别有意思的事情就是关于重排序问题，更简单的说，重排序可以分为两类：1)会改变程序执行结果的重排序。 2) 不会改变程序执行结果的重排序。 JMM对这两种不同性质的重排序，采取了不同的策略，如下： 对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM对编译器和处理器不做要求（JMM允许这种 重排序） JMM的设计图为： JMM设计示意图 从图可以看出： JMM向程序员提供的happens-before规则能满足程序员的需求。JMM的happens-before规则不但简单易懂，而且也向程序员提供了足够强的内存可见性保证（有些内存可见性保证其实并不一定真实存在，比如上面的A happens-before B）。 JMM对编译器和处理器的束缚已经尽可能少。从上面的分析可以看出，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。例如，如果编译器经过细致的分析后，认定一个锁只会被单个线程访问，那么这个锁可以被消除。再如，如果编译器经过细致的分析后，认定一个volatile变量只会被单个线程访问，那么编译器可以把这个volatile变量当作一个普通变量来对待。这些优化既不会改变程序的执行结果，又能提高程序的执行效率。 3.3 happens-before定义happens-before的概念最初由Leslie Lamport在其一篇影响深远的论文（《Time，Clocks and the Ordering of Events in a Distributed System》）中提出。JSR-133使用happens-before的概念来指定两个操作之间的执行顺序。由于这两个操作可以在一个线程之内，也可以是在不同线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）。具体的定义为： 1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。 上面的1）是JMM对程序员的承诺。从程序员的角度来说，可以这样理解happens-before关系：如果A happens-before B，那么Java内存模型将向程序员保证——A操作的结果将对B可见，且A的执行顺序排在B之前。注意，这只是Java内存模型向程序员做出的保证！ 上面的2）是JMM对编译器和处理器重排序的约束原则。正如前面所言，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。JMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。因此，happens-before关系本质上和as-if-serial语义是一回事。 3.3 happens-before对比as-if-serial as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 3.4 happens-before具体规则 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 3.5 happens-before与JMM的关系图 一个happens-before规则对应于一个或多个编译器和处理器重排序规则。对于Java程序员来说，happens-before规则简单易懂，它避免Java程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法 四、volatile、锁的内存语义4.1 volatile的内存语义当声明共享变量为volatile后，对这个变量的读/写会很特别。一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都是使用同一个锁来同步，它们之间的执行效果相同。 锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 锁的语义决定了临界区代码的执行具有原子性。这意味着，即使是64位的long型和double型变量，只要是volatile变量，对该变量的读/写就具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 简而言之，一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 2）禁止进行指令重排序。 可见性。对一个volatiole变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 有序性。volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。 volatile关键字禁止指令重排序有两层意思： 1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行； 2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。 可能上面说的比较绕，举个简单的例子： //x、y为非volatile变量 //flag为volatile变量 x = 2; //语句1 y = 0; //语句2 flag = true; //语句3 x = 4; //语句4 y = -1; //语句5 由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。 原子性。对任意单个volatile变量的读、写具有原子性，但类似于volatile++这种复合操作不具有原子性。 volatile写的内存语义：当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 volatile读的内存语义：当读一个volatile变量时，JMM会把该线程对应的本地内存置位无效。线程接下来将从主内存中读取共享变量。（强制从主内存读取共享变量，把本地内存与主内存的共享变量的值变成一致）。 volatile写和读的内存语义总结总结： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对变量所做修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。(隐式通信) 4.2 volatile内存语义的实现前面提到过编译器重排序和处理器重排序。为了实现volatile内存语义，JMM分别限制了这两种类型的重排序类型。 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写时，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。 下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图： 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是volatile写后面的StoreLoad屏障。这个屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在这里采取了保守策略：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图： 上图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： class VolatileBarrierExample { int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() { int i = v1; //第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; //普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; //第二个 volatile写 } … //其他方法 } 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 上面的优化是针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以x86处理器为例，上图中除最后的StoreLoad屏障外，其它的屏障都会被省略。 为了提供一种比锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和锁的释放-获取具有相同的内存语义。 由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。在功能上，锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。 当一个变量被定义为volatile之后，就可以保证此变量对所有线程的可见性，即当一个线程修改了此变量的值的时候，变量新的值对于其他线程来说是可以立即得知的。可以理解成：对volatile变量所有的写操作都能立刻被其他线程得知。但是这并不代表基于volatile变量的运算在并发下是安全的，因为volatile只能保证内存可见性，却没有保证对变量操作的原子性。比如下面的代码： / * * 发起20个线程，每个线程对race变量进行10000次自增操作，如果代码能够正确并发， * 则最终race的结果应为200000，但实际的运行结果却小于200000。 * * @author Colin Wang */ public class Test { public static volatile int race = 0; public static void increase() { race++; } private static final int THREADS_COUNT = 20; public static void main(String[] args) { Thread[] threads = new Thread[THREADS_COUNT]; for (int i = 0; i < THREADS_COUNT; i++) { threads[i] = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i < 10000; i++) { increase(); } } }); threads[i].start(); } while (Thread.activeCount() > 1) Thread.yield(); System.out.println(race); } } 按道理来说结果是10000，但是运行下很可能是个小于10000的值。有人可能会说volatile不是保证了可见性啊，一个线程对race的修改，另外一个线程应该立刻看到啊！可是这里的操作race++是个复合操作啊，包括读取race的值，对其自增，然后再写回主存。 假设线程A，读取了race的值为10，这时候被阻塞了，因为没有对变量进行修改，触发不了volatile规则。 线程B此时也读读race的值，主存里race的值依旧为10，做自增，然后立刻就被写回主存了，为11。 此时又轮到线程A执行，由于工作内存里保存的是10，所以继续做自增，再写回主存，11又被写了一遍。所以虽然两个线程执行了两次increase()，结果却只加了一次。 有人说，volatile不是会使缓存行无效的吗？但是这里线程A读取到线程B也进行操作之前，并没有修改inc值，所以线程B读取的时候，还是读的10。 又有人说，线程B将11写回主存，不会把线程A的缓存行设为无效吗？但是线程A的读取操作已经做过了啊，只有在做读取操作时，发现自己缓存行无效，才会去读主存的值，所以这里线程A只能继续做自增了。 综上所述，在这种复合操作的情景下，原子性的功能是维持不了了。但是volatile在上面那种设置flag值的例子里，由于对flag的读/写操作都是单步的，所以还是能保证原子性的。 要想保证原子性，只能借助于synchronized,Lock以及并发包下的atomic的原子操作类了，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。 Java 理论与实践: 正确使用 Volatile 变量 总结了volatile关键的使用场景， 只能在有限的一些情形下使用 volatile 变量替代锁。要使 volatile 变量提供理想的线程安全，必须同时满足下面两个条件： 对变量的写操作不依赖于当前值。 该变量没有包含在具有其他变量的不变式中。 实际上，这些条件表明，可以被写入 volatile 变量的这些有效值独立于任何程序的状态，包括变量的当前状态。 第一个条件的限制使 volatile 变量不能用作线程安全计数器。虽然增量操作（x++）看上去类似一个单独操作，实际上它是一个由读取－修改－写入操作序列组成的组合操作，必须以原子方式执行，而 volatile 不能提供必须的原子特性。实现正确的操作需要使x 的值在操作期间保持不变，而 volatile 变量无法实现这点。（然而，如果将值调整为只从单个线程写入，那么可以忽略第一个条件。） volatile一个使用场景是状态位；还有只有一个线程写，其余线程读的场景 4.3 锁的内存语义锁可以让临界区互斥执行。锁的释放-获取的内存语义与volatile变量写-读的内存语义很像。 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM会把该线程对应的本地内存置位无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 不难发现：锁释放与volatile写有相同的内存语音；锁获取与volatile读有相同的内存语义。 下面对锁释放和锁获取的内存语义做个总结。 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改）的消息。 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。 4.4 final域的内存语义与前面介绍的锁和volatile想比，对final域的读和写更像是普通的变量访问。 对于final域，编译器和处理器要遵循两个重排序规则： 1.在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 2.初次读一个包含final域的对象的应用，与随后初次读这个final域，这两个操作之间不能重排序 下面通过一个示例来分别说明这两个规则： public class FinalTest { int i;//普通变量 final int j; static FinalExample obj; public FinalExample(){ i = 1; j = 2; } public static void writer(){ obj = new FinalExample(); } public static void reader(){ FinalExample object = obj;//读对象引用 int a = object.i; int b = object.j; } } 这里假设一个线程A执行writer()方法，随后另一个线程B执行reader()方法。下面我们通过这两个线程的交互来说明这两个规则。 写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面两个方面。 1）JMM禁止编译器把final域的写重排序到构造函数之外。 2）编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 现在让我们分析writer方法，writer方法只包含一行代码obj = new FinalTest();这行代码包含两个步骤： 1）构造一个FinalTest类型的对象 2）把这个对象的引用赋值给obj 假设线程B的读对象引用与读对象的成员域之间没有重排序，下图是一种可能的执行时序 在上图中，写普通域的操作被编译器重排序到了构造函数之外，读线程B错误的读取到了普通变量i初始化之前的值。而写final域的操作被写final域重排序的规则限定在了构造函数之内，读线程B正确的读取到了final变量初始化之后的值。 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被初始化了，而普通变量不具有这个保证。以上图为例，读线程B看到对象obj的时候，很可能obj对象还没有构造完成（对普通域i的写操作被重排序到构造函数外，此时初始值1还没有写入普通域i） 读final域的重排序规则是：在一个线程中，初次读对象的引用与初次读这个对象包含的final域，JMM禁止重排序这两个操作(该规则仅仅针对处理器)。编译器会在读final域的操作前面加一个LoadLoad屏障。 初次读对象引用与初次读该对象包含的final域，这两个操作之间存在间接依赖关系。由于编译器遵守间接依赖关系，因此编译器不会重排序这两个操作。大多数处理器也会遵守间接依赖，也不会重排序这两个操作。但有少数处理器允许对存在间接依赖关系的操作做重排序（比如alpha处理器），这个规则就是专门用来针对这种处理器的。 上面的例子中，reader方法包含三个操作 1）初次读引用变量obj 2）初次读引用变量指向对象的普通域 3）初次读引用变量指向对象的final域 现在假设写线程A没有发生任何重排序，同时程序在不遵守间接依赖的处理器上执行，下图是一种可能的执行时序： 在上图中，读对象的普通域操作被处理器重排序到读对象引用之前。在读普通域时，该域还没有被写线程写入，这是一个错误的读取操作，而读final域的重排序规则会把读对象final域的操作“限定”在读对象引用之后，此时该final域已经被A线程初始化过了，这是一个正确的读取操作。 读final域的重排序规则可以确保：在读一个对象的final域之前，一定会先读包含这个final域的对象的引用。在这个示例程序中，如果该引用不为null，那么引用对象的final域一定已经被A线程初始化过了。 final域为引用类型，上面我们看到的final域是基础的数据类型，如果final域是引用类型呢？ public class FinalReferenceTest { final int[] arrs;//final引用 static FinalReferenceTest obj; public FinalReferenceTest(){ arrs = new int[1];//1 arrs[0] = 1;//2 } public static void write0(){//A线程 obj = new FinalReferenceTest();//3 } public static void write1(){//线程B obj.arrs[0] = 2;//4 } public static void reader(){//C线程 if(obj!=null){//5 int temp =obj.arrs[0];//6 } } } JMM可以确保读线程C至少能看到写线程A在构造函数中对final引用对象的成员域的写入。即C至少能看到数组下标0的值为1。而写线程B对数组元素的写入，读线程C可能看得到，也可能看不到。JMM不保证线程B的写入对读线程C可见，因为写线程B和读线程C之间存在数据竞争，此时的执行结果不可预知。 如果想要确保读线程C看到写线程B对数组元素的写入，写线程B和读线程C之间需要使用同步原语（lock或volatile）来确保内存可见性。 前面我们提到过，写final域的重排序规则可以确保：在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。其实，要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程所见，也就是对象引用不能在构造函数中“逸出”。 public class FinalReferenceEscapeExample {final int i;static FinalReferenceEscapeExample obj;public FinalReferenceEscapeExample () { i = 1; // 1写final域 obj = this; // 2 this引用在此"逸出" } public static void writer() {new FinalReferenceEscapeExample (); }public static void reader() {if (obj != null) { // 3 int temp = obj.i; // 4 } } } 假设一个线程A执行writer()方法，另一个线程B执行reader()方法。这里的操作2使得对象还未完成构造前就为线程B可见。即使这里的操作2是构造函数的最后一步，且在程序中操作2排在操作1后面，执行read()方法的线程仍然可能无法看到final域被初始化后的值，因为这里的操作1和操作2之间可能被重排序。 JSR-133为什么要增强final的语义： 通过为final域增加写和读重排序规则，可以为Java程序员提供初始化安全保证：只要对象是正确构造的（被构造对象的引用在构造函数中没有“逸出”），那么不需要使用同步（指lock和volatile的使用）就可以保证任意线程都能看到这个final域在构造函数中被初始化之后的值。 五、JMM是如何处理并发过程中的三大特性 JMM是围绕这在并发过程中如何处理原子性、可见性和有序性这3个特性来建立的。 原子性： Java中，对基本数据类型的读取和赋值操作是原子性操作，所谓原子性操作就是指这些操作是不可中断的，要做一定做完，要么就没有执行。比如： i = 2;j = i;i++;i = i + 1； 上面4个操作中，i=2是读取操作，必定是原子性操作，j=i你以为是原子性操作，其实吧，分为两步，一是读取i的值，然后再赋值给j,这就是2步操作了，称不上原子操作，i++和i = i + 1其实是等效的，读取i的值，加1，再写回主存，那就是3步操作了。所以上面的举例中，最后的值可能出现多种情况，就是因为满足不了原子性。 JMM只能保证对单个volatile变量的读/写具有原子性，但类似于volatile++这种符合操作不具有原子性，这时候就必须借助于synchronized和Lock来保证整块代码的原子性了。线程在释放锁之前，必然会把i的值刷回到主存的。 可见性：可见性指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改。Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性。 无论是普通变量还是volatile变量，它们的区别是：volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因为，可以说volatile保证了多线程操作时变量的可见性，而普通变量不能保证这一点。 除了volatile之外，java中还有2个关键字能实现可见性，即synchronized和final（final修饰的变量，线程安全级别最高）。同步块的可见性是由“对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store，write操作）”这条规则获得；而final关键字的可见性是指：被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事，其他线程有可能通过这个引用访问到“初始化了一半”的对象），那么在其他线程中就能看到final字段的值。 有序性：JMM的有序性在讲解volatile时详细的讨论过，java程序中天然的有序性可以总结为一句话：如果在本线程内观察，所有操作都是有序的；如果在一个线程中观察另一个线程，所有操作都是无序的。前半句是指“线程内表现为串行的语义”，后半句指的是“指令重排”现象和“工作内存与主内存同步延迟”现象。 前半句可以用JMM规定的as-if-serial语义来解决，后半句可以用JMM规定的happens-before原则来解决。Java语义提供了volatile和synchronized两个关键字来保证线程之间操作的有序性，volatile关键字本身就包含了禁止指令重排的语义，而synchronized则是由“一个变量在同一个时刻只允许一条线程对其进行lock操作”这条规则获取的。这个规则决定了持有同一个锁的两个同步块只能串行的进入。 参考链接：https://blog.csdn.net/javazejian/article/details/72772461#volatile%E7%A6%81%E6%AD%A2%E9%87%8D%E6%8E%92%E4%BC%98%E5%8C%96 https://www.cnblogs.com/_popc/p/6096517.html https://blog.csdn.net/liu_dong_liang/article/details/80391040 https://www.jb51.net/article/76006.htm https://blog.csdn.net/x_i_y_u_e/article/details/50728602 https://blog.csdn.net/FYGu18/article/details/79001688 https://blog.csdn.net/soongp/article/details/80292796 《Java 并发编程的艺术》 方腾飞 魏鹏 程晓明 著 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南5：JMM中的final关键字解析]]></title>
    <url>%2F2019%2F10%2F05%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%975%EF%BC%9AJMM%E4%B8%AD%E7%9A%84final%E5%85%B3%E9%94%AE%E5%AD%97%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言与前面介绍的锁和volatile相比较，对final域的读和写更像是普通的变量访问。对于final域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。 下面，我们通过一些示例性的代码来分别说明这两个规则： 1234567891011121314151617181920public class FinalExample &#123; int i; //普通变量 final int j; //final变量 static FinalExample obj; public void FinalExample () &#123; //构造函数 i = 1; //写普通域 j = 2; //写final域 &#125; public static void writer () &#123; //写线程A执行 obj = new FinalExample (); &#125; public static void reader () &#123; //读线程B执行 FinalExample object = obj; //读对象引用 int a = object.i; //读普通域 int b = object.j; //读final域 &#125;&#125; 这里假设一个线程A执行writer ()方法，随后另一个线程B执行reader ()方法。下面我们通过这两个线程的交互来说明这两个规则。 写final域的重排序规则写final域的重排序规则禁止把final域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM禁止编译器把final域的写重排序到构造函数之外。 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障禁止处理器把final域的写重排序到构造函数之外。 现在让我们分析writer ()方法。writer ()方法只包含一行代码：finalExample = new FinalExample ()。这行代码包含两个步骤： 构造一个FinalExample类型的对象； 把这个对象的引用赋值给引用变量obj。 假设线程B读对象引用与读对象的成员域之间没有重排序（马上会说明为什么需要这个假设），下图是一种可能的执行时序： 在上图中，写普通域的操作被编译器重排序到了构造函数之外，读线程B错误的读取了普通变量i初始化之前的值。而写final域的操作，被写final域的重排序规则“限定”在了构造函数之内，读线程B正确的读取了final变量初始化之后的值。 写final域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的final域已经被正确初始化过了，而普通域不具有这个保障。以上图为例，在读线程B“看到”对象引用obj时，很可能obj对象还没有构造完成（对普通域i的写操作被重排序到构造函数外，此时初始值2还没有写入普通域i）。 读final域的重排序规则读final域的重排序规则如下： 在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读final域操作的前面插入一个LoadLoad屏障。 初次读对象引用与初次读该对象包含的final域，这两个操作之间存在间接依赖关系。由于编译器遵守间接依赖关系，因此编译器不会重排序这两个操作。大多数处理器也会遵守间接依赖，大多数处理器也不会重排序这两个操作。但有少数处理器允许对存在间接依赖关系的操作做重排序（比如alpha处理器），这个规则就是专门用来针对这种处理器。 reader()方法包含三个操作： 初次读引用变量obj; 初次读引用变量obj指向对象的普通域j。 初次读引用变量obj指向对象的final域i。 现在我们假设写线程A没有发生任何重排序，同时程序在不遵守间接依赖的处理器上执行，下面是一种可能的执行时序： 在上图中，读对象的普通域的操作被处理器重排序到读对象引用之前。读普通域时，该域还没有被写线程A写入，这是一个错误的读取操作。而读final域的重排序规则会把读对象final域的操作“限定”在读对象引用之后，此时该final域已经被A线程初始化过了，这是一个正确的读取操作。 读final域的重排序规则可以确保：在读一个对象的final域之前，一定会先读包含这个final域的对象的引用。在这个示例程序中，如果该引用不为null，那么引用对象的final域一定已经被A线程初始化过了。 如果final域是引用类型上面我们看到的final域是基础数据类型，下面让我们看看如果final域是引用类型，将会有什么效果？ 请看下列示例代码： 1234567891011121314151617181920212223public class FinalReferenceExample &#123;final int[] intArray; //final是引用类型static FinalReferenceExample obj;public FinalReferenceExample () &#123; //构造函数 intArray = new int[1]; //1 intArray[0] = 1; //2&#125;public static void writerOne () &#123; //写线程A执行 obj = new FinalReferenceExample (); //3&#125;public static void writerTwo () &#123; //写线程B执行 obj.intArray[0] = 2; //4&#125;public static void reader () &#123; //读线程C执行 if (obj != null) &#123; //5 int temp1 = obj.intArray[0]; //6 &#125;&#125;&#125; 这里final域为一个引用类型，它引用一个int型的数组对象。对于引用类型，写final域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 对上面的示例程序，我们假设首先线程A执行writerOne()方法，执行完后线程B执行writerTwo()方法，执行完后线程C执行reader ()方法。下面是一种可能的线程执行时序： 在上图中，1是对final域的写入，2是对这个final域引用的对象的成员域的写入，3是把被构造的对象的引用赋值给某个引用变量。这里除了前面提到的1不能和3重排序外，2和3也不能重排序。 JMM可以确保读线程C至少能看到写线程A在构造函数中对final引用对象的成员域的写入。即C至少能看到数组下标0的值为1。而写线程B对数组元素的写入，读线程C可能看的到，也可能看不到。JMM不保证线程B的写入对读线程C可见，因为写线程B和读线程C之间存在数据竞争，此时的执行结果不可预知。 如果想要确保读线程C看到写线程B对数组元素的写入，写线程B和读线程C之间需要使用同步原语（lock或volatile）来确保内存可见性。 为什么final引用不能从构造函数内“逸出”前面我们提到过，写final域的重排序规则可以确保：在引用变量为任意线程可见之前，该引用变量指向的对象的final域已经在构造函数中被正确初始化过了。其实要得到这个效果，还需要一个保证：在构造函数内部，不能让这个被构造对象的引用为其他线程可见，也就是对象引用不能在构造函数中“逸出”。为了说明问题，让我们来看下面示例代码： 12345678910111213141516171819public class FinalReferenceEscapeExample &#123;final int i;static FinalReferenceEscapeExample obj;public FinalReferenceEscapeExample () &#123; i = 1; //1写final域 obj = this; //2 this引用在此“逸出”&#125;public static void writer() &#123; new FinalReferenceEscapeExample ();&#125;public static void reader &#123; if (obj != null) &#123; //3 int temp = obj.i; //4 &#125;&#125;&#125; 假设一个线程A执行writer()方法，另一个线程B执行reader()方法。这里的操作2使得对象还未完成构造前就为线程B可见。即使这里的操作2是构造函数的最后一步，且即使在程序中操作2排在操作1后面，执行read()方法的线程仍然可能无法看到final域被初始化后的值，因为这里的操作1和操作2之间可能被重排序。实际的执行时序可能如下图所示： 从上图我们可以看出：在构造函数返回前，被构造对象的引用不能为其他线程可见，因为此时的final域可能还没有被初始化。在构造函数返回后，任意线程都将保证能看到final域正确初始化之后的值。 final语义在处理器中的实现现在我们以x86处理器为例，说明final语义在处理器中的具体实现。 上面我们提到，写final域的重排序规则会要求译编器在final域的写之后，构造函数return之前，插入一个StoreStore障屏。读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障。 由于x86处理器不会对写-写操作做重排序，所以在x86处理器中，写final域需要的StoreStore障屏会被省略掉。同样，由于x86处理器不会对存在间接依赖关系的操作做重排序，所以在x86处理器中，读final域需要的LoadLoad屏障也会被省略掉。也就是说在x86处理器中，final域的读/写不会插入任何内存屏障！ JSR-133为什么要增强final的语义在旧的Java内存模型中 ，最严重的一个缺陷就是线程可能看到final域的值会改变。比如，一个线程当前看到一个整形final域的值为0（还未初始化之前的默认值），过一段时间之后这个线程再去读这个final域的值时，却发现值变为了1（被某个线程初始化之后的值）。最常见的例子就是在旧的Java内存模型中，String的值可能会改变（参考文献2中有一个具体的例子，感兴趣的读者可以自行参考，这里就不赘述了）。 为了修补这个漏洞，JSR-133专家组增强了final的语义。通过为final域增加写和读重排序规则，可以为java程序员提供初始化安全保证：只要对象是正确构造的（被构造对象的引用在构造函数中没有“逸出”），那么不需要使用同步（指lock和volatile的使用），就可以保证任意线程都能看到这个final域在构造函数中被初始化之后的值。 参考文献 Java Concurrency in Practice JSR 133 (Java Memory Model) FAQ Java Concurrency in Practice The JSR-133 Cookbook for Compiler Writers Intel® 64 and IA-32 ArchitecturesvSoftware Developer’s Manual Volume 3A: System Programming Guide, Part 1 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南4：Java中的锁 Lock和synchronized]]></title>
    <url>%2F2019%2F10%2F04%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%974%EF%BC%9AJava%E4%B8%AD%E7%9A%84%E9%94%81%20Lock%E5%92%8Csynchronized%2F</url>
    <content type="text"><![CDATA[本文转载自并发编程网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java中的锁机制及Lock类锁的释放-获取建立的happens before 关系锁是java并发编程中最重要的同步机制。锁除了让临界区互斥执行外，还可以让释放锁的线程向获取同一个锁的线程发送消息。 下面是锁释放-获取的示例代码： class MonitorExample { int a = 0; public synchronized void writer() { //1 a++; //2 } //3 public synchronized void reader() { //4 int i = a; //5 …… } //6 }根据程序次序规则，1 happens before 2, 2 happens before 3; 4 happens before 5, 5 happens before 6。假设线程A执行writer()方法，随后线程B执行reader()方法。根据happens before规则，这个过程包含的happens before 关系可以分为两类： 根据监视器锁规则，3 happens before 4。 根据happens before 的传递性，2 happens before 5。 上述happens before 关系的图形化表现形式如下： 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示监视器锁规则；蓝色箭头表示组合这些规则后提供的happens before保证。 上图表示在线程A释放了锁之后，随后线程B获取同一个锁。在上图中，2 happens before 5。因此，线程A在释放锁之前所有可见的共享变量，在线程B获取同一个锁之后，将立刻变得对B线程可见。 锁释放和获取的内存语义当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。以上面的MonitorExample程序为例，A线程释放锁后，共享数据的状态示意图如下： 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。下面是锁获取的状态示意图： 对比锁释放-获取的内存语义与volatile写-读的内存语义，可以看出：锁释放与volatile写有相同的内存语义；锁获取与volatile读有相同的内存语义。 下面对锁释放和锁获取的内存语义做个总结： 线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。 线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。 线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。 锁内存语义的实现本文将借助ReentrantLock的源代码，来分析锁内存语义的具体实现机制。 请看下面的示例代码： class ReentrantLockExample { int a = 0; ReentrantLock lock = new ReentrantLock(); public void writer() { lock.lock(); //获取锁 try { a++; } finally { lock.unlock(); //释放锁 } } public void reader () { lock.lock(); //获取锁 try { int i = a; …… } finally { lock.unlock(); //释放锁 } } }在ReentrantLock中，调用lock()方法获取锁；调用unlock()方法释放锁。 ReentrantLock的实现依赖于java同步器框架AbstractQueuedSynchronizer（本文简称之为AQS）。AQS使用一个整型的volatile变量（命名为state）来维护同步状态，马上我们会看到，这个volatile变量是ReentrantLock内存语义实现的关键。 下面是ReentrantLock的类图（仅画出与本文相关的部分）： ReentrantLock分为公平锁和非公平锁，我们首先分析公平锁。 使用公平锁时，加锁方法lock()的方法调用轨迹如下： ReentrantLock : lock() FairSync : lock() AbstractQueuedSynchronizer : acquire(int arg) ReentrantLock : tryAcquire(int acquires) 在第4步真正开始加锁，下面是该方法的源代码： protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); //获取锁的开始，首先读volatile变量state if (c == 0) { if (isFirst(current) &amp;&amp; compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; } return false; }从上面源代码中我们可以看出，加锁方法首先读volatile变量state。 在使用公平锁时，解锁方法unlock()的方法调用轨迹如下： ReentrantLock : unlock() AbstractQueuedSynchronizer : release(int arg) Sync : tryRelease(int releases) 在第3步真正开始释放锁，下面是该方法的源代码： protected final boolean tryRelease(int releases) { int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; setExclusiveOwnerThread(null); } setState(c); //释放锁的最后，写volatile变量state return free; }从上面的源代码我们可以看出，在释放锁的最后写volatile变量state。 公平锁在释放锁的最后写volatile变量state；在获取锁时首先读这个volatile变量。根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量，在获取锁的线程读取同一个volatile变量后将立即变的对获取锁的线程可见。 现在我们分析非公平锁的内存语义的实现。 非公平锁的释放和公平锁完全一样，所以这里仅仅分析非公平锁的获取。 使用公平锁时，加锁方法lock()的方法调用轨迹如下： ReentrantLock : lock() NonfairSync : lock() AbstractQueuedSynchronizer : compareAndSetState(int expect, int update) 在第3步真正开始加锁，下面是该方法的源代码： protected final boolean compareAndSetState(int expect, int update) {return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} 该方法以原子操作的方式更新state变量，本文把java的compareAndSet()方法调用简称为CAS。JDK文档对该方法的说明如下：如果当前状态值等于预期值，则以原子方式将同步状态设置为给定的更新值。此操作具有 volatile 读和写的内存语义。 这里我们分别从编译器和处理器的角度来分析,CAS如何同时具有volatile读和volatile写的内存语义。 前文我们提到过，编译器不会对volatile读与volatile读后面的任意内存操作重排序；编译器不会对volatile写与volatile写前面的任意内存操作重排序。组合这两个条件，意味着为了同时实现volatile读和volatile写的内存语义，编译器不能对CAS与CAS前面和后面的任意内存操作重排序。 下面我们来分析在常见的intel x86处理器中，CAS是如何同时具有volatile读和volatile写的内存语义的。 下面是sun.misc.Unsafe类的compareAndSwapInt()方法的源代码： protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); }可以看到这是个本地方法调用。这个本地方法在openjdk中依次调用的c++代码为：unsafe.cpp，atomic.cpp和atomicwindowsx86.inline.hpp。这个本地方法的最终实现在openjdk的如下位置：openjdk-7-fcs-src-b147-27jun2011\openjdk\hotspot\src\oscpu\windowsx86\vm\ atomicwindowsx86.inline.hpp（对应于windows操作系统，X86处理器）。下面是对应于intel x86处理器的源代码的片段： // Adding a lock prefix to an instruction on MP machine // VC++ doesn&apos;t like the lock prefix to be on a single line // so we can&apos;t insert a label after the lock prefix. // By emitting a lock prefix, we can define a label after it. #define LOCK_IF_MP(mp) __asm cmp mp, 0 \ __asm je L0 \ __asm _emit 0xF0 \ __asm L0: inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) { // alternative for InterlockedCompareExchange int mp = os::is_MP(); __asm { mov edx, dest mov ecx, exchange_value mov eax, compare_value LOCK_IF_MP(mp) cmpxchg dword ptr [edx], ecx } }如上面源代码所示，程序会根据当前处理器的类型来决定是否为cmpxchg指令添加lock前缀。如果程序是在多处理器上运行，就为cmpxchg指令加上lock前缀（lock cmpxchg）。反之，如果程序是在单处理器上运行，就省略lock前缀（单处理器自身会维护单处理器内的顺序一致性，不需要lock前缀提供的内存屏障效果）。 intel的手册对lock前缀的说明如下： 确保对内存的读-改-写操作原子执行。在Pentium及Pentium之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其他处理器暂时无法通过总线访问内存。很显然，这会带来昂贵的开销。 从Pentium 4，Intel Xeon及P6处理器开始，intel在原有总线锁的基础上做了一个很有意义的优化：如果要访问的内存区域（area of memory）在lock前缀指令执行期间已经在处理器内部的缓存中被锁定（即包含该内存区域的缓存行当前处于独占或以修改状态），并且该内存区域被完全包含在单个缓存行（cache line）中，那么处理器将直接执行该指令。 由于在指令执行期间该缓存行会一直被锁定，其它处理器无法读/写该指令要访问的内存区域，因此能保证指令执行的原子性。这个操作过程叫做缓存锁定（cache locking），缓存锁定将大大降低lock前缀指令的执行开销，但是当多处理器之间的竞争程度很高或者指令访问的内存地址未对齐时，仍然会锁住总线。 禁止该指令与之前和之后的读和写指令重排序。 把写缓冲区中的所有数据刷新到内存中。 上面的第2点和第3点所具有的内存屏障效果，足以同时实现volatile读和volatile写的内存语义。 经过上面的这些分析，现在我们终于能明白为什么JDK文档说CAS同时具有volatile读和volatile写的内存语义了。 现在对公平锁和非公平锁的内存语义做个总结： 公平锁和非公平锁释放时，最后都要写一个volatile变量state。 公平锁获取时，首先会去读这个volatile变量。 非公平锁获取时，首先会用CAS更新这个volatile变量,这个操作同时具有volatile读和volatile写的内存语义。 从本文对ReentrantLock的分析可以看出，锁释放-获取的内存语义的实现至少有下面两种方式： 利用volatile变量的写-读所具有的内存语义。 利用CAS所附带的volatile读和volatile写的内存语义。 concurrent包的实现由于java的CAS同时具有 volatile 读和volatile写的内存语义，因此Java线程之间的通信现在有了下面四种方式： A线程写volatile变量，随后B线程读这个volatile变量。 A线程写volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程用CAS更新这个volatile变量。 A线程用CAS更新一个volatile变量，随后B线程读这个volatile变量。 Java的CAS会使用现代处理器上提供的高效机器级别原子指令，这些原子指令以原子方式对内存执行读-改-写操作，这是在多处理器中实现同步的关键（从本质上来说，能够支持原子性读-改-写指令的计算机器，是顺序计算图灵机的异步等价机器，因此任何现代的多处理器都会去支持某种能对内存执行原子性读-改-写操作的原子指令）。同时，volatile变量的读/写和CAS可以实现线程之间的通信。把这些特性整合在一起，就形成了整个concurrent包得以实现的基石。如果我们仔细分析concurrent包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为volatile； 然后，使用CAS的原子条件更新来实现线程之间的同步； 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic包中的类），这些concurrent包中的基础类都是使用这种模式来实现的，而concurrent包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent包的实现示意图如下： synchronized实现原理转自：https://blog.csdn.net/chenssy/article/details/54883355 记得刚刚开始学习Java的时候，一遇到多线程情况就是synchronized。对于当时的我们来说，synchronized是如此的神奇且强大。我们赋予它一个名字“同步”，也成为我们解决多线程情况的良药，百试不爽。但是，随着学习的深入，我们知道synchronized是一个重量级锁，相对于Lock，它会显得那么笨重，以至于我们认为它不是那么的高效，并慢慢抛弃它。 诚然，随着Javs SE 1.6对synchronized进行各种优化后，synchronized不会显得那么重。 下面跟随LZ一起来探索synchronized的实现机制、Java是如何对它进行了优化、锁优化机制、锁的存储结构和升级过程。 1、实现原理 synchronized可以保证方法或者代码块在运行时，同一时刻只有一个方法可以进入到临界区，同时它还可以保证共享变量的内存可见性。 Java中每一个对象都可以作为锁，这是synchronized实现同步的基础： 普通同步方法，锁是当前实例对象； 静态同步方法，锁是当前类的class对象； 同步方法块，锁是括号里面的对象。 当一个线程访问同步代码块时，它首先是需要得到锁才能执行同步代码，当退出或者抛出异常时必须要释放锁，那么它是如何来实现这个机制的呢？ 我们先看一段简单的代码： public class SynchronizedTest{ public synchronized void test1(){ } public void test2(){ synchronized(this){ } } } 利用Javap工具查看生成的class文件信息来分析Synchronize的实现： 从上面可以看出，同步代码块是使用monitorenter和monitorexit指令实现的，同步方法（在这看不出来需要看JVM底层实现）依靠的是方法修饰符上的ACCSYNCHRONIZED实现。 同步代码块： monitorenter指令插入到同步代码块的开始位置，monitorexit指令插入到同步代码块的结束位置，JVM需要保证每一个monitorenter都有一个monitorexit与之相对应。任何对象都有一个monitor与之相关联，当且一个monitor被持有之后，他将处于锁定状态。线程执行到monitorenter指令时，将会尝试获取对象所对应的monitor所有权，即尝试获取对象的锁； 同步方法 synchronized方法则会被翻译成普通的方法调用和返回指令如:invokevirtual、areturn指令，在VM字节码层面并没有任何特别的指令来实现被synchronized修饰的方法，而是在Class文件的方法表中将该方法的accessflags字段中的synchronized标志位置1，表示该方法是同步方法并使用调用该方法的对象或该方法所属的Class在JVM的内部对象表示Klass做为锁对象。 (摘自：http://www.cnblogs.com/javaminer/p/3889023.html) 下面我们来继续分析，但是在深入之前我们需要了解两个重要的概念：Java对象头、Monitor。 Java对象头、monitor：Java对象头和monitor是实现synchronized的基础！下面就这两个概念来做详细介绍。 2、Java对象头synchronized用的锁是存在Java对象头里的，那么什么是Java对象头呢？ Hotspot虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键。 所以下面将重点阐述。 Mark Word Mark Word用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。 下图是Java对象头的存储结构（32位虚拟机）： 对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机）： 简单介绍了Java对象头，我们下面再看Monitor。 3、Monitor什么是Monitor？ 我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。 与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java的设计中 ，每一个Java对象自打娘胎里出来就带了一把看不见的锁，它叫做内部锁或者Monitor锁。 Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。 其结构如下： Owner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL。 EntryQ：关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。 RcThis：表示blocked或waiting在该monitor record上的所有线程的个数。 Nest：用来实现重入锁的计数。HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。 Candidate：用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。 Candidate只有两种可能的值0表示没有需要唤醒的线程1表示要唤醒一个继任线程来竞争锁。 我们知道synchronized是重量级锁，效率不怎么滴，同时这个观念也一直存在我们脑海里，不过在JDK 1.6中对synchronize的实现进行了各种优化，使得它显得不是那么重了，那么JVM采用了那些优化手段呢？ 4、锁优化 JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。 ** 锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态。**他们会随着竞争的激烈而逐渐升级。注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 5、自旋锁 线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作，势必会给系统的并发性能带来很大的压力。同时我们发现在许多应用上面，对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。 所以引入自旋锁。 何谓自旋锁？ ** 所谓自旋锁，就是让该线程等待一段时间，不会被立即挂起（就是不让前来获取该锁（已被占用）的线程立即阻塞），看持有锁的线程是否会很快释放锁。** 怎么等待呢？ 执行一段无意义的循环即可（自旋）。 自旋等待不能替代阻塞，先不说对处理器数量的要求（多核，貌似现在没有单核的处理器了），虽然它可以避免线程切换带来的开销，但是它占用了处理器的时间。如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好；反之，自旋的线程就会白白消耗掉处理的资源，它不会做任何有意义的工作，典型的占着茅坑不拉屎，这样反而会带来性能上的浪费。 所以说，自旋等待的时间（自旋的次数）必须要有一个限度，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起。自旋锁在JDK 1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启，在JDK1.6中默认开启。同时自旋的默认次数为10次，可以通过参数-XX:PreBlockSpin来调整。 如果通过参数-XX:preBlockSpin来调整自旋锁的自旋次数，会带来诸多不便。假如我将参数调整为10，但是系统很多线程都是等你刚刚退出的时候就释放了锁（假如你多自旋一两次就可以获取锁），你是不是很尴尬？于是JDK1.6引入自适应的自旋锁，让虚拟机会变得越来越聪明。 6、适应自旋锁 JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。 它怎么做呢？ 线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。 7、锁消除 为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。 ** 如果不存在竞争，为什么还需要加锁呢？** 所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？ 我们虽然没有显示使用锁，但是我们在使用一些JDK的内置API时，如StringBuffer、Vector、HashTable等，这个时候会存在隐形的加锁操作。 ** 比如StringBuffer的append()方法，Vector的add()方法：** public void vectorTest(){ Vector vector = new Vector(); for(int i = 0 ; i < 10 ; i++){ vector.add(i + ""); } System.out.println(vector); } 在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以大胆地将vector内部的加锁操作消除。 8、锁粗化 我们知道在使用同步锁的时候，需要让同步块的作用范围尽可能小，仅在共享数据的实际作用域中才进行同步。这样做的目的是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。 在大多数的情况下，上述观点是正确的，LZ也一直坚持着这个观点。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。 那什么是锁粗化？ 就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。 如上面实例：vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。 9、轻量级锁 引入轻量级锁的主要目的是在多没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。 当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁，其步骤如下：**获取锁。** 判断当前对象是否处于无锁状态（hashcode、0、01），若是，则JVM首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝（官方把这份拷贝加了一个Displaced前缀，即Displaced Mark Word）；否则执行步骤（3）； JVM利用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指正，如果成功表示竞争到锁，则将锁标志位变成00（表示此对象处于轻量级锁状态），执行同步操作；如果失败则执行步骤（3）； 判断当前对象的Mark Word是否指向当前线程的栈帧，如果是则表示当前线程已经持有当前对象的锁，则直接执行同步代码块；否则只能说明该锁对象已经被其他线程抢占了，这时轻量级锁需要膨胀为重量级锁，锁标志位变成10，后面等待的线程将会进入阻塞状态； 释放锁轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下： 取出在获取轻量级锁保存在Displaced Mark Word中的数据； 用CAS操作将取出的数据替换当前对象的Mark Word中，如果成功，则说明释放锁成功，否则执行（3）； 如果CAS操作替换失败，说明有其他线程尝试获取该锁，则需要在释放锁的同时需要唤醒被挂起的线程。 轻量级锁能提升程序同步性能的依据是“对于绝大部分的锁，在整个同步周期内都是不存在竞争的”，这是一个经验数据。轻量级锁在当前线程的栈帧中建立一个名为锁记录的空间，用于存储锁对象目前的指向和状态。如果没有竞争，轻量级锁使用CAS操作避免了使用互斥量的开销，但如果存在锁竞争，除了互斥量的开销外，还额外发生了CAS操作，因此在有竞争的情况下，轻量级锁会比传统的重量级锁更慢。 什么是CAS操作？ compare and swap,CAS操作需要输入两个数值，一个旧值（期望操作前的值）和一个新值，在操作期间先比较旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。 CAS详解：https://mp.weixin.qq.com/s__biz=MzIxMjE5MTE1Nw==&amp;mid=2653192625&amp;idx=1&amp;sn=cbabbd806e4874e8793332724ca9d454&amp;chksm=8c99f36bbbee7a7d169581dedbe09658d0b0edb62d2cbc9ba4c40f706cb678c7d8c768afb666&amp;scene=21#wechat_redirect https://blog.csdn.net/qq_35357656/article/details/78657373 下图是轻量级锁的获取和释放过程： 10、偏向锁 引入偏向锁主要目的是：为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径。上面提到了轻量级锁的加锁解锁操作是需要依赖多次CAS原子指令的。那么偏向锁是如何来减少不必要的CAS操作呢？我们可以查看Mark work的结构就明白了。 只需要检查是否为偏向锁、锁标识为以及ThreadID即可，处理流程如下：**获取锁。** 检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01； 若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，则执行步骤（5），否则执行步骤（3）； 如果线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，否则执行线程（4）； 通过CAS竞争锁失败，证明当前存在多线程竞争情况，当到达全局安全点，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码块； 执行同步代码块。 释放锁偏向锁的释放采用了一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。偏向锁的撤销需要等待全局安全点（这个时间点是上没有正在执行的代码）。 其步骤如下： 暂停拥有偏向锁的线程，判断锁对象石是否还处于被锁定状态； 撤销偏向苏，恢复到无锁状态（01）或者轻量级锁的状态。 下图是偏向锁的获取和释放流程： 11、重量级锁 重量级锁通过对象内部的监视器（monitor）实现，其中monitor的本质是依赖于底层操作系统的Mutex Lock实现，操作系统实现线程之间的切换需要从用户态到内核态的切换，切换成本非常高。 参考资料 周志明：《深入理解Java虚拟机》 方腾飞：《Java并发编程的艺术》 Java中synchronized的实现原理与应用微信公众号 个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南3：并发三大问题与volatile关键字，CAS操作]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%973%EF%BC%9A%E5%B9%B6%E5%8F%91%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98%E4%B8%8Evolatile%E5%85%B3%E9%94%AE%E5%AD%97%EF%BC%8CCAS%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 序言先来看如下这个简单的Java类，该类中并没有使用任何的同步。 final class SetCheck { private int a = 0; private long b = 0; void set() { a = 1; b = -1; } boolean check() { return ((b == 0) || (b == -1 &amp;&amp; a == 1)); } }如果是在一个串行执行的语言中，执行SetCheck类中的check方法永远不会返回false，即使编译器，运行时和计算机硬件并没有按照你所期望的逻辑来处理这段程序，该方法依然不会返回false。在程序执行过程中，下面这些你所不能预料的行为都是可能发生的： 编译器可能会进行指令重排序，所以b变量的赋值操作可能先于a变量。如果是一个内联方法，编译器可能更甚一步将该方法的指令与其他语句进行重排序。 处理器可能会对语句所对应的机器指令进行重排序之后再执行，甚至并发地去执行。 内存系统（由高速缓存控制单元组成）可能会对变量所对应的内存单元的写操作指令进行重排序。重排之后的写操作可能会对其他的计算/内存操作造成覆盖。 编译器，处理器以及内存系统可能会让两条语句的机器指令交错。比如在32位机器上，b变量的高位字节先被写入，然后是a变量，紧接着才会是b变量的低位字节。 编译器，处理器以及内存系统可能会导致代表两个变量的内存单元在（如果有的话）连续的check调用（如果有的话）之后的某个时刻才更新，而以这种方式保存相应的值（如在CPU寄存器中）仍会得到预期的结果（check永远不会返回false）。 在串行执行的语言中，只要程序执行遵循类似串行的语义，如上几种行为就不会有任何的影响。在一段简单的代码块中，串行执行程序不会依赖于代码的内部执行细节，因此如上的几种行为可以随意控制代码。 这样就为编译器和计算机硬件提供了基本的灵活性。基于此，在过去的数十年内很多技术（CPU的流水线操作，多级缓存，读写平衡，寄存器分配等等）应运而生，为计算机处理速度的大幅提升奠定了基础。这些操作的类似串行执行的特性可以让开发人员无须知道其内部发生了什么。对于开发人员来说，如果不创建自己的线程，那么这些行为也不会对其产生任何的影响。 然而这些情况在并发编程中就完全不一样了，上面的代码在并发过程中，当一个线程调用check方法的时候完全有可能另一个线程正在执行set方法，这种情况下check方法就会将上面提到的优化操作过程暴露出来。 如果上述任意一个操作发生，那么check方法就有可能返回false。例如，check方法读取long类型的变量b的时候可能得到的既不是0也不是-1.而是一个被写入一半的值。另一种情况，set方法中的语句的乱序执行有可能导致check方法读取变量b的值的时候是-1，然而读取变量a时却依然是0。 换句话说，不仅是并发执行会导致问题，而且在一些优化操作（比如指令重排序）进行之后也会导致代码执行结果和源代码中的逻辑有所出入。由于编译器和运行时技术的日趋成熟以及多处理器的逐渐普及，这种现象就变得越来越普遍。 对于那些一直从事串行编程背景的开发人员（其实，基本上所有的程序员）来说，这可能会导致令人诧异的结果，而这些结果可能从没在串行编程中出现过。这可能就是那些微妙难解的并发编程错误的根本源头吧。 在绝大部分的情况下，有一个很简单易行的方法来避免那些在复杂的并发程序中因代码执行优化导致的问题：使用同步。例如，如果SetCheck类中所有的方法都被声明为synchronized,那么你就可以确保那么内部处理细节都不会影响代码预期的结果了。 但是在有些情况下你却不能或者不想去使用同步，抑或着你需要推断别人未使用同步的代码。在这些情况下你只能依赖Java内存模型所阐述的结果语义所提供的最小保证。Java内存模型允许上面提到的所有操作，但是限制了它们在执行语义上潜在的结果，此外还提出了一些技术让程序员可以用来控制这些语义的某些方面。 Java内存模型是Java语言规范的一部分，主要在JLS的第17章节介绍。这里，我们只是讨论一些基本的动机，属性以及模型的程序一致性。这里对JLS第一版中所缺少的部分进行了澄清。 我们假设Java内存模型可以被看作在1.2.4中描述的那种标准的SMP机器的理想化模型。 （1.2.4） 在这个模型中，每一个线程都可以被看作为运行在不同的CPU上，然而即使是在多处理器上，这种情况也是很罕见的。但是实际上，通过模型所具备的某些特性，这种CPU和线程单一映射能够通过一些合理的方法去实现。例如，因为CPU的寄存器不能被另一个CPU直接访问，这种模型必须考虑到某个线程无法得知被另一个线程操作变量的值的情况。这种情况不仅仅存在于多处理器环境上，在单核CPU环境里，因为编译器和处理器的不可预测的行为也可能导致同样的情况。 Java内存模型没有具体讲述前面讨论的执行策略是由编译器，CPU，缓存控制器还是其它机制促成的。甚至没有用开发人员所熟悉的类，对象及方法来讨论。取而代之，Java内存模型中仅仅定义了线程和内存之间那种抽象的关系。众所周知，每个线程都拥有自己的工作存储单元（缓存和寄存器的抽象）来存储线程当前使用的变量的值。Java内存模型仅仅保证了代码指令与变量操作的有序性，大多数规则都只是指出什么时候变量值应该在内存和线程工作内存之间传输。这些规则主要是为了解决如下三个相互牵连的问题： 原子性：哪些指令必须是不可分割的。在Java内存模型中，这些规则需声明仅适用于-—实例变量和静态变量，也包括数组元素，但不包括方法中的局部变量-—的内存单元的简单读写操作。 可见性：在哪些情况下，一个线程执行的结果对另一个线程是可见的。这里需要关心的结果有，写入的字段以及读取这个字段所看到的值。 有序性：在什么情况下，某个线程的操作结果对其它线程来看是无序的。最主要的乱序执行问题主要表现在读写操作和赋值语句的相互执行顺序上。 原子性当正确的使用了同步，上面属性都会具有一个简单的特性：一个同步方法或者代码块中所做的修改对于使用了同一个锁的同步方法或代码块都具有原子性和可见性。同步方法或代码块之间的执行过程都会和代码指定的执行顺序保持一致。即使代码块内部指令也许是乱序执行的，也不会对使用了同步的其它线程造成任何影响。 当没有使用同步或者使用的不一致的时候，情况就会变得复杂。Java内存模型所提供的保障要比大多数开发人员所期望的弱，也远不及目前业界所实现的任意一款Java虚拟机。这样，开发人员就必须负起额外的义务去保证对象的一致性关系：对象间若有能被多个线程看到的某种恒定关系，所有依赖这种关系的线程就必须一直维持这种关系，而不仅仅由执行状态修改的线程来维持。 除了long型字段和double型字段外，java内存模型确保访问任意类型字段所对应的内存单元都是原子的。这包括引用其它对象的引用类型的字段。此外，volatile long 和volatile double也具有原子性 。（虽然java内存模型不保证non-volatile long 和 non-volatile double的原子性，当然它们在某些场合也具有原子性。）（译注：non-volatile long在64位JVM，OS，CPU下具有原子性） 当在一个表达式中使用一个non-long或者non-double型字段时，原子性可以确保你将获得这个字段的初始值或者某个线程对这个字段写入之后的值；但不会是两个或更多线程在同一时间对这个字段写入之后产生混乱的结果值（即原子性可以确保，获取到的结果值所对应的所有bit位，全部都是由单个线程写入的）。但是，如下面（译注：指可见性章节）将要看到的，原子性不能确保你获得的是任意线程写入之后的最新值。 因此，原子性保证通常对并发程序设计的影响很小。 可见性只有在下列情况时，一个线程对字段的修改才能确保对另一个线程可见： 一个写线程释放一个锁之后，另一个读线程随后获取了同一个锁。本质上，线程释放锁时会将强制刷新工作内存中的脏数据到主内存中，获取一个锁将强制线程装载（或重新装载）字段的值。锁提供对一个同步方法或块的互斥性执行，线程执行获取锁和释放锁时，所有对字段的访问的内存效果都是已定义的。 注意同步的双重含义：锁提供高级同步协议，同时在线程执行同步方法或块时，内存系统（有时通过内存屏障指令）保证值的一致性。这说明，与顺序程序设计相比较，并发程序设计与分布式程序设计更加类似。同步的第二个特性可以视为一种机制：一个线程在运行已同步方法时，它将发送和/或接收其他线程在同步方法中对变量所做的修改。从这一点来说，使用锁和发送消息仅仅是语法不同而已。 如果把一个字段声明为volatile型，线程对这个字段写入后，在执行后续的内存访问之前，线程必须刷新这个字段且让这个字段对其他线程可见（即该字段立即刷新）。每次对volatile字段的读访问，都要重新装载字段的值。 一个线程首次访问一个对象的字段，它将读到这个字段的初始值或被某个线程写入后的值。此外，把还未构造完成的对象的引用暴露给某个线程，这是一个错误的做法，在构造函数内部开始一个新线程也是危险的，特别是这个类可能被子类化时。Thread.start有如下的内存效果：调用start方法的线程释放了锁，随后开始执行的新线程获取了这个锁。 如果在子类构造函数执行之前，可运行的超类调用了new Thread(this).start()，当run方法执行时，对象很可能还没有完全初始化。同样，如果你创建且开始一个新线程T，这个线程使用了在执行start之后才创建的一个对象X。你不能确信X的字段值将能对线程T可见。除非你把所有用到X的引用的方法都同步。如果可行的话，你可以在开始T线程之前创建X。 线程终止时，所有写过的变量值都要刷新到主内存中。比如，一个线程使用Thread.join来终止另一个线程，那么第一个线程肯定能看到第二个线程对变量值得修改。 注意，在同一个线程的不同方法之间传递对象的引用，永远也不会出现内存可见性问题。 内存模型确保上述操作最终会发生，一个线程对一个特定字段的特定更新，最终将会对其他线程可见，但这个“最终”可能是很长一段时间。线程之间没有同步时，很难保证对字段的值能在多线程之间保持一致（指写线程对字段的写入立即能对读线程可见）。 特别是，如果字段不是volatile或没有通过同步来访问这个字段，在一个循环中等待其他线程对这个字段的写入，这种情况总是错误的。 在缺乏同步的情况下，模型还允许不一致的可见性。比如，得到一个对象的一个字段的最新值，同时得到这个对象的其他字段的过期的值。同样，可能读到一个引用变量的最新值，但读取到这个引用变量引用的对象的字段的过期值。不管怎样，线程之间的可见性并不总是失效（指线程即使没有使用同步，仍然有可能读取到字段的最新值），内存模型仅仅是允许这种失效发生而已。因此，即使多个线程之间没有使用同步，也不保证一定会发生内存可见性问题（指线程读取到过期的值），java内存模型仅仅是允许内存可见性问题发生而已。 在很多当前的JVM实现和java执行平台中，甚至是在那些使用多处理器的JVM和平台中，也很少出现内存可见性问题。共享同一个CPU的多个线程使用公共的缓存，缺少强大的编译器优化，以及存在强缓存一致性的硬件，这些都会使线程更新后的值能够立即在多线程之间传递。 这使得测试基于内存可见性的错误是不切实际的，因为这样的错误极难发生。或者这种错误仅仅在某个你没有使用过的平台上发生，或仅在未来的某个平台上发生。这些类似的解释对于多线程之间的内存可见性问题来说非常普遍。没有同步的并发程序会出现很多问题，包括内存一致性问题。 有序性有序性规则表现在以下两种场景: 线程内和线程间 从某个线程的角度看方法的执行，指令会按照一种叫“串行”（as-if-serial）的方式执行，此种方式已经应用于顺序编程语言。 这个线程“观察”到其他线程并发地执行非同步的代码时，任何代码都有可能交叉执行。唯一起作用的约束是：对于同步方法，同步块以及volatile字段的操作仍维持相对有序。 再次提醒，这些仅是最小特性的规则。具体到任何一个程序或平台上，可能存在更严格的有序性规则。所以你不能依赖它们，因为即使你的代码遵循了这些更严格的规则，仍可能在不同特性的JVM上运行失败，而且测试非常困难。 需要注意的是，线程内部的观察视角被JLS _[1] _中其他的语义的讨论所采用。例如，算术表达式的计算在线程内看来是从左到右地执行操作（JLS 15.6章节），而这种执行效果是没有必要被其他线程观察到的。 仅当某一时刻只有一个线程操作变量时，线程内的执行表现为串行。出现上述情景，可能是因为使用了同步，互斥体_[2] _或者纯属巧合。当多线程同时运行在非同步的代码里进行公用字段的读写时，会形成一种执行模式。在这种模式下，代码会任意交叉执行，原子性和可见性会失效，以及产生竞态条件。这时线程执行不再表现为串行。 尽管JLS列出了一些特定的合法和非法的重排序，如果碰到所列范围之外的问题，会降低以下这条实践保证 ：运行结果反映了几乎所有的重排序产生的代码交叉执行的情况。所以，没必要去探究这些代码的有序性。 volatile关键字详解：在JMM中volatile的内存语义是锁volatile的特性当我们声明共享变量为volatile后，对这个变量的读/写将会很特别。理解volatile特性的一个好方法是：把对volatile变量的单个读/写，看成是使用同一个监视器锁对这些单个读/写操作做了同步。下面我们通过具体的示例来说明，请看下面的示例代码： class VolatileFeaturesExample { volatile long vl = 0L; // 使用volatile声明64位的long型变量 public void set(long l) { vl = l; // 单个volatile变量的写 } public void getAndIncrement() { vl++; // 复合（多个）volatile变量的读/写 } public long get() { return vl; // 单个volatile变量的读 } }假设有多个线程分别调用上面程序的三个方法，这个程序在语意上和下面程序等价： class VolatileFeaturesExample { long vl = 0L; // 64位的long型普通变量 public synchronized void set(long l) { //对单个的普通 变量的写用同一个监视器同步 vl = l; } public void getAndIncrement () { //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 } public synchronized long get() { //对单个的普通变量的读用同一个监视器同步 return vl; } }如上面示例程序所示，对一个volatile变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个监视器锁来同步，它们之间的执行效果相同。 监视器锁的happens-before规则保证释放监视器和获取监视器的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 简而言之，volatile变量自身具有下列特性：监视器锁的语义决定了临界区代码的执行具有原子性。这意味着即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读写就将具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。 原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。 volatile写-读建立的happens before关系上面讲的是volatile变量自身的特性，对程序员来说，volatile对线程的内存可见性的影响比volatile自身的特性更为重要，也更需要我们去关注。 从JSR-133开始，volatile变量的写-读可以实现线程之间的通信。 从内存语义的角度来说，volatile与监视器锁有相同的效果：volatile写和监视器的释放有相同的内存语义；volatile读与监视器的获取有相同的内存语义。 请看下面使用volatile变量的示例代码：class VolatileExample { int a = 0; volatile boolean flag = false; public void writer() { a = 1; // 1 flag = true; // 2 } public void reader() { if (flag) { //3 int i = a; //4 …… } }} 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens before规则，这个过程建立的happens before 关系可以分为两类： 根据程序次序规则，1 happens before 2; 3 happens before 4。 根据volatile规则，2 happens before 3。 根据happens before 的传递性规则，1 happens before 4。 上述happens before 关系的图形化表现形式如下： 在上图中，每一个箭头链接的两个节点，代表了一个happens before 关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens before保证。 这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 volatile写-读的内存语义volatile写的内存语义如下： 当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。 以上面示例程序VolatileExample为例，假设线程A首先执行writer()方法，随后线程B执行reader()方法，初始时两个线程的本地内存中的flag和a都是初始状态。下图是线程A执行volatile写后，共享变量的状态示意图： 如上图所示，线程A在写flag变量后，本地内存A中被线程A更新过的两个共享变量的值被刷新到主内存中。此时，本地内存A和主内存中的共享变量的值是一致的。 volatile读的内存语义如下： 当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 下面是线程B读同一个volatile变量后，共享变量的状态示意图： 如上图所示，在读flag变量后，本地内存B已经被置为无效。此时，线程B必须从主内存中读取共享变量。线程B的读取操作将导致本地内存B与主内存中的共享变量的值也变成一致的了。 如果我们把volatile写和volatile读这两个步骤综合起来看的话，在读线程B读一个volatile变量后，写线程A在写这个volatile变量之前所有可见的共享变量的值都将立即变得对读线程B可见。 下面对volatile写和volatile读的内存语义做个总结： 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了（其对共享变量所在修改的）消息。 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile变量之前对共享变量所做修改的）消息。 线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过主内存向线程B发送消息。 volatile内存语义的实现下面，让我们来看看JMM如何实现volatile写/读的内存语义。 前文我们提到过重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型。下面是JMM针对编译器制定的volatile重排序规则表： 是否能重排序 第二个操作 第一个操作 普通读/写 普通读/写 volatile读 NO volatile写 举例来说，第三行最后一个单元格的意思是：在程序顺序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。 从上表我们可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略： 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。 下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图： 上图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是volatile写后面的StoreLoad屏障。这个屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面，是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在这里采取了保守策略：在每个volatile写的后面或在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里我们可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图： 上图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： class VolatileBarrierExample { int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() { int i = v1; // 第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; // 普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; // 第二个 volatile写 } }针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化： 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器常常会在这里插入一个StoreLoad屏障。 上面的优化是针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以x86处理器为例，上图中除最后的StoreLoad屏障外，其它的屏障都会被省略。 前面保守策略下的volatile读和写，在 x86处理器平台可以优化成： 前文提到过，x86处理器仅会对写-读操作做重排序。X86不会对读-读，读-写和写-写操作做重排序，因此在x86处理器中会省略掉这三种操作类型对应的内存屏障。在x86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在x86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 JSR-133为什么要增强volatile的内存语义在JSR-133之前的旧Java内存模型中，虽然不允许volatile变量之间重排序，但旧的Java内存模型允许volatile变量与普通变量之间重排序。在旧的内存模型中，VolatileExample示例程序可能被重排序成下列时序来执行： 在旧的内存模型中，当1和2之间没有数据依赖关系时，1和2之间就可能被重排序（3和4类似）。其结果就是：读线程B执行4时，不一定能看到写线程A在执行1时对共享变量的修改。 因此在旧的内存模型中 ，volatile的写-读没有监视器的释放-获所具有的内存语义。为了提供一种比监视器锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和监视器的释放-获取一样，具有相同的内存语义。从编译器重排序规则和处理器内存屏障插入策略来看，只要volatile变量与普通变量之间的重排序可能会破坏volatile的内存语意，这种重排序就会被编译器重排序规则和处理器内存屏障插入策略禁止。 由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而监视器锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性。在功能上，监视器锁比volatile更强大；在可伸缩性和执行性能上，volatile更有优势。如果读者想在程序中用volatile代替监视器锁，请一定谨慎。 ##CAS操作详解 本文属于作者原创，原文发表于InfoQ：http://www.infoq.com/cn/articles/atomic-operation 引言原子（atom）本意是“不能被进一步分割的最小粒子”，而原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。在多处理器上实现原子操作就变得有点复杂。本文让我们一起来聊一聊在Inter处理器和Java里是如何实现原子操作的。 术语定义 术语名称 英文 解释 缓存行 Cache line 缓存的最小操作单位 比较并交换 Compare and Swap CAS操作需要输入两个数值，一个旧值（期望操作前的值）和一个新值，在操作期间先比较下在旧值有没有发生变化，如果没有发生变化，才交换成新值，发生了变化则不交换。 CPU流水线 CPU pipeline CPU流水线的工作方式就象工业生产上的装配流水线，在CPU中由56个不同功能的电路单元组成一条指令处理流水线，然后将一条X86指令分成56步后再由这些电路单元分别执行，这样就能实现在一个CPU时钟周期完成一条指令，因此提高CPU的运算速度。 内存顺序冲突 Memory order violation 内存顺序冲突一般是由假共享引起，假共享是指多个CPU同时修改同一个缓存行的不同部分而引起其中一个CPU的操作无效，当出现这个内存顺序冲突时，CPU必须清空流水线。 3 处理器如何实现原子操作32位IA-32处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。 3.1 处理器自动保证基本内存操作的原子性首先处理器会自动保证基本的内存操作的原子性。处理器保证从系统内存当中读取或者写入一个字节是原子的，意思是当一个处理器读取一个字节时，其他处理器不能访问这个字节的内存地址。奔腾6和最新的处理器能自动保证单处理器对同一个缓存行里进行16/32/64位的操作是原子的，但是复杂的内存操作处理器不能自动保证其原子性，比如跨总线宽度，跨多个缓存行，跨页表的访问。但是处理器提供总线锁定和缓存锁定两个机制来保证复杂内存操作的原子性。 3.2 使用总线锁保证原子性第一个机制是通过总线锁保证原子性。如果多个处理器同时对共享变量进行读改写（i++就是经典的读改写操作）操作，那么共享变量就会被多个处理器同时进行操作，这样读改写操作就不是原子的，操作完之后共享变量的值会和期望的不一致，举个例子：如果i=1,我们进行两次i++操作，我们期望的结果是3，但是有可能结果是2。如下图 （例1） 原因是有可能多个处理器同时从各自的缓存中读取变量i，分别进行加一操作，然后分别写入系统内存当中。那么想要保证读改写共享变量的操作是原子的，就必须保证CPU1读改写共享变量的时候，CPU2不能操作缓存了该共享变量内存地址的缓存。 处理器使用总线锁就是来解决这个问题的。所谓总线锁就是使用处理器提供的一个LOCK＃信号，当一个处理器在总线上输出此信号时，其他处理器的请求将被阻塞住,那么该处理器可以独占使用共享内存。 3.3 使用缓存锁保证原子性第二个机制是通过缓存锁定保证原子性。在同一时刻我们只需保证对某个内存地址的操作是原子性即可，但总线锁定把CPU和内存之间通信锁住了，这使得锁定期间，其他处理器不能操作其他内存地址的数据，所以总线锁定的开销比较大，最近的处理器在某些场合下使用缓存锁定代替总线锁定来进行优化。 频繁使用的内存会缓存在处理器的L1，L2和L3高速缓存里，那么原子操作就可以直接在处理器内部缓存中进行，并不需要声明总线锁，在奔腾6和最近的处理器中可以使用“缓存锁定”的方式来实现复杂的原子性。所谓“缓存锁定”就是如果缓存在处理器缓存行中内存区域在LOCK操作期间被锁定，当它执行锁操作回写内存时，处理器不在总线上声言LOCK＃信号，而是修改内部的内存地址，并允许它的缓存一致性机制来保证操作的原子性，因为缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据，当其他处理器回写已被锁定的缓存行的数据时会起缓存行无效，在例1中，当CPU1修改缓存行中的i时使用缓存锁定，那么CPU2就不能同时缓存了i的缓存行。 但是有两种情况下处理器不会使用缓存锁定。第一种情况是：当操作的数据不能被缓存在处理器内部，或操作的数据跨多个缓存行（cache line），则处理器会调用总线锁定。第二种情况是：有些处理器不支持缓存锁定。对于Inter486和奔腾处理器,就算锁定的内存区域在处理器的缓存行中也会调用总线锁定。 以上两个机制我们可以通过Inter处理器提供了很多LOCK前缀的指令来实现。比如位测试和修改指令BTS，BTR，BTC，交换指令XADD，CMPXCHG和其他一些操作数和逻辑指令，比如ADD（加），OR（或）等，被这些指令操作的内存区域就会加锁，导致其他处理器不能同时访问它。 4 JAVA如何实现原子操作在java中可以通过锁和循环CAS的方式来实现原子操作。 4.1 使用循环CAS实现原子操作JVM中的CAS操作正是利用了上一节中提到的处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到成功为止，以下代码实现了一个基于CAS线程安全的计数器方法safeCount和一个非线程安全的计数器count。 package Test; import java.util.ArrayList; import java.util.List; import java.util.concurrent.atomic.AtomicInteger; public class Counter { private AtomicInteger atomicI = new AtomicInteger(); private int i = 0; public static void main(String[] args) { final Counter cas = new Counter(); List&lt;Thread&gt; ts = new ArrayList&lt;Thread&gt;(); long start = System.currentTimeMillis(); for (int j = 0; j &lt; 100; j++) { Thread t = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; 10000; i++) { cas.count(); cas.safeCount(); } } }); ts.add(t); } for (Thread t : ts) { t.start(); } // 等待所有线程执行完成 for (Thread t : ts) { try { t.join(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(cas.i); System.out.println(cas.atomicI.get()); System.out.println(System.currentTimeMillis() - start); } /** * * 使用CAS实现线程安全计数器 * */ private void safeCount() { for (;;) { int i = atomicI.get(); boolean suc = atomicI.compareAndSet(i, ++i); if (suc) { break; } } } /** * * 非线程安全计数器 * */ private void count() { i++; } } 结果 992362 1000000 75从Java1.5开始JDK的并发包里提供了一些类来支持原子操作，如AtomicBoolean（用原子方式更新的 boolean 值），AtomicInteger（用原子方式更新的 int 值），AtomicLong（用原子方式更新的 long 值），这些原子包装类还提供了有用的工具方法，比如以原子的方式将当前值自增1和自减1。 在Java并发包中有一些并发框架也使用了自旋CAS的方式来实现原子操作，比如LinkedTransferQueue类的Xfer方法。CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作。 ABA问题。因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。 从Java1.5开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 public boolean compareAndSet( V expectedReference,//预期引用 V newReference,//更新后的引用 int expectedStamp, //预期标志 int newStamp //更新后的标志 ) 循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 4.2 使用锁机制实现原子操作锁机制保证了只有获得锁的线程能够操作锁定的内存区域。JVM内部实现了很多种锁机制，有偏向锁，轻量级锁和互斥锁，有意思的是除了偏向锁，JVM实现锁的方式都用到的循环CAS，当一个线程想进入同步块的时候使用循环CAS的方式来获取锁，当它退出同步块的时候使用循环CAS释放锁。详细说明可以参见文章Java SE1.6中的Synchronized。 5 参考资料 Java SE1.6中的Synchronized Intel 64和IA-32架构软件开发人员手册 深入分析Volatile的实现原理微信公众号 个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南1：并发基础与Java多线程]]></title>
    <url>%2F2019%2F10%2F02%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%971%EF%BC%9A%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80%E4%B8%8EJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[转自https://javadoop.com/post/design-pattern 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 1多线程的优点 资源利用率更好 程序设计在某些情况下更简单 程序响应更快 1.1资源利用率更好案例方式1从磁盘读取一个文件需要5秒，处理一个文件需要2秒。处理两个文件则需要14秒 11 5秒读取文件A2 2秒处理文件A3 5秒读取文件B4 2秒处理文件B5 ---------------------6 总共需要14秒 方式2从磁盘中读取文件的时候，大部分的CPU非常的空闲。它可以做一些别的事情。通过改变操作的顺序，就能够更好的使用CPU资源。看下面的顺序： 11 5秒读取文件A2 5秒读取文件B + 2秒处理文件A3 2秒处理文件B4 ---------------------5 总共需要12秒 总结：多线程并发效率提高2秒 1.2程序响应更快设想一个服务器应用，它在某一个端口监听进来的请求。当一个请求到来时，它把请求传递给工作者线程(worker thread)，然后立刻返回去监听。而工作者线程则能够处理这个请求并发送一个回复给客户端。 while(server is active){ listenThread for request hand request to workerThread }这种方式，服务端线程迅速地返回去监听。因此，更多的客户端能够发送请求给服务端。这个服务也变得响应更快。 2多线程的代价2.1设计更复杂多线程一般都复杂。在多线程访问共享数据的时候，这部分代码需要特别的注意。线程之间的交互往往非常复杂。不正确的线程同步产生的错误非常难以被发现，并且重现以修复。 2.2上下文切换的开销上下文切换当CPU从执行一个线程切换到执行另外一个线程的时候，它需要先存储当前线程的本地的数据，程序指针等，然后载入另一个线程的本地数据，程序指针等，最后才开始执行。 CPU会在一个上下文中执行一个线程，然后切换到另外一个上下文中执行另外一个线程。 上下文切换并不廉价。如果没有必要，应该减少上下文切换的发生。 2.3增加资源消耗每个线程需要消耗的资源： CPU，内存（维持它本地的堆栈），操作系统资源（管理线程） 3竞态条件与临界区当多个线程竞争同一资源时，如果对资源的访问顺序敏感，就称存在竞态条件。导致竞态条件发生的代码区称作临界区。 多线程同时执行下面的代码可能会出错： public class Counter { protected long count = 0; public void add(long value) { this.count = this.count + value; } }想象下线程A和B同时执行同一个Counter对象的add()方法，我们无法知道操作系统何时会在两个线程之间切换。JVM并不是将这段代码视为单条指令来执行的，而是按照下面的顺序 从内存获取 this.count 的值放到寄存器 将寄存器中的值增加value 将寄存器中的值写回内存 观察线程A和B交错执行会发生什么 this.count = 0; A: 读取 this.count 到一个寄存器 (0) B: 读取 this.count 到一个寄存器 (0) B: 将寄存器的值加2 B: 回写寄存器值(2)到内存. this.count 现在等于 2 A: 将寄存器的值加3由于两个线程是交叉执行的，两个线程从内存中读出的初始值都是0。然后各自加了2和3，并分别写回内存。最终的值可能并不是期望的5，而是最后写回内存的那个线程的值，上面例子中最后写回内存可能是线程A，也可能是线程B 4线程的运行与创建Java 创建线程对象有两种方法： 继承 Thread 类创建线程对象 实现 Runnable 接口类创建线程对象 注意： 在java中，每次程序运行至少启动2个线程。一个是main线程，一个是垃圾收集线程。因为每当使用java命令执行一个类的时候，实际上都会启动一个jvm，每一个jvm实际上就是在操作系统中启动了一个进程。 5线程的状态和优先级线程优先级1 到 10 ，其中 1 是最低优先级，10 是最高优先级。 状态 new（新建） runnnable（可运行） blocked（阻塞） waiting（等待） time waiting （定时等待） terminated（终止） 状态转换 线程状态流程如下： 线程创建后，进入 new 状态 调用 start 或者 run 方法，进入 runnable 状态 JVM 按照线程优先级及时间分片等执行 runnable 状态的线程。开始执行时，进入 running 状态 如果线程执行 sleep、wait、join，或者进入 IO 阻塞等。进入 wait 或者 blocked 状态 线程执行完毕后，线程被线程队列移除。最后为 terminated 状态 代码 public class MyThreadInfo extends Thread { @Override // 可以省略 public void run() { System.out.println(&quot;run&quot;); // System.exit(1); } public static void main(String[] args) { MyThreadInfo thread = new MyThreadInfo(); thread.start(); System.out.println(&quot;线程唯一标识符：&quot; + thread.getId()); System.out.println(&quot;线程名称：&quot; + thread.getName()); System.out.println(&quot;线程状态：&quot; + thread.getState()); System.out.println(&quot;线程优先级：&quot; + thread.getPriority()); } } 结果： 线程唯一标识符：9 线程名称：Thread-0 run 线程状态：RUNNABLE微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发指南2：Java并发指南2：深入理解Java内存模型JMM.md]]></title>
    <url>%2F2019%2F10%2F02%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E6%8C%87%E5%8D%972%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8BJMM.md%2F</url>
    <content type="text"><![CDATA[本文转载自并发编程网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章同步发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Java并发指南》其中一篇，本文大部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何全面深入地学习Java并发技术，从Java多线程基础，再到并发编程的基础知识，从Java并发包的入门和实战，再到JUC的源码剖析，一步步地学习Java并发编程，并上手进行实战，以便让你更完整地了解整个Java并发编程知识体系，形成自己的知识框架。 为了更好地总结和检验你的学习成果，本系列文章也会提供一些对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 一：JMM基础与happens-before并发编程模型的分类在并发编程中，我们需要处理两个关键问题：线程之间如何通信及线程之间如何同步（这里的线程是指并发执行的活动实体）。通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java内存模型的抽象 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。如果编写多线程程序的Java程序员不理解隐式进行的线程之间通信的工作机制，很可能会遇到各种奇怪的内存可见性问题。 在java中，所有实例域、静态域和数组元素存储在堆内存中，堆内存在线程之间共享（本文使用“共享变量”这个术语代指实例域，静态域和数组元素）。局部变量（Local variables），方法定义参数（java语言规范称之为formal method parameters）和异常处理器参数（exception handler parameters）不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java线程之间的通信由Java内存模型（本文简称为JMM）控制，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其他的硬件和编译器优化。Java内存模型的抽象示意图如下： 从上图来看，线程A与线程B之间如要通信的话，必须要经历下面2个步骤： 首先，线程A把本地内存A中更新过的共享变量刷新到主内存中去。 然后，线程B到主内存中去读取线程A之前已更新过的共享变量。 下面通过示意图来说明这两个步骤： 如上图所示，本地内存A和B有主内存中共享变量x的副本。假设初始时，这三个内存中的x值都为0。线程A在执行时，把更新后的x值（假设值为1）临时存放在自己的本地内存A中。当线程A和线程B需要通信时，线程A首先会把自己本地内存中修改后的x值刷新到主内存中，此时主内存中的x值变为了1。随后，线程B到主内存中去读取线程A更新后的x值，此时线程B的本地内存的x值也变为了1。 从整体来看，这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为java程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从java源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上述的1属于编译器重排序，2和3属于处理器重排序。这些重排序都可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的内存屏障（memory barriers，intel称之为memory fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序与内存屏障指令现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！为了具体说明，请看下面示例： Processor A Processor B a = 1; //A1x = b; //A2 b = 2; //B1y = a; //B2 初始状态：a = b = 0处理器允许执行后得到结果：x = y = 0 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到x = y = 0的结果。具体的原因如下图所示： 这里处理器A和处理器B可以同时把共享变量写入自己的写缓冲区（A1，B1），然后从内存中读取另一个共享变量（A2，B2），最后才把自己写缓存区中保存的脏数据刷新到内存中（A3，B3）。当以这种时序执行时，程序就可以得到x = y = 0的结果。 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1-&gt;A2，但内存操作实际发生的顺序却是：A2-&gt;A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样，这里就不赘述了）。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操做重排序。 下面是常见处理器允许的重排序类型的列表： Load-Load Load-Store Store-Store Store-Load 数据依赖 sparc-TSO N N N Y N x86 N N N Y N ia64 Y Y Y Y N PowerPC Y Y Y Y N 上表单元格中的“N”表示处理器不允许两个操作重排序，“Y”表示允许重排序。 从上表我们可以看出：常见的处理器都允许Store-Load重排序；常见的处理器都不允许对存在数据依赖的操作做重排序。sparc-TSO和x86拥有相对较强的处理器内存模型，它们仅允许对写-读操作做重排序（因为它们都使用了写缓冲区）。 ※注1：sparc-TSO是指以TSO(Total Store Order)内存模型运行时，sparc处理器的特性。 ※注2：上表中的x86包括x64及AMD64。 ※注3：由于ARM处理器的内存模型与PowerPC处理器的内存模型非常类似，本文将忽略它。 ※注4：数据依赖性后文会专门说明。 为了保证内存可见性，java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（buffer fully flush）。 happens-before从JDK5开始，java使用新的JSR -133内存模型（本文除非特别说明，针对的都是JSR- 133内存模型）。JSR-133提出了happens-before的概念，通过这个概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的happens-before规则如下： 程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。 volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个volatile域的读。 传递性：如果A happens- before B，且B happens- before C，那么A happens- before C。 注意，两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens- before的定义很微妙，后文会具体说明happens-before为什么要这么定义。 happens-before与JMM的关系如下图所示： 如上图所示，一个happens-before规则通常对应于多个编译器重排序规则和处理器重排序规则。对于java程序员来说，happens-before规则简单易懂，它避免程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现。 二：重排序与JMM的as-if-serial数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1;b = a; 写一个变量之后，再读这个位置。 写后写 a = 1;a = 2; 写一个变量之后，再写这个变量。 读后写 a = b;b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 as-if-serial语义as-if-serial语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作可能被编译器和处理器重排序。为了具体说明，请看下面计算圆面积的代码示例： double pi = 3.14; //A double r = 1.0; //B double area = pi * r * r; //C上面三个操作的数据依赖关系如下图所示： 如上图所示，A和C之间存在数据依赖关系，同时B和C之间也存在数据依赖关系。因此在最终执行的指令序列中，C不能被重排序到A和B的前面（C排到A和B的前面，程序的结果将会被改变）。但A和B之间没有数据依赖关系，编译器和处理器可以重排序A和B之间的执行顺序。下图是该程序的两种执行顺序： as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器，runtime 和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。as-if-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。 程序顺序规则根据happens- before的程序顺序规则，上面计算圆的面积的示例代码存在三个happens- before关系： A happens- before B； B happens- before C； A happens- before C； 这里的第3个happens- before关系，是根据happens- before的传递性推导出来的。 这里A happens- before B，但实际执行时B却可以排在A之前执行（看上面的重排序后的执行顺序）。在第一章提到过，如果A happens- before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens- before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从happens- before的定义我们可以看出，JMM同样遵从这一目标。 重排序对多线程的影响现在让我们来看看，重排序是否会改变多线程程序的执行结果。请看下面的示例代码： class ReorderExample { int a = 0; boolean flag = false; public void writer() { a = 1; //1 flag = true; //2 } Public void reader() { if (flag) { //3 int i = a * a; //4 …… } } }flag变量是个标记，用来标识变量a是否已被写入。这里假设有两个线程A和B，A首先执行writer()方法，随后B线程接着执行reader()方法。线程B在执行操作4时，能否看到线程A在操作1对共享变量a的写入？ 答案是：不一定能看到。 由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，当操作1和操作2重排序时，可能会产生什么效果？请看下面的程序执行时序图： 如上图所示，操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还根本没有被线程A写入，在这里多线程程序的语义被重排序破坏了！ ※注：本文统一用红色的虚箭线表示错误的读操作，用绿色的虚箭线表示正确的读操作。 下面再让我们看看，当操作3和操作4重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。下面是操作3和操作4重排序后，程序的执行时序图： 在程序中，操作3和操作4存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作3的条件判断为真时，就把该计算结果写入变量i中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 三：顺序一致性内存模型与JMM数据竞争与顺序一致性保证当程序未正确同步时，就会存在数据竞争。java内存模型规范对数据竞争的定义如下： 在一个线程中写一个变量， 在另一个线程读同一个变量， 而且写和读没有通过同步来排序。 当代码中包含数据竞争时，程序的执行往往产生违反直觉的结果（前一章的示例正是如此）。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。 JMM对正确同步的多线程程序的内存一致性做了如下保证： 如果程序是正确同步的，程序的执行将具有顺序一致性（sequentially consistent）–即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同（马上我们将会看到，这对于程序员来说是一个极强的保证）。这里的同步是指广义上的同步，包括对常用同步原语（lock，volatile和final）的正确使用。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程。同时，每一个线程必须按程序的顺序来执行内存读/写操作。从上图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 为了更好的理解，下面我们通过两个示意图来对顺序一致性模型的特性做进一步的说明。 假设有两个线程A和B并发执行。其中A线程有三个操作，它们在程序中的顺序是：A1-&gt;A2-&gt;A3。B线程也有三个操作，它们在程序中的顺序是：B1-&gt;B2-&gt;B3。 假设这两个线程使用监视器来正确同步：A线程的三个操作执行后释放监视器，随后B线程获取同一个监视器。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程A和B看到的执行顺序都是：B1-&gt;A1-&gt;A2-&gt;B2-&gt;A3-&gt;B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，且还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 同步程序的顺序一致性效果下面我们对前面的示例程序ReorderExample用监视器来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码： class SynchronizedExample { int a = 0; boolean flag = false; public synchronized void writer() { a = 1; flag = true; } public synchronized void reader() { if (flag) { int i = a; …… } } }上面示例代码中，假设A线程执行writer()方法后，B线程执行reader()方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出监视器和进入监视器这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图（具体细节后文会说明）。虽然线程A在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到JMM在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 未同步程序的执行特性对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false），JMM保证线程读操作读取到的值不会无中生有（out of thin air）的冒出来。为了实现最小安全性，JVM在堆上分配对象时，首先会清零内存空间，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在以清零的内存空间（pre-zeroed memory）分配对象时，域的默认初始化已经完成了。 JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。因为未同步程序在顺序一致性模型中执行时，整体上是无序的，其执行结果无法预知。保证未同步程序在两个模型中的执行结果一致毫无意义。 和顺序一致性模型一样，未同步程序在JMM中的执行时，整体上也是无序的，其执行结果也无法预知。同时，未同步程序在这两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。这一点前面已经讲过了，这里就不再赘述。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序。这一点前面也已经讲过，这里就不再赘述。 JMM不保证对64位的long型和double型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 第3个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（bus transaction）。总线事务包括读事务（read transaction）和写事务（write transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和I/O设备执行内存的读/写。下面让我们通过一个示意图来说明总线的工作机制： 如上图所示，假设处理器A，B和C同时向总线发起总线事务，这时总线仲裁（bus arbitration）会对竞争作出裁决，这里我们假设总线在仲裁后判定处理器A在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器A继续它的总线事务，而其它两个处理器则要等待处理器A的总线事务完成后才能开始再次执行内存访问。假设在处理器A执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器D向总线发起了总线事务，此时处理器D的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些32位的处理器上，如果要求对64位数据的读/写操作具有原子性，会有比较大的开销。为了照顾这种处理器，java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的读/写具有原子性。当JVM在这种处理器上运行时，会把一个64位long/ double型变量的读/写操作拆分为两个32位的读/写操作来执行。这两个32位的读/写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的读/写将不具有原子性。 当单个内存操作不具有原子性，将可能会产生意想不到后果。请看下面示意图： 如上图所示，假设处理器A写一个long型变量，同时处理器B要读这个long型变量。处理器A中64位的写操作被拆分为两个32位的写操作，且 这两个32位的写操作被分配到不同的写事务中执行。同时处理器B中64位的读操作被拆分为两个32位的读操作，且这两个32位的读操作被分配到同一个的读事务中执行。当处理器A和B按上图的时序来执行时，处理器B将看到仅仅被处理器A“写了一半“的无效值。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java并发编程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初探Java设计模式5：一文了解Spring涉及到的9种设计模式]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%88%9D%E6%8E%A2Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F5%EF%BC%9ASpring%E6%B6%89%E5%8F%8A%E5%88%B0%E7%9A%849%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 设计模式作为工作学习中的枕边书，却时常处于勤说不用的尴尬境地，也不是我们时常忘记，只是一直没有记忆。 今天，螃蟹在IT学习者网站就设计模式的内在价值做一番探讨，并以spring为例进行讲解，只有领略了其设计的思想理念，才能在工作学习中运用到“无形”。 Spring作为业界的经典框架，无论是在架构设计方面，还是在代码编写方面，都堪称行内典范。好了，话不多说，开始今天的内容。 spring中常用的设计模式达到九种，我们举例说明： 第一种：简单工厂 又叫做静态工厂方法（StaticFactory Method）模式，但不属于23种GOF设计模式之一。简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。如下配置，就是在 HelloItxxz 类中创建一个 itxxzBean。 &lt;beans&gt; &lt;bean id=&quot;singletonBean&quot; &gt; &lt;constructor-arg&gt; &lt;value&gt;Hello! 这是singletonBean!value&gt; &lt;/constructor-arg&gt; &lt;/ bean&gt; &lt;bean id=&quot;itxxzBean&quot; singleton=&quot;false&quot;&gt; &lt;constructor-arg&gt; &lt;value&gt;Hello! 这是itxxzBean! value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;/beans&gt;第二种：工厂方法（Factory Method） 通常由应用程序直接使用new创建新的对象，为了将对象的创建和使用相分离，采用工厂模式,即应用程序将对象的创建及初始化职责交给工厂对象。 一般情况下,应用程序有自己的工厂对象来创建bean.如果将应用程序自己的工厂对象交给Spring管理,那么Spring管理的就不是普通的bean,而是工厂Bean。 螃蟹就以工厂方法中的静态方法为例讲解一下： import java.util.Random; public class StaticFactoryBean { public static Integer createRandom() { return new Integer(new Random().nextInt()); } }建一个config.xm配置文件，将其纳入Spring容器来管理,需要通过factory-method指定静态方法名称 &lt;bean id=&quot;random&quot; factory-method=&quot;createRandom&quot; //createRandom方法必须是static的,才能找到 scope=&quot;prototype&quot; /&gt;测试: public static void main(String[] args) { //调用getBean()时,返回随机数.如果没有指定factory-method,会返回StaticFactoryBean的实例,即返回工厂Bean的实例 XmlBeanFactory factory = new XmlBeanFactory(new ClassPathResource(&quot;config.xml&quot;)); System.out.println(&quot;我是IT学习者创建的实例:&quot;+factory.getBean(&quot;random&quot;).toString()); }第三种：单例模式（Singleton） 保证一个类仅有一个实例，并提供一个访问它的全局访问点。spring中的单例模式完成了后半句话，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为spring管理的是是任意的java对象。核心提示点：Spring下默认的bean均为singleton，可以通过singleton=“true|false” 或者 scope=“？”来指定 第四种：适配器（Adapter） 在Spring的Aop中，使用的Advice（通知）来增强被代理类的功能。Spring实现这一AOP功能的原理就使用代理模式（1、JDK动态代理。2、CGLib字节码生成技术代理。）对类进行方法级别的切面增强，即，生成被代理类的代理类， 并在代理类的方法前，设置拦截器，通过执行拦截器重的内容增强了代理方法的功能，实现的面向切面编程。 Adapter类接口： public interface AdvisorAdapter { boolean supportsAdvice(Advice advice); MethodInterceptor getInterceptor(Advisor advisor); } **MethodBeforeAdviceAdapter类**，Adapter class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable { public boolean supportsAdvice(Advice advice) { return (advice instanceof MethodBeforeAdvice); } public MethodInterceptor getInterceptor(Advisor advisor) { MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); return new MethodBeforeAdviceInterceptor(advice); } }第五种：包装器（Decorator） 在我们的项目中遇到这样一个问题：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。我们以往在spring和hibernate框架中总是配置一个数据源，因而sessionFactory的dataSource属性总是指向这个数据源并且恒定不变，所有DAO在使用sessionFactory的时候都是通过这个数据源访问数据库。 但是现在，由于项目的需要，我们的DAO在访问sessionFactory的时候都不得不在多个数据源中不断切换，问题就出现了：如何让sessionFactory在执行数据持久化的时候，根据客户的需求能够动态切换不同的数据源？我们能不能在spring的框架下通过少量修改得到解决？是否有什么设计模式可以利用呢？ 首先想到在spring的applicationContext中配置所有的dataSource。这些dataSource可能是各种不同类型的，比如不同的数据库：Oracle、SQL Server、MySQL等，也可能是不同的数据源：比如apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等。然后sessionFactory根据客户的每次请求，将dataSource属性设置成不同的数据源，以到达切换数据源的目的。 spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。基本上都是动态地给一个对象添加一些额外的职责。 第六种：代理（Proxy） 为其他对象提供一种代理以控制对这个对象的访问。 从结构上来看和Decorator模式类似，但Proxy是控制，更像是一种对功能的限制，而Decorator是增加职责。spring的Proxy模式在aop中有体现，比如JdkDynamicAopProxy和Cglib2AopProxy。 第七种：观察者（Observer） 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。spring中Observer模式常用的地方是listener的实现。如ApplicationListener。 第八种：策略（Strategy） 定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。spring中在实例化对象的时候用到Strategy模式在SimpleInstantiationStrategy中有如下代码说明了策略模式的使用情况： 第九种：模板方法（Template Method） 定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。 spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。 怎么办？那我们就用回调对象吧。在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式吧。 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; &#125; public Food makeNoodle() &#123; System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式 首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式 来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 123456789101112131415161718public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println(&quot;咕咕叫&quot;); &#125; public void fly() &#123; System.out.println(&quot;鸡也会飞哦&quot;); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式 废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 123456789101112131415161718192021222324252627// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator_ 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent_ 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage &#123; public String getDescription() &#123; return &quot;红茶&quot;; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return &quot;绿茶&quot;; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + &quot;, 加芒果&quot;; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 参考文章转自https://javadoop.com/post/design-pattern 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初探Java设计模式4：一文带你掌握JDK中的设计模式]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%88%9D%E6%8E%A2Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F4%EF%BC%9AJDK%E4%B8%AD%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 本文主要是归纳了JDK中所包含的设计模式，包括作用和其设计类图。首先来个总结，具体的某个模式可以一个一个慢慢写，希望能对研究JDK和设计模式有所帮助。 设计模式是什么（1）反复出现问题的解决方案（2）增强软件的灵活性（3）适应软件不断变化 学习JDK中设计模式的好处（1）借鉴优秀代码的设计，有助于提高代码设计能力（2）JDK的设计中体现了大多数设计模式，是学习设计模式的较好的方式（3）可以更加深入的了解JDK 类间关系继承、委托、依赖、聚合、组合 介绍方式（1）作用：归纳某设计模式的基本要点（2）JDK中体现：某设计模式在JDK中是怎样体现出来的（3）类图：某设计模式在JDK中所对应的类图 经典设计模式在JDK中的体现1.Singleton（单例）作用：保证类只有一个实例；提供一个全局访问点JDK中体现：（1）Runtime（2）NumberFormat类图： 静态工厂作用： （1）代替构造函数创建对象 （2）方法名比构造函数清晰 JDK中体现： （1）Integer.valueOf （2）Class.forName类图： 工厂方法作用：子类决定哪一个类实例化 JDK中体现：Collection.iterator方法类图： 建造者模式作用： （1）将构造逻辑提到单独的类中 （2）分离类的构造逻辑和表现 JDK中体现：DocumentBuilder(org.w3c.dom)类图： 原型模型作用：（1）复制对象（2）浅复制、深复制JDK中体现：Object.clone；Cloneable类图： 适配器模式作用：使不兼容的接口相容 JDK中体现： （1）java.io.InputStreamReader(InputStream) （2）java.io.OutputStreamWriter(OutputStream)类图： 桥接模式作用：将抽象部分与其实现部分分离，使它们都可以独立地变化 JDK中体现：java.util.logging中的Handler和Formatter类图： 组合模式作用：一致地对待组合对象和独立对象 JDK中体现： （1）org.w3c.dom （2）javax.swing.JComponent#add(Component)类图： 装饰者模式作用：为类添加新的功能；防止类继承带来的爆炸式增长 JDK中体现： （1）java.io包 （2）java.util.Collections#synchronizedList(List)类图： 外观模式作用： （1）封装一组交互类，一致地对外提供接口 （2）封装子系统，简化子系统调用 JDK中体现：java.util.logging包类图： 享元模式作用：共享对象，节省内存 JDK中体现： （1）Integer.valueOf(int i)；Character.valueOf(char c) （2）String常量池类图：** 代理模式作用： （1）透明调用被代理对象，无须知道复杂实现细节 （2）增加被代理类的功能 JDK中体现：动态代理；RMI类图：** 迭代器模式作用：将集合的迭代和集合本身分离 JDK中体现：Iterator、Enumeration接口类图： 观察者模式作用：通知对象状态改变 JDK中体现： （1）java.util.Observer,Observable （2）Swing中的Listener类图：** 模板方法模式作用：定义算法的结构，子类只实现不同的部分 JDK中体现：ThreadPoolExecutor.Worker类图： 策略模式作用：提供不同的算法 JDK中的体现：ThreadPoolExecutor中的四种拒绝策略类图： 责任链模式作用：请求会被链上的对象处理，但是客户端不知道请求会被哪些对象处理 JDK中体现： （1）java.util.logging.Logger会将log委托给parent logger （2）ClassLoader的委托模型类图： 命令模式作用： （1）封装操作，使接口一致 （2）将调用者和接收者在空间和时间上解耦合 JDK中体现：Runnable；Callable；ThreadPoolExecutor类图： 状态模式作用：将主对象和其状态分离，状态对象负责主对象的状态转换，使主对象类功能减轻 JDK中体现：未发现类图： 六、参考文献 1. Design Pattern（GoF） 2. Software Architecture Design Patterns in Java 3. JDK 5 Documentation 4. http://stackoverflow.com/questions/1673841/examples-of-gof-design-patterns 5. http://java.csdn.net/a/20101129/282644.html** 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; &#125; public Food makeNoodle() &#123; System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式 首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式 来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 123456789101112131415161718public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println(&quot;咕咕叫&quot;); &#125; public void fly() &#123; System.out.println(&quot;鸡也会飞哦&quot;); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式 废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 123456789101112131415161718192021222324252627// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator_ 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent_ 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage &#123; public String getDescription() &#123; return &quot;红茶&quot;; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return &quot;绿茶&quot;; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + &quot;, 加芒果&quot;; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 参考文章转自https://javadoop.com/post/design-pattern 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初探Java设计模式3：行为型模式（策略，观察者等）]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%88%9D%E6%8E%A2Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F3%EF%BC%9A%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%EF%BC%88%E7%AD%96%E7%95%A5%EF%BC%8C%E8%A7%82%E5%AF%9F%E8%80%85%E7%AD%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转自https://javadoop.com/post/design-pattern 行为型模式 策略模式 观察者模式 责任链模式 模板方法模式 状态模式 行为型模式总结 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： 123public interface Strategy &#123; public void draw(int radius, int x, int y);&#125; 然后我们定义具体的几个策略： 123456789101112131415161718public class RedPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class GreenPen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class BluePen implements Strategy &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125; 使用策略的类： 1234567891011public class Context &#123; private Strategy strategy; public Context(Strategy strategy)&#123; this.strategy = strategy; &#125; public int executeDraw(int radius, int x, int y)&#123; return strategy.draw(radius, x, y); &#125;&#125; 客户端演示： 1234public static void main(String[] args) &#123; Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);&#125; 放到一张图上，让大家看得清晰些： 存失败重新上传取消​ 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 1234567891011121314151617181920212223242526public class Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() &#123; return state; &#125; public void setState(int state) &#123; this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); &#125; public void attach(Observer observer)&#123; observers.add(observer); &#125; // 通知观察者们 public void notifyAllObservers()&#123; for (Observer observer : observers) &#123; observer.update(); &#125; &#125; &#125; 定义观察者接口： 1234public abstract class Observer &#123; protected Subject subject; public abstract void update();&#125; 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 123456789101112131415161718192021222324252627282930public class BinaryObserver extends Observer &#123; // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) &#123; this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); &#125; // 该方法由主题类在数据变更的时候进行调用 @Override public void update() &#123; String result = Integer.toBinaryString(subject.getState()); System.out.println(&quot;订阅的数据发生变化，新的数据处理为二进制值为：&quot; + result); &#125;&#125;public class HexaObserver extends Observer &#123; public HexaObserver(Subject subject) &#123; this.subject = subject; this.subject.attach(this); &#125; @Override public void update() &#123; String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println(&quot;订阅的数据发生变化，新的数据处理为十六进制值为：&quot; + result); &#125;&#125; 客户端使用也非常简单： 12345678910public static void main(String[] args) &#123; // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);&#125; output: 12订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 1234567891011121314public abstract class RuleHandler &#123; // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) &#123; this.successor = successor; &#125; public RuleHandler getSuccessor() &#123; return successor; &#125;&#125; 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 1234567891011121314public class NewUserRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; if (context.isNewUser()) &#123; // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException(&quot;该活动仅限新用户参与&quot;); &#125; &#125;&#125; 校验用户所在地区是否可以参与： 123456789101112public class LocationRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(context); &#125; &#125; else &#123; throw new RuntimeException(&quot;非常抱歉，您所在的地区无法参与本次活动&quot;); &#125; &#125;&#125; 校验奖品是否已领完： 123456789101112public class LimitRuleHandler extends RuleHandler &#123; public void apply(Context context) &#123; int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) &#123; if (this.getSuccessor() != null) &#123; this.getSuccessor().apply(userInfo); &#125; &#125; else &#123; throw new RuntimeException(&quot;您来得太晚了，奖品被领完了&quot;); &#125; &#125;&#125; 客户端： 123456789public static void main(String[] args) &#123; RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);&#125; 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。 通常会有一个抽象类： 123456789101112131415public abstract class AbstractTemplate &#123; // 这就是模板方法 public void templateMethod()&#123; init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 &#125; protected void init() &#123; System.out.println(&quot;init 抽象层已经实现，子类也可以选择覆写&quot;); &#125; // 留给子类实现 protected abstract void apply(); protected void end() &#123; &#125;&#125; 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 12345678public class ConcreteTemplate extends AbstractTemplate &#123; public void apply() &#123; System.out.println(&quot;子类实现抽象方法 apply&quot;); &#125; public void end() &#123; System.out.println(&quot;我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了&quot;); &#125;&#125; 客户端调用演示： 12345public static void main(String[] args) &#123; AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();&#125; 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： 123public interface State &#123; public void doAction(Context context);&#125; 定义减库存的状态： 12345678910111213public class DeductState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;商品卖出，准备减库存&quot;); context.setState(this); //... 执行减库存的具体操作 &#125; public String toString()&#123; return &quot;Deduct State&quot;; &#125;&#125; 定义补库存状态： 1234567891011public class RevertState implements State &#123; public void doAction(Context context) &#123; System.out.println(&quot;给此商品补库存&quot;); context.setState(this); //... 执行加库存的具体操作 &#125; public String toString() &#123; return &quot;Revert State&quot;; &#125;&#125; 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 1234567891011121314public class Context &#123; private State state; private String name; public Context(String name) &#123; this.name = name; &#125; public void setState(State state) &#123; this.state = state; &#125; public void getState() &#123; return this.state; &#125;&#125; 我们来看下客户端调用，大家就一清二楚了： 123456789101112131415public static void main(String[] args) &#123; // 我们需要操作的是 iPhone X Context context = new Context(&quot;iPhone X&quot;); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();&#125; 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 行为型模式总结行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，我就不进行介绍了。 总结学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。这次整理这篇文章，让我重新审视了一下各个设计模式，对我自己而言收获还是挺大的。我想，文章的最大收益者一般都是作者本人，为了写一篇文章，需要巩固自己的知识，需要寻找各种资料，而且，自己写过的才最容易记住，也算是我给读者的建议吧。 （全文完） 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; &#125; public Food makeNoodle() &#123; System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式 首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式 来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 123456789101112131415161718public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println(&quot;咕咕叫&quot;); &#125; public void fly() &#123; System.out.println(&quot;鸡也会飞哦&quot;); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式 废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 123456789101112131415161718192021222324252627// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator_ 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent_ 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage &#123; public String getDescription() &#123; return &quot;红茶&quot;; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return &quot;绿茶&quot;; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + &quot;, 加芒果&quot;; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 参考文章转自https://javadoop.com/post/design-pattern 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初探Java设计模式2：结构型模式（代理模式，适配器模式等）]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%88%9D%E6%8E%A2Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F2%EF%BC%9A%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%EF%BC%8C%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F%E7%AD%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public interface FoodService &#123; Food makeChicken(); Food makeNoodle();&#125;public class FoodServiceImpl implements FoodService &#123; public Food makeChicken() &#123; Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; &#125; public Food makeNoodle() &#123; Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; &#125;&#125;// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService &#123; // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() &#123; System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; &#125; public Food makeNoodle() &#123; System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; &#125;&#125; 客户端调用，注意，我们要用代理来实例化接口： 123// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配。 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式 首先，我们先看看最简单的适配器模式默认适配器模式(Default Adapter)是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 12345678910public interface FileAlterationListener &#123; void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);&#125; 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 1234567891011121314151617181920212223242526public class FileAlterationListenerAdaptor implements FileAlterationListener &#123; public void onStart(final FileAlterationObserver observer) &#123; &#125; public void onDirectoryCreate(final File directory) &#123; &#125; public void onDirectoryChange(final File directory) &#123; &#125; public void onDirectoryDelete(final File directory) &#123; &#125; public void onFileCreate(final File file) &#123; &#125; public void onFileChange(final File file) &#123; &#125; public void onFileDelete(final File file) &#123; &#125; public void onStop(final FileAlterationObserver observer) &#123; &#125;&#125; 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor &#123; public void onFileCreate(final File file) &#123; // 文件创建 doSomething(); &#125; public void onFileDelete(final File file) &#123; // 文件删除 doSomething(); &#125;&#125; 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式 来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 123456789101112131415161718public interface Duck &#123; public void quack(); // 鸭的呱呱叫 public void fly(); // 飞&#125;public interface Cock &#123; public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞&#125;public class WildCock implements Cock &#123; public void gobble() &#123; System.out.println(&quot;咕咕叫&quot;); &#125; public void fly() &#123; System.out.println(&quot;鸡也会飞哦&quot;); &#125;&#125; 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 123456789101112131415161718192021// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck &#123; Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) &#123; this.cock = cock; &#125; // 实现鸭的呱呱叫方法 @Override public void quack() &#123; // 内部其实是一只鸡的咕咕叫 cock.gobble(); &#125; @Override public void fly() &#123; cock.fly(); &#125;&#125; 客户端调用很简单了： 1234567public static void main(String[] args) &#123; // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...&#125; 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式 废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI &#123; public void draw(int radius, int x, int y);&#125; 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class GreenPen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125;public class BluePen implements DrawAPI &#123; @Override public void draw(int radius, int x, int y) &#123; System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); &#125;&#125; 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape &#123; protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI)&#123; this.drawAPI = drawAPI; &#125; public abstract void draw(); &#125; 定义抽象类的子类： 123456789101112131415161718192021222324252627// 圆形public class Circle extends Shape &#123; private int radius; public Circle(int radius, DrawAPI drawAPI) &#123; super(drawAPI); this.radius = radius; &#125; public void draw() &#123; drawAPI.draw(radius, 0, 0); &#125;&#125;// 长方形public class Rectangle extends Shape &#123; private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) &#123; super(drawAPI); this.x = x; this.y = y; &#125; public void draw() &#123; drawAPI.draw(0, x, y); &#125;&#125; 最后，我们来看客户端演示： 1234567public static void main(String[] args) &#123; Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();&#125; 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 本节引用了这里的例子，并对其进行了修改。 装饰模式要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator_ 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent_ 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage &#123; // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();&#125; 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage &#123; public String getDescription() &#123; return &quot;红茶&quot;; &#125; public double cost() &#123; return 10; &#125;&#125;public class GreenTea extends Beverage &#123; public String getDescription() &#123; return &quot;绿茶&quot;; &#125; public double cost() &#123; return 11; &#125;&#125;...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 1234// 调料public abstract class Condiment extends Beverage &#123;&#125; 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment &#123; private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; &#125; public double cost() &#123; // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 &#125;&#125;public class Mango extends Condiment &#123; private Beverage bevarage; public Mango(Beverage bevarage) &#123; this.bevarage = bevarage; &#125; public String getDescription() &#123; return bevarage.getDescription() + &quot;, 加芒果&quot;; &#125; public double cost() &#123; return beverage.cost() + 3; // 加芒果需要 3 元 &#125;&#125;...// 给每一种调料都加一个类 看客户端调用： 12345678910public static void main(String[] args) &#123; // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;&#125; 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类： 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 123DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的。 门面模式门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 123public interface Shape &#123; void draw();&#125; 定义几个实现类： 123456789101112131415public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Circle::draw()&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Rectangle::draw()&quot;); &#125;&#125; 客户端调用： 123456789public static void main(String[] args) &#123; // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();&#125; 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 12345678910111213141516171819202122232425public class ShapeMaker &#123; private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() &#123; circle = new Circle(); rectangle = new Rectangle(); square = new Square(); &#125; /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle()&#123; circle.draw(); &#125; public void drawRectangle()&#123; rectangle.draw(); &#125; public void drawSquare()&#123; square.draw(); &#125;&#125; 看看现在客户端怎么调用： 12345678public static void main(String[] args) &#123; ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); &#125; 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee &#123; private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) &#123; this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); &#125; public void add(Employee e) &#123; subordinates.add(e); &#125; public void remove(Employee e) &#123; subordinates.remove(e); &#125; public List&lt;Employee&gt; getSubordinates()&#123; return subordinates; &#125; public String toString()&#123; return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); &#125; &#125; 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 参考文章转自https://javadoop.com/post/design-pattern 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[初探Java设计模式1：创建型模式（工厂，单例等）]]></title>
    <url>%2F2019%2F09%2F30%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%88%9D%E6%8E%A2Java%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F1%EF%BC%9A%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F%EF%BC%88%E5%B7%A5%E5%8E%82%EF%BC%8C%E5%8D%95%E4%BE%8B%E7%AD%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[转自https://javadoop.com/post/design-pattern 系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章也将发表在我的个人博客，阅读体验更佳： www.how2playlife.com 一直想写一篇介绍设计模式的文章，让读者可以很快看完，而且一看就懂，看懂就会用，同时不会将各个模式搞混。自认为本文还是写得不错的，花了不少心思来写这文章和做图，力求让读者真的能看着简单同时有所收获。 设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结，其中最出名的当属 Gang of Four (GoF) 的分类了，他们将设计模式分类为 23 种经典的模式，根据用途我们又可以分为三大类，分别为创建型模式、结构型模式和行为型模式。是的，我不善于扯这些有的没的，还是少点废话吧~ 有一些重要的设计原则在开篇和大家分享下，这些原则将贯通全文： 面向接口编程，而不是面向实现。这个很重要，也是优雅的、可扩展的代码的第一步，这就不需要多说了吧。 职责单一原则。每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来。 对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。 目录 创建型模式 简单工厂模式 工厂模式 抽象工厂模式 单例模式 建造者模式 原型模式 创建型模式总结 创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式和名字一样简单，非常简单，直接上代码吧： 12345678910111213141516public class FoodFactory &#123; public static Food makeFood(String name) &#123; if (name.equals(&quot;noodle&quot;)) &#123; Food noodle = new LanZhouNoodle(); noodle.addSpicy(&quot;more&quot;); return noodle; &#125; else if (name.equals(&quot;chicken&quot;)) &#123; Food chicken = new HuangMenChicken(); chicken.addCondiment(&quot;potato&quot;); return chicken; &#125; else &#123; return null; &#125; &#125;&#125; 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 1234567891011121314151617181920212223242526272829public interface FoodFactory &#123; Food makeFood(String name);&#125;public class ChineseFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals(&quot;A&quot;)) &#123; return new ChineseFoodA(); &#125; else if (name.equals(&quot;B&quot;)) &#123; return new ChineseFoodB(); &#125; else &#123; return null; &#125; &#125;&#125;public class AmericanFoodFactory implements FoodFactory &#123; @Override public Food makeFood(String name) &#123; if (name.equals(&quot;A&quot;)) &#123; return new AmericanFoodA(); &#125; else if (name.equals(&quot;B&quot;)) &#123; return new AmericanFoodB(); &#125; else &#123; return null; &#125; &#125;&#125; 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 12345678public class APP &#123; public static void main(String[] args) &#123; // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood(&quot;A&quot;); &#125;&#125; 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。 核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样读者看着比较清晰： 存失败重新上传取消​ 抽象工厂模式当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 存失败重新上传取消​ 这个时候的客户端调用是这样的： 12345678910// 得到 Intel 的 CPUCPUFactory cpuFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果 Intel 家产的 CPU 和 AMD 产的主板不能兼容使用，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 12345678910111213public static void main(String[] args) &#123; // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);&#125; 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式单例模式用得最多，错得最多。 饿汉模式最简单： 12345678910111213public class Singleton &#123; // 首先，将 new Singleton() 堵死 private Singleton() &#123;&#125;; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 private static Singleton instance = new Singleton(); public static Singleton getInstance() &#123; return instance; &#125; // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) &#123;return new Date();&#125;&#125; 很多人都能说出饿汉模式的缺点，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 12345678910111213141516171819public class Singleton &#123; // 首先，也是先堵死 new Singleton() 这条路 private Singleton() &#123;&#125; // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() &#123; if (instance == null) &#123; // 加锁 synchronized (Singleton.class) &#123; // 这一次判断也是必须的，不然会有并发问题 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 双重检查，指的是两次检查 instance 是否为 null。 volatile 在这里是需要的，希望能引起读者的关注。 很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 1234567891011public class Singleton3 &#123; private Singleton3() &#123;&#125; // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder &#123; private static Singleton3 instance = new Singleton3(); &#125; public static Singleton3 getInstance() &#123; return Holder.instance; &#125;&#125; 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的： 12Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class User &#123; // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) &#123; this.name = name; this.password = password; this.nickName = nickName; this.age = age; &#125; // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() &#123; return new UserBuilder(); &#125; public static class UserBuilder &#123; // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() &#123; &#125; // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) &#123; this.name = name; return this; &#125; public UserBuilder password(String password) &#123; this.password = password; return this; &#125; public UserBuilder nickName(String nickName) &#123; this.nickName = nickName; return this; &#125; public UserBuilder age(int age) &#123; this.age = age; return this; &#125; // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() &#123; if (name == null || password == null) &#123; throw new RuntimeException(&quot;用户名和密码必填&quot;); &#125; if (age &lt;= 0 || age &gt;= 150) &#123; throw new RuntimeException(&quot;年龄不合法&quot;); &#125; // 还可以做赋予”默认值“的功能 if (nickName == null) &#123; nickName = name; &#125; return new User(name, password, nickName, age); &#125; &#125;&#125; 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 123456789public class APP &#123; public static void main(String[] args) &#123; User d = User.builder() .name(&quot;foo&quot;) .password(&quot;pAss12345&quot;) .age(25) .build(); &#125;&#125; 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 1234567@Builderclass User &#123; private String name; private String password; private String nickName; private int age;&#125; 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 return this 就可以了，然后就可以像下面这样调用： 1User user = new User().setName(&quot;&quot;).setPassword(&quot;&quot;).setAge(20); 原型模式这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 1protected native Object clone() throws CloneNotSupportedException; java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 创建型模式总结创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 参考文章转自https://javadoop.com/post/design-pattern 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解8：Java集合类细节精讲，细节决定成败]]></title>
    <url>%2F2019%2F09%2F29%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A38%EF%BC%9AJava%E9%9B%86%E5%90%88%E7%B1%BB%E7%BB%86%E8%8A%82%E7%B2%BE%E8%AE%B2%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 今天我们来探索一下Java集合类中的一些技术细节。主要是对一些比较容易被遗漏和误解的知识点做一些讲解和补充。可能不全面，还请谅解。 初始容量集合是我们在Java编程中使用非常广泛的，它就像大海，海纳百川，像万能容器，盛装万物，而且这个大海，万能容器还可以无限变大（如果条件允许）。当这个海、容器的量变得非常大的时候，它的初始容量就会显得很重要了，因为挖海、扩容是需要消耗大量的人力物力财力的。 同样的道理，Collection的初始容量也显得异常重要。所以：对于已知的情景，请为集合指定初始容量。 public static void main(String[] args) { StudentVO student = null; long begin1 = System.currentTimeMillis(); List&lt;StudentVO&gt; list1 = new ArrayList&lt;&gt;(); for(int i = 0 ; i &lt; 1000000; i++){ student = new StudentVO(i,&quot;chenssy_&quot;+i,i); list1.add(student); } long end1 = System.currentTimeMillis(); System.out.println(&quot;list1 time：&quot; + (end1 - begin1)); long begin2 = System.currentTimeMillis(); List&lt;StudentVO&gt; list2 = new ArrayList&lt;&gt;(1000000); for(int i = 0 ; i &lt; 1000000; i++){ student = new StudentVO(i,&quot;chenssy_&quot;+i,i); list2.add(student); } long end2 = System.currentTimeMillis(); System.out.println(&quot;list2 time：&quot; + (end2 - begin2)); }上面代码两个list都是插入1000000条数据，只不过list1没有没有申请初始化容量，而list2初始化容量1000000。那运行结果如下： list1 time：1638 list2 time：921从上面的运行结果我们可以看出list2的速度是list1的两倍左右。在前面LZ就提过，ArrayList的扩容机制是比较消耗资源的。我们先看ArrayList的add方法： public boolean add(E e) { ensureCapacity(size + 1); elementData[size++] = e; return true; } public void ensureCapacity(int minCapacity) { modCount++; //修改计数器 int oldCapacity = elementData.length; //当前需要的长度超过了数组长度，进行扩容处理 if (minCapacity &gt; oldCapacity) { Object oldData[] = elementData; //新的容量 = 旧容量 * 1.5 + 1 int newCapacity = (oldCapacity * 3)/2 + 1; if (newCapacity &lt; minCapacity) newCapacity = minCapacity; //数组拷贝，生成新的数组 elementData = Arrays.copyOf(elementData, newCapacity); } }ArrayList每次新增一个元素，就会检测ArrayList的当前容量是否已经到达临界点，如果到达临界点则会扩容1.5倍。然而ArrayList的扩容以及数组的拷贝生成新的数组是相当耗资源的。所以若我们事先已知集合的使用场景，知道集合的大概范围，我们最好是指定初始化容量，这样对资源的利用会更加好，尤其是大数据量的前提下，效率的提升和资源的利用会显得更加具有优势。 asList的缺陷在实际开发过程中我们经常使用asList讲数组转换为List，这个方法使用起来非常方便，但是asList方法存在几个缺陷： 避免使用基本数据类型数组转换为列表使用8个基本类型数组转换为列表时会存在一个比较有味的缺陷。先看如下程序： public static void main(String[] args) { int[] ints = {1,2,3,4,5}; List list = Arrays.asList(ints); System.out.println(&quot;list&apos;size：&quot; + list.size()); } ------------------------------------ outPut： list&apos;size：1程序的运行结果并没有像我们预期的那样是5而是逆天的1，这是什么情况？先看源码： public static &lt;T&gt; List&lt;T&gt; asList(T... a) { return new ArrayList&lt;&gt;(a); }asList接受的参数是一个泛型的变长参数，我们知道基本数据类型是无法发型化的，也就是说8个基本类型是无法作为asList的参数的， 要想作为泛型参数就必须使用其所对应的包装类型。但是这个这个实例中为什么没有出错呢？ 因为该实例是将int类型的数组当做其参数，而在Java中数组是一个对象，它是可以泛型化的。所以该例子是不会产生错误的。既然例子是将整个int类型的数组当做泛型参数，那么经过asList转换就只有一个int 的列表了。如下： public static void main(String[] args) { int[] ints = {1,2,3,4,5}; List list = Arrays.asList(ints); System.out.println(&quot;list 的类型:&quot; + list.get(0).getClass()); System.out.println(&quot;list.get(0) == ints：&quot; + list.get(0).equals(ints)); } outPut:list 的类型:class [Ilist.get(0) == ints：true从这个运行结果我们可以充分证明list里面的元素就是int数组。弄清楚这点了，那么修改方法也就一目了然了：将int 改变为Integer。 public static void main(String[] args) { Integer[] ints = {1,2,3,4,5}; List list = Arrays.asList(ints); System.out.println(&quot;list&apos;size：&quot; + list.size()); System.out.println(&quot;list.get(0) 的类型:&quot; + list.get(0).getClass()); System.out.println(&quot;list.get(0) == ints[0]：&quot; + list.get(0).equals(ints[0])); } ---------------------------------------- outPut: list&apos;size：5 list.get(0) 的类型:class java.lang.Integer list.get(0) == ints[0]：trueasList产生的列表不可操作对于上面的实例我们再做一个小小的修改： public static void main(String[] args) { Integer[] ints = {1,2,3,4,5}; List list = Arrays.asList(ints); list.add(6); }该实例就是讲ints通过asList转换为list 类别，然后再通过add方法加一个元素，这个实例简单的不能再简单了，但是运行结果呢？打出我们所料： Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException at java.util.AbstractList.add(Unknown Source) at java.util.AbstractList.add(Unknown Source) at com.chenssy.test.arrayList.AsListTest.main(AsListTest.java:10)运行结果尽然抛出UnsupportedOperationException异常，该异常表示list不支持add方法。这就让我们郁闷了，list怎么可能不支持add方法呢？难道jdk脑袋堵塞了？我们再看asList的源码： public static &lt;T&gt; List&lt;T&gt; asList(T... a) { return new ArrayList&lt;&gt;(a); }asList接受参数后，直接new 一个ArrayList，到这里看应该是没有错误的啊？别急，再往下看: private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable{ private static final long serialVersionUID = -2764017481108945198L; private final E[] a; ArrayList(E[] array) { if (array==null) throw new NullPointerException(); a = array; } //................. }这是ArrayList的源码,从这里我们可以看出,此ArrayList不是java.util.ArrayList，他是Arrays的内部类。 该内部类提供了size、toArray、get、set、indexOf、contains方法，而像add、remove等改变list结果的方法从AbstractList父类继承过来，同时这些方法也比较奇葩，它直接抛出UnsupportedOperationException异常： public boolean add(E e) { add(size(), e); return true; } public E set(int index, E element) { throw new UnsupportedOperationException(); } public void add(int index, E element) { throw new UnsupportedOperationException(); } public E remove(int index) { throw new UnsupportedOperationException(); }通过这些代码可以看出asList返回的列表只不过是一个披着list的外衣，它并没有list的基本特性（变长）。该list是一个长度不可变的列表，传入参数的数组有多长，其返回的列表就只能是多长。所以：：不要试图改变asList返回的列表，否则你会自食苦果。 subList的缺陷我们经常使用subString方法来对String对象进行分割处理，同时我们也可以使用subList、subMap、subSet来对List、Map、Set进行分割处理，但是这个分割存在某些瑕疵。 subList返回仅仅只是一个视图首先我们先看如下实例： public static void main(String[] args) { List list1 = new ArrayList(); list1.add(1); list1.add(2); //通过构造函数新建一个包含list1的列表 list2 List&lt;Integer&gt; list2 = new ArrayList&lt;Integer&gt;(list1); //通过subList生成一个与list1一样的列表 list3 List&lt;Integer&gt; list3 = list1.subList(0, list1.size()); //修改list3 list3.add(3); System.out.println(&quot;list1 == list2：&quot; + list1.equals(list2)); System.out.println(&quot;list1 == list3：&quot; + list1.equals(list3)); }这个例子非常简单，无非就是通过构造函数、subList重新生成一个与list1一样的list，然后修改list3，最后比较list1 == list2?、list1 == list3?。 按照我们常规的思路应该是这样的：因为list3通过add新增了一个元素，那么它肯定与list1不等，而list2是通过list1构造出来的，所以应该相等，所以结果应该是： list1 == list2：true list1 == list3: false首先我们先不论结果的正确与否，我们先看subList的源码： public List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex); }subListRangeCheck方式是判断fromIndex、toIndex是否合法，如果合法就直接返回一个subList对象，注意在产生该new该对象的时候传递了一个参数 this ，该参数非常重要，因为他代表着原始list。 /** * 继承AbstractList类，实现RandomAccess接口 */ private class SubList extends AbstractList implements RandomAccess { private final AbstractList parent; //列表 private final int parentOffset; private final int offset; int size; //构造函数 SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) { this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount; } //set方法 public E set(int index, E e) { rangeCheck(index); checkForComodification(); E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue; } //get方法 public E get(int index) { rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); } //add方法 public void add(int index, E e) { rangeCheckForAdd(index); checkForComodification(); parent.add(parentOffset + index, e); this.modCount = parent.modCount; this.size++; } //remove方法 public E remove(int index) { rangeCheck(index); checkForComodification(); E result = parent.remove(parentOffset + index); this.modCount = parent.modCount; this.size--; return result; } }该SubLsit是ArrayList的内部类，它与ArrayList一样，都是继承AbstractList和实现RandomAccess接口。同时也提供了get、set、add、remove等list常用的方法。但是它的构造函数有点特殊，在该构造函数中有两个地方需要注意： 1、this.parent = parent;而parent就是在前面传递过来的list，也就是说this.parent就是原始list的引用。 2、this.offset = offset + fromIndex;this.parentOffset = fromIndex;。同时在构造函数中它甚至将modCount（fail-fast机制）传递过来了。 我们再看get方法，在get方法中return ArrayList.this.elementData(offset + index); 这段代码可以清晰表明get所返回就是原列表offset + index位置的元素。同样的道理还有add方法里面的： parent.add(parentOffset + index, e);this.modCount = parent.modCount;remove方法里面的 E result = parent.remove(parentOffset + index);this.modCount = parent.modCount; 诚然，到了这里我们可以判断subList返回的SubList同样也是AbstractList的子类，同时它的方法如get、set、add、remove等都是在原列表上面做操作，它并没有像subString一样生成一个新的对象。 所以subList返回的只是原列表的一个视图，它所有的操作最终都会作用在原列表上。 那么从这里的分析我们可以得出上面的结果应该恰恰与我们上面的答案相反： list1 == list2：falselist1 == list3：true subList生成子列表后，不要试图去操作原列表从上面我们知道subList生成的子列表只是原列表的一个视图而已，如果我们操作子列表它产生的作用都会在原列表上面表现，但是如果我们操作原列表会产生什么情况呢？ public static void main(String[] args) { List list1 = new ArrayList(); list1.add(1); list1.add(2); //通过subList生成一个与list1一样的列表 list3 List&lt;Integer&gt; list3 = list1.subList(0, list1.size()); //修改list1 list1.add(3); System.out.println(&quot;list1&apos;size：&quot; + list1.size()); System.out.println(&quot;list3&apos;size：&quot; + list3.size()); }该实例如果不产生意外，那么他们两个list的大小都应该都是3，但是偏偏事与愿违，事实上我们得到的结果是这样的： list1&apos;size：3 Exception in thread &quot;main&quot; java.util.ConcurrentModificationException at java.util.ArrayList$SubList.checkForComodification(Unknown Source) at java.util.ArrayList$SubList.size(Unknown Source) at com.chenssy.test.arrayList.SubListTest.main(SubListTest.java:17)list1正常输出，但是list3就抛出ConcurrentModificationException异常，看过我另一篇博客的同仁肯定对这个异常非常，fail-fast？不错就是fail-fast机制，在fail-fast机制中，LZ花了很多力气来讲述这个异常，所以这里LZ就不对这个异常多讲了。我们再看size方法： public int size() { checkForComodification(); return this.size; }size方法首先会通过checkForComodification验证，然后再返回this.size。 private void checkForComodification() { if (ArrayList.this.modCount != this.modCount) throw new ConcurrentModificationException(); }该方法表明当原列表的modCount与this.modCount不相等时就会抛出ConcurrentModificationException。 同时我们知道modCount 在new的过程中 “继承”了原列表modCount，只有在修改该列表（子列表）时才会修改该值（先表现在原列表后作用于子列表）。 而在该实例中我们是操作原列表，原列表的modCount当然不会反应在子列表的modCount上啦，所以才会抛出该异常。 对于子列表视图，它是动态生成的，生成之后就不要操作原列表了，否则必然都导致视图的不稳定而抛出异常。最好的办法就是将原列表设置为只读状态，要操作就操作子列表： //通过subList生成一个与list1一样的列表 list3 List&lt;Integer&gt; list3 = list1.subList(0, list1.size());//对list1设置为只读状态 list1 = Collections.unmodifiableList(list1);推荐使用subList处理局部列表在开发过程中我们一定会遇到这样一个问题：获取一堆数据后，需要删除某段数据。例如，有一个列表存在1000条记录，我们需要删除100-200位置处的数据，可能我们会这样处理： for(int i = 0 ; i &lt; list1.size() ; i++){ if(i &gt;= 100 &amp;&amp; i &lt;= 200){ list1.remove(i); /* * 当然这段代码存在问题，list remove之后后面的元素会填充上来， * 所以需要对i进行简单的处理，当然这个不是这里讨论的问题。 */ } }这个应该是我们大部分人的处理方式吧，其实还有更好的方法，利用subList。在前面LZ已经讲过，子列表的操作都会反映在原列表上。所以下面一行代码全部搞定： list1.subList(100, 200).clear();简单而不失华丽！！！！！ 保持compareTo和equals同步在Java中我们常使用Comparable接口来实现排序，其中compareTo是实现该接口方法。我们知道compareTo返回0表示两个对象相等，返回正数表示大于，返回负数表示小于。同时我们也知道equals也可以判断两个对象是否相等，那么他们两者之间是否存在关联关系呢？ public class Student implements Comparable&lt;Student&gt;{ private String id; private String name; private int age; public Student(String id,String name,int age){ this.id = id; this.name = name; this.age = age; } public boolean equals(Object obj){ if(obj == null){ return false; } if(this == obj){ return true; } if(obj.getClass() != this.getClass()){ return false; } Student student = (Student)obj; if(!student.getName().equals(getName())){ return false; } return true; } public int compareTo(Student student) { return this.age - student.age; } /** 省略getter、setter方法 */ }Student类实现Comparable接口和实现equals方法，其中compareTo是根据age来比对的，equals是根据name来比对的。 public static void main(String[] args){ List&lt;Student&gt; list = new ArrayList&lt;&gt;(); list.add(new Student(&quot;1&quot;, &quot;chenssy1&quot;, 24)); list.add(new Student(&quot;2&quot;, &quot;chenssy1&quot;, 26)); Collections.sort(list); //排序 Student student = new Student(&quot;2&quot;, &quot;chenssy1&quot;, 26); //检索student在list中的位置 int index1 = list.indexOf(student); int index2 = Collections.binarySearch(list, student); System.out.println(&quot;index1 = &quot; + index1); System.out.println(&quot;index2 = &quot; + index2); }按照常规思路来说应该两者index是一致的，因为他们检索的是同一个对象，但是非常遗憾，其运行结果： index1 = 0index2 = 1 为什么会产生这样不同的结果呢？这是因为indexOf和binarySearch的实现机制不同。 indexOf是基于equals来实现的只要equals返回TRUE就认为已经找到了相同的元素。 而binarySearch是基于compareTo方法的，当compareTo返回0 时就认为已经找到了该元素。 在我们实现的Student类中我们覆写了compareTo和equals方法，但是我们的compareTo、equals的比较依据不同，一个是基于age、一个是基于name。 比较依据不同那么得到的结果很有可能会不同。所以知道了原因，我们就好修改了：将两者之间的比较依据保持一致即可。 对于compareTo和equals两个方法我们可以总结为：compareTo是判断元素在排序中的位置是否相等，equals是判断元素是否相等，既然一个决定排序位置，一个决定相等，所以我们非常有必要确保当排序位置相同时，其equals也应该相等。 使其相等的方式就是两者应该依附于相同的条件。当compareto相等时equals也应该相等，而compareto不相等时equals不应该相等，并且compareto依据某些属性来决定排序。 参考文章https://www.cnblogs.com/galibujianbusana/p/6600226.html http://blog.itpub.net/69906029/viewspace-2641300/ https://www.cnblogs.com/itxiaok/p/10356553.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>集合类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解7：一文搞清楚HashSet，TreeSet与LinkedHashSet的异同]]></title>
    <url>%2F2019%2F09%2F29%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A37%EF%BC%9AHashSet%EF%BC%8CTreeSet%E4%B8%8ELinkedHashSet%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 今天我们来探索一下HashSet，TreeSet与LinkedHashSet的基本原理与源码实现，由于这三个set都是基于之前文章的三个map进行实现的，所以推荐大家先看一下前面有关map的文章，结合使用味道更佳。 本文参考www.cmsblogs.com/?p=599 HashSet定义public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.SerializableHashSet继承AbstractSet类，实现Set、Cloneable、Serializable接口。其中AbstractSet提供 Set 接口的骨干实现，从而最大限度地减少了实现此接口所需的工作。==Set接口是一种不包括重复元素的Collection，它维持它自己的内部排序，所以随机访问没有任何意义。== 本文基于1.8jdk进行源码分析。 基本属性 基于HashMap实现，底层使用HashMap保存所有元素 private transient HashMap&lt;E,Object&gt; map; //定义一个Object对象作为HashMap的value private static final Object PRESENT = new Object(); 构造函数 /** * 默认构造函数 * 初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */ public HashSet() { map = new HashMap&lt;&gt;(); } /** * 构造一个包含指定 collection 中的元素的新 set。 */ public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); } /** * 构造一个新的空 set，其底层 HashMap 实例具有指定的初始容量和指定的加载因子 */ public HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;&gt;(initialCapacity, loadFactor); } /** * 构造一个新的空 set，其底层 HashMap 实例具有指定的初始容量和默认的加载因子（0.75）。 */ public HashSet(int initialCapacity) { map = new HashMap&lt;&gt;(initialCapacity); } /** * 在API中我没有看到这个构造函数，今天看源码才发现（原来访问权限为包权限，不对外公开的） * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * dummy 为标识 该构造函数主要作用是对LinkedHashSet起到一个支持作用 */ HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } 从构造函数中可以看出HashSet所有的构造都是构造出一个新的HashMap，其中最后一个构造函数，为包访问权限是不对外公开，仅仅只在使用LinkedHashSet时才会发生作用。方法 既然HashSet是基于HashMap，那么对于HashSet而言，其方法的实现过程是非常简单的。 public Iterator&lt;E&gt; iterator() { return map.keySet().iterator(); } iterator()方法返回对此 set 中元素进行迭代的迭代器。返回元素的顺序并不是特定的。 底层调用HashMap的keySet返回所有的key，这点反应了HashSet中的所有元素都是保存在HashMap的key中，value则是使用的PRESENT对象，该对象为static final。 public int size() { return map.size(); } size()返回此 set 中的元素的数量（set 的容量）。底层调用HashMap的size方法，返回HashMap容器的大小。 public boolean isEmpty() { return map.isEmpty(); } isEmpty()，判断HashSet()集合是否为空，为空返回 true，否则返回false。 public boolean contains(Object o) { return map.containsKey(o); } public boolean containsKey(Object key) { return getNode(hash(key), key) != null; } //最终调用该方法进行节点查找 final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //先检查桶的头结点是否存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //不是头结点，则遍历链表，如果是树节点则使用树节点的方法遍历，直到找到，或者为null if ((e = first.next) != null) { if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; }contains()，判断某个元素是否存在于HashSet()中，存在返回true，否则返回false。更加确切的讲应该是要满足这种关系才能返回true：(o==null ? e==null : o.equals(e))。底层调用containsKey判断HashMap的key值是否为空。 public boolean add(E e) { return map.put(e, PRESENT)==null; } public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } map的put方法： final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //确认初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果桶为空，直接插入新元素，也就是entry if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; //如果冲突，分为三种情况 //key相等时让旧entry等于新entry即可 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //红黑树情况 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { //如果key不相等，则连成链表 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } 这里注意一点，hashset只是不允许重复的元素加入，而不是不允许元素连成链表，因为只要key的equals方法判断为true时它们是相等的，此时会发生value的替换，因为所有entry的value一样，所以和没有插入时一样的。 而当两个hashcode相同但key不相等的entry插入时，仍然会连成一个链表，长度超过8时依然会和hashmap一样扩展成红黑树，看完源码之后笔者才明白自己之前理解错了。所以看源码还是蛮有好处的。hashset基本上就是使用hashmap的方法再次实现了一遍而已，只不过value全都是同一个object，让你以为相同元素没有插入，事实上只是value替换成和原来相同的值而已。 当add方法发生冲突时，如果key相同，则替换value，如果key不同，则连成链表。 add()如果此 set 中尚未包含指定元素，则添加指定元素。如果此Set没有包含满足(e==null ? e2==null : e.equals(e2)) 的e2时，则将e2添加到Set中，否则不添加且返回false。 由于底层使用HashMap的put方法将key = e，value=PRESENT构建成key-value键值对，当此e存在于HashMap的key中，则value将会覆盖原有value，但是key保持不变，所以如果将一个已经存在的e元素添加中HashSet中，新添加的元素是不会保存到HashMap中，所以这就满足了HashSet中元素不会重复的特性。 public boolean remove(Object o) { return map.remove(o)==PRESENT; }remove如果指定元素存在于此 set 中，则将其移除。底层使用HashMap的remove方法删除指定的Entry。 public void clear() { map.clear(); }clear从此 set 中移除所有元素。底层调用HashMap的clear方法清除所有的Entry。 public Object clone() { try { HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; } catch (CloneNotSupportedException e) { throw new InternalError(); } }clone返回此 HashSet 实例的浅表副本：并没有复制这些元素本身。 后记： 由于HashSet底层使用了HashMap实现，使其的实现过程变得非常简单，如果你对HashMap比较了解，那么HashSet简直是小菜一碟。有两个方法对HashMap和HashSet而言是非常重要的，下篇将详细讲解hashcode和equals。 TreeSet与HashSet是基于HashMap实现一样，TreeSet同样是基于TreeMap实现的。在《Java提高篇（二七）—–TreeMap》中LZ详细讲解了TreeMap实现机制，如果客官详情看了这篇博文或者多TreeMap有比较详细的了解，那么TreeSet的实现对您是喝口水那么简单。 TreeSet定义我们知道TreeMap是一个有序的二叉树，那么同理TreeSet同样也是一个有序的，它的作用是提供有序的Set集合。通过源码我们知道TreeSet基础AbstractSet，实现NavigableSet、Cloneable、Serializable接口。 其中AbstractSet提供 Set 接口的骨干实现，从而最大限度地减少了实现此接口所需的工作。 NavigableSet是扩展的 SortedSet，具有了为给定搜索目标报告最接近匹配项的导航方法，这就意味着它支持一系列的导航方法。比如查找与指定目标最匹配项。Cloneable支持克隆，Serializable支持序列化。 public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable同时在TreeSet中定义了如下几个变量。 private transient NavigableMap&lt;E,Object&gt; m; //PRESENT会被当做Map的value与key构建成键值对 private static final Object PRESENT = new Object();其构造方法： //默认构造方法，根据其元素的自然顺序进行排序 public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } //构造一个包含指定 collection 元素的新 TreeSet，它按照其元素的自然顺序进行排序。 public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;&gt;(comparator)); } //构造一个新的空 TreeSet，它根据指定比较器进行排序。 public TreeSet(Collection&lt;? extends E&gt; c) { this(); addAll(c); } //构造一个与指定有序 set 具有相同映射关系和相同排序的新 TreeSet。 public TreeSet(SortedSet&lt;E&gt; s) { this(s.comparator()); addAll(s); } TreeSet(NavigableMap&lt;E,Object&gt; m) { this.m = m; }TreeSet主要方法1、add：将指定的元素添加到此 set（如果该元素尚未存在于 set 中）。 public boolean add(E e) { return m.put(e, PRESENT)==null; } public V put(K key, V value) { Entry&lt;K,V&gt; t = root; if (t == null) { //空树时，判断节点是否为空 compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; } int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; //非空树，根据传入比较器进行节点的插入位置查找 if (cpr != null) { do { parent = t; //节点比根节点小，则找左子树，否则找右子树 cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; //如果key的比较返回值相等，直接更新值（一般compareto相等时equals方法也相等） else return t.setValue(value); } while (t != null); } else { //如果没有传入比较器，则按照自然排序 if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do { parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); } while (t != null); } //查找的节点为空，直接插入，默认为红节点 Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; //插入后进行红黑树调整 fixAfterInsertion(e); size++; modCount++; return null; } 2、get：获取元素 public V get(Object key) { Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value); }该方法与put的流程类似，只不过是把插入换成了查找 3、ceiling：返回此 set 中大于等于给定元素的最小元素；如果不存在这样的元素，则返回 null。 public E ceiling(E e) { return m.ceilingKey(e); }4、clear：移除此 set 中的所有元素。 public void clear() { m.clear(); }5、clone：返回 TreeSet 实例的浅表副本。属于浅拷贝。 public Object clone() { TreeSet&lt;E&gt; clone = null; try { clone = (TreeSet&lt;E&gt;) super.clone(); } catch (CloneNotSupportedException e) { throw new InternalError(); } clone.m = new TreeMap&lt;&gt;(m); return clone; }6、comparator：返回对此 set 中的元素进行排序的比较器；如果此 set 使用其元素的自然顺序，则返回 null。 public Comparator&lt;? super E&gt; comparator() { return m.comparator(); }7、contains：如果此 set 包含指定的元素，则返回 true。 public boolean contains(Object o) { return m.containsKey(o); }8、descendingIterator：返回在此 set 元素上按降序进行迭代的迭代器。 public Iterator&lt;E&gt; descendingIterator() { return m.descendingKeySet().iterator(); }9、descendingSet：返回此 set 中所包含元素的逆序视图。 public NavigableSet&lt;E&gt; descendingSet() { return new TreeSet&lt;&gt;(m.descendingMap()); }10、first：返回此 set 中当前第一个（最低）元素。 public E first() { return m.firstKey(); }11、floor：返回此 set 中小于等于给定元素的最大元素；如果不存在这样的元素，则返回 null。 public E floor(E e) { return m.floorKey(e); }12、headSet：返回此 set 的部分视图，其元素严格小于 toElement。 public SortedSet&lt;E&gt; headSet(E toElement) { return headSet(toElement, false); }13、higher：返回此 set 中严格大于给定元素的最小元素；如果不存在这样的元素，则返回 null。 public E higher(E e) { return m.higherKey(e); }14、isEmpty：如果此 set 不包含任何元素，则返回 true。 public boolean isEmpty() { return m.isEmpty(); }15、iterator：返回在此 set 中的元素上按升序进行迭代的迭代器。 public Iterator&lt;E&gt; iterator() { return m.navigableKeySet().iterator(); }16、last：返回此 set 中当前最后一个（最高）元素。 public E last() { return m.lastKey(); }17、lower：返回此 set 中严格小于给定元素的最大元素；如果不存在这样的元素，则返回 null。 public E lower(E e) { return m.lowerKey(e); }18、pollFirst：获取并移除第一个（最低）元素；如果此 set 为空，则返回 null。 public E pollFirst() { Map.Entry&lt;E,?&gt; e = m.pollFirstEntry(); return (e == null) ? null : e.getKey(); }19、pollLast：获取并移除最后一个（最高）元素；如果此 set 为空，则返回 null。 public E pollLast() { Map.Entry&lt;E,?&gt; e = m.pollLastEntry(); return (e == null) ? null : e.getKey(); }20、remove：将指定的元素从 set 中移除（如果该元素存在于此 set 中）。 public boolean remove(Object o) { return m.remove(o)==PRESENT; }该方法与put类似，只不过把插入换成了删除，并且要进行删除后调整 21、size：返回 set 中的元素数（set 的容量）。 public int size() { return m.size(); }22、subSet：返回此 set 的部分视图 /** * 返回此 set 的部分视图，其元素范围从 fromElement 到 toElement。 */ public NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive) { return new TreeSet&lt;&gt;(m.subMap(fromElement, fromInclusive, toElement, toInclusive)); } /** * 返回此 set 的部分视图，其元素从 fromElement（包括）到 toElement（不包括）。 */ public SortedSet&lt;E&gt; subSet(E fromElement, E toElement) { return subSet(fromElement, true, toElement, false); }23、tailSet：返回此 set 的部分视图 /** * 返回此 set 的部分视图，其元素大于（或等于，如果 inclusive 为 true）fromElement。 */ public NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) { return new TreeSet&lt;&gt;(m.tailMap(fromElement, inclusive)); } /** * 返回此 set 的部分视图，其元素大于等于 fromElement。 */ public SortedSet&lt;E&gt; tailSet(E fromElement) { return tailSet(fromElement, true); }最后由于TreeSet是基于TreeMap实现的，所以如果我们对treeMap有了一定的了解，对TreeSet那是小菜一碟，我们从TreeSet中的源码可以看出，其实现过程非常简单，几乎所有的方法实现全部都是基于TreeMap的。 LinkedHashSetLinkedHashSet内部是如何工作的LinkedHashSet是HashSet的一个“扩展版本”，HashSet并不管什么顺序，不同的是LinkedHashSet会维护“插入顺序”。HashSet内部使用HashMap对象来存储它的元素，而LinkedHashSet内部使用LinkedHashMap对象来存储和处理它的元素。这篇文章，我们将会看到LinkedHashSet内部是如何运作的及如何维护插入顺序的。 我们首先着眼LinkedHashSet的构造函数。在LinkedHashSet类中一共有4个构造函数。这些构造函数都只是简单地调用父类构造函数（如HashSet类的构造函数）。下面看看LinkedHashSet的构造函数是如何定义的。 //Constructor - 1 public LinkedHashSet(int initialCapacity, float loadFactor) { super(initialCapacity, loadFactor, true); //Calling super class constructor } //Constructor - 2 public LinkedHashSet(int initialCapacity) { super(initialCapacity, .75f, true); //Calling super class constructor } //Constructor - 3 public LinkedHashSet() { super(16, .75f, true); //Calling super class constructor } //Constructor - 4 public LinkedHashSet(Collection&lt;? extends E&gt; c) { super(Math.max(2*c.size(), 11), .75f, true); //Calling super class constructor addAll(c); }在上面的代码片段中，你可能注意到4个构造函数调用的是同一个父类的构造函数。这个构造函数（父类的，译者注）是一个包内私有构造函数（见下面的代码，HashSet的构造函数没有使用public公开，译者注），它只能被LinkedHashSet使用。 这个构造函数需要初始容量，负载因子和一个boolean类型的哑值（没有什么用处的参数，作为标记，译者注）等参数。这个哑参数只是用来区别这个构造函数与HashSet的其他拥有初始容量和负载因子参数的构造函数，下面是这个构造函数的定义， HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); }显然，这个构造函数内部初始化了一个LinkedHashMap对象，这个对象恰好被LinkedHashSet用来存储它的元素。 LinkedHashSet并没有自己的方法，所有的方法都继承自它的父类HashSet，因此，对LinkedHashSet的所有操作方式就好像对HashSet操作一样。 唯一的不同是内部使用不同的对象去存储元素。在HashSet中，插入的元素是被当做HashMap的键来保存的，而在LinkedHashSet中被看作是LinkedHashMap的键。 这些键对应的值都是常量PRESENT（PRESENT是HashSet的静态成员变量，译者注）。 LinkedHashSet是如何维护插入顺序的 LinkedHashSet使用LinkedHashMap对象来存储它的元素，插入到LinkedHashSet中的元素实际上是被当作LinkedHashMap的键保存起来的。 LinkedHashMap的每一个键值对都是通过内部的静态类Entry&lt;K, V&gt;实例化的。这个 Entry&lt;K, V&gt;类继承了HashMap.Entry类。 这个静态类增加了两个成员变量，before和after来维护LinkedHasMap元素的插入顺序。这两个成员变量分别指向前一个和后一个元素，这让LinkedHashMap也有类似双向链表的表现。 private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; { // These fields comprise the doubly linked list used for iteration. Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) { super(hash, key, value, next); } }从上面代码看到的LinkedHashMap内部类的前面两个成员变量——before和after负责维护LinkedHashSet的插入顺序。LinkedHashMap定义的成员变量header保存的是这个双向链表的头节点。header的定义就像下面这样， 接下来看一个例子就知道LinkedHashSet内部是如何工作的了。 public class LinkedHashSetExample { public static void main(String[] args) { //Creating LinkedHashSet LinkedHashSet&lt;String&gt; set = new LinkedHashSet&lt;String&gt;(); //Adding elements to LinkedHashSet set.add(&quot;BLUE&quot;); set.add(&quot;RED&quot;); set.add(&quot;GREEN&quot;); set.add(&quot;BLACK&quot;); } }如果你知道LinkedHashMap内部是如何工作的，就非常容易明白LinkedHashSet内部是如何工作的。看一遍LinkedHashSet和LinkedHashMap的源码，你就能够准确地理解在Java中LinkedHashSet内部是如何工作的。 参考文章http://cmsblogs.com/?p=599 https://www.cnblogs.com/one-apple-pie/p/11036309.html https://blog.csdn.net/learningcoding/article/details/79983248 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); ​ var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>HashSet</tag>
        <tag>TreeSet</tag>
        <tag>LinkedHashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解6：这一次，从头到尾带你解读Java中的红黑树]]></title>
    <url>%2F2019%2F09%2F28%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A36%EF%BC%9ATreeMap%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 什么是红黑树首先，什么是红黑树呢？ 红黑树是一种“平衡的”二叉查找树，它是一种经典高效的算法，能够保证在最坏的情况下动态集合操作的时间为O（lgn）。红黑树每个节点包含5个域，分别为color,key,left,right和p。 color是在每个节点上增加的一个存储位表示节点的颜色，可以是RED或者BLACK。key为结点中的value值，left,right为该结点的左右孩子指针，没有的话为NIL，p是一个指针，是指向该节的父节点。如下图（来自维基百科）表示就是一颗红黑树，NIL为指向外结点的指针。（外结点视为没有key的结点） 红黑树有什么性质呢？一般称为红黑性质，有以下五点： 1）每个结点或者是红的或者是黑的； 2）根结点是黑的； 3）每个叶结点（NIL）是黑的； 4）如果一个结点是红的，则它的两个孩子都是黑的； 5）对每个结点，从该结点到其他其子孙结点的所有路径上包含相同数目的黑结点。 为了后面的分析，我们还得知道以下知识点。 （1）黑高度：从某个结点x出发（不包括该结点）到达一个叶结点的任意一条路径上，黑色结点的个数称为该结点x的黑高度。 （2）一颗有n个内结点的红黑树的高度至多为2lg(n+1)。 （内结点视为红黑树中带关键字的结点） （3）包含n个内部节点的红黑树的高度是 O(log(n))。定义红黑树是特殊的二叉查找树，又名R-B树(RED-BLACK-TREE)，由于红黑树是特殊的二叉查找树，即红黑树具有了二叉查找树的特性，而且红黑树还具有以下特性： 1.每个节点要么是黑色要么是红色 2.根节点是黑色 3.每个叶子节点是黑色，并且为空节点(还有另外一种说法就是，每个叶子结点都带有两个空的黑色结点（被称为黑哨兵），如果一个结点n的只有一个左孩子，那么n的右孩子是一个黑哨兵；如果结点n只有一个右孩子，那么n的左孩子是一个黑哨兵。) 4.如果一个节点是红色，则它的子节点必须是黑色 5.从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 有几点需要注意的是： 1.特性3中指定红黑树的每个叶子节点都是空节点，但是在Java实现中红黑树将使用null代表空节点，因此遍历红黑树时看不到黑色的叶子节点，反而见到的叶子节点是红色的 2.特性4保证了从根节点到叶子节点的最长路径的长度不会超过任何其他路径的两倍，例如黑色高度为3的红黑树，其最短路径(路径指的是根节点到叶子节点)是2(黑节点-黑节点-黑节点)，其最长路径为4(黑节点-红节点-黑节点-红节点-黑节点)。 实践红黑树操作插入操作首先红黑树在插入节点的时，我们设定插入节点的颜色为红色,如果插入的是黑色节点，必然会违背特性5，即改变了红黑树的黑高度，如下插入红色结点又存在着几种情况： 1.黑父 如图所示，这种情况不会破坏红黑树的特性，即不需要任何处理 2.红父 当其父亲为红色时又会存在以下的情况 红叔 红叔的情况，其实相对来说比较简单的，如下图所示，只需要通过修改父、叔的颜色为黑色，祖的颜色为红色，而且回去递归的检查祖节点即可 黑叔 黑叔的情况有如下几种，这几种情况下是不能够通过修改颜色达到平衡的效果，因此会通过旋转的操作，红黑树种有两种旋转操作，左旋和右旋(现在存在的疑问，什么时候使用到左旋，什么时候使用到右旋) Case 1:[先右旋，在改变颜色(根节点必须为黑色，其两个子节点为红色，叔节点不用改变)],如下图所示，注意省略黑哨兵节点 Case 2:[先左旋变成Case1中的情况，再右旋，最后改变颜色(根节点必须为黑色，其两个子节点为红色，叔节点不用改变)],如下图所示，注意省略黑哨兵节点 Case 3:[先左旋，最后改变颜色(根节点必须为黑色，其两个子节点为红色，叔节点不用改变)],如下图所示，注意省略黑哨兵节点 Case 4:[先右旋变成Case 3的情况，再左旋，最后改变颜色(根节点必须为黑色，其两个子节点为红色，叔节点不用改变)],如下图所示，注意省略黑哨兵节点 以上就是红黑树新增节点所有可能的操作，下面会介绍红黑树中的删除操作 删除操作删除操作相比于插入操作情况更加复杂，删除一个节点可以大致分为三种情况： 1.删除的节点没有孩子节点，即当前节点为叶子节点，这种可以直接删除 2.删除的节点有一个孩子节点，这种需要删除当前节点，并使用其孩子节点顶替上来 3.删除的节点有两个孩子节点，这种需要先找到其后继节点(树中大于节点的最小的元素);然后将其后继节点的内容复制到该节点上，其后继节点就相当于该节点的替身， 需要注意的是其后继节点一定不会有两个孩子节点(这点应该很好理解，如果后继节点有左孩子节点，那么当前的后继节点肯定不是最小的，说明后继节点只能存在没有孩子节点或者只有一个右孩子节点)，即这样就将问题转换成为1,2中的方式。 在讲述修复操作之前，首先需要明白几点， 1.对于红黑树而言，单支节点的情况只有如下图所示的一种情况，即为当前节点为黑色，其孩子节点为红色,(1.假设当前节点为红色，其两个孩子节点必须为黑色，2.若有孙子节点，则必为黑色，导致黑子数量不等，而红黑树不平衡) 2.由于红黑树是特殊的二叉查找树，它的删除和二叉查找树类型，真正的删除点即为删除点A的中序遍历的后继(前继也可以)，通过红黑树的特性可知这个后继必然最多只能有一个孩子，其这个孩子节点必然是右孩子节点，从而为单支情况(即这个后继节点只能有一个红色孩子或没有孩子) 下面将详细介绍，在执行删除节点操作之后，将通过修复操作使得红黑树达到平衡的情况。 Case 1:被删除的节点为红色，则这节点必定为叶子节点(首先这里的被删除的节点指的是真正删除的节点，通过上文得知的真正删除的节点要么是节点本身，要么是其后继节点，若是节点本身则必须为叶子节点，不为叶子节点的话其会有左右孩子，则真正删除的是其右孩子树上的最小值，若是后继节点，也必须为叶子节点，若不是则其也会有左右孩子，从而和2中相违背)，这种情况下删除红色叶节点就可以了，不用进行其他的操作了。 Case 2:被删除的节点是黑色，其子节点是红色，将其子节点顶替上来并改变其颜色为黑色，如下图所示 Case 3:被删除的节点是黑色，其子节点也是黑色，将其子节点顶替上来，变成了双黑的问题，此时有以下情况 Case 1:新节点的兄弟节点为红色，此时若新节点在左边则做左旋操作，否则做右旋操作，之后再将其父节点颜色改变为红色，兄弟节点 从图中可以看出，操作之后红黑树并未达到平衡状态，而是变成的黑兄的情况 Case 2:新节点的兄弟节点为黑色,此时可能有如下情况 红父二黑侄：将父节点变成黑色，兄弟节点变成红色，新节点变成黑色即可,如下图所示 黑父二黑侄：将父节点变成新节点的颜色，新节点变成黑色，兄弟节点染成红色，还需要继续以父节点为判定点继续判断,如下图所示 红侄： 情况一:新节点在右子树，红侄在兄弟节点左子树，此时的操作为右旋，并将兄弟节点变为父亲的颜色，父亲节点变为黑色，侄节点变为黑色，如下图所示 情况二:新节点在右子树，红侄在兄弟节点右子树，此时的操作为先左旋，后右旋并将侄节点变为父亲的颜色，父节点变为黑色，如下图所示 情况三：新节点在左子树，红侄在兄弟节点左子树,此时的操作为先右旋在左旋并将侄节点变为父亲的颜色，父亲节点变为黑色，如下图所示 情况四：新节点在右子树，红侄在兄弟节点右子树,此时的操作为左旋，并将兄弟节点变为父节点的颜色，父亲节点变为黑色，侄节点变为黑色，如下图所示 红黑树实现如下是使用JAVA代码实现红黑树的过程，主要包括了插入、删除、左旋、右旋、遍历等操作 插入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/* 插入一个节点 * @param node */private void insert(RBTreeNode&lt;T&gt; node)&#123; int cmp; RBTreeNode&lt;T&gt; root = this.rootNode; RBTreeNode&lt;T&gt; parent = null; //定位节点添加到哪个父节点下 while(null != root)&#123; parent = root; cmp = node.key.compareTo(root.key); if (cmp &lt; 0)&#123; root = root.left; &#125; else &#123; root = root.right; &#125; &#125; node.parent = parent; //表示当前没一个节点，那么就当新增的节点为根节点 if (null == parent)&#123; this.rootNode = node; &#125; else &#123; //找出在当前父节点下新增节点的位置 cmp = node.key.compareTo(parent.key); if (cmp &lt; 0)&#123; parent.left = node; &#125; else &#123; parent.right = node; &#125; &#125; //设置插入节点的颜色为红色 node.color = COLOR_RED; //修正为红黑树 insertFixUp(node);&#125;/** * 红黑树插入修正 * @param node */private void insertFixUp(RBTreeNode&lt;T&gt; node)&#123; RBTreeNode&lt;T&gt; parent,gparent; //节点的父节点存在并且为红色 while( ((parent = getParent(node)) != null) &amp;&amp; isRed(parent))&#123; gparent = getParent(parent); //如果其祖父节点是空怎么处理 // 若父节点是祖父节点的左孩子 if(parent == gparent.left)&#123; RBTreeNode&lt;T&gt; uncle = gparent.right; if ((null != uncle) &amp;&amp; isRed(uncle))&#123; setColorBlack(uncle); setColorBlack(parent); setColorRed(gparent); node = gparent; continue; &#125; if (parent.right == node)&#123; RBTreeNode&lt;T&gt; tmp; leftRotate(parent); tmp = parent; parent = node; node = tmp; &#125; setColorBlack(parent); setColorRed(gparent); rightRotate(gparent); &#125; else &#123; RBTreeNode&lt;T&gt; uncle = gparent.left; if ((null != uncle) &amp;&amp; isRed(uncle))&#123; setColorBlack(uncle); setColorBlack(parent); setColorRed(gparent); node = gparent; continue; &#125; if (parent.left == node)&#123; RBTreeNode&lt;T&gt; tmp; rightRotate(parent); tmp = parent; parent = node; node = tmp; &#125; setColorBlack(parent); setColorRed(gparent); leftRotate(gparent); &#125; &#125; setColorBlack(this.rootNode);&#125; 插入节点的操作主要分为以下几步： 1.定位：即遍历整理红黑树，确定添加的位置，如上代码中insert方法中就是在找到添加的位置 2.修复：这也就是前面介绍的，添加元素后可能会使得红黑树不在满足其特性，这时候需要通过变色、旋转来调整红黑树，也就是如上代码中insertFixUp方法 删除节点如下为删除节点的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081private void remove(RBTreeNode&lt;T&gt; node)&#123; RBTreeNode&lt;T&gt; child,parent; boolean color; //被删除节点左右孩子都不为空的情况 if ((null != node.left) &amp;&amp; (null != node.right))&#123; //获取到被删除节点的后继节点 RBTreeNode&lt;T&gt; replace = node; replace = replace.right; while(null != replace.left)&#123; replace = replace.left; &#125; //node节点不是根节点 if (null != getParent(node))&#123; //node是左节点 if (getParent(node).left == node)&#123; getParent(node).left = replace; &#125; else &#123; getParent(node).right = replace; &#125; &#125; else &#123; this.rootNode = replace; &#125; child = replace.right; parent = getParent(replace); color = getColor(replace); if (parent == node)&#123; parent = replace; &#125; else &#123; if (null != child)&#123; setParent(child,parent); &#125; parent.left = child; replace.right = node.right; setParent(node.right, replace); &#125; replace.parent = node.parent; replace.color = node.color; replace.left = node.left; node.left.parent = replace; if (color == COLOR_BLACK)&#123; removeFixUp(child,parent); &#125; node = null; return; &#125; if (null != node.left)&#123; child = node.left; &#125; else &#123; child = node.right; &#125; parent = node.parent; color = node.color; if (null != child)&#123; child.parent = parent; &#125; if (null != parent)&#123; if (parent.left == node)&#123; parent.left = child; &#125; else &#123; parent.right = child; &#125; &#125; else &#123; this.rootNode = child; &#125; if (color == COLOR_BLACK)&#123; removeFixUp(child, parent); &#125; node = null;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 删除修复 * @param node * @param parent */private void removeFixUp(RBTreeNode&lt;T&gt; node, RBTreeNode&lt;T&gt; parent)&#123; RBTreeNode&lt;T&gt; other; //node不为空且为黑色，并且不为根节点 while ((null == node || isBlack(node)) &amp;&amp; (node != this.rootNode) )&#123; //node是父节点的左孩子 if (node == parent.left)&#123; //获取到其右孩子 other = parent.right; //node节点的兄弟节点是红色 if (isRed(other))&#123; setColorBlack(other); setColorRed(parent); leftRotate(parent); other = parent.right; &#125; //node节点的兄弟节点是黑色，且兄弟节点的两个孩子节点也是黑色 if ((other.left == null || isBlack(other.left)) &amp;&amp; (other.right == null || isBlack(other.right)))&#123; setColorRed(other); node = parent; parent = getParent(node); &#125; else &#123; //node节点的兄弟节点是黑色，且兄弟节点的右孩子是红色 if (null == other.right || isBlack(other.right))&#123; setColorBlack(other.left); setColorRed(other); rightRotate(other); other = parent.right; &#125; //node节点的兄弟节点是黑色，且兄弟节点的右孩子是红色，左孩子是任意颜色 setColor(other, getColor(parent)); setColorBlack(parent); setColorBlack(other.right); leftRotate(parent); node = this.rootNode; break; &#125; &#125; else &#123; other = parent.left; if (isRed(other))&#123; setColorBlack(other); setColorRed(parent); rightRotate(parent); other = parent.left; &#125; if ((null == other.left || isBlack(other.left)) &amp;&amp; (null == other.right || isBlack(other.right)))&#123; setColorRed(other); node = parent; parent = getParent(node); &#125; else &#123; if (null == other.left || isBlack(other.left))&#123; setColorBlack(other.right); setColorRed(other); leftRotate(other); other = parent.left; &#125; setColor(other,getColor(parent)); setColorBlack(parent); setColorBlack(other.left); rightRotate(parent); node = this.rootNode; break; &#125; &#125; &#125; if (node!=null) setColorBlack(node);&#125; 删除节点主要分为几种情况去做对应的处理： 1.删除节点,按照如下三种情况去删除节点 1.真正删除的节点没有子节点 2.真正删除的节点有一个子节点 3.正在删除的节点有两个子节点 2.修复红黑树的特性，如代码中调用removeFixUp方法修复红黑树的特性。 3.总结以上主要介绍了红黑树的一些特性，包括一些操作详细的解析了里面的过程，写的时间比较长，感觉确实比较难理清楚。后面会持续的理解更深入，若有存在问题的地方，请指正。 参考文章红黑树(五)之 Java的实现 通过分析 JDK 源代码研究 TreeMap 红黑树算法实现 红黑树 （图解）红黑树的插入和删除 红黑树深入剖析及Java实现 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
        <tag>TreeMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解5：深入理解LinkedHashMap和LRU缓存]]></title>
    <url>%2F2019%2F09%2F28%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A35%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3LinkedHashMap%E5%92%8CLRU%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 今天我们来深入探索一下LinkedHashMap的底层原理，并且使用linkedhashmap来实现LRU缓存。 摘要： HashMap和双向链表合二为一即是LinkedHashMap。所谓LinkedHashMap，其落脚点在HashMap，因此更准确地说，它是一个将所有Entry节点链入一个双向链表的HashMap。 由于LinkedHashMap是HashMap的子类，所以LinkedHashMap自然会拥有HashMap的所有特性。比如，LinkedHashMap的元素存取过程基本与HashMap基本类似，只是在细节实现上稍有不同。当然，这是由LinkedHashMap本身的特性所决定的，因为它额外维护了一个双向链表用于保持迭代顺序。 此外，LinkedHashMap可以很好的支持LRU算法，笔者在第七节便在LinkedHashMap的基础上实现了一个能够很好支持LRU的结构。 友情提示： 本文所有关于 LinkedHashMap 的源码都是基于 JDK 1.6 的，不同 JDK 版本之间也许会有些许差异，但不影响我们对 LinkedHashMap 的数据结构、原理等整体的把握和了解。后面会讲解1.8对于LinkedHashMap的改动。 由于 LinkedHashMap 是 HashMap 的子类，所以其具有HashMap的所有特性，这一点在源码共用上体现的尤为突出。因此，读者在阅读本文之前，最好对 HashMap 有一个较为深入的了解和回顾，否则很可能会导致事倍功半。可以参考我之前关于hashmap的文章。 LinkedHashMap 概述 笔者曾提到，HashMap 是 Java Collection Framework 的重要成员，也是Map族(如下图所示)中我们最为常用的一种。不过遗憾的是，HashMap是无序的，也就是说，迭代HashMap所得到的元素顺序并不是它们最初放置到HashMap的顺序。 HashMap的这一缺点往往会造成诸多不便，因为在有些场景中，我们确需要用到一个可以保持插入顺序的Map。庆幸的是，JDK为我们解决了这个问题，它为HashMap提供了一个子类 —— LinkedHashMap。虽然LinkedHashMap增加了时间和空间上的开销，但是它通过维护一个额外的双向链表保证了迭代顺序。 特别地，该迭代顺序可以是插入顺序，也可以是访问顺序。因此，根据链表中元素的顺序可以将LinkedHashMap分为：保持插入顺序的LinkedHashMap和保持访问顺序的LinkedHashMap，其中LinkedHashMap的默认实现是按插入顺序排序的。 本质上，HashMap和双向链表合二为一即是LinkedHashMap。所谓LinkedHashMap，其落脚点在HashMap，因此更准确地说，它是一个将所有Entry节点链入一个双向链表双向链表的HashMap。 在LinkedHashMapMap中，所有put进来的Entry都保存在如下面第一个图所示的哈希表中，但由于它又额外定义了一个以head为头结点的双向链表(如下面第二个图所示)，因此对于每次put进来Entry，除了将其保存到哈希表中对应的位置上之外，还会将其插入到双向链表的尾部。 更直观地，下图很好地还原了LinkedHashMap的原貌：HashMap和双向链表的密切配合和分工合作造就了LinkedHashMap。特别需要注意的是，next用于维护HashMap各个桶中的Entry链，before、after用于维护LinkedHashMap的双向链表，虽然它们的作用对象都是Entry，但是各自分离，是两码事儿。 其中，HashMap与LinkedHashMap的Entry结构示意图如下图所示： 特别地，由于LinkedHashMap是HashMap的子类，所以LinkedHashMap自然会拥有HashMap的所有特性。比如，==LinkedHashMap也最多只允许一条Entry的键为Null(多条会覆盖)，但允许多条Entry的值为Null。== 此外，LinkedHashMap 也是 Map 的一个非同步的实现。此外，LinkedHashMap还可以用来实现LRU (Least recently used, 最近最少使用)算法，这个问题会在下文的特别谈到。 LinkedHashMap 在 JDK 中的定义类结构定义 LinkedHashMap继承于HashMap，其在JDK中的定义为： public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; { ... }成员变量定义 与HashMap相比，LinkedHashMap增加了两个属性用于保证迭代顺序，分别是 双向链表头结点header 和 标志位accessOrder (值为true时，表示按照访问顺序迭代；值为false时，表示按照插入顺序迭代)。 /** * The head of the doubly linked list. */ private transient Entry&lt;K,V&gt; header; // 双向链表的表头元素 /** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ private final boolean accessOrder; //true表示按照访问顺序迭代，false时表示按照插入顺序 成员方法定义 从下图我们可以看出，LinkedHashMap中并增加没有额外方法。也就是说，LinkedHashMap与HashMap在操作上大致相同，只是在实现细节上略有不同罢了。 [外链图片转存失败(img-C2vYmjQ7-1567839753833)(http://static.zybuluo.com/Rico123/nvojgv4s0o0ciieibz1tbakc/LinkedHashMap_Outline.png)] 基本元素 Entry LinkedHashMap采用的hash算法和HashMap相同，但是它重新定义了Entry。LinkedHashMap中的Entry增加了两个指针 before 和 after，它们分别用于维护双向链接列表。特别需要注意的是，next用于维护HashMap各个桶中Entry的连接顺序，before、after用于维护Entry插入的先后顺序的，源代码如下： private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; { // These fields comprise the doubly linked list used for iteration. Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) { super(hash, key, value, next); } ... } 形象地，HashMap与LinkedHashMap的Entry结构示意图如下图所示： LinkedHashMap 的构造函数 LinkedHashMap 一共提供了五个构造函数，它们都是在HashMap的构造函数的基础上实现的，除了默认空参数构造方法，下面这个构造函数包含了大部分其他构造方法使用的参数，就不一一列举了。 LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) 该构造函数意在构造一个指定初始容量和指定负载因子的具有指定迭代顺序的LinkedHashMap，其源码如下： /** * Constructs an empty LinkedHashMap instance with the * specified initial capacity, load factor and ordering mode. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @param accessOrder the ordering mode - true for * access-order, false for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); // 调用HashMap对应的构造函数 this.accessOrder = accessOrder; // 迭代顺序的默认值 } 初始容量 和负载因子是影响HashMap性能的两个重要参数。同样地，它们也是影响LinkedHashMap性能的两个重要参数。此外，LinkedHashMap 增加了双向链表头结点 header和标志位 accessOrder两个属性用于保证迭代顺序。 LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) 该构造函数意在构造一个与指定 Map 具有相同映射的 LinkedHashMap，其 初始容量不小于 16 (具体依赖于指定Map的大小)，负载因子是 0.75，是 Java Collection Framework 规范推荐提供的，其源码如下： /** * Constructs an insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with * the same mappings as the specified map. The &lt;tt&gt;LinkedHashMap&lt;/tt&gt; * instance is created with a default load factor (0.75) and an initial * capacity sufficient to hold the mappings in the specified map. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { super(m); // 调用HashMap对应的构造函数 accessOrder = false; // 迭代顺序的默认值 }init 方法 从上面的五种构造函数我们可以看出，无论采用何种方式创建LinkedHashMap，其都会调用HashMap相应的构造函数。事实上，不管调用HashMap的哪个构造函数，HashMap的构造函数都会在最后调用一个init()方法进行初始化，只不过这个方法在HashMap中是一个空实现，而在LinkedHashMap中重写了它用于初始化它所维护的双向链表。例如，HashMap的参数为空的构造函数以及init方法的源码如下： /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); } /** * Initialization hook for subclasses. This method is called * in all constructors and pseudo-constructors (clone, readObject) * after HashMap has been initialized but before any entries have * been inserted. (In the absence of this method, readObject would * require explicit knowledge of subclasses.) */ void init() { } 在LinkedHashMap中，它重写了init方法以便初始化双向列表，源码如下： /** * Called by superclass constructors and pseudoconstructors (clone, * readObject) before any entries are inserted into the map. Initializes * the chain. */ void init() { header = new Entry&lt;K,V&gt;(-1, null, null, null); header.before = header.after = header; } 因此，我们在创建LinkedHashMap的同时就会不知不觉地对双向链表进行初始化。 LinkedHashMap 的数据结构 本质上，LinkedHashMap = HashMap + 双向链表，也就是说，HashMap和双向链表合二为一即是LinkedHashMap。 也可以这样理解，LinkedHashMap 在不对HashMap做任何改变的基础上，给HashMap的任意两个节点间加了两条连线(before指针和after指针)，使这些节点形成一个双向链表。 在LinkedHashMapMap中，所有put进来的Entry都保存在HashMap中，但由于它又额外定义了一个以head为头结点的空的双向链表，因此对于每次put进来Entry还会将其插入到双向链表的尾部。 LinkedHashMap 的快速存取 我们知道，在HashMap中最常用的两个操作就是：put(Key,Value) 和 get(Key)。同样地，在 LinkedHashMap 中最常用的也是这两个操作。 对于put(Key,Value)方法而言，LinkedHashMap完全继承了HashMap的 put(Key,Value) 方法，只是对put(Key,Value)方法所调用的recordAccess方法和addEntry方法进行了重写；对于get(Key)方法而言，LinkedHashMap则直接对它进行了重写。 下面我们结合JDK源码看 LinkedHashMap 的存取实现。 LinkedHashMap 的存储实现 : put(key, vlaue) 上面谈到，LinkedHashMap没有对 put(key,vlaue) 方法进行任何直接的修改，完全继承了HashMap的 put(Key,Value) 方法，其源码如下： public V put(K key, V value) { //当key为null时，调用putForNullKey方法，并将该键值对保存到table的第一个位置 if (key == null) return putForNullKey(value); //根据key的hashCode计算hash值 int hash = hash(key.hashCode()); //计算该键值对在数组中的存储位置（哪个桶） int i = indexFor(hash, table.length); //在table的第i个桶上进行迭代，寻找 key 保存的位置 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; //判断该条链上是否存在hash值相同且key值相等的映射，若存在，则直接覆盖 value，并返回旧value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); // LinkedHashMap重写了Entry中的recordAccess方法--- (1) return oldValue; // 返回旧值 } } modCount++; //修改次数增加1，快速失败机制 //原Map中无该映射，将该添加至该链的链头 addEntry(hash, key, value, i); // LinkedHashMap重写了HashMap中的createEntry方法 ---- (2) return null; } 上述源码反映了LinkedHashMap与HashMap保存数据的过程。特别地，在LinkedHashMap中，它对addEntry方法和Entry的recordAccess方法进行了重写。下面我们对比地看一下LinkedHashMap 和HashMap的addEntry方法的具体实现： /** * This override alters behavior of superclass put method. It causes newly * allocated entry to get inserted at the end of the linked list and * removes the eldest entry if appropriate. * * LinkedHashMap中的addEntry方法 */ void addEntry(int hash, K key, V value, int bucketIndex) { //创建新的Entry，并插入到LinkedHashMap中 createEntry(hash, key, value, bucketIndex); // 重写了HashMap中的createEntry方法 //双向链表的第一个有效节点（header后的那个节点）为最近最少使用的节点，这是用来支持LRU算法的 Entry&lt;K,V&gt; eldest = header.after; //如果有必要，则删除掉该近期最少使用的节点， //这要看对removeEldestEntry的覆写,由于默认为false，因此默认是不做任何处理的。 if (removeEldestEntry(eldest)) { removeEntryForKey(eldest.key); } else { //扩容到原来的2倍 if (size &gt;= threshold) resize(2 * table.length); } } -------------------------------我是分割线------------------------------------ /** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. * * HashMap中的addEntry方法 */ void addEntry(int hash, K key, V value, int bucketIndex) { //获取bucketIndex处的Entry Entry&lt;K,V&gt; e = table[bucketIndex]; //将新创建的 Entry 放入 bucketIndex 索引处，并让新的 Entry 指向原来的 Entry table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); //若HashMap中元素的个数超过极限了，则容量扩大两倍 if (size++ &gt;= threshold) resize(2 * table.length); } 由于LinkedHashMap本身维护了插入的先后顺序，因此其可以用来做缓存，14~19行的操作就是用来支持LRU算法的，这里暂时不用去关心它。此外，在LinkedHashMap的addEntry方法中，它重写了HashMap中的createEntry方法，我们接着看一下createEntry方法： void createEntry(int hash, K key, V value, int bucketIndex) { // 向哈希表中插入Entry，这点与HashMap中相同 //创建新的Entry并将其链入到数组对应桶的链表的头结点处， HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); table[bucketIndex] = e; //在每次向哈希表插入Entry的同时，都会将其插入到双向链表的尾部， //这样就按照Entry插入LinkedHashMap的先后顺序来迭代元素(LinkedHashMap根据双向链表重写了迭代器) //同时，新put进来的Entry是最近访问的Entry，把其放在链表末尾 ，也符合LRU算法的实现 e.addBefore(header); size++; } 由以上源码我们可以知道，在LinkedHashMap中向哈希表中插入新Entry的同时，还会通过Entry的addBefore方法将其链入到双向链表中。其中，addBefore方法本质上是一个双向链表的插入操作，其源码如下： //在双向链表中，将当前的Entry插入到existingEntry(header)的前面 private void addBefore(Entry&lt;K,V&gt; existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; } 到此为止，我们分析了在LinkedHashMap中put一条键值对的完整过程。总的来说，相比HashMap而言，LinkedHashMap在向哈希表添加一个键值对的同时，也会将其链入到它所维护的双向链表中，以便设定迭代顺序。 LinkedHashMap 的扩容操作 : resize()在HashMap中，我们知道随着HashMap中元素的数量越来越多，发生碰撞的概率将越来越大，所产生的子链长度就会越来越长，这样势必会影响HashMap的存取速度。 为了保证HashMap的效率，系统必须要在某个临界点进行扩容处理，该临界点就是HashMap中元素的数量在数值上等于threshold（table数组长度*加载因子）。 但是，不得不说，扩容是一个非常耗时的过程，因为它需要重新计算这些元素在新table数组中的位置并进行复制处理。所以，如果我们能够提前预知HashMap中元素的个数，那么在构造HashMap时预设元素的个数能够有效的提高HashMap的性能。 同样的问题也存在于LinkedHashMap中，因为LinkedHashMap本来就是一个HashMap，只是它还将所有Entry节点链入到了一个双向链表中。LinkedHashMap完全继承了HashMap的resize()方法，只是对它所调用的transfer方法进行了重写。我们先看resize()方法源码： void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; // 若 oldCapacity 已达到最大值，直接将 threshold 设为 Integer.MAX_VALUE if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; // 直接返回 } // 否则，创建一个更大的数组 Entry[] newTable = new Entry[newCapacity]; //将每条Entry重新哈希到新的数组中 transfer(newTable); //LinkedHashMap对它所调用的transfer方法进行了重写 table = newTable; threshold = (int)(newCapacity * loadFactor); // 重新设定 threshold } 从上面代码中我们可以看出，Map扩容操作的核心在于重哈希。所谓重哈希是指重新计算原HashMap中的元素在新table数组中的位置并进行复制处理的过程。鉴于性能和LinkedHashMap自身特点的考量，LinkedHashMap对重哈希过程(transfer方法)进行了重写，源码如下： /** * Transfers all entries to new table array. This method is called * by superclass resize. It is overridden for performance, as it is * faster to iterate using our linked list. */ void transfer(HashMap.Entry[] newTable) { int newCapacity = newTable.length; // 与HashMap相比，借助于双向链表的特点进行重哈希使得代码更加简洁 for (Entry&lt;K,V&gt; e = header.after; e != header; e = e.after) { int index = indexFor(e.hash, newCapacity); // 计算每个Entry所在的桶 // 将其链入桶中的链表 e.next = newTable[index]; newTable[index] = e; } } 如上述源码所示，LinkedHashMap借助于自身维护的双向链表轻松地实现了重哈希操作。 LinkedHashMap 的读取实现 ：get(Object key) 相对于LinkedHashMap的存储而言，读取就显得比较简单了。LinkedHashMap中重写了HashMap中的get方法，源码如下： public V get(Object key) { // 根据key获取对应的Entry，若没有这样的Entry，则返回null Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) // 若不存在这样的Entry，直接返回 return null; e.recordAccess(this); return e.value; } /** * Returns the entry associated with the specified key in the * HashMap. Returns null if the HashMap contains no mapping * for the key. * * HashMap 中的方法 * */ final Entry&lt;K,V&gt; getEntry(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } 在LinkedHashMap的get方法中，通过HashMap中的getEntry方法获取Entry对象。注意这里的recordAccess方法，如果链表中元素的排序规则是按照插入的先后顺序排序的话，该方法什么也不做；如果链表中元素的排序规则是按照访问的先后顺序排序的话，则将e移到链表的末尾处，笔者会在后文专门阐述这个问题。 另外，同样地，调用LinkedHashMap的get(Object key)方法后，若返回值是 NULL，则也存在如下两种可能： 该 key 对应的值就是 null;HashMap 中不存在该 key。 LinkedHashMap 存取小结 LinkedHashMap的存取过程基本与HashMap基本类似，只是在细节实现上稍有不同，这是由LinkedHashMap本身的特性所决定的，因为它要额外维护一个双向链表用于保持迭代顺序。 在put操作上，虽然LinkedHashMap完全继承了HashMap的put操作，但是在细节上还是做了一定的调整，比如，在LinkedHashMap中向哈希表中插入新Entry的同时，还会通过Entry的addBefore方法将其链入到双向链表中。 在扩容操作上，虽然LinkedHashMap完全继承了HashMap的resize操作，但是鉴于性能和LinkedHashMap自身特点的考量，LinkedHashMap对其中的重哈希过程(transfer方法)进行了重写。在读取操作上，LinkedHashMap中重写了HashMap中的get方法，通过HashMap中的getEntry方法获取Entry对象。在此基础上，进一步获取指定键对应的值。 LinkedHashMap 与 LRU(Least recently used，最近最少使用)算法 到此为止，我们已经分析完了LinkedHashMap的存取实现，这与HashMap大体相同。LinkedHashMap区别于HashMap最大的一个不同点是，前者是有序的，而后者是无序的。为此，LinkedHashMap增加了两个属性用于保证顺序，分别是双向链表头结点header和标志位accessOrder。 我们知道，header是LinkedHashMap所维护的双向链表的头结点，而accessOrder用于决定具体的迭代顺序。实际上，accessOrder标志位的作用可不像我们描述的这样简单，我们接下来仔细分析一波~ 我们知道，当accessOrder标志位为true时，表示双向链表中的元素按照访问的先后顺序排列，可以看到，虽然Entry插入链表的顺序依然是按照其put到LinkedHashMap中的顺序，但put和get方法均有调用recordAccess方法（put方法在key相同时会调用）。 recordAccess方法判断accessOrder是否为true，如果是，则将当前访问的Entry（put进来的Entry或get出来的Entry）移到双向链表的尾部（key不相同时，put新Entry时，会调用addEntry，它会调用createEntry，该方法同样将新插入的元素放入到双向链表的尾部，既符合插入的先后顺序，又符合访问的先后顺序，因为这时该Entry也被访问了）； 当标志位accessOrder的值为false时，表示双向链表中的元素按照Entry插入LinkedHashMap到中的先后顺序排序，即每次put到LinkedHashMap中的Entry都放在双向链表的尾部，这样遍历双向链表时，Entry的输出顺序便和插入的顺序一致，这也是默认的双向链表的存储顺序。 因此，当标志位accessOrder的值为false时，虽然也会调用recordAccess方法，但不做任何操作。 put操作与标志位accessOrder/ 将key/value添加到LinkedHashMap中 public V put(K key, V value) { // 若key为null，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若key不为null，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 若key对已经存在，则用新的value取代旧的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 若key不存在，则将key/value键值对添加到table中 modCount++; //将key/value键值对添加到table[i]处 addEntry(hash, key, value, i); return null; } 从上述源码我们可以看到，当要put进来的Entry的key在哈希表中已经在存在时，会调用Entry的recordAccess方法；当该key不存在时，则会调用addEntry方法将新的Entry插入到对应桶的单链表的头部。我们先来看recordAccess方法： /** * This method is invoked by the superclass whenever the value * of a pre-existing entry is read by Map.get or modified by Map.set. * If the enclosing Map is access-ordered, it moves the entry * to the end of the list; otherwise, it does nothing. */ void recordAccess(HashMap&lt;K,V&gt; m) { LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; //如果链表中元素按照访问顺序排序，则将当前访问的Entry移到双向循环链表的尾部， //如果是按照插入的先后顺序排序，则不做任何事情。 if (lm.accessOrder) { lm.modCount++; //移除当前访问的Entry remove(); //将当前访问的Entry插入到链表的尾部 addBefore(lm.header); } } LinkedHashMap重写了HashMap中的recordAccess方法（HashMap中该方法为空），当调用父类的put方法时，在发现key已经存在时，会调用该方法；当调用自己的get方法时，也会调用到该方法。 该方法提供了LRU算法的实现，它将最近使用的Entry放到双向循环链表的尾部。也就是说，当accessOrder为true时，get方法和put方法都会调用recordAccess方法使得最近使用的Entry移到双向链表的末尾；当accessOrder为默认值false时，从源码中可以看出recordAccess方法什么也不会做。我们反过头来，再看一下addEntry方法： /** * This override alters behavior of superclass put method. It causes newly * allocated entry to get inserted at the end of the linked list and * removes the eldest entry if appropriate. * * LinkedHashMap中的addEntry方法 */ void addEntry(int hash, K key, V value, int bucketIndex) { //创建新的Entry，并插入到LinkedHashMap中 createEntry(hash, key, value, bucketIndex); // 重写了HashMap中的createEntry方法 //双向链表的第一个有效节点（header后的那个节点）为最近最少使用的节点，这是用来支持LRU算法的 Entry&lt;K,V&gt; eldest = header.after; //如果有必要，则删除掉该近期最少使用的节点， //这要看对removeEldestEntry的覆写,由于默认为false，因此默认是不做任何处理的。 if (removeEldestEntry(eldest)) { removeEntryForKey(eldest.key); } else { //扩容到原来的2倍 if (size &gt;= threshold) resize(2 * table.length); } } void createEntry(int hash, K key, V value, int bucketIndex) { // 向哈希表中插入Entry，这点与HashMap中相同 //创建新的Entry并将其链入到数组对应桶的链表的头结点处， HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); table[bucketIndex] = e; //在每次向哈希表插入Entry的同时，都会将其插入到双向链表的尾部， //这样就按照Entry插入LinkedHashMap的先后顺序来迭代元素(LinkedHashMap根据双向链表重写了迭代器) //同时，新put进来的Entry是最近访问的Entry，把其放在链表末尾 ，也符合LRU算法的实现 e.addBefore(header); size++; } 同样是将新的Entry链入到table中对应桶中的单链表中，但可以在createEntry方法中看出，同时也会把新put进来的Entry插入到了双向链表的尾部。 从插入顺序的层面来说，新的Entry插入到双向链表的尾部可以实现按照插入的先后顺序来迭代Entry，而从访问顺序的层面来说，新put进来的Entry又是最近访问的Entry，也应该将其放在双向链表的尾部。在上面的addEntry方法中还调用了removeEldestEntry方法，该方法源码如下： /** * Returns &lt;tt&gt;true&lt;/tt&gt; if this map should remove its eldest entry. * This method is invoked by &lt;tt&gt;put&lt;/tt&gt; and &lt;tt&gt;putAll&lt;/tt&gt; after * inserting a new entry into the map. It provides the implementor * with the opportunity to remove the eldest entry each time a new one * is added. This is useful if the map represents a cache: it allows * the map to reduce memory consumption by deleting stale entries. * * &lt;p&gt;Sample use: this override will allow the map to grow up to 100 * entries and then delete the eldest entry each time a new entry is * added, maintaining a steady state of 100 entries. * &lt;pre&gt; * private static final int MAX_ENTRIES = 100; * * protected boolean removeEldestEntry(Map.Entry eldest) { * return size() &gt; MAX_ENTRIES; * } * &lt;/pre&gt; * * &lt;p&gt;This method typically does not modify the map in any way, * instead allowing the map to modify itself as directed by its * return value. It &lt;i&gt;is&lt;/i&gt; permitted for this method to modify * the map directly, but if it does so, it &lt;i&gt;must&lt;/i&gt; return * &lt;tt&gt;false&lt;/tt&gt; (indicating that the map should not attempt any * further modification). The effects of returning &lt;tt&gt;true&lt;/tt&gt; * after modifying the map from within this method are unspecified. * * &lt;p&gt;This implementation merely returns &lt;tt&gt;false&lt;/tt&gt; (so that this * map acts like a normal map - the eldest element is never removed). * * @param eldest The least recently inserted entry in the map, or if * this is an access-ordered map, the least recently accessed * entry. This is the entry that will be removed it this * method returns &lt;tt&gt;true&lt;/tt&gt;. If the map was empty prior * to the &lt;tt&gt;put&lt;/tt&gt; or &lt;tt&gt;putAll&lt;/tt&gt; invocation resulting * in this invocation, this will be the entry that was just * inserted; in other words, if the map contains a single * entry, the eldest entry is also the newest. * @return &lt;tt&gt;true&lt;/tt&gt; if the eldest entry should be removed * from the map; &lt;tt&gt;false&lt;/tt&gt; if it should be retained. */ protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return false; }} 该方法是用来被重写的，一般地，如果用LinkedHashmap实现LRU算法，就要重写该方法。比如可以将该方法覆写为如果设定的内存已满，则返回true，这样当再次向LinkedHashMap中putEntry时，在调用的addEntry方法中便会将近期最少使用的节点删除掉（header后的那个节点）。在第七节，笔者便重写了该方法并实现了一个名副其实的LRU结构。 get操作与标志位accessOrderpublic V get(Object key) { // 根据key获取对应的Entry，若没有这样的Entry，则返回null Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) // 若不存在这样的Entry，直接返回 return null; e.recordAccess(this); return e.value; } 在LinkedHashMap中进行读取操作时，一样也会调用recordAccess方法。上面笔者已经表述的很清楚了，此不赘述。 LinkedListMap与LRU小结 使用LinkedHashMap实现LRU的必要前提是将accessOrder标志位设为true以便开启按访问顺序排序的模式。我们可以看到，无论是put方法还是get方法，都会导致目标Entry成为最近访问的Entry，因此就把该Entry加入到了双向链表的末尾：get方法通过调用recordAccess方法来实现； put方法在覆盖已有key的情况下，也是通过调用recordAccess方法来实现，在插入新的Entry时，则是通过createEntry中的addBefore方法来实现。这样，我们便把最近使用的Entry放入到了双向链表的后面。多次操作后，双向链表前面的Entry便是最近没有使用的，这样当节点个数满的时候，删除最前面的Entry(head后面的那个Entry)即可，因为它就是最近最少使用的Entry。 使用LinkedHashMap实现LRU算法 如下所示，笔者使用LinkedHashMap实现一个符合LRU算法的数据结构，该结构最多可以缓存6个元素，但元素多余六个时，会自动删除最近最久没有被使用的元素，如下所示： public class LRU&lt;K,V&gt; extends LinkedHashMap&lt;K, V&gt; implements Map&lt;K, V&gt;{ private static final long serialVersionUID = 1L; public LRU(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor, accessOrder); } /** * @description 重写LinkedHashMap中的removeEldestEntry方法，当LRU中元素多余6个时， * 删除最不经常使用的元素 * @author rico * @created 2017年5月12日 上午11:32:51 * @param eldest * @return * @see java.util.LinkedHashMap#removeEldestEntry(java.util.Map.Entry) */ @Override protected boolean removeEldestEntry(java.util.Map.Entry&lt;K, V&gt; eldest) { // TODO Auto-generated method stub if(size() &gt; 6){ return true; } return false; } public static void main(String[] args) { LRU&lt;Character, Integer&gt; lru = new LRU&lt;Character, Integer&gt;( 16, 0.75f, true); String s = &quot;abcdefghijkl&quot;; for (int i = 0; i &lt; s.length(); i++) { lru.put(s.charAt(i), i); } System.out.println(&quot;LRU中key为h的Entry的值为： &quot; + lru.get(&apos;h&apos;)); System.out.println(&quot;LRU的大小 ：&quot; + lru.size()); System.out.println(&quot;LRU ：&quot; + lru); } } 下图是程序的运行结果： LinkedHashMap 有序性原理分析如前文所述，LinkedHashMap 增加了双向链表头结点header 和 标志位accessOrder两个属性用于保证迭代顺序。但是要想真正实现其有序性，还差临门一脚，那就是重写HashMap 的迭代器，其源码实现如下： private abstract class LinkedHashIterator&lt;T&gt; implements Iterator&lt;T&gt; { Entry&lt;K,V&gt; nextEntry = header.after; Entry&lt;K,V&gt; lastReturned = null; /** * The modCount value that the iterator believes that the backing * List should have. If this expectation is violated, the iterator * has detected concurrent modification. */ int expectedModCount = modCount; public boolean hasNext() { // 根据双向列表判断 return nextEntry != header; } public void remove() { if (lastReturned == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); LinkedHashMap.this.remove(lastReturned.key); lastReturned = null; expectedModCount = modCount; } Entry&lt;K,V&gt; nextEntry() { // 迭代输出双向链表各节点 if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (nextEntry == header) throw new NoSuchElementException(); Entry&lt;K,V&gt; e = lastReturned = nextEntry; nextEntry = e.after; return e; } } // Key 迭代器，KeySet private class KeyIterator extends LinkedHashIterator&lt;K&gt; { public K next() { return nextEntry().getKey(); } } // Value 迭代器，Values(Collection) private class ValueIterator extends LinkedHashIterator&lt;V&gt; { public V next() { return nextEntry().value; } } // Entry 迭代器，EntrySet private class EntryIterator extends LinkedHashIterator&lt;Map.Entry&lt;K,V&gt;&gt; { public Map.Entry&lt;K,V&gt; next() { return nextEntry(); } } 从上述代码中我们可以知道，LinkedHashMap重写了HashMap 的迭代器，它使用其维护的双向链表进行迭代输出。 JDK1.8的改动原文是基于JDK1.6的实现，实际上JDK1.8对其进行了改动。首先它删除了addentry，createenrty等方法（事实上是hashmap的改动影响了它而已）。 linkedhashmap同样使用了大部分hashmap的增删改查方法。新版本linkedhashmap主要是通过对hashmap内置几个方法重写来实现lru的。 hashmap不提供实现： void afterNodeAccess(Node&lt;K,V&gt; p) { } void afterNodeInsertion(boolean evict) { } void afterNodeRemoval(Node&lt;K,V&gt; p) { }linkedhashmap的实现： 处理元素被访问后的情况 void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; } }处理元素插入后的情况 void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }处理元素被删除后的情况 void afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; } }另外1.8的hashmap在链表长度超过8时自动转为红黑树，会按顺序插入链表中的元素，可以自定义比较器来定义节点的插入顺序。 1.8的linkedhashmap同样会使用这一特性，当变为红黑树以后，节点的先后顺序同样是插入红黑树的顺序，其双向链表的性质没有改表，只是原来hashmap的链表变成了红黑树而已，在此不要混淆。 总结 本文从linkedhashmap的数据结构，以及源码分析，到最后的LRU缓存实现，比较深入地剖析了linkedhashmap的底层原理。总结以下几点： 1 linkedhashmap在hashmap的数组加链表结构的基础上，将所有节点连成了一个双向链表。 2 当主动传入的accessOrder参数为false时, 使用put方法时，新加入元素不会被加入双向链表，get方法使用时也不会把元素放到双向链表尾部。 3 当主动传入的accessOrder参数为true时，使用put方法新加入的元素，如果遇到了哈希冲突，并且对key值相同的元素进行了替换，就会被放在双向链表的尾部，当元素超过上限且removeEldestEntry方法返回true时，直接删除最早元素以便新元素插入。如果没有冲突直接放入，同样加入到链表尾部。使用get方法时会把get到的元素放入双向链表尾部。 4 linkedhashmap的扩容比hashmap来的方便，因为hashmap需要将原来的每个链表的元素分别在新数组进行反向插入链化，而linkedhashmap的元素都连在一个链表上，可以直接迭代然后插入。 5 linkedhashmap的removeEldestEntry方法默认返回false，要实现lru很重要的一点就是集合满时要将最久未访问的元素删除，在linkedhashmap中这个元素就是头指针指向的元素。实现LRU可以直接实现继承linkedhashmap并重写removeEldestEntry方法来设置缓存大小。jdk中实现了LRUCache也可以直接使用。 参考文章http://cmsblogs.com/?p=176 https://www.jianshu.com/p/8f4f58b4b8ab https://blog.csdn.net/wang_8101/article/details/83067860 https://www.cnblogs.com/create-and-orange/p/11237072.html https://www.cnblogs.com/ganchuanpu/p/8908093.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>HashTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解4：一文读懂HashMap和HashTable的区别以及常见面试题]]></title>
    <url>%2F2019%2F09%2F27%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A34%EF%BC%9AHashMap%E5%92%8CHashTable%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 今天我们来探索一下HashMap和HashTable机制与比较器的源码。 本文参考www.cmsblogs.com/?p=176 HashMap HashMap也是我们使用非常多的Collection，它是基于哈希表的 Map 接口的实现，以key-value的形式存在。在HashMap中，key-value总是会当做一个整体来处理，系统会根据hash算法来来计算key-value的存储位置，我们总是可以通过key快速地存、取value。下面就来分析HashMap的存取。 定义 HashMap实现了Map接口，继承AbstractMap。其中Map接口定义了键映射到值的规则，而AbstractMap类提供 Map 接口的骨干实现，以最大限度地减少实现此接口所需的工作，其实AbstractMap类已经实现了Map，这里标注Map LZ觉得应该是更加清晰吧！ public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable构造函数HashMap提供了三个构造函数： HashMap()：构造一个具有默认初始容量 (16) 和默认加载因子 (0.75) 的空 HashMap。 HashMap(int initialCapacity)：构造一个带指定初始容量和默认加载因子 (0.75) 的空 HashMap。 HashMap(int initialCapacity, float loadFactor)：构造一个带指定初始容量和加载因子的空 HashMap。 在这里提到了两个参数：初始容量，加载因子。 这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中桶的数量，初始容量是创建哈希表时的容量，加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，它衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。 对于使用链表法的散列表来说，查找一个元素的平均时间是O(1+a)，因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低；如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费。系统默认负载因子为0.75，一般情况下我们是无需修改的。 HashMap是一种支持快速存取的数据结构，要了解它的性能必须要了解它的数据结构。 数据结构 我们知道在Java中最常用的两种结构是数组和模拟指针(引用)，几乎所有的数据结构都可以利用这两种来组合实现，HashMap也是如此。实际上HashMap是一个“链表散列”，如下是它的数据结构： HashMap数据结构图 下图的table数组的每个格子都是一个桶。负载因子就是map中的元素占用的容量百分比。比如负载因子是0.75，初始容量（桶数量）为16时，那么允许装填的元素最大个数就是16*0.75 = 12，这个最大个数也被成为阈值，就是map中定义的threshold。超过这个阈值时，map就会自动扩容。 存储实现：put(key,vlaue) 首先我们先看源码 public V put(K key, V value) { //当key为null，调用putForNullKey方法，保存null与table第一个位置中，这是HashMap允许为null的原因 if (key == null) return putForNullKey(value); //计算key的hash值，此处对原来元素的hashcode进行了再次hash int hash = hash(key.hashCode()); ------(1) //计算key hash 值在 table 数组中的位置 int i = indexFor(hash, table.length); ------(2) //从i出开始迭代 e,找到 key 保存的位置 for (Entry&lt;K, V&gt; e = table[i]; e != null; e = e.next) { Object k; //判断该条链上是否有hash值相同的(key相同) //若存在相同，则直接覆盖value，返回旧value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; //旧值 = 新值 e.value = value; e.recordAccess(this); return oldValue; //返回旧值 } } //修改次数增加1 modCount++; //将key、value添加至i位置处 addEntry(hash, key, value, i); return null; } 通过源码我们可以清晰看到HashMap保存数据的过程为：首先判断key是否为null，若为null，则直接调用putForNullKey方法。 若不为空则先计算key的hash值，然后根据hash值搜索在table数组中的索引位置，如果table数组在该位置处有元素，则通过比较是否存在相同的key，若存在则覆盖原来key的value，==否则将该元素保存在链头（最先保存的元素放在链尾）==。 若table在该处没有元素，则直接保存。这个过程看似比较简单，其实深有内幕。有如下几点： 1、 先看迭代处。此处迭代原因就是为了防止存在相同的key值，若发现两个hash值（key）相同时，HashMap的处理方式是用新value替换旧value，这里并没有处理key，这就解释了HashMap中没有两个相同的key。 2、 在看（1）、（2）处。这里是HashMap的精华所在。首先是hash方法，该方法为一个纯粹的数学计算，就是计算h的hash值。 static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 我们知道对于HashMap的table而言，数据分布需要均匀（最好每项都只有一个元素，这样就可以直接找到），不能太紧也不能太松，太紧会导致查询速度慢，太松则浪费空间。计算hash值后，怎么才能保证table元素分布均与呢？我们会想到取模，但是由于取模的消耗较大，HashMap是这样处理的：调用indexFor方法。 static int indexFor(int h, int length) { return h &amp; (length-1); } HashMap的底层数组长度总是2的n次方，在构造函数中存在：capacity &lt;&lt;= 1;这样做总是能够保证HashMap的底层数组长度为2的n次方。当length为2的n次方时，h&amp;(length - 1)就相当于对length取模，而且速度比直接取模快得多，这是HashMap在速度上的一个优化。至于为什么是2的n次方下面解释。 ==对length取模来得到hash是常用的hash索引方法，这里采用位运算的话效率更高。== 我们回到indexFor方法，该方法仅有一条语句：h&amp;(length - 1)，这句话除了上面的取模运算外还有一个非常重要的责任：均匀分布table数据和充分利用空间。 这里我们假设length为16(2^n)和15，h为5、6、7。 当n=15时，6和7的结果一样，这样表示他们在table存储的位置是相同的，也就是产生了碰撞，6、7就会在一个位置形成链表，这样就会导致查询速度降低。诚然这里只分析三个数字不是很多，那么我们就看0-15。 而当length = 16时，length – 1 = 15 即1111，那么进行低位&amp;运算时，值总是与原来hash值相同，而进行高位运算时，其值等于其低位值。所以说当length = 2^n时，不同的hash值发生碰撞的概率比较小，这样就会使得数据在table数组中分布较均匀，查询速度也较快。 这里我们再来复习put的流程：当我们想一个HashMap中添加一对key-value时，系统首先会计算key的hash值，然后根据hash值确认在table中存储的位置。若该位置没有元素，则直接插入。否则迭代该处元素链表并依此比较其key的hash值。 如果两个hash值相等且key值相等(e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))),则用新的Entry的value覆盖原来节点的value。如果两个hash值相等但key值不等 ，则将该节点插入该链表的链头。具体的实现过程见addEntry方法，如下： void addEntry(int hash, K key, V value, int bucketIndex) { //获取bucketIndex处的Entry Entry&lt;K, V&gt; e = table[bucketIndex]; //将新创建的 Entry 放入 bucketIndex 索引处，并让新的 Entry 指向原来的 Entry table[bucketIndex] = new Entry&lt;K, V&gt;(hash, key, value, e); //若HashMap中元素的个数超过极限了，则容量扩大两倍 if (size++ &gt;= threshold) resize(2 * table.length); }这个方法中有两点需要注意： 后面添加的entry反而会接到前面。 一、是链的产生。 这是一个非常优雅的设计。系统总是将新的Entry对象添加到bucketIndex处。如果bucketIndex处已经有了对象，那么新添加的Entry对象将指向原有的Entry对象，形成一条Entry链，但是若bucketIndex处没有Entry对象，也就是e==null,那么新添加的Entry对象指向null，也就不会产生Entry链了。 二、扩容问题。 随着HashMap中元素的数量越来越多，发生碰撞的概率就越来越大，所产生的链表长度就会越来越长，这样势必会影响HashMap的速度，为了保证HashMap的效率，系统必须要在某个临界点进行扩容处理。 该临界点在当HashMap中元素的数量等于table数组长度*加载因子。但是扩容是一个非常耗时的过程，因为它需要重新计算这些数据在新table数组中的位置并进行复制处理。所以如果我们已经预知HashMap中元素的个数，那么预设元素的个数能够有效的提高HashMap的性能。 JDK1.8的hashmap：put方法final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果p是红黑树节点，则用另外的处理方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //当链表节点数超过8个，则直接进行红黑树化。 treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; }JDK1.8在链表长度超过8时会转换为红黑树。转换方法如下： final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //如果节点数变小小于红黑树的节点数阈值时，调整空间 resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { //该方法直接返回一个红黑树结点。 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { //从链表头开始依次插入红黑树 p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } // For treeifyBin TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) { return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next); }扩容final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { //如果原容量大于最大空间，则让阈值为最大值。因为不能再扩容了，最大容量就是整数最大值。 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //两倍扩容，阈值也跟着变为两倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) //当后面没有节点时，直接插入即可 //每个元素重新计算索引位置，此处的hash值并没有变，只是改变索引值 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // preserve order //否则，就从头到尾依次将节点进行索引然后插入新数组，这样插入后的链表顺序会和原来的顺序相反。 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; }读取实现：get(key) 相对于HashMap的存而言，取就显得比较简单了。通过key的hash值找到在table数组中的索引处的Entry，然后返回该key对应的value即可。 public V get(Object key) { // 若为null，调用getForNullKey方法返回相对应的value if (key == null) return getForNullKey(); // 根据该 key 的 hashCode 值计算它的 hash 码 int hash = hash(key.hashCode()); // 取出 table 数组中指定索引处的值 for (Entry&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; //若搜索的key与查找的key相同，则返回相对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; } return null; } 在这里能够根据key快速的取到value除了和HashMap的数据结构密不可分外，还和Entry有莫大的关系，在前面就提到过，HashMap在存储过程中并没有将key，value分开来存储，而是当做一个整体key-value来处理的，这个整体就是Entry对象。 同时value也只相当于key的附属而已。在存储的过程中，系统根据key的hashcode来决定Entry在table数组中的存储位置，在取的过程中同样根据key的hashcode取出相对应的Entry对象。 在java中与有两个类都提供了一个多种用途的hashTable机制，他们都可以将可以key和value结合起来构成键值对通过put(key,value)方法保存起来，然后通过get(key)方法获取相对应的value值。 HashTable一个是前面提到的HashMap，还有一个就是马上要讲解的HashTable。对于HashTable而言，它在很大程度上和HashMap的实现差不多，如果我们对HashMap比较了解的话，对HashTable的认知会提高很大的帮助。他们两者之间只存在几点的不同，这个后面会阐述。 定义 HashTable在Java中的定义如下： public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable 从中可以看出HashTable继承Dictionary类，实现Map接口。其中Dictionary类是任何可将键映射到相应值的类（如 Hashtable）的抽象父类。每个键和每个值都是一个对象。在任何一个 Dictionary 对象中，每个键至多与一个值相关联。Map是&quot;key-value键值对&quot;接口。 HashTable采用&quot;拉链法&quot;实现哈希表，它定义了几个重要的参数：table、count、threshold、loadFactor、modCount。 table：为一个Entry[]数组类型，Entry代表了“拉链”的节点，每一个Entry代表了一个键值对，哈希表的&quot;key-value键值对&quot;都是存储在Entry数组中的。 count：HashTable的大小，注意这个大小并不是HashTable的容器大小，而是他所包含Entry键值对的数量。 threshold：Hashtable的阈值，用于判断是否需要调整Hashtable的容量。threshold的值=&quot;容量*加载因子&quot;。 loadFactor：加载因子。 modCount：用来实现“fail-fast”机制的（也就是快速失败）。所谓快速失败就是在并发集合中，其进行迭代操作时，若有其他线程对其进行结构性的修改，这时迭代器会立马感知到，并且立即抛出ConcurrentModificationException异常，而不是等到迭代完成之后才告诉你（你已经出错了）。构造方法 在HashTabel中存在5个构造函数。通过这5个构造函数我们构建出一个我想要的HashTable。 public Hashtable() { this(11, 0.75f); } 默认构造函数，容量为11，加载因子为0.75。 public Hashtable(int initialCapacity) { this(initialCapacity, 0.75f); } 用指定初始容量和默认的加载因子 (0.75) 构造一个新的空哈希表。 public Hashtable(int initialCapacity, float loadFactor) { //验证初始容量 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); //验证加载因子 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal Load: &quot;+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; //初始化table，获得大小为initialCapacity的table数组 table = new Entry[initialCapacity]; //计算阀值 threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1); //初始化HashSeed值 initHashSeedAsNeeded(initialCapacity); }​ 用指定初始容量和指定加载因子构造一个新的空哈希表。其中initHashSeedAsNeeded方法用于初始化hashSeed参数，其中hashSeed用于计算key的hash值，它与key的hashCode进行按位异或运算。这个hashSeed是一个与实例相关的随机值，主要用于解决hash冲突。 private int hash(Object k) { return hashSeed ^ k.hashCode(); } 构造一个与给定的 Map 具有相同映射关系的新哈希表。 public Hashtable(Map&lt;? extends K, ? extends V&gt; t) { //设置table容器大小，其值==t.size * 2 + 1 this(Math.max(2*t.size(), 11), 0.75f); putAll(t); }主要方法HashTable的API对外提供了许多方法，这些方法能够很好帮助我们操作HashTable，但是这里我只介绍两个最根本的方法：put、get。 首先我们先看put方法：将指定 key 映射到此哈希表中的指定 value。注意这里键key和值value都不可为空。 public synchronized V put(K key, V value) { // 确保value不为null if (value == null) { throw new NullPointerException(); } /* * 确保key在table[]是不重复的 * 处理过程： * 1、计算key的hash值，确认在table[]中的索引位置 * 2、迭代index索引位置，如果该位置处的链表中存在一个一样的key，则替换其value，返回旧值 */ Entry tab[] = table; int hash = hash(key); //计算key的hash值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; //确认该key的索引位置 //迭代，寻找该key，替换 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { V old = e.value; e.value = value; return old; } } modCount++; if (count &gt;= threshold) { //如果容器中的元素数量已经达到阀值，则进行扩容操作 rehash(); tab = table; hash = hash(key); index = (hash &amp; 0x7FFFFFFF) % tab.length; } // 在索引位置处插入一个新的节点 Entry&lt;K,V&gt; e = tab[index]; tab[index] = new Entry&lt;&gt;(hash, key, value, e); //容器中元素+1 count++; return null; } put方法的整个处理流程是：计算key的hash值，根据hash值获得key在table数组中的索引位置，然后迭代该key处的Entry链表（我们暂且理解为链表），若该链表中存在一个这个的key对象，那么就直接替换其value值即可，否则在将改key-value节点插入该index索引位置处 在HashTabled的put方法中有两个地方需要注意： 1、HashTable的扩容操作，在put方法中，如果需要向table[]中添加Entry元素，会首先进行容量校验，如果容量已经达到了阀值，HashTable就会进行扩容处理rehash()，如下: protected void rehash() { int oldCapacity = table.length; //元素 Entry&lt;K,V&gt;[] oldMap = table; //新容量=旧容量 * 2 + 1 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) { if (oldCapacity == MAX_ARRAY_SIZE) return; newCapacity = MAX_ARRAY_SIZE; } //新建一个size = newCapacity 的HashTable Entry&lt;K,V&gt;[] newMap = new Entry[]; modCount++; //重新计算阀值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); //重新计算hashSeed boolean rehash = initHashSeedAsNeeded(newCapacity); table = newMap; //将原来的元素拷贝到新的HashTable中 for (int i = oldCapacity ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; old = oldMap[i] ; old != null ; ) { Entry&lt;K,V&gt; e = old; old = old.next; if (rehash) { e.hash = hash(e.key); } int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = newMap[index]; newMap[index] = e; } } } 在这个rehash()方法中我们可以看到容量扩大两倍+1，同时需要将原来HashTable中的元素一一复制到新的HashTable中，这个过程是比较消耗时间的，同时还需要重新计算hashSeed的，毕竟容量已经变了。 这里对阀值啰嗦一下：比如初始值11、加载因子默认0.75，那么这个时候阀值threshold=8，当容器中的元素达到8时，HashTable进行一次扩容操作，容量 = 8 * 2 + 1 =17，而阀值threshold=17*0.75 = 13，当容器元素再一次达到阀值时，HashTable还会进行扩容操作，依次类推。 下面是计算key的hash值，这里hashSeed发挥了作用。 private int hash(Object k) { return hashSeed ^ k.hashCode(); } 相对于put方法，get方法就会比较简单，处理过程就是计算key的hash值，判断在table数组中的索引位置，然后迭代链表，匹配直到找到相对应key的value,若没有找到返回null。 public synchronized V get(Object key) { Entry tab[] = table; int hash = hash(key); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return e.value; } } return null; }HashTable与HashMap的异同点 HashTable和HashMap存在很多的相同点，但是他们还是有几个比较重要的不同点。 第一：我们从他们的定义就可以看出他们的不同，HashTable基于Dictionary类，而HashMap是基于AbstractMap。Dictionary是什么？它是任何可将键映射到相应值的类的抽象父类，而AbstractMap是基于Map接口的骨干实现，它以最大限度地减少实现此接口所需的工作。 第二：HashMap可以允许存在一个为null的key和任意个为null的value，但是HashTable中的key和value都不允许为null。如下： 当HashMap遇到为null的key时，它会调用putForNullKey方法来进行处理。对于value没有进行任何处理，只要是对象都可以。 if (key == null) return putForNullKey(value); 而当HashTable遇到null时，他会直接抛出NullPointerException异常信息。 if (value == null) { throw new NullPointerException(); } 第三：Hashtable的方法是同步的，而HashMap的方法不是。所以有人一般都建议如果是涉及到多线程同步时采用HashTable，没有涉及就采用HashMap，但是在Collections类中存在一个静态方法：synchronizedMap()，该方法创建了一个线程安全的Map对象，并把它作为一个封装的对象来返回，所以通过Collections类的synchronizedMap方法是可以我们你同步访问潜在的HashMap。这样君该如何选择呢？？？ 面试题：HashMap和HashTable的区别HashMap线程不安全，HashTable是线程安全的。HashMap内部实现没有任何线程同步相关的代码，所以相对而言性能要好一点。如果在多线程中使用HashMap需要自己管理线程同步。HashTable大部分对外接口都使用synchronized包裹，所以是线程安全的，但是性能会相对差一些。 二者的基类不一样。HashMap派生于AbstractMap，HashTable派生于Dictionary。它们都实现Map, Cloneable, Serializable这些接口。AbstractMap中提供的基础方法更多，并且实现了多个通用的方法，而在Dictionary中只有少量的接口，并且都是abstract类型。 key和value的取值范围不同。HashMap的key和value都可以为null，但是HashTablekey和value都不能为null。对于HashMap如果get返回null，并不能表明HashMap不存在这个key，如果需要判断HashMap中是否包含某个key，就需要使用containsKey这个方法来判断。 算法不一样。HashMap的initialCapacity为16，而HashTable的initialCapacity为11。HashMap中初始容量必须是2的幂,如果初始化传入的initialCapacity不是2的幂，将会自动调整为大于出入的initialCapacity最小的2的幂。HashMap使用自己的计算hash的方法(会依赖key的hashCode方法)，HashTable则使用key的hashCode方法得到。 参考文章http://cmsblogs.com/?p=176 http://mini.eastday.com/mobile/180310183019559.html# https://blog.csdn.net/lihua5419/article/details/87691965 https://www.cnblogs.com/aeolian/p/8468632.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
        <tag>HashTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解3：一文读懂Iterator，fail-fast机制与比较器]]></title>
    <url>%2F2019%2F09%2F26%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A33%EF%BC%9AIterator%EF%BC%8Cfail-fast%E6%9C%BA%E5%88%B6%E4%B8%8E%E6%AF%94%E8%BE%83%E5%99%A8%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 今天我们来探索一下LIterator，fail-fast机制与比较器的源码。 本文参考 www.cmsblogs.com/p=1185 Iterator迭代对于我们搞Java的来说绝对不陌生。我们常常使用JDK提供的迭代接口进行Java集合的迭代。 Iterator iterator = list.iterator(); while(iterator.hasNext()){ String string = iterator.next(); do something }迭代其实我们可以简单地理解为遍历，是一个标准化遍历各类容器里面的所有对象的方法类，它是一个很典型的设计模式。Iterator模式是用于遍历集合类的标准访问方法。 它可以把访问逻辑从不同类型的集合类中抽象出来，从而避免向客户端暴露集合的内部结构。 在没有迭代器时我们都是这么进行处理的。如下： 对于数组我们是使用下标来进行处理的 int[] arrays = new int[10]; for(int i = 0 ; i arrays.length ; i++){ int a = arrays[i]; do something }对于ArrayList是这么处理的 ListString list = new ArrayListString(); for(int i = 0 ; i list.size() ; i++){ String string = list.get(i); do something }对于这两种方式，我们总是都事先知道集合的内部结构，访问代码和集合本身是紧密耦合的，无法将访问逻辑从集合类和客户端代码中分离出来。同时每一种集合对应一种遍历方法，客户端代码无法复用。 在实际应用中如何需要将上面将两个集合进行整合是相当麻烦的。所以为了解决以上问题，Iterator模式腾空出世，它总是用同一种逻辑来遍历集合。 使得客户端自身不需要来维护集合的内部结构，所有的内部状态都由Iterator来维护。客户端从不直接和集合类打交道，它总是控制Iterator，向它发送向前，向后，取当前元素的命令，就可以间接遍历整个集合。 上面只是对Iterator模式进行简单的说明，下面我们看看Java中Iterator接口，看他是如何来进行实现的。 java.util.Iterator在Java中Iterator为一个接口，它只提供了迭代了基本规则，在JDK中他是这样定义的：对 collection 进行迭代的迭代器。迭代器取代了 Java Collections Framework 中的 Enumeration。迭代器与枚举有两点不同： 1、迭代器允许调用者利用定义良好的语义在迭代期间从迭代器所指向的 collection 移除元素。 2、方法名称得到了改进。其接口定义如下： public interface Iterator { boolean hasNext(); Object next(); void remove(); }其中： Object next()：返回迭代器刚越过的元素的引用，返回值是Object，需要强制转换成自己需要的类型 boolean hasNext()：判断容器内是否还有可供访问的元素 void remove()：删除迭代器刚越过的元素对于我们而言，我们只一般只需使用next()、hasNext()两个方法即可完成迭代。如下： for(Iterator it = c.iterator(); it.hasNext(); ) { Object o = it.next(); do something }==前面阐述了Iterator有一个很大的优点,就是我们不必知道集合的内部结果,集合的内部结构、状态由Iterator来维持，通过统一的方法hasNext()、next()来判断、获取下一个元素，至于具体的内部实现我们就不用关心了。== 但是作为一个合格的程序员我们非常有必要来弄清楚Iterator的实现。下面就ArrayList的源码进行分析分析。 各个集合的Iterator的实现下面就ArrayList的Iterator实现来分析，其实如果我们理解了ArrayList、Hashset、TreeSet的数据结构，内部实现，对于他们是如何实现Iterator也会胸有成竹的。因为ArrayList的内部实现采用数组，所以我们只需要记录相应位置的索引即可，其方法的实现比较简单。 ArrayList的Iterator实现 在ArrayList内部首先是定义一个内部类Itr，该内部类实现Iterator接口，如下： private class Itr implements IteratorE { do something } 而ArrayList的iterator()方法实现： public IteratorE iterator() { return new Itr(); }所以通过使用ArrayList.iterator()方法返回的是Itr()内部类，所以现在我们需要关心的就是Itr()内部类的实现： 在Itr内部定义了三个int型的变量：cursor、lastRet、expectedModCount。其中cursor表示下一个元素的索引位置，lastRet表示上一个元素的索引位置 int cursor; int lastRet = -1; int expectedModCount = modCount;从cursor、lastRet定义可以看出，lastRet一直比cursor少一所以hasNext()实现方法异常简单，只需要判断cursor和lastRet是否相等即可。 public boolean hasNext() { return cursor != size; }对于next()实现其实也是比较简单的，只要返回cursor索引位置处的元素即可，然后修改cursor、lastRet即可。 public E next() { checkForComodification(); int i = cursor; 记录索引位置 if (i = size) 如果获取元素大于集合元素个数，则抛出异常 throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i = elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; cursor + 1 return (E) elementData[lastRet = i]; lastRet + 1 且返回cursor处元素 } checkForComodification()主要用来判断集合的修改次数是否合法，即用来判断遍历过程中集合是否被修改过。 。modCount用于记录ArrayList集合的修改次数，初始化为0，，每当集合被修改一次（结构上面的修改，内部update不算），如add、remove等方法，modCount + 1，所以如果modCount不变，则表示集合内容没有被修改。 该机制主要是用于实现ArrayList集合的快速失败机制，在Java的集合中，较大一部分集合是存在快速失败机制的，这里就不多说，后面会讲到。 所以要保证在遍历过程中不出错误，我们就应该保证在遍历过程中不会对集合产生结构上的修改（当然remove方法除外），出现了异常错误，我们就应该认真检查程序是否出错而不是catch后不做处理。 final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } 对于remove()方法的是实现，它是调用ArrayList本身的remove()方法删除lastRet位置元素，然后修改modCount即可。 public void remove() { if (lastRet 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } }这里就对ArrayList的Iterator实现讲解到这里，对于Hashset、TreeSet等集合的Iterator实现，各位如果感兴趣可以继续研究，个人认为在研究这些集合的源码之前，有必要对该集合的数据结构有清晰的认识，这样会达到事半功倍的效果！！！！ fail-fast机制这部分参考http://cmsblogs.com/p=1220 在JDK的Collection中我们时常会看到类似于这样的话： 例如，ArrayList 注意，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改做出任何硬性保证。快速失败迭代器会尽最大努力抛出ConcurrentModificationException。 因此，为提高这类迭代器的正确性而编写一个依赖于此异常的程序是错误的做法：迭代器的快速失败行为应该仅用于检测 bug。 HashMap中： 注意，迭代器的快速失败行为不能得到保证，一般来说，存在非同步的并发修改时，不可能作出任何坚决的保证。快速失败迭代器尽最大努力抛出 ConcurrentModificationException。因此，编写依赖于此异常的程序的做法是错误的，正确做法是：迭代器的快速失败行为应该仅用于检测程序错误。 在这两段话中反复地提到”快速失败”。那么何为”快速失败”机制呢？ “快速失败”也就是fail-fast，它是Java集合的一种错误检测机制。当多个线程对集合进行结构上的改变的操作时，有可能会产生fail-fast机制。 记住是有可能，而不是一定。例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException异常，从而产生fail-fast机制。 fail-fast示例public class FailFastTest { private static ListInteger list = new ArrayList();​ @desc线程one迭代list @Projecttest @fileFailFastTest.java @Authrochenssy @data2014年7月26日 private static class threadOne extends Thread{ public void run() { IteratorInteger iterator = list.iterator(); while(iterator.hasNext()){ int i = iterator.next(); System.out.println(ThreadOne 遍历 + i); try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } } } }​ @desc当i == 3时，修改list @Projecttest @fileFailFastTest.java @Authrochenssy @data2014年7月26日 private static class threadTwo extends Thread{ public void run(){ int i = 0 ; while(i 6){ System.out.println(ThreadTwo run： + i); if(i == 3){ list.remove(i); } i++; } } } public static void main(String[] args) { for(int i = 0 ; i 10;i++){ list.add(i); } new threadOne().start(); new threadTwo().start(); } }运行结果： ThreadOne 遍历0 ThreadTwo run：0 ThreadTwo run：1 ThreadTwo run：2 ThreadTwo run：3 ThreadTwo run：4 ThreadTwo run：5 Exception in thread Thread-0 java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(Unknown Source) at java.util.ArrayList$Itr.next(Unknown Source) at test.ArrayListTest$threadOne.run(ArrayListTest.java23)fail-fast产生原因通过上面的示例和讲解，我初步知道fail-fast产生的原因就在于程序在对 collection 进行迭代时，某个线程对该 collection 在结构上对其做了修改，这时迭代器就会抛出 ConcurrentModificationException 异常信息，从而产生 fail-fast。 要了解fail-fast机制，我们首先要对ConcurrentModificationException 异常有所了解。当方法检测到对象的并发修改，但不允许这种修改时就抛出该异常。同时需要注意的是，该异常不会始终指出对象已经由不同线程并发修改，如果单线程违反了规则，同样也有可能会抛出改异常。 诚然，迭代器的快速失败行为无法得到保证，它不能保证一定会出现该错误，但是快速失败操作会尽最大努力抛出ConcurrentModificationException异常，所以因此，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。下面我将以ArrayList为例进一步分析fail-fast产生的原因。 从前面我们知道fail-fast是在操作迭代器时产生的。现在我们来看看ArrayList中迭代器的源代码： private class Itr implements IteratorE { int cursor; int lastRet = -1; int expectedModCount = ArrayList.this.modCount; public boolean hasNext() { return (this.cursor != ArrayList.this.size); } public E next() { checkForComodification(); 省略此处代码 } public void remove() { if (this.lastRet 0) throw new IllegalStateException(); checkForComodification(); 省略此处代码 } final void checkForComodification() { if (ArrayList.this.modCount == this.expectedModCount) return; throw new ConcurrentModificationException(); } }从上面的源代码我们可以看出，迭代器在调用next()、remove()方法时都是调用checkForComodification()方法，该方法主要就是检测modCount == expectedModCount 若不等则抛出ConcurrentModificationException 异常，从而产生fail-fast机制。所以要弄清楚为什么会产生fail-fast机制我们就必须要用弄明白为什么modCount != expectedModCount ，他们的值在什么时候发生改变的。 expectedModCount 是在Itr中定义的：int expectedModCount = ArrayList.this.modCount;所以他的值是不可能会修改的，所以会变的就是modCount。modCount是在 AbstractList 中定义的，为全局变量： protected transient int modCount = 0;那么他什么时候因为什么原因而发生改变呢？请看ArrayList的源码： public boolean add(E paramE) { ensureCapacityInternal(this.size + 1); 省略此处代码 } private void ensureCapacityInternal(int paramInt) { if (this.elementData == EMPTY_ELEMENTDATA) paramInt = Math.max(10, paramInt); ensureExplicitCapacity(paramInt); } private void ensureExplicitCapacity(int paramInt) { this.modCount += 1; 修改modCount 省略此处代码 } public boolean remove(Object paramObject) { int i; if (paramObject == null) for (i = 0; i this.size; ++i) { if (this.elementData[i] != null) continue; fastRemove(i); return true; } else for (i = 0; i this.size; ++i) { if (!(paramObject.equals(this.elementData[i]))) continue; fastRemove(i); return true; } return false; } private void fastRemove(int paramInt) { this.modCount += 1; 修改modCount 省略此处代码 } public void clear() { this.modCount += 1; 修改modCount 省略此处代码 } 从上面的源代码我们可以看出，ArrayList中无论add、remove、clear方法只要是涉及了改变ArrayList元素的个数的方法都会导致modCount的改变。 所以我们这里可以初步判断由于expectedModCount 得值与modCount的改变不同步，导致两者之间不等从而产生fail-fast机制。知道产生fail-fast产生的根本原因了，我们可以有如下场景： 有两个线程（线程A，线程B），其中线程A负责遍历list、线程B修改list。线程A在遍历list过程的某个时候（此时expectedModCount = modCount=N），线程启动，同时线程B增加一个元素，这是modCount的值发生改变（modCount + 1 = N + 1）。 线程A继续遍历执行next方法时，通告checkForComodification方法发现expectedModCount = N ，而modCount = N + 1，两者不等，这时就抛出ConcurrentModificationException 异常，从而产生fail-fast机制。 所以，直到这里我们已经完全了解了fail-fast产生的根本原因了。知道了原因就好找解决办法了。 fail-fast解决办法通过前面的实例、源码分析，我想各位已经基本了解了fail-fast的机制，下面我就产生的原因提出解决方案。这里有两种解决方案： 方案一：在遍历过程中所有涉及到改变modCount值得地方全部加上synchronized或者直接使用Collections.synchronizedList，这样就可以解决。但是不推荐，因为增删造成的同步锁可能会阻塞遍历操作。 方案二：使用CopyOnWriteArrayList来替换ArrayList。推荐使用该方案。 CopyOnWriteArrayList为何物？ArrayList 的一个线程安全的变体，其中所有可变操作（add、set 等等）都是通过对底层数组进行一次新的复制来实现的。 该类产生的开销比较大，但是在两种情况下，它非常适合使用。 1：在不能或不想进行同步遍历，但又需要从并发线程中排除冲突时。 2：当遍历操作的数量大大超过可变操作的数量时。遇到这两种情况使用CopyOnWriteArrayList来替代ArrayList再适合不过了。那么为什么CopyOnWriterArrayList可以替代ArrayList呢？ 第一、CopyOnWriterArrayList的无论是从数据结构、定义都和ArrayList一样。它和ArrayList一样，同样是实现List接口，底层使用数组实现。在方法上也包含add、remove、clear、iterator等方法。 第二、CopyOnWriterArrayList根本就不会产生ConcurrentModificationException异常，也就是它使用迭代器完全不会产生fail-fast机制。请看： private static class COWIteratorE implements ListIteratorE { 省略此处代码 public E next() { if (!(hasNext())) throw new NoSuchElementException(); return this.snapshot[(this.cursor++)]; } 省略此处代码 }CopyOnWriterArrayList的方法根本就没有像ArrayList中使用checkForComodification方法来判断expectedModCount 与 modCount 是否相等。它为什么会这么做，凭什么可以这么做呢？我们以add方法为例： public boolean add(E paramE) { ReentrantLock localReentrantLock = this.lock; localReentrantLock.lock(); try { Object[] arrayOfObject1 = getArray(); int i = arrayOfObject1.length; Object[] arrayOfObject2 = Arrays.copyOf(arrayOfObject1, i + 1); arrayOfObject2[i] = paramE; setArray(arrayOfObject2); int j = 1; return j; } finally { localReentrantLock.unlock(); } } final void setArray(Object[] paramArrayOfObject) { this.array = paramArrayOfObject; }CopyOnWriterArrayList的add方法与ArrayList的add方法有一个最大的不同点就在于，下面三句代码： Object[] arrayOfObject2 = Arrays.copyOf(arrayOfObject1, i + 1); arrayOfObject2[i] = paramE; setArray(arrayOfObject2);就是这三句代码使得CopyOnWriterArrayList不会抛ConcurrentModificationException异常。他们所展现的魅力就在于copy原来的array，再在copy数组上进行add操作，这样做就完全不会影响COWIterator中的array了。 所以CopyOnWriterArrayList所代表的核心概念就是：任何对array在结构上有所改变的操作（add、remove、clear等），CopyOnWriterArrayList都会copy现有的数据，再在copy的数据上修改，这样就不会影响COWIterator中的数据了，修改完成之后改变原有数据的引用即可。同时这样造成的代价就是产生大量的对象，同时数组的copy也是相当有损耗的。 Comparable 和 ComparatorJava 中为我们提供了两种比较机制：Comparable 和 Comparator，他们之间有什么区别呢？今天来了解一下。 ComparableComparable 在 java.lang包下，是一个接口，内部只有一个方法 compareTo()： public interface ComparableT { public int compareTo(T o); }Comparable 可以让实现它的类的对象进行比较，具体的比较规则是按照 compareTo 方法中的规则进行。这种顺序称为 自然顺序。 compareTo 方法的返回值有三种情况： e1.compareTo(e2) 0 即 e1 e2 e1.compareTo(e2) = 0 即 e1 = e2 e1.compareTo(e2) 0 即 e1 e2注意： 1.由于 null 不是一个类，也不是一个对象，因此在重写 compareTo 方法时应该注意 e.compareTo(null) 的情况，即使 e.equals(null) 返回 false，compareTo 方法也应该主动抛出一个空指针异常 NullPointerException。 2.Comparable 实现类重写 compareTo 方法时一般要求 e1.compareTo(e2) == 0 的结果要和 e1.equals(e2) 一致。这样将来使用 SortedSet 等根据类的自然排序进行排序的集合容器时可以保证保存的数据的顺序和想象中一致。 有人可能好奇上面的第二点如果违反了会怎样呢？ 举个例子，如果你往一个 SortedSet 中先后添加两个对象 a 和 b，a b 满足 (!a.equals(b) &amp;&amp; a.compareTo(b) == 0)，同时也没有另外指定个 Comparator，那当你添加完 a 再添加 b 时会添加失败返回 false, SortedSet 的 size 也不会增加，因为在 SortedSet 看来它们是相同的，而 SortedSet 中是不允许重复的。 实际上所有实现了 Comparable 接口的 Java 核心类的结果都和 equlas 方法保持一致。 实现了 Comparable 接口的 List 或则数组可以使用 Collections.sort() 或者 Arrays.sort() 方法进行排序。 实现了 Comparable 接口的对象才能够直接被用作 SortedMap (SortedSet) 的 key，要不然得在外边指定 Comparator 排序规则。 因此自己定义的类如果想要使用有序的集合类，需要实现 Comparable 接口，比如： description 测试用的实体类 书, 实现了 Comparable 接口，自然排序 author shixinzhang br data 1052016 public class BookBean implements Serializable, Comparable { private String name; private int count; public BookBean(String name, int count) { this.name = name; this.count = count; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getCount() { return count; } public void setCount(int count) { this.count = count; }​ 重写 equals @param o @return @Override public boolean equals(Object o) { if (this == o) return true; if (!(o instanceof BookBean)) return false; BookBean bean = (BookBean) o; if (getCount() != bean.getCount()) return false; return getName().equals(bean.getName()); }​ 重写 hashCode 的计算方法 根据所有属性进行 迭代计算，避免重复 计算 hashCode 时 计算因子 31 见得很多，是一个质数，不能再被除 @return @Override public int hashCode() { 调用 String 的 hashCode(), 唯一表示一个字符串内容 int result = getName().hashCode(); 乘以 31, 再加上 count result = 31 result + getCount(); return result; } @Override public String toString() { return BookBean{ + name=&apos; + name + &apos;&apos;&apos; + , count= + count + &apos;}&apos;; }​ 当向 TreeSet 中添加 BookBean 时，会调用这个方法进行排序 @param another @return @Override public int compareTo(Object another) { if (another instanceof BookBean){ BookBean anotherBook = (BookBean) another; int result; 比如这里按照书价排序 result = getCount() - anotherBook.getCount(); 或者按照 String 的比较顺序 result = getName().compareTo(anotherBook.getName()); if (result == 0){ 当书价一致时，再对比书名。 保证所有属性比较一遍 result = getName().compareTo(anotherBook.getName()); } return result; } 一样就返回 0 return 0; }上述代码还重写了 equlas(), hashCode() 方法，自定义的类将来可能会进行比较时，建议重写这些方法。 这里我想表达的是在有些场景下 equals 和 compareTo 结果要保持一致，这时候不重写 equals，使用 Object.equals 方法得到的结果会有问题，比如说 HashMap.put() 方法，会先调用 key 的 equals 方法进行比较，然后才调用 compareTo。 后面重写 compareTo 时，要判断某个相同时对比下一个属性，把所有属性都比较一次。 Comparator首先认识一下Comparator： Comparator 是javase中的接口，位于java.util包下，该接口抽象度极高，有必要掌握该接口的使用大多数文章告诉大家Comparator是用来排序，但我想说排序是Comparator能实现的功能之一，他不仅限于排序 排序例子：题目描述输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 代码实现： import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; public class Solution { public String PrintMinNumber(int [] s) { if(s==null) return null; String s1=&quot;&quot;; ArrayList&lt;Integer&gt; list=new ArrayList&lt;Integer&gt;(); for(int i=0;i&lt;s.length;i++){ list.add(s[i]); } Collections.sort(list,new Comparator&lt;Integer&gt;(){ public int compare(Integer str1,Integer str2){ String s1=str1+&quot;&quot;+str2; String s2=str2+&quot;&quot;+str1; return s1.compareTo(s2); } }); for(int j:list){ s1+=j; } return s1; } }一般需要做比较的逻辑都可以使用的上Comparator，最常用的场景就是排序和分组，排序常使用Arrays和Collections的sort方法，而分组则可以使用提供的divider方法。 排序和分组的区别在于:排序时，两个对象比较的结果有三种：大于，等于，小于。分组时，两个对象比较的结果只有两种：等于(两个对象属于同一组)，不等于(两个对象属于不同组) Java8中使用lambda实现比较器今天先看看Lambda 表达式的简单使用：首先：Lambda表达式的基本语法：(parameters) -&gt; expression或（请注意语句的花括号）(parameters) -&gt; { statements; } 第一感觉就是这个箭头感觉有点怪，不过多用几次习惯就好，它主要是为了把参数列表与Lambda主体分隔开，箭头左边的是参数列表，右边的是Lambda主体。注意：Lambda表达式可以包含多行语句。在用Lambda 之前，我们先看看之前写比较器的写法 Comparator&lt;Developer&gt; byName = new Comparator&lt;Developer&gt;() { @Override public int compare(Developer o1, Developer o2) { return o1.getName().compareTo(o2.getName()); } };感觉也不是很复杂，没几行代码，再来看看Lambda 表达式的写法： Comparator&lt;Developer&gt; byName = (Developer o1, Developer o2)-&gt;o1.getName().compareTo(o2.getName());比之前要简单许多有木有。下面再来看看排序功能示例：先用Collections.sort如下： public class TestSorting { public static void main(String[] args) { List&lt;Developer&gt; listDevs = getDevelopers(); System.out.println(&quot;Before Sort&quot;); for (Developer developer : listDevs) { System.out.println(developer); } //安装年龄排序 Collections.sort(listDevs, new Comparator&lt;Developer&gt;() { @Override public int compare(Developer o1, Developer o2) { return o1.getAge() - o2.getAge(); } }); System.out.println(&quot;After Sort&quot;); for (Developer developer : listDevs) { System.out.println(developer); } } private static List&lt;Developer&gt; getDevelopers() { List&lt;Developer&gt; result = new ArrayList&lt;Developer&gt;(); result.add(new Developer(&quot;mkyong&quot;, new BigDecimal(&quot;70000&quot;), 33)); result.add(new Developer(&quot;alvin&quot;, new BigDecimal(&quot;80000&quot;), 20)); result.add(new Developer(&quot;jason&quot;, new BigDecimal(&quot;100000&quot;), 10)); result.add(new Developer(&quot;iris&quot;, new BigDecimal(&quot;170000&quot;), 55)); return result; } } 输出结果： Before Sort Developer [name=mkyong, salary=70000, age=33] Developer [name=alvin, salary=80000, age=20] Developer [name=jason, salary=100000, age=10] Developer [name=iris, salary=170000, age=55] After Sort Developer [name=jason, salary=100000, age=10] Developer [name=alvin, salary=80000, age=20] Developer [name=mkyong, salary=70000, age=33] Developer [name=iris, salary=170000, age=55]看起来整个流程完全没毛病，下面再来看看Lambda的方式: public class TestSorting { public static void main(String[] args) { List&lt;Developer&gt; listDevs = getDevelopers(); System.out.println(&quot;Before Sort&quot;); for (Developer developer : listDevs) { System.out.println(developer); } System.out.println(&quot;After Sort&quot;); //对比上面的代码 listDevs.sort((Developer o1, Developer o2)-&gt;o1.getAge()-o2.getAge()); //这样打印感觉也不错 listDevs.forEach((developer)-&gt;System.out.println(developer)); } private static List&lt;Developer&gt; getDevelopers() { List&lt;Developer&gt; result = new ArrayList&lt;Developer&gt;(); result.add(new Developer(&quot;mkyong&quot;, new BigDecimal(&quot;70000&quot;), 33)); result.add(new Developer(&quot;alvin&quot;, new BigDecimal(&quot;80000&quot;), 20)); result.add(new Developer(&quot;jason&quot;, new BigDecimal(&quot;100000&quot;), 10)); result.add(new Developer(&quot;iris&quot;, new BigDecimal(&quot;170000&quot;), 55)); return result; } }输出结果： Before Sort Developer [name=mkyong, salary=70000, age=33] Developer [name=alvin, salary=80000, age=20] Developer [name=jason, salary=100000, age=10] Developer [name=iris, salary=170000, age=55] After Sort Developer [name=jason, salary=100000, age=10] Developer [name=alvin, salary=80000, age=20] Developer [name=mkyong, salary=70000, age=33] Developer [name=iris, salary=170000, age=55]总体来说，写法与之前有较大的改变，写的代码更少，更简便，感觉还不错。后续会带来更多有关Java8相关的东西，毕竟作为一只程序狗，得不停的学习才能不被淘汰。Java语言都在不停的改进更新，我们有啥理由不跟上节奏呢？由于时间问题这里只是一个简单的应用，想了解更多可到官网查找相关示例。 总结Java 中的两种排序方式： Comparable 自然排序。（实体类实现） Comparator 是定制排序。（无法修改实体类时，直接在调用方创建） 同时存在时采用 Comparator（定制排序）的规则进行比较。对于一些普通的数据类型（比如 String, Integer, Double…），它们默认实现了Comparable 接口，实现了 compareTo 方法，我们可以直接使用。 而对于一些自定义类，它们可能在不同情况下需要实现不同的比较策略，我们可以新创建 Comparator 接口，然后使用特定的 Comparator 实现进行比较。 这就是 Comparable 和 Comparator 的区别。 参考文章https://blog.csdn.net/weixin_30363263/article/details/80867590 https://www.cnblogs.com/shizhijie/p/7657049.html https://www.cnblogs.com/xiaweicn/p/8688216.html https://cmsblogs.com/p=1185 https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>Iterator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合类详解2：一文读懂Queue和LinkedList]]></title>
    <url>%2F2019%2F09%2F25%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A32%EF%BC%9AQueue%E5%92%8CLinkedList%2F</url>
    <content type="text"><![CDATA[《Java集合详解系列》是我在完成夯实Java基础篇的系列博客后准备开始写的新系列。 这些文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star、fork哈 文章首发于我的个人博客： www.how2playlife.com 本文参考 http://cmsblogs.com/?p=155和https://www.jianshu.com/p/0e84b8d3606c LinkedList概述 LinkedList与ArrayList一样实现List接口，只是ArrayList是List接口的大小可变数组的实现，LinkedList是List接口链表的实现。基于链表实现的方式使得LinkedList在插入和删除时更优于ArrayList，而随机访问则比ArrayList逊色些。 LinkedList实现所有可选的列表操作，并允许所有的元素包括null。 除了实现 List 接口外，LinkedList 类还为在列表的开头及结尾 get、remove 和 insert 元素提供了统一的命名方法。这些操作允许将链接列表用作堆栈、队列或双端队列。 此类实现 Deque 接口，为 add、poll 提供先进先出队列操作，以及其他堆栈和双端队列操作。 所有操作都是按照双重链接列表的需要执行的。在列表中编索引的操作将从开头或结尾遍历列表（从靠近指定索引的一端）。 同时，与ArrayList一样此实现不是同步的。 （以上摘自JDK 6.0 API）。 源码分析定义 首先我们先看LinkedList的定义： public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable 从这段代码中我们可以清晰地看出LinkedList继承AbstractSequentialList，实现List、Deque、Cloneable、Serializable。其中AbstractSequentialList提供了 List 接口的骨干实现，从而最大限度地减少了实现受“连续访问”数据存储（如链接列表）支持的此接口所需的工作,从而以减少实现List接口的复杂度。Deque一个线性 collection，支持在两端插入和移除元素，定义了双端队列的操作。属性在LinkedList中提供了两个基本属性size、header。 private transient Entry header = new Entry(null, null, null);private transient int size = 0;其中size表示的LinkedList的大小，header表示链表的表头，Entry为节点对象。 private static class Entry&lt;E&gt; { E element; //元素节点 Entry&lt;E&gt; next; //下一个元素 Entry&lt;E&gt; previous; //上一个元素 Entry(E element, Entry&lt;E&gt; next, Entry&lt;E&gt; previous) { this.element = element; this.next = next; this.previous = previous; } } 上面为Entry对象的源代码，Entry为LinkedList的内部类，它定义了存储的元素。该元素的前一个元素、后一个元素，这是典型的双向链表定义方式。构造方法LinkedList提供了两个构造方法：LinkedList()和LinkedList(Collection&lt;? extends E&gt; c)。 /** * 构造一个空列表。 */ public LinkedList() { header.next = header.previous = header; } /** * 构造一个包含指定 collection 中的元素的列表，这些元素按其 collection 的迭代器返回的顺序排列。 */ public LinkedList(Collection&lt;? extends E&gt; c) { this(); addAll(c); } LinkedList()构造一个空列表。里面没有任何元素，仅仅只是将header节点的前一个元素、后一个元素都指向自身。 LinkedList(Collection&lt;? extends E&gt; c)： 构造一个包含指定 collection 中的元素的列表，这些元素按其 collection 的迭代器返回的顺序排列。该构造函数首先会调用LinkedList()，构造一个空列表，然后调用了addAll()方法将Collection中的所有元素添加到列表中。以下是addAll()的源代码： /** * 添加指定 collection 中的所有元素到此列表的结尾，顺序是指定 collection 的迭代器返回这些元素的顺序。 */ public boolean addAll(Collection&lt;? extends E&gt; c) { return addAll(size, c); } /** * 将指定 collection 中的所有元素从指定位置开始插入此列表。其中index表示在其中插入指定collection中第一个元素的索引 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) { //若插入的位置小于0或者大于链表长度，则抛出IndexOutOfBoundsException异常 if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(&quot;Index: &quot; + index + &quot;, Size: &quot; + size); Object[] a = c.toArray(); int numNew = a.length; //插入元素的个数 //若插入的元素为空，则返回false if (numNew == 0) return false; //modCount:在AbstractList中定义的，表示从结构上修改列表的次数 modCount++; //获取插入位置的节点，若插入的位置在size处，则是头节点，否则获取index位置处的节点 Entry&lt;E&gt; successor = (index == size ? header : entry(index)); //插入位置的前一个节点，在插入过程中需要修改该节点的next引用：指向插入的节点元素 Entry&lt;E&gt; predecessor = successor.previous; //执行插入动作 for (int i = 0; i &lt; numNew; i++) { //构造一个节点e，这里已经执行了插入节点动作同时修改了相邻节点的指向引用 // Entry&lt;E&gt; e = new Entry&lt;E&gt;((E) a[i], successor, predecessor); //将插入位置前一个节点的下一个元素引用指向当前元素 predecessor.next = e; //修改插入位置的前一个节点，这样做的目的是将插入位置右移一位，保证后续的元素是插在该元素的后面，确保这些元素的顺序 predecessor = e; } successor.previous = predecessor; //修改容量大小 size += numNew; return true; } 在addAll()方法中，涉及到了两个方法，一个是entry(int index)，该方法为LinkedList的私有方法，主要是用来查找index位置的节点元素。 /** * 返回指定位置(若存在)的节点元素 */ private Entry&lt;E&gt; entry(int index) { if (index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(&quot;Index: &quot; + index + &quot;, Size: &quot; + size); //头部节点 Entry&lt;E&gt; e = header; //判断遍历的方向 if (index &lt; (size &gt;&gt; 1)) { for (int i = 0; i &lt;= index; i++) e = e.next; } else { for (int i = size; i &gt; index; i--) e = e.previous; } return e; } 从该方法有两个遍历方向中我们也可以看出LinkedList是双向链表，这也是在构造方法中为什么需要将header的前、后节点均指向自己。 如果对数据结构有点了解，对上面所涉及的内容应该问题，我们只需要清楚一点：LinkedList是双向链表，其余都迎刃而解。 由于篇幅有限，下面将就LinkedList中几个常用的方法进行源码分析。 增加方法 add(E e): 将指定元素添加到此列表的结尾。 public boolean add(E e) { addBefore(e, header); return true; } 该方法调用addBefore方法，然后直接返回true，对于addBefore()而已，它为LinkedList的私有方法。 private Entry&lt;E&gt; addBefore(E e, Entry&lt;E&gt; entry) { //利用Entry构造函数构建一个新节点 newEntry， Entry&lt;E&gt; newEntry = new Entry&lt;E&gt;(e, entry, entry.previous); //修改newEntry的前后节点的引用，确保其链表的引用关系是正确的 newEntry.previous.next = newEntry; newEntry.next.previous = newEntry; //容量+1 size++; //修改次数+1 modCount++; return newEntry; } 在addBefore方法中无非就是做了这件事：构建一个新节点newEntry，然后修改其前后的引用。 LinkedList还提供了其他的增加方法： add(int index, E element)：在此列表中指定的位置插入指定的元素。 addAll(Collection&lt;? extends E&gt; c)：添加指定 collection 中的所有元素到此列表的结尾，顺序是指定 collection 的迭代器返回这些元素的顺序。 addAll(int index, Collection&lt;? extends E&gt; c)：将指定 collection 中的所有元素从指定位置开始插入此列表。 AddFirst(E e): 将指定元素插入此列表的开头。 addLast(E e): 将指定元素添加到此列表的结尾。移除方法 remove(Object o)：从此列表中移除首次出现的指定元素（如果存在）。该方法的源代码如下： public boolean remove(Object o) { if (o==null) { for (Entry&lt;E&gt; e = header.next; e != header; e = e.next) { if (e.element==null) { remove(e); return true; } } } else { for (Entry&lt;E&gt; e = header.next; e != header; e = e.next) { if (o.equals(e.element)) { remove(e); return true; } } } return false; } 该方法首先会判断移除的元素是否为null，然后迭代这个链表找到该元素节点，最后调用remove(Entry e)，remove(Entry e)为私有方法，是LinkedList中所有移除方法的基础方法，如下： private E remove(Entry&lt;E&gt; e) { if (e == header) throw new NoSuchElementException(); //保留被移除的元素：要返回 E result = e.element; //将该节点的前一节点的next指向该节点后节点 e.previous.next = e.next; //将该节点的后一节点的previous指向该节点的前节点 //这两步就可以将该节点从链表从除去：在该链表中是无法遍历到该节点的 e.next.previous = e.previous; //将该节点归空 e.next = e.previous = null; e.element = null; size--; modCount++; return result; }其他的移除方法： clear()： 从此列表中移除所有元素。 remove()：获取并移除此列表的头（第一个元素）。 remove(int index)：移除此列表中指定位置处的元素。 remove(Objec o)：从此列表中移除首次出现的指定元素（如果存在）。 removeFirst()：移除并返回此列表的第一个元素。 removeFirstOccurrence(Object o)：从此列表中移除第一次出现的指定元素（从头部到尾部遍历列表时）。 removeLast()：移除并返回此列表的最后一个元素。 removeLastOccurrence(Object o)：从此列表中移除最后一次出现的指定元素（从头部到尾部遍历列表时）。查找方法对于查找方法的源码就没有什么好介绍了，无非就是迭代，比对，然后就是返回当前值。 get(int index)：返回此列表中指定位置处的元素。 getFirst()：返回此列表的第一个元素。 getLast()：返回此列表的最后一个元素。 indexOf(Object o)：返回此列表中首次出现的指定元素的索引，如果此列表中不包含该元素，则返回 -1。 lastIndexOf(Object o)：返回此列表中最后出现的指定元素的索引，如果此列表中不包含该元素，则返回 -1。QueueQueue接口定义了队列数据结构，元素是有序的(按插入顺序)，先进先出。Queue接口相关的部分UML类图如下： DeQueue DeQueue(Double-ended queue)为接口，继承了Queue接口，创建双向队列，灵活性更强，可以前向或后向迭代，在队头队尾均可心插入或删除元素。它的两个主要实现类是ArrayDeque和LinkedList。 ArrayDeque （底层使用循环数组实现双向队列）创建public ArrayDeque() { // 默认容量为16 elements = new Object[16]; } public ArrayDeque(int numElements) { // 指定容量的构造函数 allocateElements(numElements); } private void allocateElements(int numElements) { int initialCapacity = MIN_INITIAL_CAPACITY;// 最小容量为8 // Find the best power of two to hold elements. // Tests &quot;&lt;=&quot; because arrays aren&apos;t kept full. // 如果要分配的容量大于等于8，扩大成2的幂（是为了维护头、尾下标值）；否则使用最小容量8 if (numElements &gt;= initialCapacity) { initialCapacity = numElements; initialCapacity |= (initialCapacity &gt;&gt;&gt; 1); initialCapacity |= (initialCapacity &gt;&gt;&gt; 2); initialCapacity |= (initialCapacity &gt;&gt;&gt; 4); initialCapacity |= (initialCapacity &gt;&gt;&gt; 8); initialCapacity |= (initialCapacity &gt;&gt;&gt; 16); initialCapacity++; if (initialCapacity &lt; 0) // Too many elements, must back off initialCapacity &gt;&gt;&gt;= 1;// Good luck allocating 2 ^ 30 elements } elements = new Object[initialCapacity]; }add操作add(E e) 调用 addLast(E e) 方法： public void addLast(E e) { if (e == null) throw new NullPointerException(&quot;e == null&quot;); elements[tail] = e; // 根据尾索引，添加到尾端 // 尾索引+1，并与数组（length - 1）进行取‘&amp;’运算，因为length是2的幂，所以（length-1）转换为2进制全是1， // 所以如果尾索引值 tail 小于等于（length - 1），那么‘&amp;’运算后仍为 tail 本身；如果刚好比（length - 1）大1时， // ‘&amp;’运算后 tail 便为0（即回到了数组初始位置）。正是通过与（length - 1）进行取‘&amp;’运算来实现数组的双向循环。 // 如果尾索引和头索引重合了，说明数组满了，进行扩容。 if ((tail = (tail + 1) &amp; (elements.length - 1)) == head) doubleCapacity();// 扩容为原来的2倍 } addFirst(E e) 的实现： public void addFirst(E e) { if (e == null) throw new NullPointerException(&quot;e == null&quot;); // 此处如果head为0，则-1（1111 1111 1111 1111 1111 1111 1111 1111）与（length - 1）进行取‘&amp;’运算，结果必然是（length - 1），即回到了数组的尾部。 elements[head = (head - 1) &amp; (elements.length - 1)] = e; // 如果尾索引和头索引重合了，说明数组满了，进行扩容 if (head == tail) doubleCapacity(); }remove操作remove()方法最终都会调对应的poll()方法： public E poll() { return pollFirst(); } public E pollFirst() { int h = head; @SuppressWarnings(&quot;unchecked&quot;) E result = (E) elements[h]; // Element is null if deque empty if (result == null) return null; elements[h] = null; // Must null out slot // 头索引 + 1 head = (h + 1) &amp; (elements.length - 1); return result; } public E pollLast() { // 尾索引 - 1 int t = (tail - 1) &amp; (elements.length - 1); @SuppressWarnings(&quot;unchecked&quot;) E result = (E) elements[t]; if (result == null) return null; elements[t] = null; tail = t; return result; } PriorityQueue（底层用数组实现堆的结构） 优先队列跟普通的队列不一样，普通队列是一种遵循FIFO规则的队列，拿数据的时候按照加入队列的顺序拿取。 而优先队列每次拿数据的时候都会拿出优先级最高的数据。 优先队列内部维护着一个堆，每次取数据的时候都从堆顶拿数据（堆顶的优先级最高），这就是优先队列的原理。 add 添加方法public boolean add(E e) { return offer(e); // add方法内部调用offer方法 } public boolean offer(E e) { if (e == null) // 元素为空的话，抛出NullPointerException异常 throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) // 如果当前用堆表示的数组已经满了，调用grow方法扩容 grow(i + 1); // 扩容 size = i + 1; // 元素个数+1 if (i == 0) // 堆还没有元素的情况 queue[0] = e; // 直接给堆顶赋值元素 else // 堆中已有元素的情况 siftUp(i, e); // 重新调整堆，从下往上调整，因为新增元素是加到最后一个叶子节点 return true; } private void siftUp(int k, E x) { if (comparator != null) // 比较器存在的情况下 siftUpUsingComparator(k, x); // 使用比较器调整 else // 比较器不存在的情况下 siftUpComparable(k, x); // 使用元素自身的比较器调整 } private void siftUpUsingComparator(int k, E x) { while (k &gt; 0) { // 一直循环直到父节点还存在 int parent = (k - 1) &gt;&gt;&gt; 1; // 找到父节点索引，等同于（k - 1）/ 2 Object e = queue[parent]; // 获得父节点元素 // 新元素与父元素进行比较，如果满足比较器结果，直接跳出，否则进行调整 if (comparator.compare(x, (E) e) &gt;= 0) break; queue[k] = e; // 进行调整，新位置的元素变成了父元素 k = parent; // 新位置索引变成父元素索引，进行递归操作 } queue[k] = x; // 新添加的元素添加到堆中 } poll，出队方法public E poll() { if (size == 0) return null; int s = --size; // 元素个数-1 modCount++; E result = (E) queue[0]; // 得到堆顶元素 E x = (E) queue[s]; // 最后一个叶子节点 queue[s] = null; // 最后1个叶子节点置空 if (s != 0) siftDown(0, x); // 从上往下调整，因为删除元素是删除堆顶的元素 return result; } private void siftDown(int k, E x) { if (comparator != null) // 比较器存在的情况下 siftDownUsingComparator(k, x); // 使用比较器调整 else // 比较器不存在的情况下 siftDownComparable(k, x); // 使用元素自身的比较器调整 } private void siftDownUsingComparator(int k, E x) { int half = size &gt;&gt;&gt; 1; // 只需循环节点个数的一般即可 while (k &lt; half) { int child = (k &lt;&lt; 1) + 1; // 得到父节点的左子节点索引，即（k * 2）+ 1 Object c = queue[child]; // 得到左子元素 int right = child + 1; // 得到父节点的右子节点索引 if (right &lt; size &amp;&amp; comparator.compare((E) c, (E) queue[right]) &gt; 0) // 左子节点跟右子节点比较，取更大的值 c = queue[child = right]; if (comparator.compare(x, (E) c) &lt;= 0) // 然后这个更大的值跟最后一个叶子节点比较 break; queue[k] = c; // 新位置使用更大的值 k = child; // 新位置索引变成子元素索引，进行递归操作 } queue[k] = x; // 最后一个叶子节点添加到合适的位置 } remove，删除队列元素public boolean remove(Object o) { int i = indexOf(o); // 找到数据对应的索引 if (i == -1) // 不存在的话返回false return false; else { // 存在的话调用removeAt方法，返回true removeAt(i); return true; } } private E removeAt(int i) { modCount++; int s = --size; // 元素个数-1 if (s == i) // 如果是删除最后一个叶子节点 queue[i] = null; // 直接置空，删除即可，堆还是保持特质，不需要调整 else { // 如果是删除的不是最后一个叶子节点 E moved = (E) queue[s]; // 获得最后1个叶子节点元素 queue[s] = null; // 最后1个叶子节点置空 siftDown(i, moved); // 从上往下调整 if (queue[i] == moved) { // 如果从上往下调整完毕之后发现元素位置没变，从下往上调整 siftUp(i, moved); // 从下往上调整 if (queue[i] != moved) return moved; } } return null; }先执行 siftDown() 下滤过程： 再执行 siftUp() 上滤过程： 总结和同步的问题1、jdk内置的优先队列PriorityQueue内部使用一个堆维护数据，每当有数据add进来或者poll出去的时候会对堆做从下往上的调整和从上往下的调整。 2、PriorityQueue不是一个线程安全的类，如果要在多线程环境下使用，可以使用 PriorityBlockingQueue 这个优先阻塞队列。其中add、poll、remove方法都使用 ReentrantLock 锁来保持同步，take() 方法中如果元素为空，则会一直保持阻塞。 参考文章http://cmsblogs.com/?p=155 https://www.jianshu.com/p/0e84b8d3606c https://blog.csdn.net/Faker_Wang/article/details/80923155 https://blog.csdn.net/m0_37869177/article/details/88847569 https://www.iteye.com/blog/shmilyaw-hotmail-com-1825171 https://blog.csdn.net/weixin_36378917/article/details/81812210 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>Linkedlist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合详解1：一文读懂ArrayList,Vector与Stack使用方法和实现原理]]></title>
    <url>%2F2019%2F09%2F25%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%2FJava%E9%9B%86%E5%90%88%E8%AF%A6%E8%A7%A31%EF%BC%9A%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82ArrayList%2CVector%E4%B8%8EStack%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%E5%92%8C%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《走进JavaWeb技术世界》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 文末赠送8000G的Java架构师学习资料，需要的朋友可以到文末了解领取方式，资料包括Java基础、进阶、项目和架构师等免费学习资料，更有数据库、分布式、微服务等热门技术学习视频，内容丰富，兼顾原理和实践，另外也将赠送作者原创的Java学习指南、Java程序员面试指南等干货资源） //一般讨论集合类无非就是。这里的两种数组类型更是如此 // 1底层数据结构 // 2增删改查方式 // 3初始容量，扩容方式，扩容时机。 // 4线程安全与否 // 5是否允许空，是否允许重复，是否有序 ArrayListArrayList概述 ArrayList是实现List接口的动态数组，所谓动态就是它的大小是可变的。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。默认初始容量为10。随着ArrayList中元素的增加，它的容量也会不断的自动增长。 在每次添加新的元素时，ArrayList都会检查是否需要进行扩容操作，扩容操作带来数据向新数组的重新拷贝，所以如果我们知道具体业务数据量，在构造ArrayList时可以给ArrayList指定一个初始容量，这样就会减少扩容时数据的拷贝问题。当然在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，ArrayList实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。所以为了保证同步，最好的办法是在创建时完成，以防止意外对列表进行不同步的访问： List list = Collections.synchronizedList(new ArrayList(...)); ArrayList的继承关系ArrayList继承AbstractList抽象父类，实现了List接口（规定了List的操作规范）、RandomAccess（可随机访问）、Cloneable（可拷贝）、Serializable（可序列化）。 底层数据结构 ArrayList的底层是一个object数组，并且由trasient修饰。 //transient Object[] elementData; //non-private to simplify nested class access//ArrayList底层数组不会参与序列化，而是使用另外的序列化方式。 //使用writeobject方法进行序列化,具体为什么这么做欢迎查看我之前的关于序列化的文章 //总结一下就是只复制数组中有值的位置，其他未赋值的位置不进行序列化，可以节省空间。 // private void writeObject(java.io.ObjectOutputStream s) // throws java.io.IOException{ // // Write out element count, and any hidden stuff // int expectedModCount = modCount; // s.defaultWriteObject(); // // // Write out size as capacity for behavioural compatibility with clone() // s.writeInt(size); // // // Write out all elements in the proper order. // for (int i=0; i&lt;size; i++) { // s.writeObject(elementData[i]); // } // // if (modCount != expectedModCount) { // throw new ConcurrentModificationException(); // } // }增删改查//增删改查添加元素时，首先判断索引是否合法，然后检测是否需要扩容，最后使用System.arraycopy方法来完成数组的复制。 这个方法无非就是使用System.arraycopy()方法将C集合(先准换为数组)里面的数据复制到elementData数组中。这里就稍微介绍下System.arraycopy()，因为下面还将大量用到该方法 。该方法的原型为： public static void arraycopy(Object src, int srcPos, Object dest, int destPos, int length)。它的根本目的就是进行数组元素的复制。即从指定源数组中复制一个数组，复制从指定的位置开始，到目标数组的指定位置结束。 将源数组src从srcPos位置开始复制到dest数组中，复制长度为length，数据从dest的destPos位置开始粘贴。 // public void add(int index, E element) { // rangeCheckForAdd(index); // // ensureCapacityInternal(size + 1); // Increments modCount!! // System.arraycopy(elementData, index, elementData, index + 1, // size - index); // elementData[index] = element; // size++; // } //删除元素时，同样判断索引是否和法，删除的方式是把被删除元素右边的元素左移，方法同样是使用System.arraycopy进行拷贝。 // public E remove(int index) { // rangeCheck(index); // // modCount++; // E oldValue = elementData(index); // // int numMoved = size - index - 1; // if (numMoved &gt; 0) // System.arraycopy(elementData, index+1, elementData, index, // numMoved); // elementData[--size] = null; // clear to let GC do its work // // return oldValue; // }ArrayList提供一个清空数组的办法，方法是将所有元素置为null，这样就可以让GC自动回收掉没有被引用的元素了。 // // /** // * Removes all of the elements from this list. The list will // * be empty after this call returns. // */ // public void clear() { // modCount++; // // // clear to let GC do its work // for (int i = 0; i &lt; size; i++) // elementData[i] = null; // // size = 0; // }修改元素时，只需要检查下标即可进行修改操作。 // public E set(int index, E element) { // rangeCheck(index); // // E oldValue = elementData(index); // elementData[index] = element; // return oldValue; // } // // public E get(int index) { // rangeCheck(index); // // return elementData(index); // } //上述方法都使用了rangeCheck方法，其实就是简单地检查下标而已。 // private void rangeCheck(int index) { // if (index &gt;= size) // throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); // }modCount// protected transient int modCount = 0;由以上代码可以看出，在一个迭代器初始的时候会赋予它调用这个迭代器的对象的mCount，如何在迭代器遍历的过程中，一旦发现这个对象的mcount和迭代器中存储的mcount不一样那就抛异常 好的，下面是这个的完整解释Fail-Fast 机制我们知道 java.util.ArrayList 不是线程安全的，ArrayList，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。 这一策略在源码中的实现是通过 modCount 域，modCount 顾名思义就是修改次数，对ArrayList 内容的修改都将增加这个值，那么在迭代器初始化过程中会将这个值赋给迭代器的 expectedModCount。 在迭代过程中，判断 modCount 跟 expectedModCount 是否相等，如果不相等就表示已经有其他线程修改了 ArrayList。 所以在这里和大家建议，当大家遍历那些非线程安全的数据结构时，尽量使用迭代器 初始容量和扩容方式初始容量是10，下面是扩容方法。首先先取 // private static final int DEFAULT_CAPACITY = 10; 扩容发生在add元素时，传入当前元素容量加一 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; } 这里给出初始化时的数组 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; 这说明：如果数组还是初始数组，那么最小的扩容大小就是size+1和初始容量中较大的一个，初始容量为10。 因为addall方法也会调用该函数，所以此时需要做判断。 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //开始精确地扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code 如果此时扩容容量大于数组长度吗，执行grow，否则不执行。 if (minCapacity - elementData.length &gt; 0) grow(minCapacity); }真正执行扩容的方法grow 扩容方式是让新容量等于旧容量的1.5被。 当新容量大于最大数组容量时，执行大数扩容 // private void grow(int minCapacity) { // // overflow-conscious code // int oldCapacity = elementData.length; // int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // if (newCapacity - minCapacity &lt; 0) // newCapacity = minCapacity; // if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // newCapacity = hugeCapacity(minCapacity); // // minCapacity is usually close to size, so this is a win: // elementData = Arrays.copyOf(elementData, newCapacity); // }当新容量大于最大数组长度，有两种情况，一种是溢出，抛异常，一种是没溢出，返回整数的最大值。 private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; }在这里有一个疑问，为什么每次扩容处理会是1.5倍，而不是2.5、3、4倍呢？通过google查找，发现1.5倍的扩容是最好的倍数。因为一次性扩容太大(例如2.5倍)可能会浪费更多的内存(1.5倍最多浪费33%，而2.5被最多会浪费60%，3.5倍则会浪费71%……)。但是一次性扩容太小，需要多次对数组重新分配内存，对性能消耗比较严重。所以1.5倍刚刚好，既能满足性能需求，也不会造成很大的内存消耗。 处理这个ensureCapacity()这个扩容数组外，ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize()方法来实现。该方法可以最小化ArrayList实例的存储量。 public void trimToSize() { modCount++; int oldCapacity = elementData.length; if (size &lt; oldCapacity) { elementData = Arrays.copyOf(elementData, size); } }线程安全ArrayList是线程不安全的。在其迭代器iteator中，如果有多线程操作导致modcount改变，会执行fastfail。抛出异常。 final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); }VectorVector简介Vector可以实现可增长的对象数组。与数组一样，它包含可以使用整数索引进行访问的组件。不过，Vector的大小是可以增加或者减小的，以便适应创建Vector后进行添加或者删除操作。 Vector实现List接口，继承AbstractList类，所以我们可以将其看做队列，支持相关的添加、删除、修改、遍历等功能。 Vector实现RandmoAccess接口，即提供了随机访问功能，提供提供快速访问功能。在Vector我们可以直接访问元素。 Vector 实现了Cloneable接口，支持clone()方法，可以被克隆。 vector底层数组不加transient，序列化时会全部复制 protected Object[] elementData; // private void writeObject(java.io.ObjectOutputStream s) // throws java.io.IOException { // final java.io.ObjectOutputStream.PutField fields = s.putFields(); // final Object[] data; // synchronized (this) { // fields.put(&quot;capacityIncrement&quot;, capacityIncrement); // fields.put(&quot;elementCount&quot;, elementCount); // data = elementData.clone(); // } // fields.put(&quot;elementData&quot;, data); // s.writeFields(); // }Vector除了iterator外还提供Enumeration枚举方法，不过现在比较过时。 // public Enumeration&lt;E&gt; elements() { // return new Enumeration&lt;E&gt;() { // int count = 0; // // public boolean hasMoreElements() { // return count &lt; elementCount; // } // // public E nextElement() { // synchronized (Vector.this) { // if (count &lt; elementCount) { // return elementData(count++); // } // } // throw new NoSuchElementException(&quot;Vector Enumeration&quot;); // } // }; // } //增删改查vector的增删改查既提供了自己的实现，也继承了abstractList抽象类的部分方法。下面的方法是vector自己实现的。 // // public synchronized E elementAt(int index) { // if (index &gt;= elementCount) { // throw new ArrayIndexOutOfBoundsException(index + &quot; &gt;= &quot; + elementCount); // } // // return elementData(index); // } // // // public synchronized void setElementAt(E obj, int index) { // if (index &gt;= elementCount) { // throw new ArrayIndexOutOfBoundsException(index + &quot; &gt;= &quot; + // elementCount); // } // elementData[index] = obj; // } // // public synchronized void removeElementAt(int index) { // modCount++; // if (index &gt;= elementCount) { // throw new ArrayIndexOutOfBoundsException(index + &quot; &gt;= &quot; + // elementCount); // } // else if (index &lt; 0) { // throw new ArrayIndexOutOfBoundsException(index); // } // int j = elementCount - index - 1; // if (j &gt; 0) { // System.arraycopy(elementData, index + 1, elementData, index, j); // } // elementCount--; // elementData[elementCount] = null; /* to let gc do its work */ // } // public synchronized void insertElementAt(E obj, int index) { // modCount++; // if (index &gt; elementCount) { // throw new ArrayIndexOutOfBoundsException(index // + &quot; &gt; &quot; + elementCount); // } // ensureCapacityHelper(elementCount + 1); // System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); // elementData[index] = obj; // elementCount++; // } // // public synchronized void addElement(E obj) { // modCount++; // ensureCapacityHelper(elementCount + 1); // elementData[elementCount++] = obj; // }初始容量和扩容扩容方式与ArrayList基本一样，但是扩容时不是1.5倍扩容，而是有一个扩容增量。 // protected int elementCount; // protected int capacityIncrement; // // // } // public Vector() { // this(10); // }capacityIncrement：向量的大小大于其容量时，容量自动增加的量。如果在创建Vector时，指定了capacityIncrement的大小；则，每次当Vector中动态数组容量增加时&gt;，增加的大小都是capacityIncrement。如果容量的增量小于等于零，则每次需要增大容量时，向量的容量将增大一倍。 // public synchronized void ensureCapacity(int minCapacity) { // if (minCapacity &gt; 0) { // modCount++; // ensureCapacityHelper(minCapacity); // } // } // private void ensureCapacityHelper(int minCapacity) { // // overflow-conscious code // if (minCapacity - elementData.length &gt; 0) // grow(minCapacity); // } // // private void grow(int minCapacity) { // // overflow-conscious code // int oldCapacity = elementData.length; // int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? // capacityIncrement : oldCapacity); // if (newCapacity - minCapacity &lt; 0) // newCapacity = minCapacity; // if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // newCapacity = hugeCapacity(minCapacity); // elementData = Arrays.copyOf(elementData, newCapacity); // }下面是扩容过程示意图 线程安全vector大部分方法都使用了synchronized修饰符，所以他是线层安全的集合类。 Stack我们最常用的数据结构之一大概就是stack了。在实际的程序执行，方法调用的过程中都离不开stack。那么，在一个成熟的类库里面，它的实现是怎么样的呢？也许平时我们实践的时候也会尝试着去写一个stack的实现玩玩。这里，我们就仔细的分析一下jdk里的详细实现。 Stack如果我们去查jdk的文档，我们会发现stack是在java.util这个包里。它对应的一个大致的类关系图如下： 通过继承Vector类，Stack类可以很容易的实现他本身的功能。因为大部分的功能在Vector里面已经提供支持了。在Java中Stack类表示后进先出（LIFO）的对象堆栈。栈是一种非常常见的数据结构，它采用典型的先进后出的操作方式完成的。 Stack通过五个操作对Vector进行扩展，允许将向量视为堆栈。这个五个操作如下： empty() 测试堆栈是否为空。 peek() 查看堆栈顶部的对象，但不从堆栈中移除它。 pop() 移除堆栈顶部的对象，并作为此函数的值返回该对象。 push(E item) 把项压入堆栈顶部。 search(Object o) 返回对象在堆栈中的位置，以 1 为基数。 Stack继承Vector，他对Vector进行了简单的扩展： public class Stack extends Vector Stack的实现非常简单，仅有一个构造方法，五个实现方法（从Vector继承而来的方法不算与其中），同时其实现的源码非常简单 /** * 构造函数 */ public Stack() { } /** * push函数：将元素存入栈顶 */ public E push(E item) { // 将元素存入栈顶。 // addElement()的实现在Vector.java中 addElement(item); return item; } /** * pop函数：返回栈顶元素，并将其从栈中删除 */ public synchronized E pop() { E obj; int len = size(); obj = peek(); // 删除栈顶元素，removeElementAt()的实现在Vector.java中 removeElementAt(len - 1); return obj; } /** * peek函数：返回栈顶元素，不执行删除操作 */ public synchronized E peek() { int len = size(); if (len == 0) throw new EmptyStackException(); // 返回栈顶元素，elementAt()具体实现在Vector.java中 return elementAt(len - 1); } /** * 栈是否为空 */ public boolean empty() { return size() == 0; } /** * 查找“元素o”在栈中的位置：由栈底向栈顶方向数 */ public synchronized int search(Object o) { // 获取元素索引，elementAt()具体实现在Vector.java中 int i = lastIndexOf(o); if (i &gt;= 0) { return size() - i; } return -1; }Stack的源码很多都是基于Vector，所以这里不再累述 三个集合类之间的区别ArrayList的优缺点 从上面的几个过程总结一下ArrayList的优缺点。ArrayList的优点如下： 1、ArrayList底层以数组实现，是一种随机访问模式，再加上它实现了RandomAccess接口，因此查找也就是get的时候非常快 2、ArrayList在顺序添加一个元素的时候非常方便，只是往数组里面添加了一个元素而已 不过ArrayList的缺点也十分明显： 1、删除元素的时候，涉及到一次元素复制，如果要复制的元素很多，那么就会比较耗费性能 2、插入元素的时候，涉及到一次元素复制，如果要复制的元素很多，那么就会比较耗费性能 因此，ArrayList比较适合顺序添加、随机访问的场景。 ArrayList和Vector的区别 ArrayList是线程非安全的，这很明显，因为ArrayList中所有的方法都不是同步的，在并发下一定会出现线程安全问题。那么我们想要使用ArrayList并且让它线程安全怎么办？一个方法是用Collections.synchronizedList方法把你的ArrayList变成一个线程安全的List，比如： List&lt;String&gt; synchronizedList = Collections.synchronizedList(list); synchronizedList.add(&quot;aaa&quot;); synchronizedList.add(&quot;bbb&quot;); for (int i = 0; i &lt; synchronizedList.size(); i++) { System.out.println(synchronizedList.get(i)); } 另一个方法就是Vector，它是ArrayList的线程安全版本，其实现90%和ArrayList都完全一样，区别在于： 1、Vector是线程安全的，ArrayList是线程非安全的 2、Vector可以指定增长因子，如果该增长因子指定了，那么扩容的时候会每次新的数组大小会在原数组的大小基础上加上增长因子；如果不指定增长因子，那么就给原数组大小*2，源代码是这样的： int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity);参考文章https://www.cnblogs.com/williamjie/p/11158523.html https://www.cnblogs.com/shenzhichipingguo/p/10075212.html https://www.cnblogs.com/rnmb/p/6553711.html https://blog.csdn.net/u011419651/article/details/83831156 https://www.jianshu.com/p/c4027084ac43 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java集合类</category>
      </categories>
      <tags>
        <tag>Linkedlist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列23：深入理解Java继承、封装、多态的底层实现原理]]></title>
    <url>%2F2019%2F09%2F23%2F23%E7%BB%A7%E6%89%BF%E3%80%81%E5%B0%81%E8%A3%85%E3%80%81%E5%A4%9A%E6%80%81%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 从JVM结构开始谈多态Java 对于方法调用动态绑定的实现主要依赖于方法表，但通过类引用调用和接口引用调用的实现则有所不同。总体而言，当某个方法被调用时，JVM 首先要查找相应的常量池，得到方法的符号引用，并查找调用类的方法表以确定该方法的直接引用，最后才真正调用该方法。以下分别对该过程中涉及到的相关部分做详细介绍。 JVM 的结构典型的 Java 虚拟机的运行时结构如下图所示 图 1.JVM 运行时结构 此结构中，我们只探讨和本文密切相关的方法区 (method area)。当程序运行需要某个类的定义时，载入子系统 (class loader subsystem) 装入所需的 class 文件，并在内部建立该类的类型信息，这个类型信息就存贮在方法区。类型信息一般包括该类的方法代码、类变量、成员变量的定义等等。可以说，类型信息就是类的 Java 文件在运行时的内部结构，包含了改类的所有在 Java 文件中定义的信息。 注意到，该类型信息和 class 对象是不同的。class 对象是 JVM 在载入某个类后于堆 (heap) 中创建的代表该类的对象，可以通过该 class 对象访问到该类型信息。比如最典型的应用，在 Java 反射中应用 class 对象访问到该类支持的所有方法，定义的成员变量等等。可以想象，JVM 在类型信息和 class 对象中维护着它们彼此的引用以便互相访问。两者的关系可以类比于进程对象与真正的进程之间的关系。 Java 的方法调用方式Java 的方法调用有两类，动态方法调用与静态方法调用。静态方法调用是指对于类的静态方法的调用方式，是静态绑定的；而动态方法调用需要有方法调用所作用的对象，是动态绑定的。类调用 (invokestatic) 是在编译时刻就已经确定好具体调用方法的情况，而实例调用 (invokevirtual) 则是在调用的时候才确定具体的调用方法，这就是动态绑定，也是多态要解决的核心问题。 JVM 的方法调用指令有四个，分别是 invokestatic，invokespecial，invokesvirtual 和 invokeinterface。前两个是静态绑定，后两个是动态绑定的。本文也可以说是对于 JVM 后两种调用实现的考察。 常量池（constant pool）常量池中保存的是一个 Java 类引用的一些常量信息，包含一些字符串常量及对于类的符号引用信息等。Java 代码编译生成的类文件中的常量池是静态常量池，当类被载入到虚拟机内部的时候，在内存中产生类的常量池叫运行时常量池。 常量池在逻辑上可以分成多个表，每个表包含一类的常量信息，本文只探讨对于 Java 调用相关的常量池表。 CONSTANT_Utf8_info 字符串常量表，该表包含该类所使用的所有字符串常量，比如代码中的字符串引用、引用的类名、方法的名字、其他引用的类与方法的字符串描述等等。其余常量池表中所涉及到的任何常量字符串都被索引至该表。 CONSTANT_Class_info 类信息表，包含任何被引用的类或接口的符号引用，每一个条目主要包含一个索引，指向 CONSTANT_Utf8_info 表，表示该类或接口的全限定名。 CONSTANT_NameAndType_info 名字类型表，包含引用的任意方法或字段的名称和描述符信息在字符串常量表中的索引。 CONSTANT_InterfaceMethodref_info 接口方法引用表，包含引用的任何接口方法的描述信息，主要包括类信息索引和名字类型索引。 CONSTANT_Methodref_info 类方法引用表，包含引用的任何类型方法的描述信息，主要包括类信息索引和名字类型索引。 图 2. 常量池各表的关系 可以看到，给定任意一个方法的索引，在常量池中找到对应的条目后，可以得到该方法的类索引（class_index）和名字类型索引 (name_and_type_index), 进而得到该方法所属的类型信息和名称及描述符信息（参数，返回值等）。注意到所有的常量字符串都是存储在 CONSTANT_Utf8_info 中供其他表索引的。 方法表与方法调用方法表是动态调用的核心，也是 Java 实现动态调用的主要方式。它被存储于方法区中的类型信息，包含有该类型所定义的所有方法及指向这些方法代码的指针，注意这些具体的方法代码可能是被覆写的方法，也可能是继承自基类的方法。 如有类定义 Person, Girl, Boy, 清单 1 class Person { public String toString(){ return "I'm a person."; } public void eat(){} public void speak(){} } class Boy extends Person{ public String toString(){ return "I'm a boy"; } public void speak(){} public void fight(){} } class Girl extends Person{ public String toString(){ return "I'm a girl"; } public void speak(){} public void sing(){} } 当这三个类被载入到 Java 虚拟机之后，方法区中就包含了各自的类的信息。Girl 和 Boy 在方法区中的方法表可表示如下： 图 3.Boy 和 Girl 的方法表 可以看到，Girl 和 Boy 的方法表包含继承自 Object 的方法，继承自直接父类 Person 的方法及各自新定义的方法。注意方法表条目指向的具体的方法地址，如 Girl 的继承自 Object 的方法中，只有 toString() 指向自己的实现（Girl 的方法代码），其余皆指向 Object 的方法代码；其继承自于 Person 的方法 eat() 和 speak() 分别指向 Person 的方法实现和本身的实现。 Person 或 Object 的任意一个方法，在它们的方法表和其子类 Girl 和 Boy 的方法表中的位置 (index) 是一样的。这样 JVM 在调用实例方法其实只需要指定调用方法表中的第几个方法即可。 如调用如下： 清单 2 class Party{ … void happyHour(){ Person girl = new Girl(); girl.speak(); … } } 当编译 Party 类的时候，生成 girl.speak()的方法调用假设为： Invokevirtual #12 设该调用代码对应着 girl.speak(); #12 是 Party 类的常量池的索引。JVM 执行该调用指令的过程如下所示： 图 4. 解析调用过程 JVM 首先查看 Party 的常量池索引为 12 的条目（应为 CONSTANT_Methodref_info 类型，可视为方法调用的符号引用），进一步查看常量池（CONSTANT_Class_info，CONSTANT_NameAndType_info ，CONSTANT_Utf8_info）可得出要调用的方法是 Person 的 speak 方法（注意引用 girl 是其基类 Person 类型），查看 Person 的方法表，得出 speak 方法在该方法表中的偏移量 15（offset），这就是该方法调用的直接引用。 当解析出方法调用的直接引用后（方法表偏移量 15），JVM 执行真正的方法调用：根据实例方法调用的参数 this 得到具体的对象（即 girl 所指向的位于堆中的对象），据此得到该对象对应的方法表 (Girl 的方法表 )，进而调用方法表中的某个偏移量所指向的方法（Girl 的 speak() 方法的实现）。 接口调用因为 Java 类是可以同时实现多个接口的，而当用接口引用调用某个方法的时候，情况就有所不同了。Java 允许一个类实现多个接口，从某种意义上来说相当于多继承，这样同样的方法在基类和派生类的方法表的位置就可能不一样了。 清单 3 interface IDance{ void dance(); } class Person { public String toString(){ return "I'm a person."; } public void eat(){} public void speak(){} } class Dancer extends Person implements IDance { public String toString(){ return "I'm a dancer."; } public void dance(){} } class Snake implements IDance{ public String toString(){ return "A snake."; } public void dance(){ //snake dance } } 图 5.Dancer 的方法表（查看大图） 可以看到，由于接口的介入，继承自于接口 IDance 的方法 dance（）在类 Dancer 和 Snake 的方法表中的位置已经不一样了，显然我们无法通过给出方法表的偏移量来正确调用 Dancer 和 Snake 的这个方法。这也是 Java 中调用接口方法有其专有的调用指令（invokeinterface）的原因。 Java 对于接口方法的调用是采用搜索方法表的方式，对如下的方法调用 invokeinterface #13 JVM 首先查看常量池，确定方法调用的符号引用（名称、返回值等等），然后利用 this 指向的实例得到该实例的方法表，进而搜索方法表来找到合适的方法地址。 因为每次接口调用都要搜索方法表，所以从效率上来说，接口方法的调用总是慢于类方法的调用的。 执行结果如下：可以看到System.out.println(dancer); 调用的是Person的toString方法。 继承的实现原理Java 的继承机制是一种复用类的技术，从原理上来说，是更好的使用了组合技术，因此要理解继承，首先需要了解类的组合技术是如何实现类的复用的。 使用组合技术复用类假设现在的需求是要创建一个具有基本类型，String 类型以及一个其他非基本类型的对象。该如何处理呢？ 对于基本类型的变量，在新类中成员变量处直接定义即可，但对于非基本类型变量，不仅需要在类中声明其引用，并且还需要手动初始化这个对象。 这里需要注意的是，编译器并不会默认将所有的引用都创建对象，因为这样的话在很多情况下会增加不必要的负担，因此，在合适的时机初始化合适的对象，可以通过以下几个位置做初始化操作： 在定义对象的地方，先于构造方法执行。在构造方法中。在正要使用之前，这个被称为惰性初始化。使用实例初始化。 class Soap { private String s; Soap() { System.out.println(&quot;Soap()&quot;); s = &quot;Constructed&quot;; } public String tiString(){ return s; } } public class Bath { // s1 初始化先于构造函数 private String s1 = &quot;Happy&quot;, s2 = &quot;Happy&quot;, s3, s4; private Soap soap; private int i; private float f; public Both() { System.out.println(&quot;inSide Both&quot;); s3 = &quot;Joy&quot;; f = 3.14f; soap = new Soap(); } { i = 88; } public String toString() { if(s4 == null){ s4 = &quot;Joy&quot; } return &quot;s1 = &quot; + s1 +&quot;\n&quot; + &quot;s2 = &quot; + s2 +&quot;\n&quot; + &quot;s3 = &quot; + s3 +&quot;\n&quot; + &quot;s4 = &quot; + s4 +&quot;\n&quot; + &quot;i = &quot; + i +&quot;\n&quot; + &quot;f = &quot; + f +&quot;\n&quot; + &quot;soap = &quot; + soap; } }继承Java 中的继承由 extend 关键字实现，组合的语法比较平实，而继承是一种特殊的语法。当一个类继承自另一个类时，那么这个类就可以拥有另一个类的域和方法。 class Cleanser{ private String s = &quot;Cleanser&quot;; public void append(String a){ s += a; } public void apply(){ append(&quot;apply&quot;); } public void scrub(){ append(&quot;scrub&quot;); } public String toString(){ return s; } public static void main(String args){ Cleanser c = new Cleanser(); c.apply(); System.out.println(c); } } public class Deter extends Cleanser{ public void apply(){ append(&quot;Deter.apply&quot;); super.scrub(); } public void foam(){ append(&quot;foam&quot;); } public static void main(String args){ Deter d = new Deter(); d.apply(); d.scrub(); d.foam(); System.out.println(d); Cleanser.main(args); } }上面的代码中，展示了继承语法中的一些特性： 子类可以直接使用父类中公共的方法和成员变量（通常为了保护数据域，成员变量均为私有）子类中可以覆盖父类中的方法，也就是子类重写了父类的方法，此时若还需要调用被覆盖的父类的方法，则需要用到 super 来指定是调用父类中的方法。子类中可以自定义父类中没有的方法。可以发现上面两个类中均有 main 方法，命令行中调用的哪个类就执行哪个类的 main 方法，例如：java Deter。继承语法的原理接下来我们将通过创建子类对象来分析继承语法在我们看不到的地方做了什么样的操作。 可以先思考一下，如何理解使用子类创建的对象呢，首先这个对象中包含子类的所有信息，但是也包含父类的所有公共的信息。 下面来看一段代码，观察一下子类在创建对象初始化的时候，会不会用到父类相关的方法。 class Art{ Art() { System.out.println(&quot;Art Construct&quot;); } } class Drawing extends Art { Drawing() { System.out.println(&quot;Drawing Construct&quot;); } } public class Cartoon extends Drawing { public Cartoon() { System.out.println(&quot;Cartoon construct&quot;); } public void static main(String args) { Cartoon c = new Cartoon(); } } /*output: Art Construct Drawing Construct Cartoon construct */通过观察代码可以发现，在实例化Cartoon时，事实上是从最顶层的父类开始向下逐个实例化，也就是最终实例化了三个对象。编译器会默认在子类的构造方法中增加调用父类默认构造方法的代码。 因此，继承可以理解为编译器帮我们完成了类的特殊组合技术，即在子类中存在一个父类的对象，使得我们可以用子类对象调用父类的方法。而在开发者看来只不过是使用了一个关键字。 注意：虽然继承很接近组合技术，但是继承拥有其他更多的区别于组合的特性，例如父类的对象我们是不可见的，对于父类中的方法也做了相应的权限校验等。 那么，如果类中的构造方法是带参的，该如何操作呢？（使用super关键字显示调用） 见代码： class Game { Game(int i){ System.out.println(&quot;Game Construct&quot;); } } class BoardGame extends Game { BoardGame(int j){ super(j); System.out.println(&quot;BoardGame Construct&quot;); } } public class Chess extends BoardGame{ Chess(){ super(99); System.out.println(&quot;Chess construct&quot;); } public static void main(String args) { Chess c = new Chess(); } } /*output: Game Construct BoardGame Construct Chess construc */重载和重写的实现原理刚开始学习Java的时候，就了解了Java这个比较有意思的特性：重写 和 重载。开始的有时候从名字上还总是容易弄混。我相信熟悉Java这门语言的同学都应该了解这两个特性，可能只是从语言层面上了解这种写法，但是jvm是如何实现他们的呢 ?重载官方给出的介绍： 一. overload:The Java programming language supports overloading methods, and Java can distinguish between methods with different method signatures. This means that methods within a class can have the same name if they have different parameter lists . Overloaded methods are differentiated by the number and the type of the arguments passed into the method. You cannot declare more than one method with the same name and the same number and type of arguments, because the compiler cannot tell them apart. The compiler does not consider return type when differentiating methods, so you cannot declare two methods with the same signature even if they have a different return type. 首先看一段代码，来看看代码的执行结果： public class OverrideTest { class Father{} class Sun extends Father {} public void doSomething(Father father){ System.out.println(&quot;Father do something&quot;); } public void doSomething(Sun father){ System.out.println(&quot;Sun do something&quot;); } public static void main(String [] args){ OverrideTest overrideTest = new OverrideTest(); Father sun = overrideTest.new Sun(); Father father = overrideTest.new Father(); overrideTest.doSomething(father); overrideTest.doSomething(sun); } }看下这段代码的执行结果，最后会打印： Father do somethingFather do something 为什么会打印出这样的结果呢？ 首先要介绍两个概念：静态分派和动态分派 静态分派：依赖静态类型来定位方法执行版本的分派动作称为静态分派 动态分派：运行期根据实际类型确定方法执行版本的分派过程。 他们的区别是： 静态分派发生在编译期，动态分派发生在运行期； private,static,final 方法发生在编译期，并且不能被重写，一旦发生了重写，将会在运行期处理。 重载是静态分派，重写是动态分派 回到上面的问题，因为重载是发生在编译期，所以在编译期已经确定两次 doSomething 方法的参数都是Father类型，在class文件中已经指向了Father类的符号引用，所以最后会打印两次Father do something。 二. override:An instance method in a subclass with the same signature (name, plus the number and the type of its parameters) and return type as an instance method in the superclass overrides the superclass’s method. The ability of a subclass to override a method allows a class to inherit from a superclass whose behavior is “close enough” and then to modify behavior as needed. The overriding method has the same name, number and type of parameters, and return type as the method that it overrides. An overriding method can also return a subtype of the type returned by the overridden method. This subtype is called a covariant return type. 还是上面那个代码，稍微改动下 public class OverrideTest { class Father{} class Sun extends Father {} public void doSomething(){ System.out.println(&quot;Father do something&quot;); } public void doSomething(){ System.out.println(&quot;Sun do something&quot;); } public static void main(String [] args){ OverrideTest overrideTest = new OverrideTest(); Father sun = overrideTest.new Sun(); Father father = overrideTest.new Father(); overrideTest.doSomething(); overrideTest.doSomething(); } }​最后会打印： Father do something Sun do something 相信大家都会知道这个结果，那么这个结果jvm是怎么实现的呢？ 在编译期，只会识别到是调用Father类的doSomething方法，到运行期才会真正找到对象的实际类型。 首先该方法的执行，jvm会调用invokevirtual指令，该指令会找栈顶第一个元素所指向的对象的实际类型，如果该类型存在调用的方法，则会走验证流程，否则继续找其父类。这也是为什么子类可以直接调用父类具有访问权限的方法的原因。简而言之，就是在运行期才会去确定对象的实际类型，根据这个实际类型确定方法执行版本，这个过程称为动态分派。override 的实现依赖jvm的动态分派。 参考文章https://blog.csdn.net/dj_dengjian/article/details/80811348https://blog.csdn.net/chenssy/article/details/12757911https://blog.csdn.net/fan2012huan/article/details/51007517https://blog.csdn.net/fan2012huan/article/details/50999777https://www.cnblogs.com/serendipity-fly/p/9469289.htmlhttps://blog.csdn.net/m0_37264516/article/details/86709537 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>继承</tag>
        <tag>封装</tag>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列22：一文读懂Java序列化和反序列化]]></title>
    <url>%2F2019%2F09%2F22%2F22%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文参考 http://www.importnew.com/17964.html和https://www.ibm.com/developerworks/cn/java/j-lo-serial/ 序列化与反序列化概念序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。一般将一个对象存储至一个储存媒介，例如档案或是记亿体缓冲等。在网络传输过程中，可以是字节或是XML等格式。而字节的或XML编码格式可以还原完全相等的对象。这个相反的过程又称为反序列化。 Java对象的序列化与反序列化在Java中，我们可以通过多种方式来创建对象，并且只要对象没有被回收我们都可以复用该对象。但是，我们创建出来的这些Java对象都是存在于JVM的堆内存中的。 只有JVM处于运行状态的时候，这些对象才可能存在。一旦JVM停止运行，这些对象的状态也就随之而丢失了。 但是在真实的应用场景中，我们需要将这些对象持久化下来，并且能够在需要的时候把对象重新读取出来。Java的对象序列化可以帮助我们实现该功能。 对象序列化机制（object serialization）是Java语言内建的一种对象持久化方式，通过对象序列化，可以把对象的状态保存为字节数组，并且可以在有需要的时候将这个字节数组通过反序列化的方式再转换成对象。 对象序列化可以很容易的在JVM中的活动对象和字节数组（流）之间进行转换。 在Java中，对象的序列化与反序列化被广泛应用到RMI(远程方法调用)及网络传输中。 相关接口及类Java为了方便开发人员将Java对象进行序列化及反序列化提供了一套方便的API来支持。其中包括以下接口和类： java.io.Serializable java.io.Externalizable ObjectOutput ObjectInput ObjectOutputStream ObjectInputStream Serializable 接口类通过实现 java.io.Serializable 接口以启用其序列化功能。 未实现此接口的类将无法使其任何状态序列化或反序列化。可序列化类的所有子类型本身都是可序列化的。序列化接口没有方法或字段，仅用于标识可序列化的语义。 (该接口并没有方法和字段，为什么只有实现了该接口的类的对象才能被序列化呢？) 当试图对一个对象进行序列化的时候，如果遇到不支持 Serializable 接口的对象。在此情况下，将抛出NotSerializableException。 如果要序列化的类有父类，要想同时将在父类中定义过的变量持久化下来，那么父类也应该集成java.io.Serializable接口。 下面是一个实现了java.io.Serializable接口的类 public class 序列化和反序列化 {​​ public static void main(String[] args) {​ } //注意，内部类不能进行序列化，因为它依赖于外部类 @Test public void test() throws IOException { A a = new A(); a.i = 1; a.s = “a”; FileOutputStream fileOutputStream = null; FileInputStream fileInputStream = null; try { //将obj写入文件 fileOutputStream = new FileOutputStream(“temp”); ObjectOutputStream objectOutputStream = new ObjectOutputStream(fileOutputStream); objectOutputStream.writeObject(a); fileOutputStream.close(); //通过文件读取obj fileInputStream = new FileInputStream(“temp”); ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream); A a2 = (A) objectInputStream.readObject(); fileInputStream.close(); System.out.println(a2.i); System.out.println(a2.s); //打印结果和序列化之前相同 } catch (IOException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } } } class A implements Serializable { int i; String s; }Externalizable接口 除了Serializable 之外，java中还提供了另一个序列化接口Externalizable 为了了解Externalizable接口和Serializable接口的区别，先来看代码，我们把上面的代码改成使用Externalizable的形式。 class B implements Externalizable { //必须要有公开无参构造函数。否则报错 public B() { } int i; String s; @Override public void writeExternal(ObjectOutput out) throws IOException { } @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException { } } @Test public void test2() throws IOException, ClassNotFoundException { B b = new B(); b.i = 1; b.s = &quot;a&quot;; //将obj写入文件 FileOutputStream fileOutputStream = new FileOutputStream(&quot;temp&quot;); ObjectOutputStream objectOutputStream = new ObjectOutputStream(fileOutputStream); objectOutputStream.writeObject(b); fileOutputStream.close(); //通过文件读取obj FileInputStream fileInputStream = new FileInputStream(&quot;temp&quot;); ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream); B b2 = (B) objectInputStream.readObject(); fileInputStream.close(); System.out.println(b2.i); System.out.println(b2.s); //打印结果为0和null，即初始值，没有被赋值 //0 //null }通过上面的实例可以发现，对B类进行序列化及反序列化之后得到的对象的所有属性的值都变成了默认值。也就是说，之前的那个对象的状态并没有被持久化下来。这就是Externalizable接口和Serializable接口的区别： Externalizable继承了Serializable，该接口中定义了两个抽象方法：writeExternal()与readExternal()。 当使用Externalizable接口来进行序列化与反序列化的时候需要开发人员重写writeExternal()与readExternal()方法。由于上面的代码中，并没有在这两个方法中定义序列化实现细节，所以输出的内容为空。 还有一点值得注意：在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器。 class C implements Externalizable { int i; int j; String s; public C() { } //实现下面两个方法可以选择序列化中需要被复制的成员。 //并且，写入顺序和读取顺序要一致，否则报错。 //可以写入多个同类型变量，顺序保持一致即可。 @Override public void writeExternal(ObjectOutput out) throws IOException { out.writeInt(i); out.writeInt(j); out.writeObject(s); } @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException { i = in.readInt(); j = in.readInt(); s = (String) in.readObject(); } } @Test public void test3() throws IOException, ClassNotFoundException { C c = new C(); c.i = 1; c.j = 2; c.s = &quot;a&quot;; //将obj写入文件 FileOutputStream fileOutputStream = new FileOutputStream(&quot;temp&quot;); ObjectOutputStream objectOutputStream = new ObjectOutputStream(fileOutputStream); objectOutputStream.writeObject(c); fileOutputStream.close(); //通过文件读取obj FileInputStream fileInputStream = new FileInputStream(&quot;temp&quot;); ObjectInputStream objectInputStream = new ObjectInputStream(fileInputStream); C c2 = (C) objectInputStream.readObject(); fileInputStream.close(); System.out.println(c2.i); System.out.println(c2.j); System.out.println(c2.s); //打印结果为0和null，即初始值，没有被赋值 //0 //null }序列化ID序列化 ID 问题情境：两个客户端 A 和 B 试图通过网络传递对象数据，A 端将对象 C 序列化为二进制数据再传给 B，B 反序列化得到 C。 问题：C 对象的全类路径假设为 com.inout.Test，在 A 和 B 端都有这么一个类文件，功能代码完全一致。也都实现了 Serializable 接口，但是反序列化时总是提示不成功。 解决：虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID = 1L）。清单 1 中，虽然两个类的功能代码完全一致，但是序列化 ID 不同，他们无法相互序列化和反序列化。 package com.inout; import java.io.Serializable; public class A implements Serializable { private static final long serialVersionUID = 1L; private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } } package com.inout; import java.io.Serializable; public class A implements Serializable { private static final long serialVersionUID = 2L; private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } }静态变量不参与序列化清单 2 中的 main 方法，将对象序列化后，修改静态变量的数值，再将序列化对象读取出来，然后通过读取出来的对象获得静态变量的数值并打印出来。依照清单 2，这个 System.out.println(t.staticVar) 语句输出的是 10 还是 5 呢？ public class Test implements Serializable { private static final long serialVersionUID = 1L; public static int staticVar = 5; public static void main(String[] args) { try { //初始时staticVar为5 ObjectOutputStream out = new ObjectOutputStream( new FileOutputStream(&quot;result.obj&quot;)); out.writeObject(new Test()); out.close(); //序列化后修改为10 Test.staticVar = 10; ObjectInputStream oin = new ObjectInputStream(new FileInputStream( &quot;result.obj&quot;)); Test t = (Test) oin.readObject(); oin.close(); //再读取，通过t.staticVar打印新的值 System.out.println(t.staticVar); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } } }最后的输出是 10，对于无法理解的读者认为，打印的 staticVar 是从读取的对象里获得的，应该是保存时的状态才对。之所以打印 10 的原因在于序列化时，并不保存静态变量，这其实比较容易理解，序列化保存的是对象的状态，静态变量属于类的状态，因此 序列化并不保存静态变量。 探究ArrayList的序列化ArrayList的序列化在介绍ArrayList序列化之前，先来考虑一个问题： 如何自定义的序列化和反序列化策略 带着这个问题，我们来看java.util.ArrayList的源码 public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; transient Object[] elementData; // non-private to simplify nested class access private int size; }笔者省略了其他成员变量，从上面的代码中可以知道ArrayList实现了java.io.Serializable接口，那么我们就可以对它进行序列化及反序列化。 因为elementData是transient的（1.8好像改掉了这一点)，所以我们认为这个成员变量不会被序列化而保留下来。我们写一个Demo，验证一下我们的想法： public class ArrayList的序列化 { public static void main(String[] args) throws IOException, ClassNotFoundException { ArrayList list = new ArrayList(); list.add(&quot;a&quot;); list.add(&quot;b&quot;); ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream(&quot;arr&quot;)); objectOutputStream.writeObject(list); objectOutputStream.close(); ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(&quot;arr&quot;)); ArrayList list1 = (ArrayList) objectInputStream.readObject(); objectInputStream.close(); System.out.println(Arrays.toString(list.toArray())); //序列化成功，里面的元素保持不变。 }了解ArrayList的人都知道，ArrayList底层是通过数组实现的。那么数组elementData其实就是用来保存列表中的元素的。通过该属性的声明方式我们知道，他是无法通过序列化持久化下来的。那么为什么code 4的结果却通过序列化和反序列化把List中的元素保留下来了呢？ writeObject和readObject方法 在ArrayList中定义了来个方法： writeObject和readObject。 这里先给出结论: 在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。 如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。 用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。 来看一下这两个方法的具体实现： private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) { // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) { a[i] = s.readObject(); } } }​​ private void writeObject(java.io.ObjectOutputStream s)​ throws java.io.IOException{​ // Write out element count, and any hidden stuff​ int expectedModCount = modCount;​ s.defaultWriteObject();​ // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); } }那么为什么ArrayList要用这种方式来实现序列化呢？ why transient ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。 why writeObject and readObject 前面说过，为了防止一个包含大量空对象的数组被序列化，为了优化存储，所以，ArrayList使用transient来声明elementData。 但是，作为一个集合，在序列化过程中还必须保证其中的元素可以被持久化下来，所以，通过重写writeObject 和 readObject方法的方式把其中的元素保留下来。 writeObject方法把elementData数组中的元素遍历的保存到输出流（ObjectOutputStream）中。 readObject方法从输入流（ObjectInputStream）中读出对象并保存赋值到elementData数组中。如何自定义的序列化和反序列化策略延续上一部分，刚刚我们明白了ArrayList序列化数组元素的原理。 至此，我们先试着来回答刚刚提出的问题： 如何自定义的序列化和反序列化策略 答：可以通过在被序列化的类中增加writeObject 和 readObject方法。那么问题又来了： 虽然ArrayList中写了writeObject 和 readObject 方法，但是这两个方法并没有显示的被调用啊。 那么如果一个类中包含writeObject 和 readObject 方法，那么这两个方法是怎么被调用的呢? ObjectOutputStream从code 4中，我们可以看出，对象的序列化过程通过ObjectOutputStream和ObjectInputputStream来实现的，那么带着刚刚的问题，我们来分析一下ArrayList中的writeObject 和 readObject 方法到底是如何被调用的呢？ 为了节省篇幅，这里给出ObjectOutputStream的writeObject的调用栈： writeObject —&gt; writeObject0 —&gt;writeOrdinaryObject—&gt;writeSerialData—&gt;invokeWriteObject 这里看一下invokeWriteObject： void invokeWriteObject(Object obj, ObjectOutputStream out) throws IOException, UnsupportedOperationException { if (writeObjectMethod != null) { try { writeObjectMethod.invoke(obj, new Object[]{ out }); } catch (InvocationTargetException ex) { Throwable th = ex.getTargetException(); if (th instanceof IOException) { throw (IOException) th; } else { throwMiscException(th); } } catch (IllegalAccessException ex) { // should not occur, as access checks have been suppressed throw new InternalError(ex); } } else { throw new UnsupportedOperationException(); } }其中writeObjectMethod.invoke(obj, new Object[]{ out });是关键，通过反射的方式调用writeObjectMethod方法。官方是这么解释这个writeObjectMethod的： class-defined writeObject method, or null if none 在我们的例子中，这个方法就是我们在ArrayList中定义的writeObject方法。通过反射的方式被调用了。 至此，我们先试着来回答刚刚提出的问题： 如果一个类中包含writeObject 和 readObject 方法，那么这两个方法是怎么被调用的? 答：在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用。​ 为什么要实现Serializable至此，我们已经介绍完了ArrayList的序列化方式。那么，不知道有没有人提出这样的疑问： Serializable明明就是一个空的接口，它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢？ Serializable接口的定义： public interface Serializable { } 读者可以尝试把code 1中的继承Serializable的代码去掉，再执行code 2，会抛出java.io.NotSerializableException。其实这个问题也很好回答，我们再回到刚刚ObjectOutputStream的writeObject的调用栈： writeObject ---&gt; writeObject0 ---&gt;writeOrdinaryObject---&gt;writeSerialData---&gt;invokeWriteObjectwriteObject0方法中有这么一段代码： if (obj instanceof String) { writeString((String) obj, unshared); } else if (cl.isArray()) { writeArray(obj, desc, unshared); } else if (obj instanceof Enum) { writeEnum((Enum&lt;?&gt;) obj, desc, unshared); } else if (obj instanceof Serializable) { writeOrdinaryObject(obj, desc, unshared); } else { if (extendedDebugInfo) { throw new NotSerializableException( cl.getName() + &quot;\n&quot; + debugInfoStack.toString()); } else { throw new NotSerializableException(cl.getName()); } }在进行序列化操作时，会判断要被序列化的类是否是Enum、Array和Serializable类型，如果不是则直接抛出NotSerializableException。 序列化知识点总结 1、如果一个类想被序列化，需要实现Serializable接口。否则将抛出NotSerializableException异常，这是因为，在序列化操作过程中会对类型进行检查，要求被序列化的类必须属于Enum、Array和Serializable类型其中的任何一种。 2、通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化 3、虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID） 序列化 ID 在 Eclipse 下提供了两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化 ID 有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。 4、序列化并不保存静态变量。 5、要想将父类对象也序列化，就需要让父类也实现Serializable 接口。 6、Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。 7、服务器端给客户端发送序列化对象数据，对象中有一些数据是敏感的，比如密码字符串等，希望对该密码字段在序列化时，进行加密，而客户端如果拥有解密的密钥，只有在客户端进行反序列化时，才可以对密码进行读取，这样可以一定程度保证序列化对象的数据安全。 8、在类中增加writeObject 和 readObject 方法可以实现自定义序列化策略 参考文章https://blog.csdn.net/qq_34988624/article/details/86592229https://www.meiwen.com.cn/subject/slhvhqtx.htmlhttps://blog.csdn.net/qq_34988624/article/details/86592229https://segmentfault.com/a/1190000012220863https://my.oschina.net/wuxinshui/blog/1511484https://blog.csdn.net/hukailee/article/details/81107412 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java序列化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列21：Java8新特性终极指南]]></title>
    <url>%2F2019%2F09%2F21%2F21%E3%80%81Java8%E6%96%B0%E7%89%B9%E6%80%A7%E7%BB%88%E6%9E%81%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 这是一个Java8新增特性的总结图。接下来让我们一次实践一下这些新特性吧 Java语言新特性Lambda表达式Lambda表达式（也称为闭包）是整个Java 8发行版中最受期待的在Java语言层面上的改变，Lambda允许把函数作为一个方法的参数（函数作为参数传递进方法中），或者把代码看成数据：函数式程序员对这一概念非常熟悉。在JVM平台上的很多语言（Groovy，Scala，……）从一开始就有Lambda，但是Java程序员不得不使用毫无新意的匿名类来代替lambda。 关于Lambda设计的讨论占用了大量的时间与社区的努力。可喜的是，最终找到了一个平衡点，使得可以使用一种即简洁又紧凑的新方式来构造Lambdas。在最简单的形式中，一个lambda可以由用逗号分隔的参数列表、–&gt;符号与函数体三部分表示。例如： Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).forEach( e -&gt; System.out.println( e ) );请注意参数e的类型是由编译器推测出来的。同时，你也可以通过把参数类型与参数包括在括号中的形式直接给出参数的类型： Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).forEach( ( String e ) -&gt; System.out.println( e ) );在某些情况下lambda的函数体会更加复杂，这时可以把函数体放到在一对花括号中，就像在Java中定义普通函数一样。例如： Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).forEach( e -&gt; { System.out.print( e ); System.out.print( e ); } );Lambda可以引用类的成员变量与局部变量（如果这些变量不是final的话，它们会被隐含的转为final，这样效率更高）。例如，下面两个代码片段是等价的： String separator = &quot;,&quot;; Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).forEach( ( String e ) -&gt; System.out.print( e + separator ) );和： final String separator = &quot;,&quot;; Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).forEach( ( String e ) -&gt; System.out.print( e + separator ) );Lambda可能会返回一个值。返回值的类型也是由编译器推测出来的。如果lambda的函数体只有一行的话，那么没有必要显式使用return语句。下面两个代码片段是等价的： Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).sort( ( e1, e2 ) -&gt; e1.compareTo( e2 ) );和： Arrays.asList( &quot;a&quot;, &quot;b&quot;, &quot;d&quot; ).sort( ( e1, e2 ) -&gt; { int result = e1.compareTo( e2 ); return result; } );语言设计者投入了大量精力来思考如何使现有的函数友好地支持lambda。 最终采取的方法是：增加函数式接口的概念。函数式接口就是一个具有一个方法的普通接口。像这样的接口，可以被隐式转换为lambda表达式。 java.lang.Runnable与java.util.concurrent.Callable是函数式接口最典型的两个例子。 在实际使用过程中，函数式接口是容易出错的：如有某个人在接口定义中增加了另一个方法，这时，这个接口就不再是函数式的了，并且编译过程也会失败。 为了克服函数式接口的这种脆弱性并且能够明确声明接口作为函数式接口的意图，Java8增加了一种特殊的注解@FunctionalInterface（Java8中所有类库的已有接口都添加了@FunctionalInterface注解）。让我们看一下这种函数式接口的定义： @FunctionalInterfacepublic interface Functional { void method();}需要记住的一件事是：默认方法与静态方法并不影响函数式接口的契约，可以任意使用： @FunctionalInterfacepublic interface FunctionalDefaultMethods { void method(); default void defaultMethod() { } }Lambda是Java 8最大的卖点。它具有吸引越来越多程序员到Java平台上的潜力，并且能够在纯Java语言环境中提供一种优雅的方式来支持函数式编程。更多详情可以参考官方文档。 下面看一个例子： public class lambda和函数式编程 { @Test public void test1() { List names = Arrays.asList(&quot;peter&quot;, &quot;anna&quot;, &quot;mike&quot;, &quot;xenia&quot;); Collections.sort(names, new Comparator&lt;String&gt;() { @Override public int compare(String a, String b) { return b.compareTo(a); } }); System.out.println(Arrays.toString(names.toArray())); } @Test public void test2() { List&lt;String&gt; names = Arrays.asList(&quot;peter&quot;, &quot;anna&quot;, &quot;mike&quot;, &quot;xenia&quot;); Collections.sort(names, (String a, String b) -&gt; { return b.compareTo(a); }); Collections.sort(names, (String a, String b) -&gt; b.compareTo(a)); Collections.sort(names, (a, b) -&gt; b.compareTo(a)); System.out.println(Arrays.toString(names.toArray())); } } static void add(double a,String b) { System.out.println(a + b); } @Test public void test5() { D d = (a,b) -&gt; add(a,b); // interface D { // void get(int i,String j); // } //这里要求，add的两个参数和get的两个参数吻合并且返回类型也要相等，否则报错 // static void add(double a,String b) { // System.out.println(a + b); // } } @FunctionalInterface interface D { void get(int i,String j); }函数式接口所谓的函数式接口就是只有一个抽象方法的接口，注意这里说的是抽象方法，因为Java8中加入了默认方法的特性，但是函数式接口是不关心接口中有没有默认方法的。 一般函数式接口可以使用@FunctionalInterface注解的形式来标注表示这是一个函数式接口，该注解标注与否对函数式接口没有实际的影响， 不过一般还是推荐使用该注解，就像使用@Override注解一样。 lambda表达式是如何符合 Java 类型系统的？每个lambda对应于一个给定的类型，用一个接口来说明。而这个被称为函数式接口（functional interface）的接口必须仅仅包含一个抽象方法声明。每个那个类型的lambda表达式都将会被匹配到这个抽象方法上。因此默认的方法并不是抽象的，你可以给你的函数式接口自由地增加默认的方法。 我们可以使用任意的接口作为lambda表达式，只要这个接口只包含一个抽象方法。为了保证你的接口满足需求，你需要增加@FunctionalInterface注解。编译器知道这个注解，一旦你试图给这个接口增加第二个抽象方法声明时，它将抛出一个编译器错误。 下面举几个例子 public class 函数式接口使用 { @FunctionalInterface interface A { void say(); default void talk() { } } @Test public void test1() { A a = () -&gt; System.out.println(&quot;hello&quot;); a.say(); } @FunctionalInterface interface B { void say(String i); } public void test2() { //下面两个是等价的，都是通过B接口来引用一个方法，而方法可以直接使用::来作为方法引用 B b = System.out::println; B b1 = a -&gt; Integer.parseInt(&quot;s&quot;);//这里的a其实换成别的也行，只是将方法传给接口作为其方法实现 B b2 = Integer::valueOf;//i与方法传入参数的变量类型一直时，可以直接替换 B b3 = String::valueOf; //B b4 = Integer::parseInt;类型不符，无法使用 } @FunctionalInterface interface C { int say(String i); } public void test3() { C c = Integer::parseInt;//方法参数和接口方法的参数一样，可以替换。 int i = c.say(&quot;1&quot;); //当我把C接口的int替换为void时就会报错，因为返回类型不一致。 System.out.println(i); //综上所述，lambda表达式提供了一种简便的表达方式，可以将一个方法传到接口中。 //函数式接口是只提供一个抽象方法的接口，其方法由lambda表达式注入，不需要写实现类， //也不需要写匿名内部类，可以省去很多代码，比如实现runnable接口。 //函数式编程就是指把方法当做一个参数或引用来进行操作。除了普通方法以外，静态方法，构造方法也是可以这样操作的。 } }请记住如果@FunctionalInterface 这个注解被遗漏，此代码依然有效。 方法引用Lambda表达式和方法引用 有了函数式接口之后，就可以使用Lambda表达式和方法引用了。其实函数式接口的表中的函数描述符就是Lambda表达式，在函数式接口中Lambda表达式相当于匿名内部类的效果。 举个简单的例子： public class TestLambda { public static void execute(Runnable runnable) { runnable.run(); } public static void main(String[] args) { //Java8之前 execute(new Runnable() { @Override public void run() { System.out.println(&quot;run&quot;); } }); //使用Lambda表达式 execute(() -&gt; System.out.println(&quot;run&quot;)); }} 可以看到，相比于使用匿名内部类的方式，Lambda表达式可以使用更少的代码但是有更清晰的表述。注意，Lambda表达式也不是完全等价于匿名内部类的， 两者的不同点在于this的指向和本地变量的屏蔽上。 方法引用可以看作Lambda表达式的更简洁的一种表达形式，使用::操作符，方法引用主要有三类： 指向静态方法的方法引用(例如Integer的parseInt方法，写作Integer::parseInt)； 指向任意类型实例方法的方法引用(例如String的length方法，写作String::length)； 指向现有对象的实例方法的方法引用(例如假设你有一个本地变量localVariable用于存放Variable类型的对象，它支持实例方法getValue，那么可以写成localVariable::getValue)。举个方法引用的简单的例子： Function&lt;String, Integer&gt; stringToInteger = (String s) -&gt; Integer.parseInt(s);//使用方法引用 Function&lt;String, Integer&gt; stringToInteger = Integer::parseInt;方法引用中还有一种特殊的形式，构造函数引用，假设一个类有一个默认的构造函数，那么使用方法引用的形式为： Supplier&lt;SomeClass&gt; c1 = SomeClass::new; SomeClass s1 = c1.get();//等价于 Supplier&lt;SomeClass&gt; c1 = () -&gt; new SomeClass(); SomeClass s1 = c1.get();如果是构造函数有一个参数的情况： Function&lt;Integer, SomeClass&gt; c1 = SomeClass::new; SomeClass s1 = c1.apply(100);//等价于 Function&lt;Integer, SomeClass&gt; c1 = i -&gt; new SomeClass(i); SomeClass s1 = c1.apply(100);接口的默认方法Java 8 使我们能够使用default 关键字给接口增加非抽象的方法实现。这个特性也被叫做 扩展方法（Extension Methods）。如下例所示： public class 接口的默认方法 { class B implements A { // void a(){}实现类方法不能重名 } interface A { //可以有多个默认方法 public default void a(){ System.out.println(&quot;a&quot;); } public default void b(){ System.out.println(&quot;b&quot;); } //报错static和default不能同时使用 // public static default void c(){ // System.out.println(&quot;c&quot;); // } } public void test() { B b = new B(); b.a(); } }默认方法出现的原因是为了对原有接口的扩展，有了默认方法之后就不怕因改动原有的接口而对已经使用这些接口的程序造成的代码不兼容的影响。 在Java8中也对一些接口增加了一些默认方法，比如Map接口等等。一般来说，使用默认方法的场景有两个：可选方法和行为的多继承。 默认方法的使用相对来说比较简单，唯一要注意的点是如何处理默认方法的冲突。关于如何处理默认方法的冲突可以参考以下三条规则： 类中的方法优先级最高。类或父类中声明的方法的优先级高于任何声明为默认方法的优先级。 如果无法依据第一条规则进行判断，那么子接口的优先级更高：函数签名相同时，优先选择拥有最具体实现的默认方法的接口。即如果B继承了A，那么B就比A更具体。 最后，如果还是无法判断，继承了多个接口的类必须通过显式覆盖和调用期望的方法，显式地选择使用哪一个默认方法的实现。那么如何显式地指定呢: public class C implements B, A { public void hello() { B.super().hello(); } }使用X.super.m(..)显式地调用希望调用的方法。 Java 8用默认方法与静态方法这两个新概念来扩展接口的声明。默认方法使接口有点像Traits（Scala中特征(trait)类似于Java中的Interface，但它可以包含实现代码，也就是目前Java8新增的功能），但与传统的接口又有些不一样，它允许在已有的接口中添加新方法，而同时又保持了与旧版本代码的兼容性。 默认方法与抽象方法不同之处在于抽象方法必须要求实现，但是默认方法则没有这个要求。相反，每个接口都必须提供一个所谓的默认实现，这样所有的接口实现者将会默认继承它（如果有必要的话，可以覆盖这个默认实现）。让我们看看下面的例子： private interface Defaulable { // Interfaces now allow default methods, the implementer may or // may not implement (override) them. default String notRequired() { return &quot;Default implementation&quot;; } } private static class DefaultableImpl implements Defaulable { } private static class OverridableImpl implements Defaulable { @Override public String notRequired() { return &quot;Overridden implementation&quot;; } }Defaulable接口用关键字default声明了一个默认方法notRequired()，Defaulable接口的实现者之一DefaultableImpl实现了这个接口，并且让默认方法保持原样。Defaulable接口的另一个实现者OverridableImpl用自己的方法覆盖了默认方法。 Java 8带来的另一个有趣的特性是接口可以声明（并且可以提供实现）静态方法。例如： private interface DefaulableFactory { // Interfaces now allow static methods static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) { return supplier.get(); } }下面的一小段代码片段把上面的默认方法与静态方法黏合到一起。 public static void main( String[] args ) { Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new ); System.out.println( defaulable.notRequired() ); defaulable = DefaulableFactory.create( OverridableImpl::new ); System.out.println( defaulable.notRequired() ); }这个程序的控制台输出如下： Default implementationOverridden implementation在JVM中，默认方法的实现是非常高效的，并且通过字节码指令为方法调用提供了支持。默认方法允许继续使用现有的Java接口，而同时能够保障正常的编译过程。这方面好的例子是大量的方法被添加到java.util.Collection接口中去：stream()，parallelStream()，forEach()，removeIf()，…… 尽管默认方法非常强大，但是在使用默认方法时我们需要小心注意一个地方：在声明一个默认方法前，请仔细思考是不是真的有必要使用默认方法，因为默认方法会带给程序歧义，并且在复杂的继承体系中容易产生编译错误。更多详情请参考官方文档 重复注解自从Java 5引入了注解机制，这一特性就变得非常流行并且广为使用。然而，使用注解的一个限制是相同的注解在同一位置只能声明一次，不能声明多次。Java 8打破了这条规则，引入了重复注解机制，这样相同的注解可以在同一地方声明多次。 重复注解机制本身必须用@Repeatable注解。事实上，这并不是语言层面上的改变，更多的是编译器的技巧，底层的原理保持不变。让我们看一个快速入门的例子： package com.javacodegeeks.java8.repeatable.annotations; import java.lang.annotation.ElementType; import java.lang.annotation.Repeatable; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; public class RepeatingAnnotations { @Target( ElementType.TYPE ) @Retention( RetentionPolicy.RUNTIME ) public @interface Filters { Filter[] value(); } @Target( ElementType.TYPE ) @Retention( RetentionPolicy.RUNTIME ) @Repeatable( Filters.class ) public @interface Filter { String value(); }; @Filter( &quot;filter1&quot; ) @Filter( &quot;filter2&quot; ) public interface Filterable { } public static void main(String[] args) { for( Filter filter: Filterable.class.getAnnotationsByType( Filter.class ) ) { System.out.println( filter.value() ); } } }正如我们看到的，这里有个使用@Repeatable( Filters.class )注解的注解类Filter，Filters仅仅是Filter注解的数组，但Java编译器并不想让程序员意识到Filters的存在。这样，接口Filterable就拥有了两次Filter（并没有提到Filter）注解。 同时，反射相关的API提供了新的函数getAnnotationsByType()来返回重复注解的类型（请注意Filterable.class.getAnnotation( Filters.class )经编译器处理后将会返回Filters的实例）。 程序输出结果如下： filter1filter2更多详情请参考官方文档 Java编译器的新特性方法参数名字可以反射获取很长一段时间里，Java程序员一直在发明不同的方式使得方法参数的名字能保留在Java字节码中，并且能够在运行时获取它们（比如，Paranamer类库）。最终，在Java 8中把这个强烈要求的功能添加到语言层面（通过反射API与Parameter.getName()方法）与字节码文件（通过新版的javac的–parameters选项）中。 package com.javacodegeeks.java8.parameter.names; import java.lang.reflect.Method;import java.lang.reflect.Parameter; public class ParameterNames { public static void main(String[] args) throws Exception { Method method = ParameterNames.class.getMethod( “main”, String[].class ); for( final Parameter parameter: method.getParameters() ) { System.out.println( “Parameter: “ + parameter.getName() ); } }}如果不使用–parameters参数来编译这个类，然后运行这个类，会得到下面的输出： Parameter: arg0如果使用–parameters参数来编译这个类，程序的结构会有所不同（参数的真实名字将会显示出来）： Parameter: args Java 类库的新特性Java 8 通过增加大量新类，扩展已有类的功能的方式来改善对并发编程、函数式编程、日期/时间相关操作以及其他更多方面的支持。 Optional到目前为止，臭名昭著的空指针异常是导致Java应用程序失败的最常见原因。以前，为了解决空指针异常，Google公司著名的Guava项目引入了Optional类，Guava通过使用检查空值的方式来防止代码污染，它鼓励程序员写更干净的代码。受到Google Guava的启发，Optional类已经成为Java 8类库的一部分。 Optional实际上是个容器：它可以保存类型T的值，或者仅仅保存null。Optional提供很多有用的方法，这样我们就不用显式进行空值检测。更多详情请参考官方文档。 我们下面用两个小例子来演示如何使用Optional类：一个允许为空值，一个不允许为空值。 public class 空指针Optional { public static void main(String[] args) { //使用of方法，仍然会报空指针异常 // Optional optional = Optional.of(null); // System.out.println(optional.get()); //抛出没有该元素的异常 //Exception in thread &quot;main&quot; java.util.NoSuchElementException: No value present // at java.util.Optional.get(Optional.java:135) // at com.javase.Java8.空指针Optional.main(空指针Optional.java:14) // Optional optional1 = Optional.ofNullable(null); // System.out.println(optional1.get()); Optional optional = Optional.ofNullable(null); System.out.println(optional.isPresent()); System.out.println(optional.orElse(0));//当值为空时给与初始值 System.out.println(optional.orElseGet(() -&gt; new String[]{&quot;a&quot;}));//使用回调函数设置默认值 //即使传入Optional容器的元素为空，使用optional.isPresent()方法也不会报空指针异常 //所以通过optional.orElse这种方式就可以写出避免空指针异常的代码了 //输出Optional.empty。 } }如果Optional类的实例为非空值的话，isPresent()返回true，否从返回false。为了防止Optional为空值，orElseGet()方法通过回调函数来产生一个默认值。map()函数对当前Optional的值进行转化，然后返回一个新的Optional实例。orElse()方法和orElseGet()方法类似，但是orElse接受一个默认值而不是一个回调函数。下面是这个程序的输出： Full Name is set? falseFull Name: [none]Hey Stranger!让我们来看看另一个例子： Optional&lt; String &gt; firstName = Optional.of( &quot;Tom&quot; ); System.out.println( &quot;First Name is set? &quot; + firstName.isPresent() ); System.out.println( &quot;First Name: &quot; + firstName.orElseGet( () -&gt; &quot;[none]&quot; ) ); System.out.println( firstName.map( s -&gt; &quot;Hey &quot; + s + &quot;!&quot; ).orElse( &quot;Hey Stranger!&quot; ) ); System.out.println();下面是程序的输出： First Name is set? trueFirst Name: TomHey Tom! Stream最新添加的Stream API（java.util.stream） 把真正的函数式编程风格引入到Java中。这是目前为止对Java类库最好的补充，因为Stream API可以极大提供Java程序员的生产力，让程序员写出高效率、干净、简洁的代码。 Stream API极大简化了集合框架的处理（但它的处理的范围不仅仅限于集合框架的处理，这点后面我们会看到）。让我们以一个简单的Task类为例进行介绍： Task类有一个分数的概念（或者说是伪复杂度），其次是还有一个值可以为OPEN或CLOSED的状态.让我们引入一个Task的小集合作为演示例子： final Collection&lt; Task &gt; tasks = Arrays.asList( new Task( Status.OPEN, 5 ), new Task( Status.OPEN, 13 ), new Task( Status.CLOSED, 8 ) );我们下面要讨论的第一个问题是所有状态为OPEN的任务一共有多少分数？在Java 8以前，一般的解决方式用foreach循环，但是在Java 8里面我们可以使用stream：一串支持连续、并行聚集操作的元素。 // Calculate total points of all active tasks using sum() final long totalPointsOfOpenTasks = tasks .stream() .filter( task -&gt; task.getStatus() == Status.OPEN ) .mapToInt( Task::getPoints ) .sum(); System.out.println( &quot;Total points: &quot; + totalPointsOfOpenTasks );程序在控制台上的输出如下： Total points: 18 这里有几个注意事项。 第一，task集合被转换化为其相应的stream表示。然后，filter操作过滤掉状态为CLOSED的task。 下一步，mapToInt操作通过Task::getPoints这种方式调用每个task实例的getPoints方法把Task的stream转化为Integer的stream。最后，用sum函数把所有的分数加起来，得到最终的结果。 在继续讲解下面的例子之前，关于stream有一些需要注意的地方（详情在这里）.stream操作被分成了中间操作与最终操作这两种。 中间操作返回一个新的stream对象。中间操作总是采用惰性求值方式，运行一个像filter这样的中间操作实际上没有进行任何过滤，相反它在遍历元素时会产生了一个新的stream对象，这个新的stream对象包含原始stream中符合给定谓词的所有元素。 像forEach、sum这样的最终操作可能直接遍历stream，产生一个结果或副作用。当最终操作执行结束之后，stream管道被认为已经被消耗了，没有可能再被使用了。在大多数情况下，最终操作都是采用及早求值方式，及早完成底层数据源的遍历。 stream另一个有价值的地方是能够原生支持并行处理。让我们来看看这个算task分数和的例子。 stream另一个有价值的地方是能够原生支持并行处理。让我们来看看这个算task分数和的例子。 // Calculate total points of all tasks final double totalPoints = tasks .stream() .parallel() .map( task -&gt; task.getPoints() ) // or map( Task::getPoints ) .reduce( 0, Integer::sum ); System.out.println( &quot;Total points (all tasks): &quot; + totalPoints );这个例子和第一个例子很相似，但这个例子的不同之处在于这个程序是并行运行的，其次使用reduce方法来算最终的结果。下面是这个例子在控制台的输出： Total points (all tasks): 26.0经常会有这个一个需求：我们需要按照某种准则来对集合中的元素进行分组。Stream也可以处理这样的需求，下面是一个例子： // Group tasks by their status final Map&lt; Status, List&lt; Task &gt; &gt; map = tasks .stream() .collect( Collectors.groupingBy( Task::getStatus ) ); System.out.println( map );这个例子的控制台输出如下： {CLOSED=[[CLOSED, 8]], OPEN=[[OPEN, 5], [OPEN, 13]]}让我们来计算整个集合中每个task分数（或权重）的平均值来结束task的例子。 // Calculate the weight of each tasks (as percent of total points) final Collection&lt; String &gt; result = tasks .stream() // Stream&lt; String &gt; .mapToInt( Task::getPoints ) // IntStream .asLongStream() // LongStream .mapToDouble( points -&gt; points / totalPoints ) // DoubleStream .boxed() // Stream&lt; Double &gt; .mapToLong( weigth -&gt; ( long )( weigth * 100 ) ) // LongStream .mapToObj( percentage -&gt; percentage + &quot;%&quot; ) // Stream&lt; String&gt; .collect( Collectors.toList() ); // List&lt; String &gt; System.out.println( result );下面是这个例子的控制台输出： [19%, 50%, 30%]最后，就像前面提到的，Stream API不仅仅处理Java集合框架。像从文本文件中逐行读取数据这样典型的I/O操作也很适合用Stream API来处理。下面用一个例子来应证这一点。 final Path path = new File( filename ).toPath(); try( Stream&lt; String &gt; lines = Files.lines( path, StandardCharsets.UTF_8 ) ) { lines.onClose( () -&gt; System.out.println(&quot;Done!&quot;) ).forEach( System.out::println ); }对一个stream对象调用onClose方法会返回一个在原有功能基础上新增了关闭功能的stream对象，当对stream对象调用close()方法时，与关闭相关的处理器就会执行。 Stream API、Lambda表达式与方法引用在接口默认方法与静态方法的配合下是Java 8对现代软件开发范式的回应。更多详情请参考官方文档。 Date/Time API (JSR 310)Java 8通过发布新的Date-Time API (JSR 310)来进一步加强对日期与时间的处理。对日期与时间的操作一直是Java程序员最痛苦的地方之一。标准的 java.util.Date以及后来的java.util.Calendar一点没有改善这种情况（可以这么说，它们一定程度上更加复杂）。 这种情况直接导致了Joda-Time——一个可替换标准日期/时间处理且功能非常强大的Java API的诞生。Java 8新的Date-Time API (JSR 310)在很大程度上受到Joda-Time的影响，并且吸取了其精髓。新的java.time包涵盖了所有处理日期，时间，日期/时间，时区，时刻（instants），过程（during）与时钟（clock）的操作。在设计新版API时，十分注重与旧版API的兼容性：不允许有任何的改变（从java.util.Calendar中得到的深刻教训）。如果需要修改，会返回这个类的一个新实例。 让我们用例子来看一下新版API主要类的使用方法。第一个是Clock类，它通过指定一个时区，然后就可以获取到当前的时刻，日期与时间。Clock可以替换System.currentTimeMillis()与TimeZone.getDefault()。 // Get the system clock as UTC offset final Clock clock = Clock.systemUTC(); System.out.println( clock.instant() ); System.out.println( clock.millis() );下面是程序在控制台上的输出： 2014-04-12T15:19:29.282Z 1397315969360我们需要关注的其他类是LocaleDate与LocalTime。LocaleDate只持有ISO-8601格式且无时区信息的日期部分。相应的，LocaleTime只持有ISO-8601格式且无时区信息的时间部分。LocaleDate与LocalTime都可以从Clock中得到。 // Get the local date and local time final LocalDate date = LocalDate.now(); final LocalDate dateFromClock = LocalDate.now( clock ); System.out.println( date ); System.out.println( dateFromClock ); // Get the local date and local time final LocalTime time = LocalTime.now(); final LocalTime timeFromClock = LocalTime.now( clock ); System.out.println( time ); System.out.println( timeFromClock );下面是程序在控制台上的输出： 2014-04-12 2014-04-12 11:25:54.568 15:25:54.568下面是程序在控制台上的输出： 2014-04-12T11:47:01.017-04:00[America/New_York] 2014-04-12T15:47:01.017Z 2014-04-12T08:47:01.017-07:00[America/Los_Angeles]最后，让我们看一下Duration类：在秒与纳秒级别上的一段时间。Duration使计算两个日期间的不同变的十分简单。下面让我们看一个这方面的例子。 // Get duration between two dates final LocalDateTime from = LocalDateTime.of( 2014, Month.APRIL, 16, 0, 0, 0 ); final LocalDateTime to = LocalDateTime.of( 2015, Month.APRIL, 16, 23, 59, 59 ); final Duration duration = Duration.between( from, to ); System.out.println( &quot;Duration in days: &quot; + duration.toDays() ); System.out.println( &quot;Duration in hours: &quot; + duration.toHours() );上面的例子计算了两个日期2014年4月16号与2014年4月16号之间的过程。下面是程序在控制台上的输出： Duration in days: 365Duration in hours: 8783对Java 8在日期/时间API的改进整体印象是非常非常好的。一部分原因是因为它建立在“久战杀场”的Joda-Time基础上，另一方面是因为用来大量的时间来设计它，并且这次程序员的声音得到了认可。更多详情请参考官方文档。 并行（parallel）数组Java 8增加了大量的新方法来对数组进行并行处理。可以说，最重要的是parallelSort()方法，因为它可以在多核机器上极大提高数组排序的速度。下面的例子展示了新方法（parallelXxx）的使用。 package com.javacodegeeks.java8.parallel.arrays; import java.util.Arrays; import java.util.concurrent.ThreadLocalRandom; public class ParallelArrays { public static void main( String[] args ) { long[] arrayOfLong = new long [ 20000 ]; Arrays.parallelSetAll( arrayOfLong, index -&gt; ThreadLocalRandom.current().nextInt( 1000000 ) ); Arrays.stream( arrayOfLong ).limit( 10 ).forEach( i -&gt; System.out.print( i + &quot; &quot; ) ); System.out.println(); Arrays.parallelSort( arrayOfLong ); Arrays.stream( arrayOfLong ).limit( 10 ).forEach( i -&gt; System.out.print( i + &quot; &quot; ) ); System.out.println(); } }上面的代码片段使用了parallelSetAll()方法来对一个有20000个元素的数组进行随机赋值。然后，调用parallelSort方法。这个程序首先打印出前10个元素的值，之后对整个数组排序。这个程序在控制台上的输出如下（请注意数组元素是随机生产的）： Unsorted: 591217 891976 443951 424479 766825 351964 242997 642839 119108 552378Sorted: 39 220 263 268 325 607 655 678 723 793 CompletableFuture在Java8之前，我们会使用JDK提供的Future接口来进行一些异步的操作，其实CompletableFuture也是实现了Future接口， 并且基于ForkJoinPool来执行任务，因此本质上来讲，CompletableFuture只是对原有API的封装， 而使用CompletableFuture与原来的Future的不同之处在于可以将两个Future组合起来，或者如果两个Future是有依赖关系的，可以等第一个执行完毕后再实行第二个等特性。 先来看看基本的使用方式： public Future&lt;Double&gt; getPriceAsync(final String product) { final CompletableFuture&lt;Double&gt; futurePrice = new CompletableFuture&lt;&gt;(); new Thread(() -&gt; { double price = calculatePrice(product); futurePrice.complete(price); //完成后使用complete方法，设置future的返回值 }).start(); return futurePrice; }得到Future之后就可以使用get方法来获取结果，CompletableFuture提供了一些工厂方法来简化这些API，并且使用函数式编程的方式来使用这些API，例如： Fufure price = CompletableFuture.supplyAsync(() -&gt; calculatePrice(product));代码是不是一下子简洁了许多呢。之前说了，CompletableFuture可以组合多个Future，不管是Future之间有依赖的，还是没有依赖的。 如果第二个请求依赖于第一个请求的结果，那么可以使用thenCompose方法来组合两个Future public List&lt;String&gt; findPriceAsync(String product) { List&lt;CompletableFutute&lt;String&gt;&gt; priceFutures = tasks.stream() .map(task -&gt; CompletableFuture.supplyAsync(() -&gt; task.getPrice(product),executor)) .map(future -&gt; future.thenApply(Work::parse)) .map(future -&gt; future.thenCompose(work -&gt; CompletableFuture.supplyAsync(() -&gt; Count.applyCount(work), executor))) .collect(Collectors.toList()); return priceFutures.stream().map(CompletableFuture::join).collect(Collectors.toList()); }上面这段代码使用了thenCompose来组合两个CompletableFuture。supplyAsync方法第二个参数接受一个自定义的Executor。 首先使用CompletableFuture执行一个任务，调用getPrice方法，得到一个Future，之后使用thenApply方法，将Future的结果应用parse方法， 之后再使用执行完parse之后的结果作为参数再执行一个applyCount方法，然后收集成一个CompletableFuture的List， 最后再使用一个流，调用CompletableFuture的join方法，这是为了等待所有的异步任务执行完毕，获得最后的结果。 注意，这里必须使用两个流，如果在一个流里调用join方法，那么由于Stream的延迟特性，所有的操作还是会串行的执行，并不是异步的。 再来看一个两个Future之间没有依赖关系的例子： Future&lt;String&gt; futurePriceInUsd = CompletableFuture.supplyAsync(() -&gt; shop.getPrice(“price1”)) .thenCombine(CompletableFuture.supplyAsync(() -&gt; shop.getPrice(“price2”)), (s1, s2) -&gt; s1 + s2);这里有两个异步的任务，使用thenCombine方法来组合两个Future，thenCombine方法的第二个参数就是用来合并两个Future方法返回值的操作函数。 有时候，我们并不需要等待所有的异步任务结束，只需要其中的一个完成就可以了，CompletableFuture也提供了这样的方法： //假设getStream方法返回一个Stream&lt;CompletableFuture&lt;String&gt;&gt; CompletableFuture[] futures = getStream(“listen”).map(f -&gt; f.thenAccept(System.out::println)).toArray(CompletableFuture[]::new); //等待其中的一个执行完毕 CompletableFuture.anyOf(futures).join(); 使用anyOf方法来响应CompletableFuture的completion事件。Java虚拟机（JVM）的新特性PermGen空间被移除了，取而代之的是Metaspace（JEP 122）。JVM选项-XX:PermSize与-XX:MaxPermSize分别被-XX:MetaSpaceSize与-XX:MaxMetaspaceSize所代替。 总结更多展望：Java 8通过发布一些可以增加程序员生产力的特性来推进这个伟大的平台的进步。现在把生产环境迁移到Java 8还为时尚早，但是在接下来的几个月里，它会被大众慢慢的接受。毫无疑问，现在是时候让你的代码与Java 8兼容，并且在Java 8足够安全稳定的时候迁移到Java 8。 参考文章https://blog.csdn.net/shuaicihai/article/details/72615495https://blog.csdn.net/qq_34908167/article/details/79286697https://www.jianshu.com/p/4df02599aeb2https://www.cnblogs.com/yangzhilong/p/10973006.htmlhttps://www.cnblogs.com/JackpotHan/p/9701147.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列20：从IDE的实现原理聊起，谈谈那些年我们用过的Java命令]]></title>
    <url>%2F2019%2F09%2F20%2F20%E3%80%81javac%E5%92%8Cjavap%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 聊聊IDE的实现原理 IDE是把双刃剑，它可以什么都帮你做了，你只要敲几行代码，点几下鼠标，程序就跑起来了，用起来相当方便。 你不用去关心它后面做了些什么，执行了哪些命令，基于什么原理。然而也是这种过分的依赖往往让人散失了最基本的技能，当到了一个没有IDE的地方，你便觉得无从下手，给你个代码都不知道怎么去跑。好比给你瓶水，你不知道怎么打开去喝，然后活活给渴死。 之前用惯了idea，Java文件编译运行的命令基本忘得一干二净。 那好，不如咱们先来了解一下IDE的实现原理，这样一来，即使离开IDE，我们还是知道如何运行Java程序了。 像Eclipse等java IDE是怎么编译和查找java源代码的呢？ 源代码保存这个无需多说，在编译器写入代码，并保存到文件。这个利用流来实现。 编译为class文件java提供了JavaCompiler，我们可以通过它来编译java源文件为class文件。 查找class可以通过Class.forName(fullClassPath)或自定义类加载器来实现。 生成对象，并调用对象方法通过上面一个查找class，得到Class对象后，可以通过newInstance()或构造器的newInstance()得到对象。然后得到Method，最后调用方法，传入相关参数即可。 示例代码： public class MyIDE { public static void main(String[] args) throws IOException, ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException { // 定义java代码，并保存到文件（Test.java） StringBuilder sb = new StringBuilder(); sb.append(&quot;package com.tommy.core.test.reflect;\n&quot;); sb.append(&quot;public class Test {\n&quot;); sb.append(&quot; private String name;\n&quot;); sb.append(&quot; public Test(String name){\n&quot;); sb.append(&quot; this.name = name;\n&quot;); sb.append(&quot; System.out.println(\&quot;hello,my name is \&quot; + name);\n&quot;); sb.append(&quot; }\n&quot;); sb.append(&quot; public String sayHello(String name) {\n&quot;); sb.append(&quot; return \&quot;hello,\&quot; + name;\n&quot;); sb.append(&quot; }\n&quot;); sb.append(&quot;}\n&quot;); System.out.println(sb.toString()); String baseOutputDir = &quot;F:\\output\\classes\\&quot;; String baseDir = baseOutputDir + &quot;com\\tommy\\core\\test\\reflect\\&quot;; String targetJavaOutputPath = baseDir + &quot;Test.java&quot;; // 保存为java文件 FileWriter fileWriter = new FileWriter(targetJavaOutputPath); fileWriter.write(sb.toString()); fileWriter.flush(); fileWriter.close(); // 编译为class文件 JavaCompiler compiler = ToolProvider.getSystemJavaCompiler(); StandardJavaFileManager manager = compiler.getStandardFileManager(null,null,null); List&lt;File&gt; files = new ArrayList&lt;&gt;(); files.add(new File(targetJavaOutputPath)); Iterable compilationUnits = manager.getJavaFileObjectsFromFiles(files); // 编译 // 设置编译选项，配置class文件输出路径 Iterable&lt;String&gt; options = Arrays.asList(&quot;-d&quot;,baseOutputDir); JavaCompiler.CompilationTask task = compiler.getTask(null, manager, null, options, null, compilationUnits); // 执行编译任务 task.call();​​ // 通过反射得到对象​ // Class clazz = Class.forName(“com.tommy.core.test.reflect.Test”);​ // 使用自定义的类加载器加载class​ Class clazz = new MyClassLoader(baseOutputDir).loadClass(“com.tommy.core.test.reflect.Test”);​ // 得到构造器​ Constructor constructor = clazz.getConstructor(String.class);​ // 通过构造器new一个对象​ Object test = constructor.newInstance(“jack.tsing”);​ // 得到sayHello方法​ Method method = clazz.getMethod(“sayHello”, String.class);​ // 调用sayHello方法​ String result = (String) method.invoke(test, “jack.ma”);​ System.out.println(result);​ }​ } 自定义类加载器代码： ​​ public class MyClassLoader extends ClassLoader {​ private String baseDir;​ public MyClassLoader(String baseDir) {​ this.baseDir = baseDir;​ }​ @Override​ protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException {​ String fullClassFilePath = this.baseDir + name.replace(“\.”,”/“) + “.class”;​ File classFilePath = new File(fullClassFilePath);​ if (classFilePath.exists()) {​ FileInputStream fileInputStream = null;​ ByteArrayOutputStream byteArrayOutputStream = null;​ try {​ fileInputStream = new FileInputStream(classFilePath);​ byte[] data = new byte[1024];​ int len = -1;​ byteArrayOutputStream = new ByteArrayOutputStream();​ while ((len = fileInputStream.read(data)) != -1) {​ byteArrayOutputStream.write(data,0,len);​ }​ return defineClass(name,byteArrayOutputStream.toByteArray(),0,byteArrayOutputStream.size()); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } finally { if (null != fileInputStream) { try { fileInputStream.close(); } catch (IOException e) { e.printStackTrace(); } } if (null != byteArrayOutputStream) { try { byteArrayOutputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } return super.findClass(name); } } javac命令初窥注：以下红色标记的参数在下文中有所讲解。 本部分参考https://www.cnblogs.com/xiazdong/p/3216220.html 用法: javac 其中, 可能的选项包括: -g 生成所有调试信息 -g:none 不生成任何调试信息 -g:{lines,vars,source} 只生成某些调试信息 -nowarn 不生成任何警告 -verbose 输出有关编译器正在执行的操作的消息 -deprecation 输出使用已过时的 API 的源位置 -classpath &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -cp &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -sourcepath &lt;路径&gt; 指定查找输入源文件的位置 -bootclasspath &lt;路径&gt; 覆盖引导类文件的位置 -extdirs &lt;目录&gt; 覆盖所安装扩展的位置 -endorseddirs &lt;目录&gt; 覆盖签名的标准路径的位置 -proc:{none,only} 控制是否执行注释处理和/或编译。 -processor [,,…] 要运行的注释处理程序的名称; 绕过默认的搜索进程 -processorpath &lt;路径&gt; 指定查找注释处理程序的位置 -d &lt;目录&gt; 指定放置生成的类文件的位置 -s &lt;目录&gt; 指定放置生成的源文件的位置 -implicit:{none,class} 指定是否为隐式引用文件生成类文件 -encoding &lt;编码&gt; 指定源文件使用的字符编码 -source &lt;发行版&gt; 提供与指定发行版的源兼容性 -target &lt;发行版&gt; 生成特定 VM 版本的类文件 -version 版本信息 -help 输出标准选项的提要 -A关键字[=值] 传递给注释处理程序的选项 -X 输出非标准选项的提要 -J&lt;标记&gt; 直接将 &lt;标记&gt; 传递给运行时系统 -Werror 出现警告时终止编译 @&lt;文件名&gt; 从文件读取选项和文件名 在详细介绍javac命令之前，先看看这个classpath是什么 classpath是什么在dos下编译java程序，就要用到classpath这个概念，尤其是在没有设置环境变量的时候。classpath就是存放.class等编译后文件的路径。 javac：如果当前你要编译的java文件中引用了其它的类(比如说：继承)，但该引用类的.class文件不在当前目录下，这种情况下就需要在javac命令后面加上-classpath参数，通过使用以下三种类型的方法 来指导编译器在编译的时候去指定的路径下查找引用类。 (1).绝对路径：javac -classpath c:/junit3.8.1/junit.jar Xxx.java (2).相对路径：javac -classpath ../junit3.8.1/Junit.javr Xxx.java (3).系统变量：javac -classpath %CLASSPATH% Xxx.java (注意：%CLASSPATH%表示使用系统变量CLASSPATH的值进行查找，这里假设Junit.jar的路径就包含在CLASSPATH系统变量中) IDE中的classpath对于一个普通的Javaweb项目，一般有这样的配置： 1 WEB-INF/classes,lib才是classpath，WEB-INF/ 是资源目录, 客户端不能直接访问。 2、WEB-INF/classes目录存放src目录java文件编译之后的class文件，xml、properties等资源配置文件，这是一个定位资源的入口。 3、引用classpath路径下的文件，只需在文件名前加classpath: classpath:applicationContext-*.xml classpath:context/conf/controller.xml 4、lib和classes同属classpath，两者的访问优先级为: lib&gt;classes。 5、classpath 和 classpath* 区别： classpath：只会到你的class路径中查找找文件;classpath*：不仅包含class路径，还包括jar文件中(class路径)进行查找。 总结： (1).何时需要使用-classpath：当你要编译或执行的类引用了其它的类，但被引用类的.class文件不在当前目录下时，就需要通过-classpath来引入类 (2).何时需要指定路径：当你要编译的类所在的目录和你执行javac命令的目录不是同一个目录时，就需要指定源文件的路径(CLASSPATH是用来指定.class路径的，不是用来指定.java文件的路径的) Java项目和Java web项目的本质区别（看清IDE及classpath本质） 现在只是说说Java Project和Web Project，那么二者有区别么？回答：没有！都是Java语言的应用，只是应用场合不同罢了，那么他们的本质到底是什么？ 回答：编译后路径！虚拟机执行的是class文件而不是java文件，那么我们不管是何种项目都是写的java文件，怎么就不一样了呢？分成java和web两种了呢？ 从.classpath文件入手来看，这个文件在每个项目目录下都是存在的，很少有人打开看吧，那么我们就来一起看吧。这是一个XML文件，使用文本编辑器打开即可。 这里展示一个web项目的.classpath Xml代码 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;classpath&gt; &lt;classpathentry kind=&quot;src&quot; path=&quot;src&quot;/&gt; &lt;classpathentry kind=&quot;src&quot; path=&quot;resources&quot;/&gt; &lt;classpathentry kind=&quot;src&quot; path=&quot;test&quot;/&gt; &lt;classpathentry kind=&quot;con&quot; path=&quot;org.eclipse.jdt.launching.JRE_CONTAINER&quot;/&gt; &lt;classpathentry kind=&quot;lib&quot; path=&quot;lib/servlet-api.jar&quot;/&gt; &lt;classpathentry kind=&quot;lib&quot; path=&quot;webapp/WEB-INF/lib/struts2-core-2.1.8.1.jar&quot;/&gt; …… &lt;classpathentry kind=&quot;output&quot; path=&quot;webapp/WEB-INF/classes&quot;/&gt; &lt;/classpath&gt; XML文档包含一个根元素，就是classpath，类路径，那么这里面包含了什么信息呢？子元素是classpathentry，kind属性区别了种 类信息，src源码，con你看看后面的path就知道是JRE容器的信息。lib是项目依赖的第三方类库，output是src编译后的位置。 既然是web项目，那么就是WEB-INF/classes目录，可能用MyEclipse的同学会说他们那里是WebRoot或者是WebContext而不是webapp，有区别么？回答：完全没有！ 既然看到了编译路径的本来面目后，还区分什么java项目和web项目么？回答:不区分！普通的java 项目你这样写就行了：，看看Eclipse是不是这样生成的？这个问题解决了吧。 再说说webapp目录命名的问题，这个无所谓啊，web项目是要发布到服务器上的对吧，那么服务器读取的是类文件和页面文件吧，它不管源文件，它也无法去理解源文件。那么webapp目录的命名有何关系呢？只要让服务器找到不就行了。 javac命令后缀-g、-g:none、-g:{lines,vars,source} •-g：在生成的class文件中包含所有调试信息（行号、变量、源文件）•-g:none ：在生成的class文件中不包含任何调试信息。 这个参数在javac编译中是看不到什么作用的，因为调试信息都在class文件中，而我们看不懂这个class文件。 为了看出这个参数的作用，我们在eclipse中进行实验。在eclipse中，我们经常做的事就是“debug”，而在debug的时候，我们会•加入“断点”，这个是靠-g:lines起作用，如果不记录行号，则不能加断点。•在“variables”窗口中查看当前的变量，如下图所示，这是靠-g:vars起作用，否则不能查看变量信息。•在多个文件之间来回调用，比如 A.java的main()方法中调用了B.java的fun()函数，而我想看看程序进入fun()后的状态，这是靠-g:source，如果没有这个参数，则不能查看B.java的源代码。 -bootclasspath、-extdirs -bootclasspath和-extdirs 几乎不需要用的，因为他是用来改变 “引导类”和“扩展类”。•引导类(组成Java平台的类)：Java\jdk1.7.0_25\jre\lib\rt.jar等，用-bootclasspath设置。•扩展类：Java\jdk1.7.0_25\jre\lib\ext目录中的文件，用-extdirs设置。•用户自定义类：用-classpath设置。 我们用-verbose编译后出现的“类文件的搜索路径”，就是由上面三个路径组成，如下： [类文件的搜索路径: C:\Java\jdk1.7.0_25\jre\lib\resources.jar,C:\Java\jdk1.7.0_25 \jre\lib\rt.jar,C:\Java\jdk1.7.0_25\jre\lib\sunrsasign.jar,C:\Java\jdk1.7.0_25\j re\lib\jsse.jar,C:\Java\jdk1.7.0_25\jre\lib\jce.jar,C:\Java\jdk1.7.0_25\jre\lib\ charsets.jar,C:\Java\jdk1.7.0_25\jre\lib\jfr.jar,C:\Java\jdk1.7.0_25\jre\classes ,C:\Java\jdk1.7.0_25\jre\lib\ext\access-bridge-32.jar,C:\Java\jdk1.7.0_25\jre\li b\ext\dnsns.jar,C:\Java\jdk1.7.0_25\jre\lib\ext\jaccess.jar,C:\Java\jdk1.7.0_25\ jre\lib\ext\localedata.jar,C:\Java\jdk1.7.0_25\jre\lib\ext\sunec.jar,C:\Java\jdk 1.7.0_25\jre\lib\ext\sunjce_provider.jar,C:\Java\jdk1.7.0_25\jre\lib\ext\sunmsca pi.jar,C:\Java\jdk1.7.0_25\jre\lib\ext\sunpkcs11.jar,C:\Java\jdk1.7.0_25\jre\lib \ext\zipfs.jar,..\bin] 如果利用 -bootclasspath 重新定义： javac -bootclasspath src Xxx.java，则会出现下面错误： 致命错误: 在类路径或引导类路径中找不到程序包 java.lang -sourcepath和-classpath（-cp）•-classpath(-cp)指定你依赖的类的class文件的查找位置。在Linux中，用“:”分隔classpath，而在windows中，用“;”分隔。•-sourcepath指定你依赖的类的java文件的查找位置。 举个例子， public class A { public static void main(String[] args) { B b = new B(); b.print(); } }​​​ public class B​ {​ public void print()​ {​ System.out.println(“old”);​ }​ } 目录结构如下： sourcepath //此处为当前目录 123456|-src |-com |- B.java |- A.java |-bin |- B.class //是 B.java 编译后的类文件 如果要编译 A.java，则必须要让编译器找到类B的位置，你可以指定B.class的位置，也可以是B.java的位置，也可以同时都存在。 javac -classpath bin src/A.java //查找到B.class javac -sourcepath src/com src/A.java //查找到B.java javac -sourcepath src/com -classpath bin src/A.java //同时查找到B.class和B.java如果同时找到了B.class和B.java，则：•如果B.class和B.java内容一致，则遵循B.class。•如果B.class和B.java内容不一致，则遵循B.java，并编译B.java。 以上规则可以通过 -verbose选项看出。 -d•d就是 destination，用于指定.class文件的生成目录，在eclipse中，源文件都在src中，编译的class文件都是在bin目录中。 这里我用来实现一下这个功能，假设项目名称为project，此目录为当前目录，且在src/com目录中有一个Main.java文件。‘ ​​ package com;​ public class Main​ {​ public static void main(String[] args) {​ System.out.println(“Hello”);​ }​ } ​​​ javac -d bin src/com/Main.java 上面的语句将Main.class生成在bin/com目录下。 -implicit:{none,class}•如果有文件为A.java（其中有类A），且在类A中使用了类B，类B在B.java中，则编译A.java时，默认会自动编译B.java，且生成B.class。•implicit:none：不自动生成隐式引用的类文件。•implicit:class（默认）：自动生成隐式引用的类文件。 public class A { public static void main(String[] args) { B b = new B(); } } public class B { } 如果使用：​​ javac -implicit:none A.java 则不会生成 B.class。 -source和-target•-source：使用指定版本的JDK编译，比如：-source 1.4表示用JDK1.4的标准编译，如果在源文件中使用了泛型，则用JDK1.4是不能编译通过的。•-target：指定生成的class文件要运行在哪个JVM版本，以后实际运行的JVM版本必须要高于这个指定的版本。 javac -source 1.4 Xxx.java javac -target 1.4 Xxx.java -encoding默认会使用系统环境的编码，比如我们一般用的中文windows就是GBK编码，所以直接javac时会用GBK编码，而Java文件一般要使用utf-8，如果用GBK就会出现乱码。 •指定源文件的编码格式，如果源文件是UTF-8编码的，而-encoding GBK，则源文件就变成了乱码（特别是有中文时）。 javac -encoding UTF-8 Xxx.java -verbose输出详细的编译信息，包括：classpath、加载的类文件信息。 比如，我写了一个最简单的HelloWorld程序，在命令行中输入： D:\Java&gt;javac -verbose -encoding UTF-8 HelloWorld01.java 输出： [语法分析开始时间 RegularFileObject[HelloWorld01.java]] [语法分析已完成, 用时 21 毫秒] [源文件的搜索路径: .,D:\大三下\编译原理\cup\java-cup-11a.jar,E:\java\jflex\lib\J //-sourcepath Flex.jar] [类文件的搜索路径: C:\Java\jdk1.7.0_25\jre\lib\resources.jar,C:\Java\jdk1.7.0_25 //-classpath、-bootclasspath、-extdirs 省略............................................ [正在加载ZipFileIndexFileObject[C:\Java\jdk1.7.0_25\lib\ct.sym(META-INF/sym/rt.j ar/java/lang/Object.class)]] [正在加载ZipFileIndexFileObject[C:\Java\jdk1.7.0_25\lib\ct.sym(META-INF/sym/rt.j ar/java/lang/String.class)]] [正在检查Demo] 省略............................................ [已写入RegularFileObject[Demo.class]] [共 447 毫秒]编写一个程序时，比如写了一句：System.out.println(“hello”)，实际上还需要加载：Object、PrintStream、String等类文件，而上面就显示了加载的全部类文件。 其他命令-J &lt;标记&gt;•传递一些信息给 Java Launcher. javac -J-Xms48m Xxx.java //set the startup memory to 48M.-@&lt;文件名&gt; 如果同时需要编译数量较多的源文件(比如1000个)，一个一个编译是不现实的（当然你可以直接 javac *.java ），比较好的方法是：将你想要编译的源文件名都写在一个文件中（比如sourcefiles.txt），其中每行写一个文件名，如下所示： HelloWorld01.javaHelloWorld02.javaHelloWorld03.java 则使用下面的命令： javac @sourcefiles.txt 编译这三个源文件。 使用javac构建项目这部分参考：https://blog.csdn.net/mingover/article/details/57083176 一个简单的javac编译 新建两个文件夹,src和 buildsrc/com/yp/test/HelloWorld.javabuild/ 123456├─build└─src └─com └─yp └─test HelloWorld.java java文件非常简单 package com.yp.test; public class HelloWorld { public static void main(String[] args) { System.out.println(&quot;helloWorld&quot;); } }编译:javac src/com/yp/test/HelloWorld.java -d build -d 表示编译到 build文件夹下 123456789101112查看build文件夹├─build│ └─com│ └─yp│ └─test│ HelloWorld.class│└─src └─com └─yp └─test HelloWorld.java 运行文件 E:\codeplace\n_learn\java\javacmd&gt; java com/yp/test/HelloWorld.class错误: 找不到或无法加载主类 build.com.yp.test.HelloWorld.class 运行时要指定mainE:\codeplace\n_learn\java\javacmd\build&gt; java com.yp.test.HelloWorldhelloWorld 如果引用到多个其他的类，应该怎么做呢 ？ 编译 E:\codeplace\n_learn\java\javacmd&gt;javac src/com/yp/test/HelloWorld.java -sourcepath src -d build -g1-sourcepath 表示 从指定的源文件目录中找到需要的.java文件并进行编译。也可以用-cp指定编译好的class的路径运行,注意:运行在build目录下 E:\codeplace\n_learn\java\javacmd\build&gt;java com.yp.test.HelloWorld 怎么打成jar包? 生成:E:\codeplace\n_learn\java\javacmd\build&gt;jar cvf h.jar *运行:E:\codeplace\n_learn\java\javacmd\build&gt;java h.jar错误: 找不到或无法加载主类 h.jar 这个错误是没有指定main类，所以类似这样来指定:E:\codeplace\n_learn\java\javacmd\build&gt;java -cp h.jar com.yp.test.HelloWorld 生成可以运行的jar包 需要指定jar包的应用程序入口点，用-e选项： E:\codeplace\n_learn\java\javacmd\build&gt; jar cvfe h.jar com.yp.test.HelloWorld * 已添加清单 正在添加: com/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: com/yp/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: com/yp/test/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: com/yp/test/entity/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: com/yp/test/entity/Cat.class(输入 = 545) (输出 = 319)(压缩了 41%) 正在添加: com/yp/test/HelloWorld.class(输入 = 844) (输出 = 487)(压缩了 42%)直接运行 java -jar h.jar 额外发现 指定了Main类后，jar包里面的 META-INF/MANIFEST.MF 是这样的， 比原来多了一行Main-Class…. Manifest-Version: 1.0 Created-By: 1.8.0 (Oracle Corporation) Main-Class: com.yp.test.HelloWorld如果类里有引用jar包呢? 先下一个jar包 这里直接下 log4j * main函数改成 import com.yp.test.entity.Cat; import org.apache.log4j.Logger; public class HelloWorld { static Logger log = Logger.getLogger(HelloWorld.class); public static void main(String[] args) { Cat c = new Cat(&quot;keyboard&quot;); log.info(&quot;这是log4j&quot;); System.out.println(&quot;hello,&quot; + c.getName()); } }现的文件是这样的 123456789101112├─build├─lib│ log4j-1.2.17.jar│└─src └─com └─yp └─test │ HelloWorld.java │ └─entity Cat.java 这个时候 javac命令要接上 -cp ./lib/*.jar E:\codeplace\n_learn\java\javacmd&gt;javac -encoding &quot;utf8&quot; src/com/yp/test/HelloWorld.java -sourcepath src -d build -g -cp ./lib/*.jar 运行要加上-cp, -cp 选项貌似会把工作目录给换了， 所以要加上 ;../build E:\codeplace\n_learn\java\javacmd\build&gt;java -cp ../lib/log4j-1.2.17.jar;../build com.yp.test.HelloWorld结果: log4j:WARN No appenders could be found for logger(com.yp.test.HelloWorld). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. hello,keyboard由于没有 log4j的配置文件，所以提示上面的问题,往 build 里面加上 log4j.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt; &lt;log4j:configuration xmlns:log4j=&apos;http://jakarta.apache.org/log4j/&apos;&gt; &lt;appender name=&quot;stdout&quot; class=&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%d{ABSOLUTE} %-5p [%c{1}] %m%n&quot; /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;root&gt; &lt;level value=&quot;info&quot; /&gt; &lt;appender-ref ref=&quot;stdout&quot; /&gt; &lt;/root&gt; &lt;/log4j:configuration&gt;再运行 E:\codeplace\n_learn\java\javacmd&gt;java -cp lib/log4j-1.2.17.jar;build com.yp.tes t.HelloWorld 15:19:57,359 INFO [HelloWorld] 这是log4j hello,keyboard说明:这个log4j配置文件，习惯的做法是放在src目录下, 在编译过程中 copy到build中的,但根据ant的做法，不是用javac的，而是用来处理,我猜测javac是不能copy的，如果想在命令行直接 使用，应该是用cp命令主动去执行 copy操作 ok 一个简单的java 工程就运行完了但是 貌似有些繁琐, 需要手动键入 java文件 以及相应的jar包 很是麻烦,so 可以用 shell 来脚本来简化相关操作shell 文件整理如下: #!/bin/bash echo &quot;build start&quot; JAR_PATH=libs BIN_PATH=bin SRC_PATH=src # java文件列表目录 SRC_FILE_LIST_PATH=src/sources.list #生所有的java文件列表 放入列表文件中 rm -f $SRC_PATH/sources find $SRC_PATH/ -name *.java &gt; $SRC_FILE_LIST_PATH #删除旧的编译文件 生成bin目录 rm -rf $BIN_PATH/ mkdir $BIN_PATH/ #生成依赖jar包 列表 for file in ${JAR_PATH}/*.jar; do jarfile=${jarfile}:${file} done echo &quot;jarfile = &quot;$jarfile #编译 通过-cp指定所有的引用jar包，将src下的所有java文件进行编译 javac -d $BIN_PATH/ -cp $jarfile @$SRC_FILE_LIST_PATH #运行 通过-cp指定所有的引用jar包，指定入口函数运行 java -cp $BIN_PATH$jarfile com.zuiapps.danmaku.server.Main 有一点需要注意的是, javac -d $BIN_PATH/ -cp $jarfile @$SRC_FILE_LIST_PATH在要编译的文件很多时候，一个个敲命令会显得很长，也不方便修改， 可以把要编译的源文件列在文件中，在文件名前加@，这样就可以对多个文件进行编译， 以上就是吧java文件放到 $SRC_FILE_LIST_PATH 中去了 编译 : 1. 需要编译所有的java文件 2. 依赖的java 包都需要加入到 classpath 中去 3. 最后设置 编译后的 class 文件存放目录 即 -d bin/ 4. java文件过多是可以使用 @$SRC_FILE_LIST_PATH 把他们放到一个文件中去 运行: 1.需要吧 编译时设置的bin目录和 所有jar包加入到 classpath 中去​ javap 的使用 javap是jdk自带的一个工具，可以对代码反编译，也可以查看java编译器生成的字节码。 情况下，很少有人使用javap对class文件进行反编译，因为有很多成熟的反编译工具可以使用，比如jad。但是，javap还可以查看java编译器为我们生成的字节码。通过它，可以对照源代码和字节码，从而了解很多编译器内部的工作。 javap命令分解一个class文件，它根据options来决定到底输出什么。如果没有使用options,那么javap将会输出包，类里的protected和public域以及类里的所有方法。javap将会把它们输出在标准输出上。来看这个例子，先编译(javac)下面这个类。 import java.awt.*; import java.applet.*; public class DocFooter extends Applet { String date; String email; public void init() { resize(500,100); date = getParameter(&quot;LAST_UPDATED&quot;); email = getParameter(&quot;EMAIL&quot;); } }在命令行上键入javap DocFooter后，输出结果如下 Compiled from “DocFooter.java” public class DocFooter extends java.applet.Applet { java.lang.String date; java.lang.String email; public DocFooter(); public void init(); }如果加入了-c，即javap -c DocFooter，那么输出结果如下 Compiled from “DocFooter.java” public class DocFooter extends java.applet.Applet { java.lang.String date; java.lang.String email; public DocFooter(); Code: 0: aload_0 1: invokespecial #1 // Method java/applet/Applet.&quot;&lt;init&gt;&quot;:()V 4: return public void init(); Code: 0: aload_0 1: sipush 500 4: bipush 100 6: invokevirtual #2 // Method resize:(II)V 9: aload_0 10: aload_0 11: ldc #3 // String LAST_UPDATED 13: invokevirtual #4 // Method getParameter:(Ljava/lang/String;)Ljava/lang/String; 16: putfield #5 // Field date:Ljava/lang/String; 19: aload_0 20: aload_0 21: ldc #6 // String EMAIL 23: invokevirtual #4 // Method getParameter:(Ljava/lang/String;)Ljava/lang/String; 26: putfield #7 // Field email:Ljava/lang/String; 29: return }上面输出的内容就是字节码。 用法摘要 -help 帮助-l 输出行和变量的表-public 只输出public方法和域-protected 只输出public和protected类和成员-package 只输出包，public和protected类和成员，这是默认的-p -private 输出所有类和成员-s 输出内部类型签名-c 输出分解后的代码，例如，类中每一个方法内，包含java字节码的指令，-verbose 输出栈大小，方法参数的个数-constants 输出静态final常量总结 javap可以用于反编译和查看编译器编译后的字节码。平时一般用javap -c比较多，该命令用于列出每个方法所执行的JVM指令，并显示每个方法的字节码的实际作用。可以通过字节码和源代码的对比，深入分析java的编译原理，了解和解决各种Java原理级别的问题。 参考文章https://blog.csdn.net/Anbernet/article/details/81449390https://www.cnblogs.com/luobiao320/p/7975442.htmlhttps://www.jianshu.com/p/f7330dbdc051https://www.jianshu.com/p/6a8997560b05https://blog.csdn.net/w372426096/article/details/81664431https://blog.csdn.net/qincidong/article/details/82492140 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java命令行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列19：一文搞懂Java集合类框架，以及常见面试题]]></title>
    <url>%2F2019%2F09%2F19%2F19%E3%80%81Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文参考 https://www.cnblogs.com/chenssy/p/3495238.html 集合类大图在编写java程序中，我们最常用的除了八种基本数据类型，String对象外还有一个集合类，在我们的的程序中到处充斥着集合类的身影！ java中集合大家族的成员实在是太丰富了，有常用的ArrayList、HashMap、HashSet，也有不常用的Stack、Queue，有线程安全的Vector、HashTable，也有线程不安全的LinkedList、TreeMap等等！ 上面的图展示了整个集合大家族的成员以及他们之间的关系。下面就上面的各个接口、基类做一些简单的介绍(主要介绍各个集合的特点。区别)。 下面几张图更清晰地介绍了结合类接口间的关系： Collections和Collection。Arrays和Collections。 Collection的子接口 map的实现类 Collection接口 Collection接口是最基本的集合接口，它不提供直接的实现，Java SDK提供的类都是继承自Collection的“子接口”如List和Set。Collection所代表的是一种规则，它所包含的元素都必须遵循一条或者多条规则。如有些允许重复而有些则不能重复、有些必须要按照顺序插入而有些则是散列，有些支持排序但是有些则不支持。 在Java中所有实现了Collection接口的类都必须提供两套标准的构造函数，一个是无参，用于创建一个空的Collection，一个是带有Collection参数的有参构造函数，用于创建一个新的Collection，这个新的Collection与传入进来的Collection具备相同的元素。//要求实现基本的增删改查方法，并且需要能够转换为数组类型 public class Collection接口 { class collect implements Collection { @Override public int size() { return 0; } @Override public boolean isEmpty() { return false; } @Override public boolean contains(Object o) { return false; } @Override public Iterator iterator() { return null; } @Override public Object[] toArray() { return new Object[0]; } @Override public boolean add(Object o) { return false; } @Override public boolean remove(Object o) { return false; } @Override public boolean addAll(Collection c) { return false; } @Override public void clear() { } //省略部分代码 @Override public Object[] toArray(Object[] a) { return new Object[0]; } } }List接口 List接口为Collection直接接口。List所代表的是有序的Collection，即它用某种特定的插入顺序来维护元素顺序。用户可以对列表中每个元素的插入位置进行精确地控制，同时可以根据元素的整数索引（在列表中的位置）访问元素，并搜索列表中的元素。实现List接口的集合主要有：ArrayList、LinkedList、Vector、Stack。 2.1、ArrayList ArrayList是一个动态数组，也是我们最常用的集合。它允许任何符合规则的元素插入甚至包括null。每一个ArrayList都有一个初始容量（10），该容量代表了数组的大小。随着容器中的元素不断增加，容器的大小也会随着增加。在每次向容器中增加元素的同时都会进行容量检查，当快溢出时，就会进行扩容操作。所以如果我们明确所插入元素的多少，最好指定一个初始容量值，避免过多的进行扩容操作而浪费时间、效率。 size、isEmpty、get、set、iterator 和 listIterator 操作都以固定时间运行。add 操作以分摊的固定时间运行，也就是说，添加 n 个元素需要 O(n) 时间（由于要考虑到扩容，所以这不只是添加元素会带来分摊固定时间开销那样简单）。 ArrayList擅长于随机访问。同时ArrayList是非同步的。 2.2、LinkedList 同样实现List接口的LinkedList与ArrayList不同，ArrayList是一个动态数组，而LinkedList是一个双向链表。所以它除了有ArrayList的基本操作方法外还额外提供了get，remove，insert方法在LinkedList的首部或尾部。 由于实现的方式不同，LinkedList不能随机访问，它所有的操作都是要按照双重链表的需要执行。在列表中索引的操作将从开头或结尾遍历列表（从靠近指定索引的一端）。这样做的好处就是可以通过较低的代价在List中进行插入和删除操作。 与ArrayList一样，LinkedList也是非同步的。如果多个线程同时访问一个List，则必须自己实现访问同步。一种解决方法是在创建List时构造一个同步的List： List list = Collections.synchronizedList(new LinkedList(…)); 2.3、Vector 与ArrayList相似，但是Vector是同步的。所以说Vector是线程安全的动态数组。它的操作与ArrayList几乎一样。 2.4、Stack Stack继承自Vector，实现一个后进先出的堆栈。Stack提供5个额外的方法使得Vector得以被当作堆栈使用。基本的push和pop 方法，还有peek方法得到栈顶的元素，empty方法测试堆栈是否为空，search方法检测一个元素在堆栈中的位置。Stack刚创建后是空栈。。 public class List接口 { //下面是List的继承关系，由于List接口规定了包括诸如索引查询，迭代器的实现，所以实现List接口的类都会有这些方法。 //所以不管是ArrayList和LinkedList底层都可以使用数组操作，但一般不提供这样外部调用方法。 // public interface Iterable&lt;T&gt; // public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; // public interface List&lt;E&gt; extends Collection&lt;E&gt; class MyList implements List { @Override public int size() { return 0; } @Override public boolean isEmpty() { return false; } @Override public boolean contains(Object o) { return false; } @Override public Iterator iterator() { return null; } @Override public Object[] toArray() { return new Object[0]; } @Override public boolean add(Object o) { return false; } @Override public boolean remove(Object o) { return false; } @Override public void clear() { } //省略部分代码 @Override public Object get(int index) { return null; } @Override public ListIterator listIterator() { return null; } @Override public ListIterator listIterator(int index) { return null; } @Override public List subList(int fromIndex, int toIndex) { return null; } @Override public Object[] toArray(Object[] a) { return new Object[0]; } } }Set接口 Set是一种不包括重复元素的Collection。它维持它自己的内部排序，所以随机访问没有任何意义。与List一样，它同样运行null的存在但是仅有一个。由于Set接口的特殊性，所有传入Set集合中的元素都必须不同，同时要注意任何可变对象，如果在对集合中元素进行操作时，导致e1.equals(e2)==true，则必定会产生某些问题。实现了Set接口的集合有：EnumSet、HashSet、TreeSet。 3.1、EnumSet 是枚举的专用Set。所有的元素都是枚举类型。 3.2、HashSet HashSet堪称查询速度最快的集合，因为其内部是以HashCode来实现的。它内部元素的顺序是由哈希码来决定的，所以它不保证set 的迭代顺序；特别是它不保证该顺序恒久不变。 public class Set接口 { // Set接口规定将set看成一个集合，并且使用和数组类似的增删改查方式，同时提供iterator迭代器 // public interface Set&lt;E&gt; extends Collection&lt;E&gt; // public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; // public interface Iterable&lt;T&gt; class MySet implements Set { @Override public int size() { return 0; } @Override public boolean isEmpty() { return false; } @Override public boolean contains(Object o) { return false; } @Override public Iterator iterator() { return null; } @Override public Object[] toArray() { return new Object[0]; } @Override public boolean add(Object o) { return false; } @Override public boolean remove(Object o) { return false; } @Override public boolean addAll(Collection c) { return false; } @Override public void clear() { } @Override public boolean removeAll(Collection c) { return false; } @Override public boolean retainAll(Collection c) { return false; } @Override public boolean containsAll(Collection c) { return false; } @Override public Object[] toArray(Object[] a) { return new Object[0]; } } }Map接口 Map与List、Set接口不同，它是由一系列键值对组成的集合，提供了key到Value的映射。同时它也没有继承Collection。在Map中它保证了key与value之间的一一对应关系。也就是说一个key对应一个value，所以它不能存在相同的key值，当然value值可以相同。实现map的有：HashMap、TreeMap、HashTable、Properties、EnumMap。 4.1、HashMap 以哈希表数据结构实现，查找对象时通过哈希函数计算其位置，它是为快速查询而设计的，其内部定义了一个hash表数组（Entry[] table），元素会通过哈希转换函数将元素的哈希地址转换成数组中存放的索引，如果有冲突，则使用散列链表的形式将所有相同哈希地址的元素串起来，可能通过查看HashMap.Entry的源码它是一个单链表结构。 4.2、TreeMap 键以某种排序规则排序，内部以red-black（红-黑）树数据结构实现，实现了SortedMap接口 4.3、HashTable 也是以哈希表数据结构实现的，解决冲突时与HashMap也一样也是采用了散列链表的形式，不过性能比HashMap要低 public class Map接口 { //Map接口是最上层接口，Map接口实现类必须实现put和get等哈希操作。 //并且要提供keyset和values，以及entryset等查询结构。 //public interface Map&lt;K,V&gt; class MyMap implements Map { @Override public int size() { return 0; } @Override public boolean isEmpty() { return false; } @Override public boolean containsKey(Object key) { return false; } @Override public boolean containsValue(Object value) { return false; } @Override public Object get(Object key) { return null; } @Override public Object put(Object key, Object value) { return null; } @Override public Object remove(Object key) { return null; } @Override public void putAll(Map m) { } @Override public void clear() { } @Override public Set keySet() { return null; } @Override public Collection values() { return null; } @Override public Set&lt;Entry&gt; entrySet() { return null; } } }Queue 队列，它主要分为两大类，一类是阻塞式队列，队列满了以后再插入元素则会抛出异常，主要包括ArrayBlockQueue、PriorityBlockingQueue、LinkedBlockingQueue。另一种队列则是双端队列，支持在头、尾两端插入和移除元素，主要包括：ArrayDeque、LinkedBlockingDeque、LinkedList。 public class Queue接口 { //queue接口是对队列的一个实现，需要提供队列的进队出队等方法。一般使用linkedlist作为实现类 class MyQueue implements Queue { @Override public int size() { return 0; } @Override public boolean isEmpty() { return false; } @Override public boolean contains(Object o) { return false; } @Override public Iterator iterator() { return null; } @Override public Object[] toArray() { return new Object[0]; } @Override public Object[] toArray(Object[] a) { return new Object[0]; } @Override public boolean add(Object o) { return false; } @Override public boolean remove(Object o) { return false; } //省略部分代码 @Override public boolean offer(Object o) { return false; } @Override public Object remove() { return null; } @Override public Object poll() { return null; } @Override public Object element() { return null; } @Override public Object peek() { return null; } } }关于Java集合的小抄这部分内容转自我偶像 江南白衣 的博客：http://calvin1978.blogcn.com/articles/collection.html在尽可能短的篇幅里，将所有集合与并发集合的特征、实现方式、性能捋一遍。适合所有”精通Java”，其实还不那么自信的人阅读。 期望能不止用于面试时，平时选择数据结构，也能考虑一下其成本与效率，不要看着API合适就用了。 ListArrayList以数组实现。节约空间，但数组有容量限制。超出限制时会增加50%容量，用System.arraycopy（）复制到新的数组。因此最好能给出数组大小的预估值。默认第一次插入元素时创建大小为10的数组。 按数组下标访问元素－get（i）、set（i,e） 的性能很高，这是数组的基本优势。 如果按下标插入元素、删除元素－add（i,e）、 remove（i）、remove（e），则要用System.arraycopy（）来复制移动部分受影响的元素，性能就变差了。 越是前面的元素，修改时要移动的元素越多。直接在数组末尾加入元素－常用的add（e），删除最后一个元素则无影响。 LinkedList以双向链表实现。链表无容量限制，但双向链表本身使用了更多空间，每插入一个元素都要构造一个额外的Node对象，也需要额外的链表指针操作。 按下标访问元素－get（i）、set（i,e） 要悲剧的部分遍历链表将指针移动到位 （如果i&gt;数组大小的一半，会从末尾移起）。 插入、删除元素时修改前后节点的指针即可，不再需要复制移动。但还是要部分遍历链表的指针才能移动到下标所指的位置。 只有在链表两头的操作－add（）、addFirst（）、removeLast（）或用iterator（）上的remove（）倒能省掉指针的移动。 Apache Commons 有个TreeNodeList，里面是棵二叉树，可以快速移动指针到位。 CopyOnWriteArrayList并发优化的ArrayList。基于不可变对象策略，在修改时先复制出一个数组快照来修改，改好了，再让内部指针指向新数组。 因为对快照的修改对读操作来说不可见，所以读读之间不互斥，读写之间也不互斥，只有写写之间要加锁互斥。但复制快照的成本昂贵，典型的适合读多写少的场景。 虽然增加了addIfAbsent（e）方法，会遍历数组来检查元素是否已存在，性能可想像的不会太好。 遗憾无论哪种实现，按值返回下标contains（e）, indexOf（e）, remove（e） 都需遍历所有元素进行比较，性能可想像的不会太好。 没有按元素值排序的SortedList。 除了CopyOnWriteArrayList，再没有其他线程安全又并发优化的实现如ConcurrentLinkedList。凑合着用Set与Queue中的等价类时，会缺少一些List特有的方法如get（i）。如果更新频率较高，或数组较大时，还是得用Collections.synchronizedList（list），对所有操作用同一把锁来保证线程安全。 MapHashMap以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。 插入元素时，如果两条Key落在同一个桶（比如哈希值1和17取模16后都属于第一个哈希桶），我们称之为哈希冲突。 JDK的做法是链表法，Entry用一个next属性实现多个Entry以单向链表存放。查找哈希值为17的key时，先定位到哈希桶，然后链表遍历桶里所有元素，逐个比较其Hash值然后key值。 在JDK8里，新增默认为8的阈值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。 当然，最好还是桶里只有一个元素，不用去比较。所以默认当Entry数量达到桶数量的75%时，哈希冲突已比较严重，就会成倍扩容桶数组，并重新分配所有原来的Entry。扩容成本不低，所以也最好有个预估值。 取模用与操作（hash &amp; （arrayLength-1））会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。 iterator（）时顺着哈希桶数组来遍历，看起来是个乱序。 LinkedHashMap扩展HashMap，每个Entry增加双向链表，号称是最占内存的数据结构。 支持iterator（）时按Entry的插入顺序来排序（如果设置accessOrder属性为true，则所有读写访问都排序）。 插入时，Entry把自己加到Header Entry的前面去。如果所有读写访问都要排序，还要把前后Entry的before/after拼接起来以在链表中删除掉自己，所以此时读操作也是线程不安全的了。 TreeMap以红黑树实现，红黑树又叫自平衡二叉树： 对于任一节点而言，其到叶节点的每一条路径都包含相同数目的黑结点。上面的规定，使得树的层数不会差的太远，使得所有操作的复杂度不超过 O（lgn），但也使得插入，修改时要复杂的左旋右旋来保持树的平衡。 支持iterator（）时按Key值排序，可按实现了Comparable接口的Key的升序排序，或由传入的Comparator控制。可想象的，在树上插入/删除元素的代价一定比HashMap的大。 支持SortedMap接口，如firstKey（），lastKey（）取得最大最小的key，或sub（fromKey, toKey）, tailMap（fromKey）剪取Map的某一段。 EnumMapEnumMap的原理是，在构造函数里要传入枚举类，那它就构建一个与枚举的所有值等大的数组，按Enum. ordinal（）下标来访问数组。性能与内存占用俱佳。 美中不足的是，因为要实现Map接口，而 V get（Object key）中key是Object而不是泛型K，所以安全起见，EnumMap每次访问都要先对Key进行类型判断，在JMC里录得不低的采样命中频率。 ConcurrentHashMap并发优化的HashMap。 在JDK5里的经典设计，默认16把写锁（可以设置更多），有效分散了阻塞的概率。数据结构为Segment[]，每个Segment一把锁。Segment里面才是哈希桶数组。Key先算出它在哪个Segment里，再去算它在哪个哈希桶里。 也没有读锁，因为put/remove动作是个原子动作（比如put的整个过程是一个对数组元素/Entry 指针的赋值操作），读操作不会看到一个更新动作的中间状态。 但在JDK8里，Segment[]的设计被抛弃了，改为精心设计的，只在需要锁的时候加锁。 支持ConcurrentMap接口，如putIfAbsent（key，value）与相反的replace（key，value）与以及实现CAS的replace（key, oldValue, newValue）。 ConcurrentSkipListMapJDK6新增的并发优化的SortedMap，以SkipList结构实现。Concurrent包选用它是因为它支持基于CAS的无锁算法，而红黑树则没有好的无锁算法。 原理上，可以想象为多个链表组成的N层楼，其中的元素从稀疏到密集，每个元素有往右与往下的指针。从第一层楼开始遍历，如果右端的值比期望的大，那就往下走一层，继续往前走。 典型的空间换时间。每次插入，都要决定在哪几层插入，同时，要决定要不要多盖一层楼。 它的size（）同样不能随便调，会遍历来统计。 Set所有Set几乎都是内部用一个Map来实现, 因为Map里的KeySet就是一个Set，而value是假值，全部使用同一个Object即可。 Set的特征也继承了那些内部的Map实现的特征。 HashSet：内部是HashMap。 LinkedHashSet：内部是LinkedHashMap。 TreeSet：内部是TreeMap的SortedSet。 ConcurrentSkipListSet：内部是ConcurrentSkipListMap的并发优化的SortedSet。 CopyOnWriteArraySet：内部是CopyOnWriteArrayList的并发优化的Set，利用其addIfAbsent（）方法实现元素去重，如前所述该方法的性能很一般。 好像少了个ConcurrentHashSet，本来也该有一个内部用ConcurrentHashMap的简单实现，但JDK偏偏没提供。Jetty就自己简单封了一个，Guava则直接用java.util.Collections.newSetFromMap（new ConcurrentHashMap（）） 实现。 QueueQueue是在两端出入的List，所以也可以用数组或链表来实现。 普通队列LinkedList是的，以双向链表实现的LinkedList既是List，也是Queue。 ArrayDeque以循环数组实现的双向Queue。大小是2的倍数，默认是16。 为了支持FIFO，即从数组尾压入元素（快），从数组头取出元素（超慢），就不能再使用普通ArrayList的实现了，改为使用循环数组。 有队头队尾两个下标：弹出元素时，队头下标递增；加入元素时，队尾下标递增。如果加入元素时已到数组空间的末尾，则将元素赋值到数组[0]，同时队尾下标指向0，再插入下一个元素则赋值到数组[1]，队尾下标指向1。如果队尾的下标追上队头，说明数组所有空间已用完，进行双倍的数组扩容。 PriorityQueue用平衡二叉最小堆实现的优先级队列，不再是FIFO，而是按元素实现的Comparable接口或传入Comparator的比较结果来出队，数值越小，优先级越高，越先出队。但是注意其iterator（）的返回不会排序。 平衡最小二叉堆，用一个简单的数组即可表达，可以快速寻址，没有指针什么的。最小的在queue[0] ，比如queue[4]的两个孩子，会在queue[24+1] 和 queue[2（4+1）]，即queue[9]和queue[10]。 入队时，插入queue[size]，然后二叉地往上比较调整堆。 出队时，弹出queue[0]，然后把queque[size]拿出来二叉地往下比较调整堆。 初始大小为11，空间不够时自动50%扩容。 线程安全的队列ConcurrentLinkedQueue/Deque无界的并发优化的Queue，基于链表，实现了依赖于CAS的无锁算法。 ConcurrentLinkedQueue的结构是单向链表和head/tail两个指针，因为入队时需要修改队尾元素的next指针，以及修改tail指向新入队的元素两个CAS动作无法原子，所以需要的特殊的算法。 线程安全的阻塞队列BlockingQueue，一来如果队列已空不用重复的查看是否有新数据而会阻塞在那里，二来队列的长度受限，用以保证生产者与消费者的速度不会相差太远。当入队时队列已满，或出队时队列已空，不同函数的效果见下表 ArrayBlockingQueue定长的并发优化的BlockingQueue，也是基于循环数组实现。有一把公共的锁与notFull、notEmpty两个Condition管理队列满或空时的阻塞状态。 LinkedBlockingQueue/Deque可选定长的并发优化的BlockingQueue，基于链表实现，所以可以把长度设为Integer.MAX_VALUE成为无界无等待的。 利用链表的特征，分离了takeLock与putLock两把锁，继续用notEmpty、notFull管理队列满或空时的阻塞状态。 PriorityBlockingQueue无界的PriorityQueue，也是基于数组存储的二叉堆（见前）。一把公共的锁实现线程安全。因为无界，空间不够时会自动扩容，所以入列时不会锁，出列为空时才会锁。 DelayQueue内部包含一个PriorityQueue，同样是无界的，同样是出列时才会锁。一把公共的锁实现线程安全。元素需实现Delayed接口，每次调用时需返回当前离触发时间还有多久，小于0表示该触发了。 pull（）时会用peek（）查看队头的元素，检查是否到达触发时间。ScheduledThreadPoolExecutor用了类似的结构。 同步队列SynchronousQueue同步队列本身无容量，放入元素时，比如等待元素被另一条线程的消费者取走再返回。JDK线程池里用它。 JDK7还有个LinkedTransferQueue，在普通线程安全的BlockingQueue的基础上，增加一个transfer（e） 函数，效果与SynchronousQueue一样。 参考文章https://blog.csdn.net/zzw1531439090/article/details/87872424https://blog.csdn.net/weixin_40374341/article/details/86496343https://www.cnblogs.com/uodut/p/7067162.htmlhttps://www.jb51.net/article/135672.htmhttps://www.cnblogs.com/suiyue-/p/6052456.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java集合类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列18：深入理解Java内部类及其实现原理]]></title>
    <url>%2F2019%2F09%2F18%2F18%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%86%85%E9%83%A8%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 内部类初探什么是内部类？ 内部类是指在一个外部类的内部再定义一个类。内部类作为外部类的一个成员，并且依附于外部类而存在的。内部类可为静态，可用protected和private修饰（而外部类只能使用public和缺省的包访问权限）。内部类主要有以下几类：成员内部类、局部内部类、静态内部类、匿名内部类 内部类的共性 (1)内部类仍然是一个独立的类，在编译之后内部类会被编译成独立的.class文件，但是前面冠以外部类的类名和$符号 。 (2)内部类不能用普通的方式访问。 (3)内部类声明成静态的，就不能随便的访问外部类的成员变量了，此时内部类只能访问外部类的静态成员变量 。 (4)外部类不能直接访问内部类的的成员，但可以通过内部类对象来访问 内部类是外部类的一个成员，因此内部类可以自由地访问外部类的成员变量，无论是否是private的。 因为当某个外围类的对象创建内部类的对象时，此内部类会捕获一个隐式引用，它引用了实例化该内部对象的外围类对象。通过这个指针，可以访问外围类对象的全部状态。 通过反编译内部类的字节码，分析之后主要是通过以下几步做到的： 1 编译器自动为内部类添加一个成员变量， 这个成员变量的类型和外部类的类型相同， 这个成员变量就是指向外部类对象的引用； 2 编译器自动为内部类的构造方法添加一个参数， 参数的类型是外部类的类型， 在构造方法内部使用这个参数为1中添加的成员变量赋值； 3 在调用内部类的构造函数初始化内部类对象时， 会默认传入外部类的引用。 使用内部类的好处： 静态内部类的作用： 1 只是为了降低包的深度，方便类的使用，静态内部类适用于包含类当中，但又不依赖与外在的类。 2 由于Java规定静态内部类不能用使用外在类的非静态属性和方法，所以只是为了方便管理类结构而定义。于是我们在创建静态内部类的时候，不需要外部类对象的引用。 非静态内部类的作用： 1 内部类继承自某个类或实现某个接口，内部类的代码操作创建其他外围类的对象。所以你可以认为内部类提供了某种进入其外围类的窗口。 2 使用内部类最吸引人的原因是:每个内部类都能独立地继承自一个(接口的)实现，所以无论外围类是否已经继承了某个(接口的)实现，对于内部类都没有影响 3 如果没有内部类提供的可以继承多个具体的或抽象的类的能力，一些设计与编程问题就很难解决。 从这个角度看，内部类使得多重继承的解决方案变得完整。接口解决了部分问题，而内部类有效地实现了”多重继承”。 那静态内部类与普通内部类有什么区别呢？问得好，区别如下： （1）静态内部类不持有外部类的引用在普通内部类中，我们可以直接访问外部类的属性、方法，即使是private类型也可以访问，这是因为内部类持有一个外部类的引用，可以自由访问。而静态内部类，则只可以访问外部类的静态方法和静态属性（如果是private权限也能访问，这是由其代码位置所决定的），其他则不能访问。 （2）静态内部类不依赖外部类普通内部类与外部类之间是相互依赖的关系，内部类实例不能脱离外部类实例，也就是说它们会同生同死，一起声明，一起被垃圾回收器回收。而静态内部类是可以独立存在的，即使外部类消亡了，静态内部类还是可以存在的。 （3）普通内部类不能声明static的方法和变量普通内部类不能声明static的方法和变量，注意这里说的是变量，常量（也就是final static修饰的属性）还是可以的，而静态内部类形似外部类，没有任何限制。 为什么普通内部类不能有静态变量呢？ 1 成员内部类 之所以叫做成员 就是说他是类实例的一部分 而不是类的一部分 2 结构上来说 他和你声明的成员变量是一样的地位 一个特殊的成员变量 而静态的变量是类的一部分和实例无关 3 你若声明一个成员内部类 让他成为主类的实例一部分 然后又想在内部类声明和实例无关的静态的东西 你让JVM情何以堪啊 4 若想在内部类内声明静态字段 就必须将其内部类本身声明为静态 非静态内部类有一个很大的优点：可以自由使用外部类的所有变量和方法 下面的例子大概地介绍了 1 非静态内部类和静态内部类的区别。 2 不同访问权限的内部类的使用。 3 外部类和它的内部类之间的关系 //本节讨论内部类以及不同访问权限的控制 //内部类只有在使用时才会被加载。 //外部类B public class B{ int i = 1; int j = 1; static int s = 1; static int ss = 1; A a; AA aa; AAA aaa; //内部类A public class A { // static void go () { // // } // static { // // } // static int b = 1;//非静态内部类不能有静态成员变量和静态代码块和静态方法， // 因为内部类在外部类加载时并不会被加载和初始化。 //所以不会进行静态代码的调用 int i = 2;//外部类无法读取内部类的成员，而内部类可以直接访问外部类成员 public void test() { System.out.println(j); j = 2; System.out.println(j); System.out.println(s);//可以访问类的静态成员变量 } public void test2() { AA aa = new AA(); AAA aaa = new AAA(); } } //静态内部类S，可以被外部访问 public static class S { int i = 1;//访问不到非静态变量。 static int s = 0;//可以有静态变量 public static void main(String[] args) { System.out.println(s); } @Test public void test () { // System.out.println(j);//报错，静态内部类不能读取外部类的非静态变量 System.out.println(s); System.out.println(ss); s = 2; ss = 2; System.out.println(s); System.out.println(ss); } } //内部类AA，其实这里加protected相当于default //因为外部类要调用内部类只能通过B。并且无法直接继承AA，所以必须在同包 //的类中才能调用到(这里不考虑静态内部类)，那么就和default一样了。 protected class AA{ int i = 2;//内部类之间不共享变量 public void test (){ A a = new A(); AAA aaa = new AAA(); //内部类之间可以互相访问。 } } //包外部依然无法访问，因为包没有继承关系，所以找不到这个类 protected static class SS{ int i = 2;//内部类之间不共享变量 public void test (){ //内部类之间可以互相访问。 } } //私有内部类A，对外不可见，但对内部类和父类可见 private class AAA { int i = 2;//内部类之间不共享变量 public void test() { A a = new A(); AA aa = new AA(); //内部类之间可以互相访问。 } } @Test public void test(){ A a = new A(); a.test(); //内部类可以修改外部类的成员变量 //打印出 1 2 B b = new B(); } }​ //另一个外部类 class C { @Test public void test() { //首先，其他类内部类只能通过外部类来获取其实例。 B.S s = new B.S(); //静态内部类可以直接通过B类直接获取，不需要B的实例，和静态成员变量类似。 //B.A a = new B.A(); //当A不是静态类时这行代码会报错。 //需要使用B的实例来获取A的实例 B b = new B(); B.A a = b.new A(); B.AA aa = b.new AA();//B和C同包，所以可以访问到AA // B.AAA aaa = b.new AAA();AAA为私有内部类，外部类不可见 //当A使用private修饰时，使用B的实例也无法获取A的实例，这一点和私有变量是一样的。 //所有普通的内部类与类中的一个变量是类似的。静态内部类则与静态成员类似。 } } 内部类的加载可能刚才的例子中没办法直观地看到内部类是如何加载的，接下来用例子展示一下内部类加载的过程。 1 内部类是延时加载的，也就是说只会在第一次使用时加载。不使用就不加载，所以可以很好的实现单例模式。 2 不论是静态内部类还是非静态内部类都是在第一次使用时才会被加载。 3 对于非静态内部类是不能出现静态模块（包含静态块，静态属性，静态方法等） 4 非静态类的使用需要依赖于外部类的对象，详见上述对象innerClass 的初始化。 简单来说，类的加载都是发生在类要被用到的时候。内部类也是一样 1 普通内部类在第一次用到时加载，并且每次实例化时都会执行内部成员变量的初始化，以及代码块和构造方法。 2 静态内部类也是在第一次用到时被加载。但是当它加载完以后就会将静态成员变量初始化，运行静态代码块，并且只执行一次。当然，非静态成员和代码块每次实例化时也会执行。 总结一下Java类代码加载的顺序，万变不离其宗。 规律一、初始化构造时，先父后子；只有在父类所有都构造完后子类才被初始化 规律二、类加载先是静态、后非静态、最后是构造函数。 静态构造块、静态类属性按出现在类定义里面的先后顺序初始化，同理非静态的也是一样的，只是静态的只在加载字节码时执行一次，不管你new多少次，非静态会在new多少次就执行多少次 规律三、java中的类只有在被用到的时候才会被加载 规律四、java类只有在类字节码被加载后才可以被构造成对象实例 成员内部类在方法中定义的内部类称为局部内部类。与局部变量类似，局部内部类不能有访问说明符，因为它不是外围类的一部分，但是它可以访问当前代码块内的常量，和此外围类所有的成员。 需要注意的是：局部内部类只能在定义该内部类的方法内实例化，不可以在此方法外对其实例化。 public class 局部内部类 { class A {//局部内部类就是写在方法里的类，只在方法执行时加载，一次性使用。 public void test() { class B { public void test () { class C { } } } } } @Test public void test () { int i = 1; final int j = 2; class A { @Test public void test () { System.out.println(i); System.out.println(j); } } A a = new A(); System.out.println(a); } static class B { public static void test () { //static class A报错，方法里不能定义静态内部类。 //因为只有在方法调用时才能进行类加载和初始化。 } } }匿名内部类简单地说：匿名内部类就是没有名字的内部类，并且，匿名内部类是局部内部类的一种特殊形式。什么情况下需要使用匿名内部类？如果满足下面的一些条件，使用匿名内部类是比较合适的：只用到类的一个实例。类在定义后马上用到。类非常小（SUN推荐是在4行代码以下）给类命名并不会导致你的代码更容易被理解。在使用匿名内部类时，要记住以下几个原则： 1 匿名内部类不能有构造方法。 2 匿名内部类不能定义任何静态成员、方法和类。 3 匿名内部类不能是public,protected,private,static。 4 只能创建匿名内部类的一个实例。 5 一个匿名内部类一定是在new的后面，用其隐含实现一个接口或实现一个类。 6 因匿名内部类为局部内部类，所以局部内部类的所有限制都对其生效。 一个匿名内部类的例子： public class 匿名内部类 { } interface D{ void run (); } abstract class E{ E (){ } abstract void work(); } class A { @Test public void test (int k) { //利用接口写出一个实现该接口的类的实例。 //有且仅有一个实例，这个类无法重用。 new Runnable() { @Override public void run() { // k = 1;报错，当外部方法中的局部变量在内部类使用中必须改为final类型。 //因为方外部法中即使改变了这个变量也不会反映到内部类中。 //所以对于内部类来讲这只是一个常量。 System.out.println(100); System.out.println(k); } }; new D(){ //实现接口的匿名类 int i =1; @Override public void run() { System.out.println(&quot;run&quot;); System.out.println(i); System.out.println(k); } }.run(); new E(){ //继承抽象类的匿名类 int i = 1; void run (int j) { j = 1; } @Override void work() { } }; } }匿名内部类里的final使用的形参为何要为final 参考文件：http://android.blog.51cto.com/268543/384844 我们给匿名内部类传递参数的时候，若该形参在内部类中需要被使用，那么该形参必须要为final。也就是说：当所在的方法的形参需要被内部类里面使用时，该形参必须为final。 为什么必须要为final呢？ 首先我们知道在内部类编译成功后，它会产生一个class文件，该class文件与外部类并不是同一class文件，仅仅只保留对外部类的引用。当外部类传入的参数需要被内部类调用时，从java程序的角度来看是直接被调用： public class OuterClass { public void display(final String name,String age){ class InnerClass{ void display(){ System.out.println(name); } } } }从上面代码中看好像name参数应该是被内部类直接调用？其实不然，在java编译之后实际的操作如下： public class OuterClass$InnerClass { public InnerClass(String name,String age){ this.InnerClass$name = name; this.InnerClass$age = age; }​ public void display(){ System.out.println(this.InnerClass$name + “—-“ + this.InnerClass$age ); } } 所以从上面代码来看，内部类并不是直接调用方法传递的参数，而是利用自身的构造器对传入的参数进行备份，自己内部方法调用的实际上时自己的属性而不是外部方法传递进来的参数。 直到这里还没有解释为什么是final 在内部类中的属性和外部方法的参数两者从外表上看是同一个东西，但实际上却不是，所以他们两者是可以任意变化的，也就是说在内部类中我对属性的改变并不会影响到外部的形参，而然这从程序员的角度来看这是不可行的。 毕竟站在程序的角度来看这两个根本就是同一个，如果内部类该变了，而外部方法的形参却没有改变这是难以理解和不可接受的，所以为了保持参数的一致性，就规定使用final来避免形参的不改变。 简单理解就是，拷贝引用，为了避免引用值发生改变，例如被外部类的方法修改等，而导致内部类得到的值不一致，于是用final来让该引用不可改变。 故如果定义了一个匿名内部类，并且希望它使用一个其外部定义的参数，那么编译器会要求该参数引用是final的。 内部类初始化我们一般都是利用构造器来完成某个实例的初始化工作的，但是匿名内部类是没有构造器的！那怎么来初始化匿名内部类呢？使用构造代码块！利用构造代码块能够达到为匿名内部类创建一个构造器的效果。 public class OutClass { public InnerClass getInnerClass(final int age,final String name){ return new InnerClass() { int age_ ; String name_; //构造代码块完成初始化工作 { if(0 &lt; age &amp;&amp; age &lt; 200){ age_ = age; name_ = name; } } public String getName() { return name_; } public int getAge() { return age_; } }; }​ 内部类的重载 如果你创建了一个内部类，然后继承其外围类并重新定义此内部类时，会发生什么呢？也就是说，内部类可以被重载吗？这看起来似乎是个很有用的点子，但是“重载”内部类就好像它是外围类的一个方法，其实并不起什么作用： class Egg { private Yolk y; protected class Yolk { public Yolk() { System.out.println(&quot;Egg.Yolk()&quot;); } } public Egg() { System.out.println(&quot;New Egg()&quot;); y = new Yolk(); } } public class BigEgg extends Egg { public class Yolk { public Yolk() { System.out.println(&quot;BigEgg.Yolk()&quot;); } } public static void main(String[] args) { new BigEgg(); } } 复制代码 输出结果为： New Egg() Egg.Yolk()缺省的构造器是编译器自动生成的，这里是调用基类的缺省构造器。你可能认为既然创建了BigEgg 的对象，那么所使用的应该是被“重载”过的Yolk，但你可以从输出中看到实际情况并不是这样的。这个例子说明，当你继承了某个外围类的时候，内部类并没有发生什么特别神奇的变化。这两个内部类是完全独立的两个实体，各自在自己的命名空间内。 内部类的继承因为内部类的构造器要用到其外围类对象的引用，所以在你继承一个内部类的时候，事情变得有点复杂。问题在于，那个“秘密的”外围类对象的引用必须被初始化，而在被继承的类中并不存在要联接的缺省对象。要解决这个问题，需使用专门的语法来明确说清它们之间的关联： class WithInner { class Inner { Inner(){ System.out.println(&quot;this is a constructor in WithInner.Inner&quot;); }; } } public class InheritInner extends WithInner.Inner { // ! InheritInner() {} // Won&apos;t compile InheritInner(WithInner wi) { wi.super(); System.out.println(&quot;this is a constructor in InheritInner&quot;); } public static void main(String[] args) { WithInner wi = new WithInner(); InheritInner ii = new InheritInner(wi); } }复制代码输出结果为：this is a constructor in WithInner.Innerthis is a constructor in InheritInner 可以看到，InheritInner 只继承自内部类，而不是外围类。但是当要生成一个构造器时，缺省的构造器并不算好，而且你不能只是传递一个指向外围类对象的引用。此外，你必须在构造器内使用如下语法：enclosingClassReference.super();这样才提供了必要的引用，然后程序才能编译通过。 有关匿名内部类实现回调，事件驱动，委托等机制的文章将在下一节讲述。 Java内部类的实现原理内部类为什么能够访问外部类的成员？ 定义内部类如下： 使用javap命令进行反编译。 编译后得到Main.class Main$Inner.class两个文件，反编译Main$Inner.class文件如下： 可以看到，内部类其实拥有外部类的一个引用，在构造函数中将外部类的引用传递进来。 匿名内部类为什么只能访问局部的final变量？ 其实可以这样想，当方法执行完毕后，局部变量的生命周期就结束了，而局部内部类对象的生命周期可能还没有结束，那么在局部内部类中访问局部变量就不可能了，所以将局部变量改为final，改变其生命周期。 编写代码如下： 这段代码编译为Main.class Main$1.class两个文件，反编译Main$1.class文件如下： 可以看到，java将编译时已经确定的值直接复制，进行替换，将无法确定的值放到了内部类的常量池中，并在构造函数中将其从常量池取出到字段中。 可以看出，java将局部变量m直接进行复制，所以其并不是原来的值，若在内部类中将m更改，局部变量的m值不会变，就会出现数据不一致，所以java就将其限制为final，使其不能进行更改，这样数据不一致的问题就解决了。 参考文章https://www.cnblogs.com/hujingnb/p/10181621.htmlhttps://blog.csdn.net/codingtu/article/details/79336026https://www.cnblogs.com/woshimrf/p/java-inner-class.htmlhttps://www.cnblogs.com/dengchengchao/p/9713979.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java内部类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列17：一文搞懂Java多线程使用方式、实现原理以及常见面试题]]></title>
    <url>%2F2019%2F09%2F17%2F17%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java中的线程Java之父对线程的定义是： 线程是一个独立执行的调用序列，同一个进程的线程在同一时刻共享一些系统资源（比如文件句柄等）也能访问同一个进程所创建的对象资源（内存资源）。java.lang.Thread对象负责统计和控制这种行为。 每个程序都至少拥有一个线程-即作为Java虚拟机(JVM)启动参数运行在主类main方法的线程。在Java虚拟机初始化过程中也可能启动其他的后台线程。这种线程的数目和种类因JVM的实现而异。然而所有用户级线程都是显式被构造并在主线程或者是其他用户线程中被启动。 本文主要讲了java中多线程的使用方法、线程同步、线程数据传递、线程状态及相应的一些线程函数用法、概述等。在这之前，首先让我们来了解下在操作系统中进程和线程的区别： 进程：每个进程都有独立的代码和数据空间（进程上下文），进程间的切换会有较大的开销，一个进程包含1--n个线程。（进程是资源分配的最小单位） 线程：同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC)，线程切换开销小。（线程是cpu调度的最小单位） 线程和进程一样分为五个阶段：创建、就绪、运行、阻塞、终止。 多进程是指操作系统能同时运行多个任务（程序）。 多线程是指在同一程序中有多个顺序流在执行。 在java中要想实现多线程，有两种手段，一种是继续Thread类，另外一种是实现Runable接口.(其实准确来讲，应该有三种，还有一种是实现Callable接口，并与Future、线程池结合使用Java线程状态机Java 给多线程编程提供了内置的支持。 一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 多线程是多任务的一种特别的形式，但多线程使用了更小的资源开销。 这里定义和线程相关的另一个术语 - 进程：一个进程包括由操作系统分配的内存空间，包含一个或多个线程。一个线程不能独立的存在，它必须是进程的一部分。一个进程一直运行，直到所有的非守护线程都结束运行后才能结束。 多线程能满足程序员编写高效率的程序来达到充分利用 CPU 的目的。 一个线程的生命周期线程是一个动态执行的过程，它也有一个从产生到死亡的过程。 下图显示了一个线程完整的生命周期。 新建状态: 使用 new 关键字和 Thread 类或其子类建立一个线程对象后，该线程对象就处于新建状态。它保持这个状态直到程序 start() 这个线程。 就绪状态: 当线程对象调用了start()方法之后，该线程就进入就绪状态。就绪状态的线程处于就绪队列中，要等待JVM里线程调度器的调度。 运行状态: 如果就绪状态的线程获取 CPU 资源，就可以执行 run()，此时线程便处于运行状态。处于运行状态的线程最为复杂，它可以变为阻塞状态、就绪状态和死亡状态。 阻塞状态: 如果一个线程执行了sleep（睡眠）、suspend（挂起）等方法，失去所占用资源之后，该线程就从运行状态进入阻塞状态。在睡眠时间已到或获得设备资源后可以重新进入就绪状态。可以分为三种： 等待阻塞：运行状态中的线程执行 wait() 方法，使线程进入到等待阻塞状态。 同步阻塞：线程在获取 synchronized 同步锁失败(因为同步锁被其他线程占用)。 其他阻塞：通过调用线程的 sleep() 或 join() 发出了 I/O 请求时，线程就会进入到阻塞状态。当sleep() 状态超时，join() 等待线程终止或超时，或者 I/O 处理完毕，线程重新转入就绪状态。 死亡状态: 一个运行状态的线程完成任务或者其他终止条件发生时，该线程就切换到终止状态。 Java多线程实战多线程的实现public class 多线程实例 { //继承thread @Test public void test1() { class A extends Thread { @Override public void run() { System.out.println(&quot;A run&quot;); } } A a = new A(); a.start(); } //实现Runnable @Test public void test2() { class B implements Runnable { @Override public void run() { System.out.println(&quot;B run&quot;); } } B b = new B(); //Runable实现类需要由Thread类包装后才能执行 new Thread(b).start(); } //有返回值的线程 @Test public void test3() { Callable callable = new Callable() { int sum = 0; @Override public Object call() throws Exception { for (int i = 0;i &lt; 5;i ++) { sum += i; } return sum; } }; //这里要用FutureTask，否则不能加入Thread构造方法 FutureTask futureTask = new FutureTask(callable); new Thread(futureTask).start(); try { System.out.println(futureTask.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } //线程池实现 @Test public void test4() { ExecutorService executorService = Executors.newFixedThreadPool(5); //execute直接执行线程 executorService.execute(new Thread()); executorService.execute(new Runnable() { @Override public void run() { System.out.println(&quot;runnable&quot;); } }); //submit提交有返回结果的任务，运行完后返回结果。 Future future = executorService.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { return &quot;a&quot;; } }); try { System.out.println(future.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); //有返回值的线程组将返回值存进集合 for (int i = 0;i &lt; 5;i ++ ) { int finalI = i; Future future1 = executorService.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { return &quot;res&quot; + finalI; } }); try { list.add((String) future1.get()); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } for (String s : list) { System.out.println(s); } }} 线程状态转换public class 线程的状态转换 { //一开始线程是init状态，结束时是terminated状态 class t implements Runnable { private String name; public t(String name) { this.name = name; } @Override public void run() { System.out.println(name + &quot;run&quot;); } } //测试join，父线程在子线程运行时进入waiting状态 @Test public void test1() throws InterruptedException { Thread dad = new Thread(new Runnable() { Thread son = new Thread(new t(&quot;son&quot;)); @Override public void run() { System.out.println(&quot;dad init&quot;); son.start(); try { //保证子线程运行完再运行父线程 son.join(); System.out.println(&quot;dad run&quot;); } catch (InterruptedException e) { e.printStackTrace(); } } }); //调用start，线程进入runnable状态，等待系统调度 dad.start(); //在父线程中对子线程实例使用join，保证子线程在父线程之前执行完 } //测试sleep @Test public void test2(){ Thread t1 = new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;t1 run&quot;); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } }); //主线程休眠。进入time waiting状态 try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } t1.start(); } //线程2进入blocked状态。 public static void main(String[] args) { test4(); Thread.yield();//进入runnable状态 } //测试blocked状态 public static void test4() { class A { //线程1获得实例锁以后线程2无法获得实例锁，所以进入blocked状态 synchronized void run() { while (true) { System.out.println(&quot;run&quot;); } } } A a = new A(); new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;t1 get lock&quot;); a.run(); } }).start(); new Thread(new Runnable() { @Override public void run() { System.out.println(&quot;t2 get lock&quot;); a.run(); } }).start(); } //volatile保证线程可见性 volatile static int flag = 1; //object作为锁对象，用于线程使用wait和notify方法 volatile static Object o = new Object(); //测试wait和notify //wait后进入waiting状态，被notify进入blocked（阻塞等待锁释放）或者runnable状态（获取到锁） public void test5() { new Thread(new Runnable() { @Override public void run() { //wait和notify只能在同步代码块内使用 synchronized (o) { while (true) { if (flag == 0) { try { Thread.sleep(2000); System.out.println(&quot;thread1 wait&quot;); //释放锁，线程挂起进入object的等待队列，后续代码运行 o.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(&quot;thread1 run&quot;); System.out.println(&quot;notify t2&quot;); flag = 0; //通知等待队列的一个线程获取锁 o.notify(); } } } }).start(); //解释同上 new Thread(new Runnable() { @Override public void run() { while (true) { synchronized (o) { if (flag == 1) { try { Thread.sleep(2000); System.out.println(&quot;thread2 wait&quot;); o.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(&quot;thread2 run&quot;); System.out.println(&quot;notify t1&quot;); flag = 1; o.notify(); } } } }).start(); } //输出结果是 // thread1 run // notify t2 // thread1 wait // thread2 run // notify t1 // thread2 wait // thread1 run // notify t2 //不断循环 }Java Thread常用方法Thread#yield()：执行此方法会向系统线程调度器（Schelduler）发出一个暗示，告诉其当前JAVA线程打算放弃对CPU的使用，但该暗示，有可能被调度器忽略。使用该方法，可以防止线程对CPU的过度使用，提高系统性能。 Thread#sleep(time)或Thread.sleep(time, nanos)： 使当前线程进入休眠阶段，状态变为：TIME_WAITING Thread.interrupt()：中断当前线程的执行，允许当前线程对自身进行中断，否则将会校验调用方线程是否有对该线程的权限。 如果当前线程因被调用Object#wait(),Object#wait(long, int), 或者线程本身的join(), join(long),sleep()处于阻塞状态中，此时调用interrupt方法会使抛出InterruptedException，而且线程的阻塞状态将会被清除。 Thread#interrupted()，返回true或者false：查看当前线程是否处于中断状态，这个方法比较特殊之处在于，如果调用成功，会将当前线程的interrupt status清除。所以如果连续2次调用该方法，第二次将返回false。 Thread.isInterrupted()，返回true或者false：与上面方法相同的地方在于，该方法返回当前线程的中断状态。不同的地方在于，它不会清除当前线程的interrupt status状态。 Thread#join()，Thread#join(time)：A线程调用B线程的join()方法，将会使A等待B执行，直到B线程终止。如果传入time参数，将会使A等待B执行time的时间，如果time时间到达，将会切换进A线程，继续执行A线程。 构造方法和守护线程构造方法 Thread类中不同的构造方法接受如下参数的不同组合： 一个Runnable对象，这种情况下，Thread.start方法将会调用对应Runnable对象的run方法。如果没有提供Runnable对象，那么就会立即得到一个Thread.run的默认实现。 一个作为线程标识名的String字符串，该标识在跟踪和调试过程中会非常有用，除此别无它用。 线程组（ThreadGroup），用来放置新创建的线程，如果提供的ThreadGroup不允许被访问，那么就会抛出一个SecurityException 。 Thread对象拥有一个守护(daemon)标识属性，这个属性无法在构造方法中被赋值，但是可以在线程启动之前设置该属性(通过setDaemon方法)。 当程序中所有的非守护线程都已经终止，调用setDaemon方法可能会导致虚拟机粗暴的终止线程并退出。 isDaemon方法能够返回该属性的值。守护状态的作用非常有限，即使是后台线程在程序退出的时候也经常需要做一些清理工作。 （daemon的发音为”day-mon”,这是系统编程传统的遗留，系统守护进程是一个持续运行的进程，比如打印机队列管理，它总是在系统中运行。）启动线程的方式和isAlive方法启动线程调用start方法会触发Thread实例以一个新的线程启动其run方法。新线程不会持有调用线程的任何同步锁。 当一个线程正常地运行结束或者抛出某种未检测的异常（比如，运行时异常(RuntimeException)，错误(ERROR) 或者其子类）线程就会终止。 当线程终止之后，是不能被重新启动的。在同一个Thread上调用多次start方法会抛出InvalidThreadStateException异常。 如果线程已经启动但是还没有终止，那么调用isAlive方法就会返回true.即使线程由于某些原因处于阻塞(Blocked)状态该方法依然返回true。 如果线程已经被取消(cancelled),那么调用其isAlive在什么时候返回false就因各Java虚拟机的实现而异了。没有方法可以得知一个处于非活动状态的线程是否已经被启动过了。 Java多线程优先级Java的线程实现基本上都是内核级线程的实现，所以Java线程的具体执行还取决于操作系统的特性。 Java虚拟机为了实现跨平台(不同的硬件平台和各种操作系统)的特性，Java语言在线程调度与调度公平性上未作出任何的承诺，甚至都不会严格保证线程会被执行。但是Java线程却支持优先级的方法，这些方法会影响线程的调度： 每个线程都有一个优先级，分布在Thread.MIN_PRIORITY和Thread.MAX_PRIORITY之间（分别为1和10）默认情况下，新创建的线程都拥有和创建它的线程相同的优先级。main方法所关联的初始化线程拥有一个默认的优先级，这个优先级是Thread.NORM_PRIORITY (5). 线程的当前优先级可以通过getPriority方法获得。线程的优先级可以通过setPriority方法来动态的修改，一个线程的最高优先级由其所在的线程组限定。 Java多线程面试题这篇文章主要是对多线程的问题进行总结的，因此罗列了40个多线程的问题。 这些多线程的问题，有些来源于各大网站、有些来源于自己的思考。可能有些问题网上有、可能有些问题对应的答案也有、也可能有些各位网友也都看过，但是本文写作的重心就是所有的问题都会按照自己的理解回答一遍，不会去看网上的答案，因此可能有些问题讲的不对，能指正的希望大家不吝指教。 1、多线程有什么用？ 一个可能在很多人看来很扯淡的一个问题：我会用多线程就好了，还管它有什么用？在我看来，这个回答更扯淡。所谓”知其然知其所以然”，”会用”只是”知其然”，”为什么用”才是”知其所以然”，只有达到”知其然知其所以然”的程度才可以说是把一个知识点运用自如。OK，下面说说我对这个问题的看法： 1）发挥多核CPU的优势 随着工业的进步，现在的笔记本、台式机乃至商用的应用服务器至少也都是双核的，4核、8核甚至16核的也都不少见，如果是单线程的程序，那么在双核CPU上就浪费了50%，在4核CPU上就浪费了75%。单核CPU上所谓的”多线程”那是假的多线程，同一时间处理器只会处理一段逻辑，只不过线程之间切换得比较快，看着像多个线程”同时”运行罢了。多核CPU上的多线程才是真正的多线程，它能让你的多段逻辑同时工作，多线程，可以真正发挥出多核CPU的优势来，达到充分利用CPU的目的。 2）防止阻塞 从程序运行效率的角度来看，单核CPU不但不会发挥出多线程的优势，反而会因为在单核CPU上运行多线程导致线程上下文的切换，而降低程序整体的效率。但是单核CPU我们还是要应用多线程，就是为了防止阻塞。试想，如果单核CPU使用单线程，那么只要这个线程阻塞了，比方说远程读取某个数据吧，对端迟迟未返回又没有设置超时时间，那么你的整个程序在数据返回回来之前就停止运行了。多线程可以防止这个问题，多条线程同时运行，哪怕一条线程的代码执行读取数据阻塞，也不会影响其它任务的执行。 3）便于建模 这是另外一个没有这么明显的优点了。假设有一个大的任务A，单线程编程，那么就要考虑很多，建立整个程序模型比较麻烦。但是如果把这个大的任务A分解成几个小任务，任务B、任务C、任务D，分别建立程序模型，并通过多线程分别运行这几个任务，那就简单很多了。 2、创建线程的方式 比较常见的一个问题了，一般就是两种： 1）继承Thread类 2）实现Runnable接口 至于哪个好，不用说肯定是后者好，因为实现接口的方式比继承类的方式更灵活，也能减少程序之间的耦合度，面向接口编程也是设计模式6大原则的核心。 3、start()方法和run()方法的区别 只有调用了start()方法，才会表现出多线程的特性，不同线程的run()方法里面的代码交替执行。如果只是调用run()方法，那么代码还是同步执行的，必须等待一个线程的run()方法里面的代码全部执行完毕之后，另外一个线程才可以执行其run()方法里面的代码。 4、Runnable接口和Callable接口的区别 有点深的问题了，也看出一个Java程序员学习知识的广度。 Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已；Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 这其实是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，真的是非常有用。 5、CyclicBarrier和CountDownLatch的区别 两个看上去有点像的类，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： 1）CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行。 2）CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务。 3) CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了。 6、volatile关键字的作用 一个非常重要的问题，是每个学习、应用多线程的Java程序员都必须掌握的。理解volatile关键字的作用的前提是要理解Java内存模型，这里就不讲Java内存模型了，可以参见第31点，volatile关键字的作用主要有两个： 1）多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据。 2）代码底层执行不像我们看到的高级语言—-Java程序这么简单，它的执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率。 从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger，更多详情请点击这里进行学习。 7、什么是线程安全 又是一个理论的问题，各式各样的答案有很多，我给出一个个人认为解释地最好的：如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么你的代码就是线程安全的。 这个问题有值得一提的地方，就是线程安全也是有几个级别的： 1）不可变 像String、Integer、Long这些，都是final类型的类，任何一个线程都改变不了它们的值，要改变除非新创建一个，因此这些不可变对象不需要任何同步手段就可以直接在多线程环境下使用 2）绝对线程安全 不管运行时环境如何，调用者都不需要额外的同步措施。要做到这一点通常需要付出许多额外的代价，Java中标注自己是线程安全的类，实际上绝大多数都不是线程安全的，不过绝对线程安全的类，Java中也有，比方说CopyOnWriteArrayList、CopyOnWriteArraySet 3）相对线程安全 相对线程安全也就是我们通常意义上所说的线程安全，像Vector这种，add、remove方法都是原子操作，不会被打断，但也仅限于此，如果有个线程在遍历某个Vector、有个线程同时在add这个Vector，99%的情况下都会出现ConcurrentModificationException，也就是fail-fast机制。 4）线程非安全 这个就没什么好说的了，ArrayList、LinkedList、HashMap等都是线程非安全的类，点击这里了解为什么不安全。 8、Java中如何获取到线程dump文件 死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： 1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java 2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈。 9、一个线程如果出现了运行时异常会怎么样 如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放 10、如何在两个线程之间共享数据 通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 11、sleep方法和wait方法有什么区别 这个问题常问，sleep方法和wait方法都可以用来放弃CPU一定的时间，不同点在于如果线程持有某个对象的监视器，sleep方法不会放弃这个对象的监视器，wait方法会放弃这个对象的监视器 12、生产者消费者模型的作用是什么 这个问题很理论，但是很重要： 1）通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用 2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 13、ThreadLocal有什么用 简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了 14、为什么wait()方法和notify()/notifyAll()方法要在同步块中被调用 这是JDK强制的，wait()方法和notify()/notifyAll()方法在调用前都必须先获得对象的锁 15、wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别 wait()方法和notify()/notifyAll()方法在放弃对象监视器的时候的区别在于：wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器。 16、为什么要使用线程池 避免频繁地创建和销毁线程，达到线程对象的重用。另外，使用线程池还可以根据项目灵活地控制并发的数目。点击这里学习线程池详解。 17、怎么唤醒一个阻塞的线程 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 18、不可变对象对多线程有什么帮助 前面有提到过的一个问题，不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 19、什么是多线程的上下文切换 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 20、线程类的构造方法、静态块是被哪个线程调用的 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被new这个线程类所在的线程所调用的，而run方法里面的代码才是被线程自身所调用的。 如果说上面的说法让你感到困惑，那么我举个例子，假设Thread2中new了Thread1，main函数中new了Thread2，那么： 1）Thread2的构造方法、静态块是main线程调用的，Thread2的run()方法是Thread2自己调用的 2）Thread1的构造方法、静态块是Thread2调用的，Thread1的run()方法是Thread1自己调用的 21、高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ 这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是： 1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 c）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考其他有关线程池的文章。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。 参考文章https://blog.csdn.net/zl1zl2zl3/article/details/81868173https://www.runoob.com/java/java-multithreading.htmlhttps://blog.csdn.net/qq_38038480/article/details/80584715https://blog.csdn.net/tongxuexie/article/details/80145663https://www.cnblogs.com/snow-flower/p/6114765.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列16：一文读懂Java IO流和常见面试题]]></title>
    <url>%2F2019%2F09%2F16%2F16%E3%80%81JavaIO%E6%B5%81%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文参考 并发编程网 – ifeve.com IO概述 在这一小节，我会试着给出Java IO(java.io)包下所有类的概述。更具体地说，我会根据类的用途对类进行分组。这个分组将会使你在未来的工作中，进行类的用途判定时，或者是为某个特定用途选择类时变得更加容易。 ​输入和输出 术语“输入”和“输出”有时候会有一点让人疑惑。一个应用程序的输入往往是另外一个应用程序的输出 那么OutputStream流到底是一个输出到目的地的流呢，还是一个产生输出的流？InputStream流到底会不会输出它的数据给读取数据的程序呢？就我个人而言，在第一天学习Java IO的时候我就感觉到了一丝疑惑。 为了消除这个疑惑，我试着给输入和输出起一些不一样的别名，让它们从概念上与数据的来源和数据的流向相联系。Java的IO包主要关注的是从原始数据源的读取以及输出原始数据到目标媒介。以下是最典型的数据源和目标媒介： 文件 管道 网络连接 内存缓存 System.in, System.out, System.error(注：Java标准输入、输出、错误输出)下面这张图描绘了一个程序从数据源读取数据，然后将数据输出到其他媒介的原理： 流 在Java IO中，流是一个核心的概念。流从概念上来说是一个连续的数据流。你既可以从流中读取数据，也可以往流中写数据。流与数据源或者数据流向的媒介相关联。在Java IO中流既可以是字节流(以字节为单位进行读写)，也可以是字符流(以字符为单位进行读写)。类InputStream, OutputStream, Reader 和Writer一个程序需要InputStream或者Reader从数据源读取数据，需要OutputStream或者Writer将数据写入到目标媒介中。以下的图说明了这一点： InputStream和Reader与数据源相关联，OutputStream和writer与目标媒介相关联。 Java IO的用途和特征 Java IO中包含了许多InputStream、OutputStream、Reader、Writer的子类。这样设计的原因是让每一个类都负责不同的功能。这也就是为什么IO包中有这么多不同的类的缘故。各类用途汇总如下： 文件访问 网络访问 内存缓存访问 线程内部通信(管道) 缓冲 过滤 解析 读写文本 (Readers / Writers) 读写基本类型数据 (long, int etc.) 读写对象当通读过Java IO类的源代码之后，我们很容易就能了解这些用途。这些用途或多或少让我们更加容易地理解，不同的类用于针对不同业务场景。 Java IO类概述表已经讨论了数据源、目标媒介、输入、输出和各类不同用途的Java IO类，接下来是一张通过输入、输出、基于字节或者字符、以及其他比如缓冲、解析之类的特定用途划分的大部分Java IO类的表格。 Java IO类图 什么是Java IO流Java IO流是既可以从中读取，也可以写入到其中的数据流。正如这个系列教程之前提到过的，流通常会与数据源、数据流向目的地相关联，比如文件、网络等等。 流和数组不一样，不能通过索引读写数据。在流中，你也不能像数组那样前后移动读取数据，除非使用RandomAccessFile 处理文件。流仅仅只是一个连续的数据流。 某些类似PushbackInputStream 流的实现允许你将数据重新推回到流中，以便重新读取。然而你只能把有限的数据推回流中，并且你不能像操作数组那样随意读取数据。流中的数据只能够顺序访问。 Java IO流通常是基于字节或者基于字符的。字节流通常以“stream”命名，比如InputStream和OutputStream。除了DataInputStream 和DataOutputStream 还能够读写int, long, float和double类型的值以外，其他流在一个操作时间内只能读取或者写入一个原始字节。 字符流通常以“Reader”或者“Writer”命名。字符流能够读写字符(比如Latin1或者Unicode字符)。可以浏览Java Readers and Writers获取更多关于字符流输入输出的信息。 InputStream java.io.InputStream类是所有Java IO输入流的基类。如果你正在开发一个从流中读取数据的组件，请尝试用InputStream替代任何它的子类(比如FileInputStream)进行开发。这么做能够让你的代码兼容任何类型而非某种确定类型的输入流。 组合流 你可以将流整合起来以便实现更高级的输入和输出操作。比如，一次读取一个字节是很慢的，所以可以从磁盘中一次读取一大块数据，然后从读到的数据块中获取字节。为了实现缓冲，可以把InputStream包装到BufferedInputStream中。 代码示例 InputStream input = new BufferedInputStream(new FileInputStream(“c:\data\input-file.txt”)); 缓冲同样可以应用到OutputStream中。你可以实现将大块数据批量地写入到磁盘(或者相应的流)中，这个功能由BufferedOutputStream实现。 缓冲只是通过流整合实现的其中一个效果。你可以把InputStream包装到PushbackInputStream中，之后可以将读取过的数据推回到流中重新读取，在解析过程中有时候这样做很方便。或者，你可以将两个InputStream整合成一个SequenceInputStream。 将不同的流整合到一个链中，可以实现更多种高级操作。通过编写包装了标准流的类，可以实现你想要的效果和过滤器。 IO文件在Java应用程序中，文件是一种常用的数据源或者存储数据的媒介。所以这一小节将会对Java中文件的使用做一个简短的概述。这篇文章不会对每一个技术细节都做出解释，而是会针对文件存取的方法提供给你一些必要的知识点。在之后的文章中，将会更加详细地描述这些方法或者类，包括方法示例等等。 通过Java IO读文件 如果你需要在不同端之间读取文件，你可以根据该文件是二进制文件还是文本文件来选择使用FileInputStream或者FileReader。 这两个类允许你从文件开始到文件末尾一次读取一个字节或者字符，或者将读取到的字节写入到字节数组或者字符数组。你不必一次性读取整个文件，相反你可以按顺序地读取文件中的字节和字符。如果你需要跳跃式地读取文件其中的某些部分，可以使用RandomAccessFile。 通过Java IO写文件 如果你需要在不同端之间进行文件的写入，你可以根据你要写入的数据是二进制型数据还是字符型数据选用FileOutputStream或者FileWriter。 你可以一次写入一个字节或者字符到文件中，也可以直接写入一个字节数组或者字符数据。数据按照写入的顺序存储在文件当中。通过Java IO随机存取文件 正如我所提到的，你可以通过RandomAccessFile对文件进行随机存取。 随机存取并不意味着你可以在真正随机的位置进行读写操作，它只是意味着你可以跳过文件中某些部分进行操作，并且支持同时读写，不要求特定的存取顺序。 这使得RandomAccessFile可以覆盖一个文件的某些部分、或者追加内容到它的末尾、或者删除它的某些内容，当然它也可以从文件的任何位置开始读取文件。下面是具体例子： @Test //文件流范例，打开一个文件的输入流，读取到字节数组，再写入另一个文件的输出流 public void test1() { try { FileInputStream fileInputStream = new FileInputStream(new File(&quot;a.txt&quot;)); FileOutputStream fileOutputStream = new FileOutputStream(new File(&quot;b.txt&quot;)); byte []buffer = new byte[128]; while (fileInputStream.read(buffer) != -1) { fileOutputStream.write(buffer); } //随机读写，通过mode参数来决定读或者写 RandomAccessFile randomAccessFile = new RandomAccessFile(new File(&quot;c.txt&quot;), &quot;rw&quot;); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } }字符流和字节流Java IO的Reader和Writer除了基于字符之外，其他方面都与InputStream和OutputStream非常类似。他们被用于读写文本。InputStream和OutputStream是基于字节的，还记得吗？ ReaderReader类是Java IO中所有Reader的基类。子类包括BufferedReader，PushbackReader，InputStreamReader，StringReader和其他Reader。 WriterWriter类是Java IO中所有Writer的基类。子类包括BufferedWriter和PrintWriter等等。 这是一个简单的Java IO Reader的例子： Reader reader = new FileReader(&quot;c:\\data\\myfile.txt&quot;); int data = reader.read(); while(data != -1){ char dataChar = (char) data; data = reader.read(); }你通常会使用Reader的子类，而不会直接使用Reader。Reader的子类包括InputStreamReader，CharArrayReader，FileReader等等。可以查看Java IO概述浏览完整的Reader表格。 整合Reader与InputStream 一个Reader可以和一个InputStream相结合。如果你有一个InputStream输入流，并且想从其中读取字符，可以把这个InputStream包装到InputStreamReader中。把InputStream传递到InputStreamReader的构造函数中： Reader reader = new InputStreamReader(inputStream);在构造函数中可以指定解码方式。 Writer Writer类是Java IO中所有Writer的基类。子类包括BufferedWriter和PrintWriter等等。这是一个Java IO Writer的例子： Writer writer = new FileWriter(&quot;c:\\data\\file-output.txt&quot;); writer.write(&quot;Hello World Writer&quot;); writer.close();同样，你最好使用Writer的子类，不需要直接使用Writer，因为子类的实现更加明确，更能表现你的意图。常用子类包括OutputStreamWriter，CharArrayWriter，FileWriter等。Writer的write(int c)方法，会将传入参数的低16位写入到Writer中，忽略高16位的数据。 整合Writer和OutputStream 与Reader和InputStream类似，一个Writer可以和一个OutputStream相结合。把OutputStream包装到OutputStreamWriter中，所有写入到OutputStreamWriter的字符都将会传递给OutputStream。这是一个OutputStreamWriter的例子： Writer writer = new OutputStreamWriter(outputStream);IO管道Java IO中的管道为运行在同一个JVM中的两个线程提供了通信的能力。所以管道也可以作为数据源以及目标媒介。 你不能利用管道与不同的JVM中的线程通信(不同的进程)。在概念上，Java的管道不同于Unix/Linux系统中的管道。在Unix/Linux中，运行在不同地址空间的两个进程可以通过管道通信。在Java中，通信的双方应该是运行在同一进程中的不同线程。 通过Java IO创建管道 可以通过Java IO中的PipedOutputStream和PipedInputStream创建管道。一个PipedInputStream流应该和一个PipedOutputStream流相关联。 一个线程通过PipedOutputStream写入的数据可以被另一个线程通过相关联的PipedInputStream读取出来。Java IO管道示例这是一个如何将PipedInputStream和PipedOutputStream关联起来的简单例子： //使用管道来完成两个线程间的数据点对点传递 @Test public void test2() throws IOException { PipedInputStream pipedInputStream = new PipedInputStream(); PipedOutputStream pipedOutputStream = new PipedOutputStream(pipedInputStream); new Thread(new Runnable() { @Override public void run() { try { pipedOutputStream.write(&quot;hello input&quot;.getBytes()); pipedOutputStream.close(); } catch (IOException e) { e.printStackTrace(); } } }).start(); new Thread(new Runnable() { @Override public void run() { try { byte []arr = new byte[128]; while (pipedInputStream.read(arr) != -1) { System.out.println(Arrays.toString(arr)); } pipedInputStream.close(); } catch (IOException e) { e.printStackTrace(); } } }).start();管道和线程请记得，当使用两个相关联的管道流时，务必将它们分配给不同的线程。read()方法和write()方法调用时会导致流阻塞，这意味着如果你尝试在一个线程中同时进行读和写，可能会导致线程死锁。 管道的替代除了管道之外，一个JVM中不同线程之间还有许多通信的方式。实际上，线程在大多数情况下会传递完整的对象信息而非原始的字节数据。但是，如果你需要在线程之间传递字节数据，Java IO的管道是一个不错的选择。 Java IO：网络Java中网络的内容或多或少的超出了Java IO的范畴。关于Java网络更多的是在我的Java网络教程中探讨。但是既然网络是一个常见的数据来源以及数据流目的地，并且因为你使用Java IO的API通过网络连接进行通信，所以本文将简要的涉及网络应用。 当两个进程之间建立了网络连接之后，他们通信的方式如同操作文件一样：利用InputStream读取数据，利用OutputStream写入数据。换句话来说，Java网络API用来在不同进程之间建立网络连接，而Java IO则用来在建立了连接之后的进程之间交换数据。 基本上意味着如果你有一份能够对文件进行写入某些数据的代码，那么这些数据也可以很容易地写入到网络连接中去。你所需要做的仅仅只是在代码中利用OutputStream替代FileOutputStream进行数据的写入。因为FileOutputStream是OuputStream的子类，所以这么做并没有什么问题。 //从网络中读取字节流也可以直接使用OutputStream public void test3() { //读取网络进程的输出流 OutputStream outputStream = new OutputStream() { @Override public void write(int b) throws IOException { } }; } public void process(OutputStream ouput) throws IOException { //处理网络信息 //do something with the OutputStream }字节和字符数组从InputStream或者Reader中读入数组 从OutputStream或者Writer中写数组 在java中常用字节和字符数组在应用中临时存储数据。而这些数组又是通常的数据读取来源或者写入目的地。如果你需要在程序运行时需要大量读取文件里的内容，那么你也可以把一个文件加载到数组中。 前面的例子中，字符数组或字节数组是用来缓存数据的临时存储空间，不过它们同时也可以作为数据来源或者写入目的地。举个例子： //字符数组和字节数组在io过程中的作用 public void test4() { //arr和brr分别作为数据源 char []arr = {&apos;a&apos;,&apos;c&apos;,&apos;d&apos;}; CharArrayReader charArrayReader = new CharArrayReader(arr); byte []brr = {1,2,3,4,5}; ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(brr); }System.in, System.out, System.errSystem.in, System.out, System.err这3个流同样是常见的数据来源和数据流目的地。使用最多的可能是在控制台程序里利用System.out将输出打印到控制台上。 JVM启动的时候通过Java运行时初始化这3个流，所以你不需要初始化它们(尽管你可以在运行时替换掉它们)。 System.in System.in是一个典型的连接控制台程序和键盘输入的InputStream流。通常当数据通过命令行参数或者配置文件传递给命令行Java程序的时候，System.in并不是很常用。图形界面程序通过界面传递参数给程序，这是一块单独的Java IO输入机制。 System.out System.out是一个PrintStream流。System.out一般会把你写到其中的数据输出到控制台上。System.out通常仅用在类似命令行工具的控制台程序上。System.out也经常用于打印程序的调试信息(尽管它可能并不是获取程序调试信息的最佳方式)。 System.err System.err是一个PrintStream流。System.err与System.out的运行方式类似，但它更多的是用于打印错误文本。一些类似Eclipse的程序，为了让错误信息更加显眼，会将错误信息以红色文本的形式通过System.err输出到控制台上。System.out和System.err的简单例子：这是一个System.out和System.err结合使用的简单示例： //测试System.in, System.out, System.err public static void main(String[] args) { int in = new Scanner(System.in).nextInt(); System.out.println(in); System.out.println(&quot;out&quot;); System.err.println(&quot;err&quot;); //输入10，结果是 // err（红色） // 10 // out }字符流的Buffered和FilterBufferedReader能为字符输入流提供缓冲区，可以提高许多IO处理的速度。你可以一次读取一大块的数据，而不需要每次从网络或者磁盘中一次读取一个字节。特别是在访问大量磁盘数据时，缓冲通常会让IO快上许多。 BufferedReader和BufferedInputStream的主要区别在于，BufferedReader操作字符，而BufferedInputStream操作原始字节。只需要把Reader包装到BufferedReader中，就可以为Reader添加缓冲区(译者注：默认缓冲区大小为8192字节，即8KB)。代码如下： Reader input = new BufferedReader(new FileReader(&quot;c:\\data\\input-file.txt&quot;));你也可以通过传递构造函数的第二个参数，指定缓冲区大小，代码如下： Reader input = new BufferedReader(new FileReader(&quot;c:\\data\\input-file.txt&quot;), 8 * 1024);这个例子设置了8KB的缓冲区。最好把缓冲区大小设置成1024字节的整数倍，这样能更高效地利用内置缓冲区的磁盘。 除了能够为输入流提供缓冲区以外，其余方面BufferedReader基本与Reader类似。BufferedReader还有一个额外readLine()方法，可以方便地一次性读取一整行字符。 BufferedWriter 与BufferedReader类似，BufferedWriter可以为输出流提供缓冲区。可以构造一个使用默认大小缓冲区的BufferedWriter(译者注：默认缓冲区大小8 * 1024B)，代码如下： Writer writer = new BufferedWriter(new FileWriter(&quot;c:\\data\\output-file.txt&quot;));也可以手动设置缓冲区大小，代码如下： Writer writer = new BufferedWriter(new FileWriter(&quot;c:\\data\\output-file.txt&quot;), 8 * 1024);为了更好地使用内置缓冲区的磁盘，同样建议把缓冲区大小设置成1024的整数倍。除了能够为输出流提供缓冲区以外，其余方面BufferedWriter基本与Writer类似。类似地，BufferedWriter也提供了writeLine()方法，能够把一行字符写入到底层的字符输出流中。 值得注意是，你需要手动flush()方法确保写入到此输出流的数据真正写入到磁盘或者网络中。 FilterReader 与FilterInputStream类似，FilterReader是实现自定义过滤输入字符流的基类，基本上它仅仅只是简单覆盖了Reader中的所有方法。 就我自己而言，我没发现这个类明显的用途。除了构造函数取一个Reader变量作为参数之外，我没看到FilterReader任何对Reader新增或者修改的地方。如果你选择继承FilterReader实现自定义的类，同样也可以直接继承自Reader从而避免额外的类层级结构。 JavaIO流面试题什么是IO流？它是一种数据的流从源头流到目的地。比如文件拷贝，输入流和输出流都包括了。输入流从文件中读取数据存储到进程(process)中，输出流从进程中读取数据然后写入到目标文件。 字节流和字符流的区别。字节流在JDK1.0中就被引进了，用于操作包含ASCII字符的文件。JAVA也支持其他的字符如Unicode，为了读取包含Unicode字符的文件，JAVA语言设计者在JDK1.1中引入了字符流。ASCII作为Unicode的子集，对于英语字符的文件，可以可以使用字节流也可以使用字符流。 Java中流类的超类主要由那些？java.io.InputStreamjava.io.OutputStreamjava.io.Readerjava.io.Writer FileInputStream和FileOutputStream是什么？这是在拷贝文件操作的时候，经常用到的两个类。在处理小文件的时候，它们性能表现还不错，在大文件的时候，最好使用BufferedInputStream (或 BufferedReader) 和 BufferedOutputStream (或 BufferedWriter) System.out.println()是什么？println是PrintStream的一个方法。out是一个静态PrintStream类型的成员变量，System是一个java.lang包中的类，用于和底层的操作系统进行交互。 什么是Filter流？Filter Stream是一种IO流主要作用是用来对存在的流增加一些额外的功能，像给目标文件增加源文件中不存在的行数，或者增加拷贝的性能。 有哪些可用的Filter流？在java.io包中主要由4个可用的filter Stream。两个字节filter stream，两个字符filter stream. 分别是FilterInputStream, FilterOutputStream, FilterReader and FilterWriter.这些类是抽象类，不能被实例化的。 在文件拷贝的时候，那一种流可用提升更多的性能？在字节流的时候，使用BufferedInputStream和BufferedOutputStream。在字符流的时候，使用BufferedReader 和 BufferedWriter 说说管道流(Piped Stream)有四种管道流， PipedInputStream, PipedOutputStream, PipedReader 和 PipedWriter.在多个线程或进程中传递数据的时候管道流非常有用。 说说File类它不属于 IO流，也不是用于文件操作的，它主要用于知道一个文件的属性，读写权限，大小等信息。 说说RandomAccessFile?它在java.io包中是一个特殊的类，既不是输入流也不是输出流，它两者都可以做到。他是Object的直接子类。通常来说，一个流只有一个功能，要么读，要么写。但是RandomAccessFile既可以读文件，也可以写文件。 DataInputStream 和 DataOutStream有的方法，在RandomAccessFile中都存在。 参考文章https://www.imooc.com/article/24305https://www.cnblogs.com/UncleWang001/articles/10454685.htmlhttps://www.cnblogs.com/Jixiangwei/p/Java.htmlhttps://blog.csdn.net/baidu_37107022/article/details/76890019 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java IO流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列15：Java注解简介和最佳实践]]></title>
    <url>%2F2019%2F09%2F15%2F15%E3%80%81Java%E6%B3%A8%E8%A7%A3%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java注解简介Annotation 中文译过来就是注解、标释的意思，在 Java 中注解是一个很重要的知识点，但经常还是有点让新手不容易理解。 我个人认为，比较糟糕的技术文档主要特征之一就是：用专业名词来介绍专业名词。比如： Java 注解用于为 Java 代码提供元数据。作为元数据，注解不直接影响你的代码执行，但也有一些类型的注解实际上可以用于这一目的。Java 注解是从 Java5 开始添加到 Java 的。这是大多数网站上对于 Java 注解，解释确实正确，但是说实在话，我第一次学习的时候，头脑一片空白。这什么跟什么啊？听了像没有听一样。因为概念太过于抽象，所以初学者实在是比较吃力才能够理解，然后随着自己开发过程中不断地强化练习，才会慢慢对它形成正确的认识。 我在写这篇文章的时候，我就在思考。如何让自己或者让读者能够比较直观地认识注解这个概念？是要去官方文档上翻译说明吗？我马上否定了这个答案。 后来，我想到了一样东西————墨水，墨水可以挥发、可以有不同的颜色，用来解释注解正好。 不过，我继续发散思维后，想到了一样东西能够更好地代替墨水，那就是印章。印章可以沾上不同的墨水或者印泥，可以定制印章的文字或者图案，如果愿意它也可以被戳到你任何想戳的物体表面。 但是，我再继续发散思维后，又想到一样东西能够更好地代替印章，那就是标签。标签是一张便利纸，标签上的内容可以自由定义。常见的如货架上的商品价格标签、图书馆中的书本编码标签、实验室中化学材料的名称类别标签等等。 并且，往抽象地说，标签并不一定是一张纸，它可以是对人和事物的属性评价。也就是说，标签具备对于抽象事物的解释。 所以，基于如此，我完成了自我的知识认知升级，我决定用标签来解释注解。 注解如同标签之前某新闻客户端的评论有盖楼的习惯，于是 “乔布斯重新定义了手机、罗永浩重新定义了傻X” 就经常极为工整地出现在了评论楼层中，并且广大网友在相当长的一段时间内对于这种行为乐此不疲。这其实就是等同于贴标签的行为。在某些网友眼中，罗永浩就成了傻X的代名词。 广大网友给罗永浩贴了一个名为“傻x”的标签，他们并不真正了解罗永浩，不知道他当教师、砸冰箱、办博客的壮举，但是因为“傻x”这样的标签存在，这有助于他们直接快速地对罗永浩这个人做出评价，然后基于此，罗永浩就可以成为茶余饭后的谈资，这就是标签的力量。 而在网络的另一边，老罗靠他的人格魅力自然收获一大批忠实的拥泵，他们对于老罗贴的又是另一种标签。 老罗还是老罗，但是由于人们对于它贴上的标签不同，所以造成对于他的看法大相径庭，不喜欢他的人整天在网络上评论抨击嘲讽，而崇拜欣赏他的人则会愿意挣钱购买锤子手机的发布会门票。 我无意于评价这两种行为，我再引个例子。 《奇葩说》是近年网络上非常火热的辩论节目，其中辩手陈铭被另外一个辩手马薇薇攻击说是————“站在宇宙中心呼唤爱”，然后贴上了一个大大的标签————“鸡汤男”，自此以后，观众再看到陈铭的时候，首先映入脑海中便是“鸡汤男”三个大字，其实本身而言陈铭非常优秀，为人师表、作风正派、谈吐举止得体，但是在网络中，因为娱乐至上的环境所致，人们更愿意以娱乐的心态来认知一切，于是“鸡汤男”就如陈铭自己所说成了一个撕不了的标签。 我们可以抽象概括一下，标签是对事物行为的某些角度的评价与解释。 到这里，终于可以引出本文的主角注解了。 初学者可以这样理解注解：想像代码具有生命，注解就是对于代码中某些鲜活个体的贴上去的一张标签。简化来讲，注解如同一张标签。 在未开始学习任何注解具体语法而言，你可以把注解看成一张标签。这有助于你快速地理解它的大致作用。如果初学者在学习过程有大脑放空的时候，请不要慌张，对自己说： 注解，标签。注解，标签。 Java 注解概述什么是注解？ 对于很多初次接触的开发者来说应该都有这个疑问？Annontation是Java5开始引入的新特征，中文名称叫注解。它提供了一种安全的类似注释的机制，用来将任何的信息或元数据（metadata）与程序元素（类、方法、成员变量等）进行关联。为程序的元素（类、方法、成员变量）加上更直观更明了的说明，这些说明信息是与程序的业务逻辑无关，并且供指定的工具或框架使用。Annontation像一种修饰符一样，应用于包、类型、构造方法、方法、成员变量、参数及本地变量的声明语句中。 Java注解是附加在代码中的一些元信息，用于一些工具在编译、运行时进行解析和使用，起到说明、配置的功能。注解不会也不能影响代码的实际逻辑，仅仅起到辅助性的作用。包含在 java.lang.annotation 包中。 注解的用处 1、生成文档。这是最常见的，也是java 最早提供的注解。常用的有@param @return 等 2、跟踪代码依赖性，实现替代配置文件功能。比如Dagger 2依赖注入，未来java开发，将大量注解配置，具有很大用处; 3、在编译时进行格式检查。如@override 放在方法前，如果你这个方法并不是覆盖了超类方法，则编译时就能检查出。 注解的原理 注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。而我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象$Proxy1。通过代理对象调用自定义注解（接口）的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。 元注解java.lang.annotation提供了四种元注解，专门注解其他的注解（在自定义注解的时候，需要使用到元注解）： @Documented –注解是否将包含在JavaDoc中 @Retention –什么时候使用该注解 @Target –注解用于什么地方 @Inherited – 是否允许子类继承该注解 1.）@Retention– 定义该注解的生命周期 ● RetentionPolicy.SOURCE : 在编译阶段丢弃。这些注解在编译结束之后就不再有任何意义，所以它们不会写入字节码。@Override, @SuppressWarnings都属于这类注解。 ● RetentionPolicy.CLASS : 在类加载的时候丢弃。在字节码文件的处理中有用。注解默认使用这种方式 ● RetentionPolicy.RUNTIME : 始终不会丢弃，运行期也保留该注解，因此可以使用反射机制读取该注解的信息。我们自定义的注解通常使用这种方式。 2.）Target – 表示该注解用于什么地方。默认值为任何元素，表示该注解用于什么地方。可用的ElementType参数包括 ● ElementType.CONSTRUCTOR:用于描述构造器 ● ElementType.FIELD:成员变量、对象、属性（包括enum实例） ● ElementType.LOCAL_VARIABLE:用于描述局部变量 ● ElementType.METHOD:用于描述方法 ● ElementType.PACKAGE:用于描述包 ● ElementType.PARAMETER:用于描述参数 ● ElementType.TYPE:用于描述类、接口(包括注解类型) 或enum声明 3.)@Documented–一个简单的Annotations标记注解，表示是否将注解信息添加在java文档中。 4.)@Inherited – 定义该注释和子类的关系 @Inherited 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。 JDK里的注解JDK 内置注解先来看几个 Java 内置的注解，让大家热热身。 @Override 演示 class Parent { public void run() { } } class Son extends Parent { /** * 这个注解是为了检查此方法是否真的是重写父类的方法 * 这时候就不用我们用肉眼去观察到底是不是重写了 */ @Override public void run() { } }@Deprecated 演示class Parent { /** * 此注解代表过时了，但是如果可以调用到，当然也可以正常使用 * 但是，此方法有可能在以后的版本升级中会被慢慢的淘汰 * 可以放在类，变量，方法上面都起作用 */ @Deprecated public void run() { } } public class JDKAnnotationDemo { public static void main(String[] args) { Parent parent = new Parent(); parent.run(); // 在编译器中此方法会显示过时标志 } }@SuppressWarnings 演示class Parent { // 因为定义的 name 没有使用，那么编译器就会有警告，这时候使用此注解可以屏蔽掉警告 // 即任意不想看到的编译时期的警告都可以用此注解屏蔽掉，但是不推荐，有警告的代码最好还是处理一下 @SuppressWarnings(&quot;all&quot;) private String name; }@FunctionalInterface 演示/** 此注解是 Java8 提出的函数式接口，接口中只允许有一个抽象方法 加上这个注解之后，类中多一个抽象方法或者少一个抽象方法都会报错 /@FunctionalInterfaceinterface Func { void run();} 注解处理器实战注解处理器注解处理器才是使用注解整个流程中最重要的一步了。所有在代码中出现的注解，它到底起了什么作用，都是在注解处理器中定义好的。概念：注解本身并不会对程序的编译方式产生影响，而是注解处理器起的作用；注解处理器能够通过在运行时使用反射获取在程序代码中的使用的注解信息，从而实现一些额外功能。前提是我们自定义的注解使用的是 RetentionPolicy.RUNTIME 修饰的。这也是我们在开发中使用频率很高的一种方式。 我们先来了解下如何通过在运行时使用反射获取在程序中的使用的注解信息。如下类注解和方法注解。 类注解 Class aClass = ApiController.class; Annotation[] annotations = aClass.getAnnotations(); for(Annotation annotation : annotations) { if(annotation instanceof ApiAuthAnnotation) { ApiAuthAnnotation apiAuthAnnotation = (ApiAuthAnnotation) annotation; System.out.println(&quot;name: &quot; + apiAuthAnnotation.name()); System.out.println(&quot;age: &quot; + apiAuthAnnotation.age()); } } 方法注解 Method method = ... //通过反射获取方法对象 Annotation[] annotations = method.getDeclaredAnnotations(); for(Annotation annotation : annotations) { if(annotation instanceof ApiAuthAnnotation) { ApiAuthAnnotation apiAuthAnnotation = (ApiAuthAnnotation) annotation; System.out.println(&quot;name: &quot; + apiAuthAnnotation.name()); System.out.println(&quot;age: &quot; + apiAuthAnnotation.age()); } } 此部分内容可参考: 通过反射获取注解信息 注解处理器实战接下来我通过在公司中的一个实战改编来演示一下注解处理器的真实使用场景。需求: 网站后台接口只能是年龄大于 18 岁的才能访问，否则不能访问前置准备: 定义注解（这里使用上文的完整注解），使用注解（这里使用上文中使用注解的例子）接下来要做的事情: 写一个切面，拦截浏览器访问带注解的接口，取出注解信息，判断年龄来确定是否可以继续访问。 在 dispatcher-servlet.xml 文件中定义 aop 切面 &lt;aop:config&gt; &lt;!--定义切点，切的是我们自定义的注解--&gt; &lt;aop:pointcut id=&quot;apiAuthAnnotation&quot; expression=&quot;@annotation(cn.caijiajia.devops.aspect.ApiAuthAnnotation)&quot;/&gt; &lt;!--定义切面，切点是 apiAuthAnnotation，切面类即注解处理器是 apiAuthAspect，主处理逻辑在方法名为 auth 的方法中--&gt; &lt;aop:aspect ref=&quot;apiAuthAspect&quot;&gt; &lt;aop:around method=&quot;auth&quot; pointcut-ref=&quot;apiAuthAnnotation&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;切面类处理逻辑即注解处理器代码如 @Component(&quot;apiAuthAspect&quot;) public class ApiAuthAspect { public Object auth(ProceedingJoinPoint pjp) throws Throwable { Method method = ((MethodSignature) pjp.getSignature()).getMethod(); ApiAuthAnnotation apiAuthAnnotation = method.getAnnotation(ApiAuthAnnotation.class); Integer age = apiAuthAnnotation.age(); if (age &gt; 18) { return pjp.proceed(); } else { throw new RuntimeException(&quot;你未满18岁，禁止访问&quot;); } } }不同类型的注解类注解你可以在运行期访问类，方法或者变量的注解信息，下是一个访问类注解的例子： 12345678910 Class aClass = TheClass.class;Annotation[] annotations = aClass.getAnnotations();for(Annotation annotation : annotations)&#123; if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value()); &#125;&#125; 你还可以像下面这样指定访问一个类的注解： 12345678Class aClass = TheClass.class;Annotation annotation = aClass.getAnnotation(MyAnnotation.class);if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value());&#125; 方法注解下面是一个方法注解的例子： 1234public class TheClass &#123; @MyAnnotation(name=&quot;someName&quot;, value = &quot;Hello World&quot;) public void doSomething()&#123;&#125;&#125; 你可以像这样访问方法注解： 12345678910Method method = ... //获取方法对象Annotation[] annotations = method.getDeclaredAnnotations();for(Annotation annotation : annotations)&#123; if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value()); &#125;&#125; 你可以像这样访问指定的方法注解： 12345678Method method = ... // 获取方法对象Annotation annotation = method.getAnnotation(MyAnnotation.class);if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value());&#125; 参数注解方法参数也可以添加注解，就像下面这样： 12345public class TheClass &#123; public static void doSomethingElse( @MyAnnotation(name=&quot;aName&quot;, value=&quot;aValue&quot;) String parameter)&#123; &#125;&#125; 你可以通过 Method对象来访问方法参数注解： 1234567891011121314151617Method method = ... //获取方法对象Annotation[][] parameterAnnotations = method.getParameterAnnotations();Class[] parameterTypes = method.getParameterTypes();int i=0;for(Annotation[] annotations : parameterAnnotations)&#123; Class parameterType = parameterTypes[i++]; for(Annotation annotation : annotations)&#123; if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;param: &quot; + parameterType.getName()); System.out.println(&quot;name : &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value()); &#125; &#125;&#125; 需要注意的是 Method.getParameterAnnotations()方法返回一个注解类型的二维数组，每一个方法的参数包含一个注解数组。 变量注解下面是一个变量注解的例子： 12345public class TheClass &#123; @MyAnnotation(name=&quot;someName&quot;, value = &quot;Hello World&quot;) public String myField = null;&#125; 你可以像这样来访问变量的注解： 12345678910Field field = ... //获取方法对象&lt;/pre&gt;&lt;pre&gt;Annotation[] annotations = field.getDeclaredAnnotations();for(Annotation annotation : annotations)&#123; if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value()); &#125;&#125; 你可以像这样访问指定的变量注解： 123456789Field field = ...//获取方法对象&lt;/pre&gt;&lt;pre&gt;Annotation annotation = field.getAnnotation(MyAnnotation.class);if(annotation instanceof MyAnnotation)&#123; MyAnnotation myAnnotation = (MyAnnotation) annotation; System.out.println(&quot;name: &quot; + myAnnotation.name()); System.out.println(&quot;value: &quot; + myAnnotation.value());&#125; Java注解相关面试题什么是注解？他们的典型用例是什么？注解是绑定到程序源代码元素的元数据，对运行代码的操作没有影响。 他们的典型用例是： 编译器的信息 - 使用注解，编译器可以检测错误或抑制警告 编译时和部署时处理 - 软件工具可以处理注解并生成代码，配置文件等。 运行时处理 - 可以在运行时检查注解以自定义程序的行为 描述标准库中一些有用的注解。java.lang和java.lang.annotation包中有几个注解，更常见的包括但不限于此： @Override -标记方法是否覆盖超类中声明的元素。如果它无法正确覆盖该方法，编译器将发出错误 @Deprecated - 表示该元素已弃用且不应使用。如果程序使用标有此批注的方法，类或字段，编译器将发出警告 @SuppressWarnings - 告诉编译器禁止特定警告。在与泛型出现之前编写的遗留代码接口时最常用的 @FunctionalInterface - 在Java 8中引入，表明类型声明是一个功能接口，可以使用Lambda Expression提供其实现 可以从注解方法声明返回哪些对象类型？返回类型必须是基本类型，String，Class，Enum或数组类型之一。否则，编译器将抛出错误。 这是一个成功遵循此原则的示例代码： 1234567891011enum Complexity &#123; LOW, HIGH&#125;public @interface ComplexAnnotation &#123; Class&lt;? extends Object&gt; value(); int[] types(); Complexity complexity();&#125; 下一个示例将无法编译，因为Object不是有效的返回类型： 123public @interface FailingAnnotation &#123; Object complexity();&#125; 哪些程序元素可以注解？注解可以应用于整个源代码的多个位置。它们可以应用于类，构造函数和字段的声明： 12345678910@SimpleAnnotationpublic class Apply &#123; @SimpleAnnotation private String aField; @SimpleAnnotation public Apply() &#123; // ... &#125;&#125; 方法及其参数： 1234@SimpleAnnotationpublic void aMethod(@SimpleAnnotation String param) &#123; // ...&#125; 局部变量，包括循环和资源变量： 123456789101112@SimpleAnnotationint i = 10;for (@SimpleAnnotation int j = 0; j &lt; i; j++) &#123; // ...&#125;try (@SimpleAnnotation FileWriter writer = getWriter()) &#123; // ...&#125; catch (Exception ex) &#123; // ...&#125; 其他注解类型： 1234@SimpleAnnotationpublic @interface ComplexAnnotation &#123; // ...&#125; 甚至包，通过package-info.java文件： 12@PackageAnnotationpackage com.baeldung.interview.annotations; 从Java 8开始，它们也可以应用于类型的使用。为此，注解必须指定值为ElementType.USE的@Target注解： 1234@Target(ElementType.TYPE_USE)public @interface SimpleAnnotation &#123; // ...&#125; 现在，注解可以应用于类实例创建： 1new @SimpleAnnotation Apply(); 类型转换： 1aString = (@SimpleAnnotation String) something; 接口中： 1234public class SimpleList&lt;T&gt; implements @SimpleAnnotation List&lt;@SimpleAnnotation T&gt; &#123; // ...&#125; 抛出异常上： 123void aMethod() throws @SimpleAnnotation Exception &#123; // ...&#125; 有没有办法限制可以应用注解的元素？有，@ Target注解可用于此目的。如果我们尝试在不适用的上下文中使用注解，编译器将发出错误。 以下是仅将@SimpleAnnotation批注的用法限制为字段声明的示例： 1234@Target(ElementType.FIELD)public @interface SimpleAnnotation &#123; // ...&#125; 如果我们想让它适用于更多的上下文，我们可以传递多个常量： 1@Target(&#123; ElementType.FIELD, ElementType.METHOD, ElementType.PACKAGE &#125;) 我们甚至可以制作一个注解，因此它不能用于注解任何东西。当声明的类型仅用作复杂注解中的成员类型时，这可能会派上用场： 1234@Target(&#123;&#125;)public @interface NoTargetAnnotation &#123; // ...&#125; 什么是元注解？元注解适用于其他注解的注解。 所有未使用@Target标记或使用它标记但包含ANNOTATION_TYPE常量的注解也是元注解： 1234@Target(ElementType.ANNOTATION_TYPE)public @interface SimpleAnnotation &#123; // ...&#125; 下面的代码会编译吗？1234@Target(&#123; ElementType.FIELD, ElementType.TYPE, ElementType.FIELD &#125;)public @interface TestAnnotation &#123; int[] value() default &#123;&#125;;&#125; 不能。如果在@Target注解中多次出现相同的枚举常量，那么这是一个编译时错误。 删除重复常量将使代码成功编译： 1@Target(&#123; ElementType.FIELD, ElementType.TYPE&#125;) 参考文章https://blog.fundodoo.com/2018/04/19/130.htmlhttps://blog.csdn.net/qq_37939251/article/details/83215703https://blog.51cto.com/4247649/2109129https://www.jianshu.com/p/2f2460e6f8e7https://blog.csdn.net/yuzongtao/article/details/83306182 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>annotation</tag>
        <tag>Java注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析6：SpringMVC的视图解析原理]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%906%EF%BC%9ASpringMVC%E7%9A%84%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[转自 SpringMVC视图机制详解[附带源码分析] 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 目录 前言 重要接口和类介绍 源码分析 编码自定义的ViewResolver 总结 参考资料 前言SpringMVC是目前主流的Web MVC框架之一。 如果有同学对它不熟悉，那么请参考它的入门blog：http://www.cnblogs.com/fangjian0423/p/springMVC-introduction.html 本文将分析SpringMVC的视图这部分内容，让读者了解SpringMVC视图的设计原理。 重要接口和类介绍1. View接口 视图基础接口，它的各种实现类是无状态的，因此是线程安全的。 该接口定义了两个方法： 2. AbstractView抽象类 View接口的基础实现类。我们稍微介绍一下这个抽象类。 首先看下这个类的属性： 再看下抽象类中接口方法的实现： getContentType方法直接返回contentType属性即可。 render方法： 3. AbstractUrlBasedView抽象类 继承自AbstractView抽象类，增加了1个类型为String的url参数。 4. InternalResourceView类 继承自AbstractUrlBasedView抽象类的类，表示JSP视图。 我们看下这个类的renderMergedOutputModel方法(AbstractView抽象类定义的抽象方法，为View接口提供的render方法服务)。 5. JstlView类 JSTL视图，继承自InternalResourceView，该类大致上与InternalResourceView类一致。 6. AbstractTemplateView抽象类 继承自AbstractUrlBasedView抽象类，重写了renderMergedOutputModel方法，在该方法中会调用renderMergedTemplateModel方法，renderMergedTemplateModel方法为新定义的抽象方法。 该抽象类有几个boolean属性exposeSessionAttributes，exposeRequestAttributes。 设置为true的话会将request和session中的键值和值丢入到renderMergedTemplateModel方法中的model这个Map参数中。 这个类是某些模板引擎视图类的父类。 比如FreemarkerView，VelocityView。 7. FreeMarkerView类 继承自AbstractTemplateView抽象类。 直接看renderMergedTemplateModel方法，renderMergedTemplateModel内部会调用doRender方法： 8. RedirectView类 继承自AbstractUrlBasedView，并实现SmartView接口。SmartView接口定义了1个boolean isRedirectView();方法。 该视图的renderMergedOutputModel方法主要就是通过response.sendRedirect进行重定向。 有关RedirectView方面的知识楼主另外写了1篇博客。http://www.cnblogs.com/fangjian0423/p/springMVC-redirectView-analysis.html 9. ViewResolver接口 视图解释器，用来解析视图View，与View接口配合使用。 该接口只有1个方法，通过视图名称viewName和Locale对象得到View接口实现类： 1View resolveViewName(String viewName, Locale locale) throws Exception; 10. AbstractCachingViewResolver抽象类 带有缓存功能的ViewResolver接口基础实现抽象类，该类有个属性名为viewAccessCache的以 “viewName_locale” 为key， View接口为value的Map。 该抽象类实现的resolveViewName方法内部会调用createView方法，方法内部会调用loadView抽象方法。 11. UrlBasedViewResolver类 继承自AbstractCachingViewResolver抽象类、并实现Ordered接口的类，是ViewResolver接口简单的实现类。 该类复写了createView方法： 父类(AbstractCachingViewResolver)的createView方法内部会调用loadView抽象方法，UrlBasedViewResolver实现了这个抽象方法： 下面对UrlBasedViewResolver做1个test，配置如下： 123456789101112131415161718192021&lt;bean class=&quot;org.springframework.web.servlet.view.UrlBasedViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/view/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.InternalResourceView&quot;/&gt; &lt;property name=&quot;viewNames&quot;&gt; &lt;value type=&quot;java.lang.String&quot;&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;contentType&quot; value=&quot;text/html;charset=utf-8&quot;/&gt; &lt;property name=&quot;attributesMap&quot;&gt; &lt;map&gt; &lt;entry key=&quot;mytest&quot; value=&quot;mytestvalue&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;attributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;test&quot;&gt;testvalue&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 我们看到：以InternalResourceView这个JSP视图作为视图；viewNames我们设置了_，这里的_代表全部视图名(这个viewNames属性不设置也可以，代表全部视图名都处理)；http响应头部contentType信息：text/html;charset=utf-8；attributesMap和attributes传入的Map和Properties参数都会被丢入到staticAttributes属性中，这个staticAttributes会被设置成AbstractView的staticAttributes属性，也就是request域中的参数。 我们看到request域中没有设置mytest和testvalue值。但是页面中会显示，因为我们配置了attributesMap和attributes参数。 如果我们把viewNames中的”*”改成”index1”。那么就报错了，因为处理视图名的时候index匹配不上index1。 12. InternalResourceViewResolver类 继承自UrlBasedViewResolver，以InternalResourceView作为视图，若项目中存在“javax.servlet.jsp.jstl.core.Config”该类，那么会以JstlView作为视图。重写了buildView方法，主要就是为了给InternalResourceView视图设置属性。 13. AbstractTemplateViewResolver类 继承自UrlBasedViewResolver，重写了buildView方法，主要就是构造AbstractTemplateView以及为它设置相应的属性。 14. FreeMarkerViewResolver类 继承自AbstractTemplateViewResolver，将视图设置为FreeMarkerView。 15. ModelAndView对象 顾名思义，带有视图和Model属性的一个模型和视图类。 值得注意的是，这个视图属性是一个Object类型的数据，可以直接是View接口的实现类或者视图名(字符串)。 源码分析下面我们来分析SpringMVC处理视图的源码。 SpringMVC在处理请求的时候，通过RequestMappingHandlerMapping得到HandlerExecutionChain，然后通过RequestMappingHandlerAdapter得到1个ModelAndView对象，之后通过processDispatchResult方法处理。 processDispatchResult方法如下： 如果配置的ViewResolver如下： 1234&lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/view/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;&lt;/bean&gt; 那么就是使用InternalResourceViewResolver来解析视图。 之前分析过，InternalResourceViewResolver重写了UrlBasedViewResolver的buildView方法。但是还是会调用UrlBasedViewResolver的buildView方法。 最终得到InternalResourceView或JstlView视图。这两个视图的render方法本文介绍重要接口及类的时候已分析。 PS：DispathcerServlet中的viewResolvers属性是个集合，如果存在多个ViewResolver对象，必定会有优先级的问题，这部分的内容请参考楼主的另一篇博客： http://www.cnblogs.com/fangjian0423/p/spring-Ordered-interface.html 编码自定义的ViewResolver下面，我们就来编写自定义的ViewResolver。 自定义的ViewResolver处理视图名的时候，遇到 “jsp:” 开头的会找jsp页面，遇到 “freemarker:” 开头的找freemarker页面。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class CustomViewResolver extends UrlBasedViewResolver &#123; public static final String JSP_URL_PREFIX = &quot;jsp:&quot;; public static final String FTL_URL_PREFIX = &quot;freemarker:&quot;; private static final boolean jstlPresent = ClassUtils.isPresent( &quot;javax.servlet.jsp.jstl.core.Config&quot;, CustomViewResolver.class.getClassLoader()); private Boolean exposePathVariables = false; private boolean exposeRequestAttributes = false; private boolean allowRequestOverride = false; private boolean exposeSessionAttributes = false; private boolean allowSessionOverride = false; private boolean exposeSpringMacroHelpers = true; public CustomViewResolver() &#123; this.setViewClass(FreeMarkerView.class); &#125; @Override protected AbstractUrlBasedView buildView(String viewName) throws Exception &#123; if(viewName.startsWith(FTL_URL_PREFIX)) &#123; return buildFreemarkerView(viewName.substring(FTL_URL_PREFIX.length())); &#125; else if(viewName.startsWith(JSP_URL_PREFIX)) &#123; Class viewCls = jstlPresent ? JstlView.class : InternalResourceView.class; return buildView(viewCls, viewName.substring(JSP_URL_PREFIX.length()), getPrefix(), &quot;.jsp&quot;); &#125; else &#123; //默认以freemarker处理 return buildFreemarkerView(viewName); &#125; &#125; private AbstractUrlBasedView build(Class viewClass, String viewName, String prefix, String suffix) &#123; AbstractUrlBasedView view = (AbstractUrlBasedView) BeanUtils.instantiateClass(viewClass); view.setUrl(prefix + viewName + suffix); String contentType = getContentType(); if (contentType != null) &#123; view.setContentType(contentType); &#125; view.setRequestContextAttribute(getRequestContextAttribute()); view.setAttributesMap(getAttributesMap()); if (this.exposePathVariables != null) &#123; view.setExposePathVariables(exposePathVariables); &#125; return view; &#125; private AbstractUrlBasedView buildFreemarkerView(String viewName) throws Exception &#123; AbstractTemplateView view = (AbstractTemplateView) build(FreeMarkerView.class, viewName, &quot;&quot;, getSuffix()); view.setExposeRequestAttributes(this.exposeRequestAttributes); view.setAllowRequestOverride(this.allowRequestOverride); view.setExposeSessionAttributes(this.exposeSessionAttributes); view.setAllowSessionOverride(this.allowSessionOverride); view.setExposeSpringMacroHelpers(this.exposeSpringMacroHelpers); return view; &#125; //get set方法省略&#125; xml配置： 123456789&lt;bean class=&quot;org.format.demo.support.viewResolver.CustomViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/view/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.ftl&quot;/&gt; &lt;property name=&quot;contentType&quot; value=&quot;text/html;charset=utf-8&quot;/&gt; &lt;property name=&quot;exposeRequestAttributes&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;exposeSessionAttributes&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;exposeSpringMacroHelpers&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;requestContextAttribute&quot; value=&quot;request&quot;/&gt;&lt;/bean&gt; 12345678910111213&lt;bean id=&quot;freemarkerConfig&quot; class=&quot;org.springframework.web.servlet.view.freemarker.FreeMarkerConfigurer&quot;&gt; &lt;property name=&quot;templateLoaderPath&quot; value=&quot;/WEB-INF/view/&quot;/&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;utf-8&quot;/&gt; &lt;property name=&quot;freemarkerSettings&quot;&gt; &lt;props&gt; &lt;prop key=&quot;template_update_delay&quot;&gt;10&lt;/prop&gt; &lt;prop key=&quot;locale&quot;&gt;zh_CN&lt;/prop&gt; &lt;prop key=&quot;datetime_format&quot;&gt;yyyy-MM-dd&lt;/prop&gt; &lt;prop key=&quot;date_format&quot;&gt;yyyy-MM-dd&lt;/prop&gt; &lt;prop key=&quot;number_format&quot;&gt;#.##&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 简单解释一下：CustomViewResolver解析视图名的时候，判断 “jsp:” 和 “freemarker:” 开头的名字，如果是 “jsp:” 开头的，如果有JSTL依赖，构造JSTLView视图，否则构造InternalResourceView视图。如果是 “freemarker:” 构造FreemarkerView。在构造视图之前分别会设置一些属性。 xml配置：配置prefix是为了给jsp视图用的，freemarker视图不需要prefix，因为FreemarkerView内部会使用配置的FreeMarkerConfigurer，并用FreeMarkerConfigurer内部的templateLoaderPath属性作为前缀，配置的suffix是为了让FreemarkerView使用，当后缀。 最后附上Controller代码： 1234567891011121314151617@Controller@RequestMapping(value = &quot;/tvrc&quot;)public class TestViewResolverController &#123; @RequestMapping(&quot;jsp&quot;) public ModelAndView jsp(ModelAndView view) &#123; view.setViewName(&quot;jsp:trvc/index&quot;); return view; &#125; @RequestMapping(&quot;/ftl&quot;) public ModelAndView freemarker(ModelAndView view) &#123; view.setViewName(&quot;freemarker:trvc/index&quot;); return view; &#125;&#125; 视图 /WEB-INF/view/trvc/index.jsp 中的的内容是输出 This is jsp page视图 /WEB-INF/view/trvc/index.ftl 中的的内容是输出 This is freemarker page 总结本文分析了SpringMVC中的视图机制，View和ViewResolver这两个接口是视图机制的核心，并分析了几个重要的View和ViewResolver接口实现类，最终写了一个区别jsp和freemarker视图的ViewResolver实现类，让读者更加理解视图机制。 希望这篇文章能帮助读者了解SpringMVC视图机制。 文中难免有错误，希望读者能够指明出来。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码剖析5：消息转换器HttpMessageConverter与@ResponseBody注解]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%905%EF%BC%9A%E6%B6%88%E6%81%AF%E8%BD%AC%E6%8D%A2%E5%99%A8HttpMessageConverter%E4%B8%8E%40ResponseBody%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[转自 SpringMVC关于json、xml自动转换的原理研究[附带源码分析] 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 目录 前言 现象 源码分析 实例讲解 关于配置 总结 参考资料 前言SpringMVC是目前主流的Web MVC框架之一。 如果有同学对它不熟悉，那么请参考它的入门blog：http://www.cnblogs.com/fangjian0423/p/springMVC-introduction.html 现象本文使用的demo基于maven，是根据入门blog的例子继续写下去的。 我们先来看一看对应的现象。 我们这里的配置文件 *-dispatcher.xml中的关键配置如下(其他常规的配置文件不在讲解，可参考本文一开始提到的入门blog)： (视图配置省略) 123&lt;mvc:resources location=&quot;/static/&quot; mapping=&quot;/static/**&quot;/&gt;&lt;mvc:annotation-driven/&gt;&lt;context:component-scan base-package=&quot;org.format.demo.controller&quot;/&gt; pom中需要有以下依赖(Spring依赖及其他依赖不显示)： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt; jackson-core-asl &lt;version&gt;1.9.13&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.codehaus.jackson&lt;/groupId&gt; jackson-mapper-asl &lt;version&gt;1.9.13&lt;/version&gt;&lt;/dependency&gt; 这个依赖是json序列化的依赖。 ok。我们在Controller中添加一个method： @RequestMapping("/xmlOrJson") @ResponseBody public Map xmlOrJson() { Map map = new HashMap(); map.put("list", employeeService.list()); return map; } 直接访问地址： 我们看到，短短几行配置。使用@ResponseBody注解之后，Controller返回的对象 自动被转换成对应的json数据，在这里不得不感叹SpringMVC的强大。 我们好像也没看到具体的配置，唯一看到的就是*-dispatcher.xml中的一句配置：mvc:annotation-driven。其实就是这个配置，导致了java对象自动转换成json对象的现象。 那么spring到底是如何实现java对象到json对象的自动转换的呢？ 为什么转换成了json数据，如果想转换成xml数据，那该怎么办？ 源码分析本文使用的spring版本是4.0.2。 在讲解mvc:annotation-driven这个配置之前，我们先了解下Spring的消息转换机制。@ResponseBody这个注解就是使用消息转换机制，最终通过json的转换器转换成json数据的。 HttpMessageConverter接口就是Spring提供的http消息转换接口。有关这方面的知识大家可以参考”参考资料”中的第二条链接，里面讲的很清楚。 下面开始分析mvc:annotation-driven这句配置: 这句代码在spring中的解析类是： 在AnnotationDrivenBeanDefinitionParser源码的152行parse方法中： 分别实例化了RequestMappingHandlerMapping，ConfigurableWebBindingInitializer，RequestMappingHandlerAdapter等诸多类。 其中RequestMappingHandlerMapping和RequestMappingHandlerAdapter这两个类比较重要。 RequestMappingHandlerMapping处理请求映射的，处理@RequestMapping跟请求地址之间的关系。 RequestMappingHandlerAdapter是请求处理的适配器，也就是请求之后处理具体逻辑的执行，关系到哪个类的哪个方法以及转换器等工作，这个类是我们讲的重点，其中它的属性messageConverters是本文要讲的重点。 私有方法:getMessageConverters 从代码中我们可以，RequestMappingHandlerAdapter设置messageConverters的逻辑： 1.如果mvc:annotation-driven节点有子节点message-converters，那么它的转换器属性messageConverters也由这些子节点组成。 message-converters的子节点配置如下： 123456&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.example.MyHttpMessageConverter&quot;/&gt; &lt;bean class=&quot;org.example.MyOtherHttpMessageConverter&quot;/&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 2.message-converters子节点不存在或它的属性register-defaults为true的话，加入其他的转换器：ByteArrayHttpMessageConverter、StringHttpMessageConverter、ResourceHttpMessageConverter等。 我们看到这么一段： 这些boolean属性是哪里来的呢，它们是AnnotationDrivenBeanDefinitionParser的静态变量。 其中ClassUtils中的isPresent方法如下： 看到这里，读者应该明白了为什么本文一开始在pom文件中需要加入对应的jackson依赖，为了让json转换器jackson成为默认转换器之一。 mvc:annotation-driven的作用读者也明白了。 下面我们看如何通过消息转换器将java对象进行转换的。 RequestMappingHandlerAdapter在进行handle的时候，会委托给HandlerMethod（具体由子类ServletInvocableHandlerMethod处理）的invokeAndHandle方法进行处理，这个方法又转接给HandlerMethodReturnValueHandlerComposite处理。 HandlerMethodReturnValueHandlerComposite维护了一个HandlerMethodReturnValueHandler列表。HandlerMethodReturnValueHandler是一个对返回值进行处理的策略接口，这个接口非常重要。关于这个接口的细节，请参考楼主的另外一篇博客：http://www.cnblogs.com/fangjian0423/p/springMVC-request-param-analysis.html。然后找到对应的HandlerMethodReturnValueHandler对结果值进行处理。 最终找到RequestResponseBodyMethodProcessor这个Handler（由于使用了@ResponseBody注解）。 RequestResponseBodyMethodProcessor的supportsReturnType方法： 然后使用handleReturnValue方法进行处理： 我们看到，这里使用了转换器。 具体的转换方法： 至于为何是请求头部的Accept数据，读者可以进去debug这个getAcceptableMediaTypes方法看看。 我就不罗嗦了～～～ ok。至此，我们走遍了所有的流程。 现在，回过头来看。为什么一开始的demo输出了json数据？ 我们来分析吧。 由于我们只配置了mvc:annotation-driven，因此使用spring默认的那些转换器。 很明显，我们看到了2个xml和1个json转换器。 要看能不能转换，得看HttpMessageConverter接口的public boolean canWrite(Class&lt;?&gt; clazz, MediaType mediaType)方法是否返回true来决定的。 我们先分析SourceHttpMessageConverter： 它的canWrite方法被父类AbstractHttpMessageConverter重写了。 发现SUPPORTED_CLASSES中没有Map类(本文demo返回的是Map类)，因此不支持。 下面看Jaxb2RootElementHttpMessageConverter： 这个类直接重写了canWrite方法。 需要有XmlRootElement注解。 很明显，Map类当然没有。 最终MappingJackson2HttpMessageConverter匹配，进行json转换。（为何匹配，请读者自行查看源码） 实例讲解 我们分析了转换器的转换过程之后，下面就通过实例来验证我们的结论吧。 首先，我们先把xml转换器实现。 之前已经分析，默认的转换器中是支持xml的。下面我们加上注解试试吧。 由于Map是jdk源码中的部分，因此我们用Employee来做demo。 因此，Controller加上一个方法： @RequestMapping("/xmlOrJsonSimple") @ResponseBody public Employee xmlOrJsonSimple() { return employeeService.getById(1); } 实体中加上@XmlRootElement注解 我们发现，解析成了xml。 这里为什么解析成xml，而不解析成json呢？ 之前分析过，消息转换器是根据class和mediaType决定的。 我们使用firebug看到： 我们发现Accept有xml，没有json。因此解析成xml了。 我们再来验证，同一地址，HTTP头部不同Accept。看是否正确。 $.ajax({ url: "${request.contextPath}/employee/xmlOrJsonSimple", success: function(res) { console.log(res); }, headers: { "Accept": "application/xml" } }); $.ajax({ url: "${request.contextPath}/employee/xmlOrJsonSimple", success: function(res) { console.log(res); }, headers: { "Accept": "application/json" } }); 验证成功。 关于配置如果不想使用mvc:annotation-driven中默认的RequestMappingHandlerAdapter的话，我们可以在重新定义这个bean，spring会覆盖掉默认的RequestMappingHandlerAdapter。 为何会覆盖，请参考楼主的另外一篇博客：http://www.cnblogs.com/fangjian0423/p/spring-Ordered-interface.html ` ` 或者如果只想换messageConverters的话。 123456&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.example.MyHttpMessageConverter&quot;/&gt; &lt;bean class=&quot;org.example.MyOtherHttpMessageConverter&quot;/&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 如果还想用其他converters的话。 以上是spring-mvc jar包中的converters。 这里我们使用转换xml的MarshallingHttpMessageConverter。 这个converter里面使用了marshaller进行转换 我们这里使用XStreamMarshaller。 json没有转换器，返回406. 至于xml格式的问题，大家自行解决吧。 这里用的是XStream～。 使用这种方式，pom别忘记了加入xstream的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.thoughtworks.xstream&lt;/groupId&gt; xstream &lt;version&gt;1.4.7&lt;/version&gt;&lt;/dependency&gt; 总结 写了这么多，可能读者觉得有点罗嗦。 毕竟这也是自己的一些心得，希望都能说出来与读者共享。 刚接触SpringMVC的时候，发现这种自动转换机制很牛逼，但是一直没有研究它的原理，目前，算是了了一个小小心愿吧，SpringMVC还有很多内容，以后自己研究其他内容的时候还会与大家一起共享的。 文章难免会出现一些错误，希望读者们能指明出来。 详解RequestBody和@ResponseBody注解概述 在SpringMVC中，可以使用@RequestBody和@ResponseBody两个注解，分别完成请求报文到对象和对象到响应报文的转换，底层这种灵活的消息转换机制，就是Spring3.x中新引入的HttpMessageConverter即消息转换器机制。 Http请求的抽象 还是回到请求-响应，也就是解析请求体，然后返回响应报文这个最基本的Http请求过程中来。我们知道，在servlet标准中，可以用javax.servlet.ServletRequest接口中的以下方法： 1public ServletInputStream getInputStream() throws IOException; 来得到一个ServletInputStream。这个ServletInputStream中，可以读取到一个原始请求报文的所有内容。同样的，在javax.servlet.ServletResponse接口中，可以用以下方法： 1public ServletOutputStream getOutputStream() throws IOException; 来得到一个ServletOutputStream，这个ServletOutputSteam，继承自java中的OutputStream，可以让你输出Http的响应报文内容。 让我们尝试着像SpringMVC的设计者一样来思考一下。我们知道，Http请求和响应报文本质上都是一串字符串，当请求报文来到java世界，它会被封装成为一个ServletInputStream的输入流，供我们读取报文。响应报文则是通过一个ServletOutputStream的输出流，来输出响应报文。 我们从流中，只能读取到原始的字符串报文，同样，我们往输出流中，也只能写原始的字符。而在java世界中，处理业务逻辑，都是以一个个有业务意义的对象为处理维度的，那么在报文到达SpringMVC和从SpringMVC出去，都存在一个字符串到java对象的阻抗问题。这一过程，不可能由开发者手工转换。我们知道，在Struts2中，采用了OGNL来应对这个问题，而在SpringMVC中，它是HttpMessageConverter机制。我们先来看两个接口。 HttpInputMessage 这个类是SpringMVC内部对一次Http请求报文的抽象，在HttpMessageConverter的read()方法中，有一个HttpInputMessage的形参，它正是SpringMVC的消息转换器所作用的受体“请求消息”的内部抽象，消息转换器从“请求消息”中按照规则提取消息，转换为方法形参中声明的对象。 12345678910package org.springframework.http;import java.io.IOException;import java.io.InputStream;public interface HttpInputMessage extends HttpMessage &#123; InputStream getBody() throws IOException;&#125; HttpOutputMessage 这个类是SpringMVC内部对一次Http响应报文的抽象，在HttpMessageConverter的write()方法中，有一个HttpOutputMessage的形参，它正是SpringMVC的消息转换器所作用的受体“响应消息”的内部抽象，消息转换器将“响应消息”按照一定的规则写到响应报文中。 12345678910package org.springframework.http;import java.io.IOException;import java.io.OutputStream;public interface HttpOutputMessage extends HttpMessage &#123; OutputStream getBody() throws IOException;&#125; HttpMessageConverter 对消息转换器最高层次的接口抽象，描述了一个消息转换器的一般特征，我们可以从这个接口中定义的方法，来领悟Spring3.x的设计者对这一机制的思考过程。 123456789101112131415161718192021222324package org.springframework.http.converter;import java.io.IOException;import java.util.List;import org.springframework.http.HttpInputMessage;import org.springframework.http.HttpOutputMessage;import org.springframework.http.MediaType;public interface HttpMessageConverter&lt;T&gt; &#123; boolean canRead(Class&lt;?&gt; clazz, MediaType mediaType); boolean canWrite(Class&lt;?&gt; clazz, MediaType mediaType); List&lt;MediaType&gt; getSupportedMediaTypes(); T read(Class&lt;? extends T&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException; void write(T t, MediaType contentType, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException;&#125; HttpMessageConverter接口的定义出现了成对的canRead()，read()和canWrite()，write()方法，MediaType是对请求的Media Type属性的封装。举个例子，当我们声明了下面这个处理方法。 1234@RequestMapping(value=&quot;/string&quot;, method=RequestMethod.POST)public @ResponseBody String readString(@RequestBody String string) &#123; return &quot;Read string &apos;&quot; + string + &quot;&apos;&quot;;&#125; 在SpringMVC进入readString方法前，会根据@RequestBody注解选择适当的HttpMessageConverter实现类来将请求参数解析到string变量中，具体来说是使用了StringHttpMessageConverter类，它的canRead()方法返回true，然后它的read()方法会从请求中读出请求参数，绑定到readString()方法的string变量中。 当SpringMVC执行readString方法后，由于返回值标识了@ResponseBody，SpringMVC将使用StringHttpMessageConverter的write()方法，将结果作为String值写入响应报文，当然，此时canWrite()方法返回true。 我们可以用下面的图，简单描述一下这个过程。 RequestResponseBodyMethodProcessor 将上述过程集中描述的一个类是org.springframework.web.servlet.mvc.method.annotation.RequestResponseBodyMethodProcessor，这个类同时实现了HandlerMethodArgumentResolver和HandlerMethodReturnValueHandler两个接口。前者是将请求报文绑定到处理方法形参的策略接口，后者则是对处理方法返回值进行处理的策略接口。两个接口的源码如下： 123456789101112131415161718192021222324252627282930313233package org.springframework.web.method.support;import org.springframework.core.MethodParameter;import org.springframework.web.bind.WebDataBinder;import org.springframework.web.bind.support.WebDataBinderFactory;import org.springframework.web.context.request.NativeWebRequest;public interface HandlerMethodArgumentResolver &#123; boolean supportsParameter(MethodParameter parameter); Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception;&#125;package org.springframework.web.method.support;import org.springframework.core.MethodParameter;import org.springframework.web.context.request.NativeWebRequest;public interface HandlerMethodReturnValueHandler &#123; boolean supportsReturnType(MethodParameter returnType); void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception;&#125; RequestResponseBodyMethodProcessor这个类，同时充当了方法参数解析和返回值处理两种角色。我们从它的源码中，可以找到上面两个接口的方法实现。 对HandlerMethodArgumentResolver接口的实现： 1234567891011121314151617181920public boolean supportsParameter(MethodParameter parameter) &#123; return parameter.hasParameterAnnotation(RequestBody.class);&#125;public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception &#123; Object argument = readWithMessageConverters(webRequest, parameter, parameter.getGenericParameterType()); String name = Conventions.getVariableNameForParameter(parameter); WebDataBinder binder = binderFactory.createBinder(webRequest, argument, name); if (argument != null) &#123; validate(binder, parameter); &#125; mavContainer.addAttribute(BindingResult.MODEL_KEY_PREFIX + name, binder.getBindingResult()); return argument;&#125; 对HandlerMethodReturnValueHandler接口的实现 12345678910111213public boolean supportsReturnType(MethodParameter returnType) &#123; return returnType.getMethodAnnotation(ResponseBody.class) != null;&#125; public void handleReturnValue(Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws IOException, HttpMediaTypeNotAcceptableException &#123; mavContainer.setRequestHandled(true); if (returnValue != null) &#123; writeWithMessageConverters(returnValue, returnType, webRequest); &#125;&#125; 看完上面的代码，整个HttpMessageConverter消息转换的脉络已经非常清晰。因为两个接口的实现，分别是以是否有@RequestBody和@ResponseBody为条件，然后分别调用HttpMessageConverter来进行消息的读写。 如果你想问，怎么样跟踪到RequestResponseBodyMethodProcessor中，请你按照前面几篇博文的思路，然后到这里spring-mvc-showcase下载源码回来，对其中HttpMessageConverter相关的例子进行debug，只要你肯下功夫，相信你一定会有属于自己的收获的。 思考 张小龙在谈微信的本质时候说：“微信只是个平台，消息在其中流转”。在我们对SpringMVC源码分析的过程中，我们可以从HttpMessageConverter机制中领悟到类似的道理。在SpringMVC的设计者眼中，一次请求报文和一次响应报文，分别被抽象为一个请求消息HttpInputMessage和一个响应消息HttpOutputMessage。 处理请求时，由合适的消息转换器将请求报文绑定为方法中的形参对象，在这里，同一个对象就有可能出现多种不同的消息形式，比如json和xml。同样，当响应请求时，方法的返回值也同样可能被返回为不同的消息形式，比如json和xml。 在SpringMVC中，针对不同的消息形式，我们有不同的HttpMessageConverter实现类来处理各种消息形式。但是，只要这些消息所蕴含的“有效信息”是一致的，那么各种不同的消息转换器，都会生成同样的转换结果。至于各种消息间解析细节的不同，就被屏蔽在不同的HttpMessageConverter实现类中了。 参考资料http://my.oschina.net/HeliosFly/blog/205343 http://my.oschina.net/lichhao/blog/172562 http://docs.spring.io/spring/docs/current/spring-framework-reference/html/mvc.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析4：DispatcherServlet如何找到正确的Controller]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%EF%BC%9ADispatcherServlet%E5%A6%82%E4%BD%95%E6%89%BE%E5%88%B0%E6%AD%A3%E7%A1%AE%E7%9A%84Controller%2F</url>
    <content type="text"><![CDATA[本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言SpringMVC是目前主流的Web MVC框架之一。 我们使用浏览器通过地址 http://ip:port/contextPath/path 进行访问，SpringMVC是如何得知用户到底是访问哪个Controller中的方法，这期间到底发生了什么。 本文将分析SpringMVC是如何处理请求与Controller之间的映射关系的，让读者知道这个过程中到底发生了什么事情。 本文实际上是在上文基础上，深入分析 HandlerMapping里的 HandlerExecutionChain getHandler(HttpServletRequest var1) throws Exception; 该方法的具体实现，包括它如何找到对应的方法，以及如何把结果保存在map里，以便让请求转发到对应的handler上，同时也分析了handleradaptor具体做了什么事情。 源码分析在分析源码之前，我们先了解一下几个东西。 1.这个过程中重要的接口和类。 HandlerMethod类： Spring3.1版本之后引入的。 是一个封装了方法参数、方法注解，方法返回值等众多元素的类。 它的子类InvocableHandlerMethod有两个重要的属性WebDataBinderFactory和HandlerMethodArgumentResolverComposite， 很明显是对请求进行处理的。 InvocableHandlerMethod的子类ServletInvocableHandlerMethod有个重要的属性HandlerMethodReturnValueHandlerComposite，很明显是对响应进行处理的。 ServletInvocableHandlerMethod这个类在HandlerAdapter对每个请求处理过程中，都会实例化一个出来(上面提到的属性由HandlerAdapter进行设置)，分别对请求和返回进行处理。 (RequestMappingHandlerAdapter源码，实例化ServletInvocableHandlerMethod的时候分别set了上面提到的重要属性) MethodParameter类： HandlerMethod类中的parameters属性类型，是一个MethodParameter数组。MethodParameter是一个封装了方法参数具体信息的工具类，包括参数的的索引位置，类型，注解，参数名等信息。 HandlerMethod在实例化的时候，构造函数中会初始化这个数组，这时只初始化了部分数据，在HandlerAdapter对请求处理过程中会完善其他属性，之后交予合适的HandlerMethodArgumentResolver接口处理。 以类DeptController为例： 123456789101112131415@Controller@RequestMapping(value = &quot;/dept&quot;)public class DeptController &#123; @Autowired private IDeptService deptService; @RequestMapping(&quot;/update&quot;) @ResponseBody public String update(Dept dept) &#123; deptService.saveOrUpdate(dept); return &quot;success&quot;; &#125;&#125; (刚初始化时的数据) (HandlerAdapter处理后的数据) RequestCondition接口： Spring3.1版本之后引入的。 是SpringMVC的映射基础中的请求条件，可以进行combine, compareTo，getMatchingCondition操作。这个接口是映射匹配的关键接口，其中getMatchingCondition方法关乎是否能找到合适的映射。 RequestMappingInfo类： Spring3.1版本之后引入的。 是一个封装了各种请求映射条件并实现了RequestCondition接口的类。 有各种RequestCondition实现类属性，patternsCondition，methodsCondition，paramsCondition，headersCondition，consumesCondition以及producesCondition，这个请求条件看属性名也了解，分别代表http请求的路径模式、方法、参数、头部等信息。 RequestMappingHandlerMapping类： 处理请求与HandlerMethod映射关系的一个类。 2.Web服务器启动的时候，SpringMVC到底做了什么。 先看AbstractHandlerMethodMapping的initHandlerMethods方法中。 我们进入createRequestMappingInfo方法看下是如何构造RequestMappingInfo对象的。 PatternsRequestCondition构造函数： 类对应的RequestMappingInfo存在的话，跟方法对应的RequestMappingInfo进行combine操作。 然后使用符合条件的method来注册各种HandlerMethod。 下面我们来看下各种RequestCondition接口的实现类的combine操作。 PatternsRequestCondition： RequestMethodsRequestCondition： 方法的请求条件，用个set直接add即可。 其他相关的RequestConditon实现类读者可自行查看源码。 最终，RequestMappingHandlerMapping中两个比较重要的属性 private final Map&lt;T, HandlerMethod&gt; handlerMethods = new LinkedHashMap&lt;T, HandlerMethod&gt;(); private final MultiValueMap&lt;String, T&gt; urlMap = new LinkedMultiValueMap&lt;String, T&gt;(); T为RequestMappingInfo。 构造完成。 我们知道，SpringMVC的分发器DispatcherServlet会根据浏览器的请求地址获得HandlerExecutionChain。 这个过程我们看是如何实现的。 首先看HandlerMethod的获得(直接看关键代码了)： 这里的比较器是使用RequestMappingInfo的compareTo方法(RequestCondition接口定义的)。 然后构造HandlerExecutionChain加上拦截器 实例写了这么多，来点例子让我们验证一下吧。 12345678910111213141516171819202122232425262728293031323334353637@Controller@RequestMapping(value = &quot;/wildcard&quot;)public class TestWildcardController &#123; @RequestMapping(&quot;/test/**&quot;) @ResponseBody public String test1(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;TestWildcardController -&gt; /test/**&quot;); return view; &#125; @RequestMapping(&quot;/test/*&quot;) @ResponseBody public String test2(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;TestWildcardController -&gt; /test*&quot;); return view; &#125; @RequestMapping(&quot;test?&quot;) @ResponseBody public String test3(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;TestWildcardController -&gt; test?&quot;); return view; &#125; @RequestMapping(&quot;test/*&quot;) @ResponseBody public String test4(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;TestWildcardController -&gt; test/*&quot;); return view; &#125;&#125; 由于这里的每个pattern都带了*因此，都不会加入到urlMap中，但是handlerMethods还是有的。 当我们访问：http://localhost:8888/SpringMVCDemo/wildcard/test1的时候。 会先根据 “/wildcard/test1” 找urlMap对应的RequestMappingInfo集合，找不到的话取handlerMethods集合中所有的key集合(也就是RequestMappingInfo集合)。 然后进行匹配，匹配根据RequestCondition的getMatchingCondition方法。 最终匹配到2个RequestMappingInfo： 然后会使用比较器进行排序。 之前也分析过，比较器是有优先级的。 我们看到，RequestMappingInfo除了pattern，其他属性都是一样的。 我们看下PatternsRequestCondition比较的逻辑： 因此，/test*的通配符比/test?的多，因此，最终选择了/test? 直接比较优先于通配符。 123456789101112131415161718192021@Controller@RequestMapping(value = &quot;/priority&quot;)public class TestPriorityController &#123; @RequestMapping(method = RequestMethod.GET) @ResponseBody public String test1(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;其他condition相同，带有method属性的优先级高&quot;); return view; &#125; @RequestMapping() @ResponseBody public String test2(ModelAndView view) &#123; view.setViewName(&quot;/test/test&quot;); view.addObject(&quot;attr&quot;, &quot;其他condition相同，不带method属性的优先级高&quot;); return view; &#125;&#125; 这里例子，其他requestCondition都一样，只有RequestMethodCondition不一样。 看出，方法多的优先级越多。 至于其他的RequestCondition，大家自行查看源码吧。 资源文件映射以上分析均是基于Controller方法的映射(RequestMappingHandlerMapping)。 SpringMVC中还有静态文件的映射，SimpleUrlHandlerMapping。 DispatcherServlet找对应的HandlerExecutionChain的时候会遍历属性handlerMappings，这个一个实现了HandlerMapping接口的集合。 由于我们在*-dispatcher.xml中加入了以下配置： 1&lt;mvc:resources location=&quot;/static/&quot; mapping=&quot;/static/**&quot;/&gt; Spring解析配置文件会使用ResourcesBeanDefinitionParser进行解析的时候，会实例化出SimpleUrlHandlerMapping。 其中注册的HandlerMethod为ResourceHttpRequestHandler。 访问地址：http://localhost:8888/SpringMVCDemo/static/js/jquery-1.11.0.js 地址匹配到/static/**。 最终SimpleUrlHandlerMapping找到对应的Handler -&gt; ResourceHttpRequestHandler。 ResourceHttpRequestHandler进行handleRequest的时候，直接输出资源文件的文本内容。 总结大致上整理了一下SpringMVC对请求的处理，包括其中比较关键的类和接口，希望对读者有帮助。 让自己对SpringMVC有了更深入的认识，也为之后分析数据绑定，拦截器、HandlerAdapter等打下基础。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析3：DispatcherServlet的初始化与请求转发]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%EF%BC%9ADispatcherServlet%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%AF%B7%E6%B1%82%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言在我们第一次学Servlet编程，学java web的时候，还没有那么多框架。我们开发一个简单的功能要做的事情很简单，就是继承HttpServlet，根据需要重写一下doGet，doPost方法，跳转到我们定义好的jsp页面。Servlet类编写完之后在web.xml里注册这个Servlet类。 除此之外，没有其他了。我们启动web服务器，在浏览器中输入地址，就可以看到浏览器上输出我们写好的页面。为了更好的理解上面这个过程，你需要学习关于Servlet生命周期的三个阶段，就是所谓的“init-service-destroy”。 以上的知识，我觉得对于你理解SpringMVC的设计思想，已经足够了。SpringMVC当然可以称得上是一个复杂的框架，但是同时它又遵循Servlet世界里最简单的法则，那就是“init-service-destroy”。我们要分析SpringMVC的初始化流程，其实就是分析DispatcherServlet类的init()方法，让我们带着这种单纯的观点，打开DispatcherServlet的源码一窥究竟吧。 配置元素读取用Eclipse IDE打开DispatcherServlet类的源码，ctrl+T看一下。 DispatcherServlet类的初始化入口方法init()定义在HttpServletBean这个父类中，HttpServletBean类作为一个直接继承于HttpServlet类的类，覆写了HttpServlet类的init()方法，实现了自己的初始化行为。 123456789101112131415161718192021222324252627@Override public final void init() throws ServletException &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Initializing servlet &apos;&quot; + getServletName() + &quot;&apos;&quot;); &#125; // Set bean properties from init parameters. try &#123; PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, this.environment)); initBeanWrapper(bw); bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; logger.error(&quot;Failed to set bean properties on servlet &apos;&quot; + getServletName() + &quot;&apos;&quot;, ex); throw ex; &#125; // Let subclasses do whatever initialization they like. initServletBean(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Servlet &apos;&quot; + getServletName() + &quot;&apos; configured successfully&quot;); &#125; &#125; 这里的initServletBean()方法在HttpServletBean类中是一个没有任何实现的空方法，它的目的就是留待子类实现自己的初始化逻辑，也就是我们常说的模板方法设计模式。SpringMVC在此生动的运用了这个模式，init()方法就是模版方法模式中的模板方法，SpringMVC真正的初始化过程，由子类FrameworkServlet中覆写的initServletBean()方法触发。 再看一下init()方法内被try,catch块包裹的代码，里面涉及到BeanWrapper，PropertyValues，ResourceEditor这些Spring内部非常底层的类。要深究具体代码实现上面的细节，需要对Spring框架源码具有相当深入的了解。我们这里先避繁就简，从代码效果和设计思想上面来分析这段try,catch块内的代码所做的事情： 注册一个字符串到资源文件的编辑器，让Servlet下面的配置元素可以使用形如“classpath:”这种方式指定SpringMVC框架bean配置文件的来源。 将web.xml中在DispatcherServlet这个Servlet下面的配置元素利用JavaBean的方式（即通过setter方法）读取到DispatcherServlet中来。 这两点，我想通过下面一个例子来说明一下。 我在web.xml中注册的DispatcherServlet配置如下： 123456789101112131415&lt;!-- springMVC配置开始 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;appServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;!-- springMVC配置结束 --&gt; 可以看到，我注册了一个名为contextConfigLocation的元素，其值为“classpath:spring/spring-servlet.xml”，这也是大家常常用来指定SpringMVC配置文件路径的方法。上面那段try,catch块包裹的代码发挥的作用，一个是将“classpath:spring/spring-servlet.xml”这段字符串转换成classpath路径下的一个资源文件，供框架初始化读取配置元素。在我的工程中是在spring文件夹下面的配置文件spring-servlet.xml。 另外一个作用，就是将contextConfigLocation的值读取出来，然后通过setContextConfigLocation()方法设置到DispatcherServlet中，这个setContextConfigLocation()方法是在FrameworkServlet类中定义的，也就是上面继承类图中DispatcherServlet的直接父类。 我们在setContextConfigLocation()方法上面打上一个断点，启动web工程，可以看到下面的调试结果。 HttpServletBean类的作者是大名鼎鼎的Spring之父Rod Johnson。作为POJO编程哲学的大师，他在HttpServletBean这个类的设计中，运用了依赖注入思想完成了配置元素的读取。他抽离出HttpServletBean这个类的目的也在于此，就是“以依赖注入的方式来读取Servlet类的配置信息”，而且这里很明显是一种setter注入。 明白了HttpServletBean类的设计思想，我们也就知道可以如何从中获益。具体来说，我们继承HttpServletBean类（就像DispatcherServlet做的那样），在类中定义一个属性，为这个属性加上setter方法后，我们就可以在元素中为其定义值。在类被初始化后，值就会被注入进来，我们可以直接使用它，避免了样板式的getInitParameter()方法的使用，而且还免费享有Spring中资源编辑器的功能，可以在web.xml中，通过“classpath:”直接指定类路径下的资源文件。 注意，虽然SpringMVC本身为了后面初始化上下文的方便，使用了字符串来声明和设置contextConfigLocation参数，但是将其声明为Resource类型，同样能够成功获取。鼓励读者们自己继承HttpServletBean写一个测试用的Servlet类，并设置一个参数来调试一下，这样能够帮助你更好的理解获取配置参数的过程。 容器上下文的建立上一篇文章中提到过，SpringMVC使用了Spring容器来容纳自己的配置元素，拥有自己的bean容器上下文。在SpringMVC初始化的过程中，非常关键的一步就是要建立起这个容器上下文，而这个建立上下文的过程，发生在FrameworkServlet类中，由上面init()方法中的initServletBean()方法触发。 123456789101112131415161718192021222324252627@Override protected final void initServletBean() throws ServletException &#123; getServletContext().log(&quot;Initializing Spring FrameworkServlet &apos;&quot; + getServletName() + &quot;&apos;&quot;); if (this.logger.isInfoEnabled()) &#123; this.logger.info(&quot;FrameworkServlet &apos;&quot; + getServletName() + &quot;&apos;: initialization started&quot;); &#125; long startTime = System.currentTimeMillis(); try &#123; this.webApplicationContext = initWebApplicationContext(); initFrameworkServlet(); &#125; catch (ServletException ex) &#123; this.logger.error(&quot;Context initialization failed&quot;, ex); throw ex; &#125; catch (RuntimeException ex) &#123; this.logger.error(&quot;Context initialization failed&quot;, ex); throw ex; &#125; if (this.logger.isInfoEnabled()) &#123; long elapsedTime = System.currentTimeMillis() - startTime; this.logger.info(&quot;FrameworkServlet &apos;&quot; + getServletName() + &quot;&apos;: initialization completed in &quot; + elapsedTime + &quot; ms&quot;); &#125; &#125; initFrameworkServlet()方法是一个没有任何实现的空方法，除去一些样板式的代码，那么这个initServletBean()方法所做的事情已经非常明白： 1this.webApplicationContext = initWebApplicationContext(); 这一句简单直白的代码，道破了FrameworkServlet这个类，在SpringMVC类体系中的设计目的，它是 用来抽离出建立 WebApplicationContext 上下文这个过程的。 initWebApplicationContext()方法，封装了建立Spring容器上下文的整个过程，方法内的逻辑如下： 获取由ContextLoaderListener初始化并注册在ServletContext中的根上下文，记为rootContext 如果webApplicationContext已经不为空，表示这个Servlet类是通过编程式注册到容器中的（Servlet 3.0+中的ServletContext.addServlet() ），上下文也由编程式传入。若这个传入的上下文还没被初始化，将rootContext上下文设置为它的父上下文，然后将其初始化，否则直接使用。 通过wac变量的引用是否为null，判断第2步中是否已经完成上下文的设置（即上下文是否已经用编程式方式传入），如果wac==null成立，说明该Servlet不是由编程式注册到容器中的。此时以contextAttribute属性的值为键，在ServletContext中查找上下文，查找得到，说明上下文已经以别的方式初始化并注册在contextAttribute下，直接使用。 检查wac变量的引用是否为null，如果wac==null成立，说明2、3两步中的上下文初始化策略都没成功，此时调用createWebApplicationContext(rootContext)，建立一个全新的以rootContext为父上下文的上下文，作为SpringMVC配置元素的容器上下文。大多数情况下我们所使用的上下文，就是这个新建的上下文。 以上三种初始化上下文的策略，都会回调onRefresh(ApplicationContext context)方法（回调的方式根据不同策略有不同），onRefresh方法在DispatcherServlet类中被覆写，以上面得到的上下文为依托，完成SpringMVC中默认实现类的初始化。 最后，将这个上下文发布到ServletContext中，也就是将上下文以一个和Servlet类在web.xml中注册名字有关的值为键，设置为ServletContext的一个属性。你可以通过改变publishContext的值来决定是否发布到ServletContext中，默认为true。 以上面6点跟踪FrameworkServlet类中的代码，可以比较清晰的了解到整个容器上下文的建立过程，也就能够领会到FrameworkServlet类的设计目的，它是用来建立一个和Servlet关联的Spring容器上下文，并将其注册到ServletContext中的。跳脱开SpringMVC体系，我们也能通过继承FrameworkServlet类，得到与Spring容器整合的好处，FrameworkServlet和HttpServletBean一样，是一个可以独立使用的类。整个SpringMVC设计中，处处体现开闭原则，这里显然也是其中一点。 初始化SpringMVC默认实现类初始化流程在FrameworkServlet类中流转，建立了上下文后，通过onRefresh(ApplicationContext context)方法的回调，进入到DispatcherServlet类中。 1234@Override protected void onRefresh(ApplicationContext context) &#123; initStrategies(context); &#125; DispatcherServlet类覆写了父类FrameworkServlet中的onRefresh(ApplicationContext context)方法，提供了SpringMVC各种编程元素的初始化。当然这些编程元素，都是作为容器上下文中一个个bean而存在的。具体的初始化策略，在initStrategies()方法中封装。 1234567891011protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context); &#125; 我们以其中initHandlerMappings(context)方法为例，分析一下这些SpringMVC编程元素的初始化策略，其他的方法，都是以类似的策略初始化的。 1234567891011121314151617181920212223242526272829303132private void initHandlerMappings(ApplicationContext context) &#123; this.handlerMappings = null; if (this.detectAllHandlerMappings) &#123; // Find all HandlerMappings in the ApplicationContext, including ancestor contexts. Map&lt;String, HandlerMapping&gt; matchingBeans = BeanFactoryUtils.beansOfTypeIncludingAncestors(context, HandlerMapping.class, true, false); if (!matchingBeans.isEmpty()) &#123; this.handlerMappings = new ArrayList&lt;HandlerMapping&gt;(matchingBeans.values()); // We keep HandlerMappings in sorted order. OrderComparator.sort(this.handlerMappings); &#125; &#125; else &#123; try &#123; HandlerMapping hm = context.getBean(HANDLER_MAPPING_BEAN_NAME, HandlerMapping.class); this.handlerMappings = Collections.singletonList(hm); &#125; catch (NoSuchBeanDefinitionException ex) &#123; // Ignore, we&apos;ll add a default HandlerMapping later. &#125; &#125; // Ensure we have at least one HandlerMapping, by registering // a default HandlerMapping if no other mappings are found. if (this.handlerMappings == null) &#123; this.handlerMappings = getDefaultStrategies(context, HandlerMapping.class); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;No HandlerMappings found in servlet &apos;&quot; + getServletName() + &quot;&apos;: using default&quot;); &#125; &#125; &#125; detectAllHandlerMappings变量默认为true，所以在初始化HandlerMapping接口默认实现类的时候，会把上下文中所有HandlerMapping类型的Bean都注册在handlerMappings这个List变量中。如果你手工将其设置为false，那么将尝试获取名为handlerMapping的Bean，新建一个只有一个元素的List，将其赋给handlerMappings。如果经过上面的过程，handlerMappings变量仍为空，那么说明你没有在上下文中提供自己HandlerMapping类型的Bean定义。此时，SpringMVC将采用默认初始化策略来初始化handlerMappings。 点进去getDefaultStrategies看一下。 123456789101112131415161718192021222324252627282930@SuppressWarnings(&quot;unchecked&quot;) protected &lt;T&gt; List&lt;T&gt; getDefaultStrategies(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; String key = strategyInterface.getName(); String value = defaultStrategies.getProperty(key); if (value != null) &#123; String[] classNames = StringUtils.commaDelimitedListToStringArray(value); List&lt;T&gt; strategies = new ArrayList&lt;T&gt;(classNames.length); for (String className : classNames) &#123; try &#123; Class&lt;?&gt; clazz = ClassUtils.forName(className, DispatcherServlet.class.getClassLoader()); Object strategy = createDefaultStrategy(context, clazz); strategies.add((T) strategy); &#125; catch (ClassNotFoundException ex) &#123; throw new BeanInitializationException( &quot;Could not find DispatcherServlet&apos;s default strategy class [&quot; + className + &quot;] for interface [&quot; + key + &quot;]&quot;, ex); &#125; catch (LinkageError err) &#123; throw new BeanInitializationException( &quot;Error loading DispatcherServlet&apos;s default strategy class [&quot; + className + &quot;] for interface [&quot; + key + &quot;]: problem with class file or dependent class&quot;, err); &#125; &#125; return strategies; &#125; else &#123; return new LinkedList&lt;T&gt;(); &#125; &#125; 它是一个范型的方法，承担所有SpringMVC编程元素的默认初始化策略。方法的内容比较直白，就是以传递类的名称为键，从defaultStrategies这个Properties变量中获取实现类，然后反射初始化。 需要说明一下的是defaultStrategies变量的初始化，它是在DispatcherServlet的静态初始化代码块中加载的。 1234567891011121314private static final Properties defaultStrategies; static &#123; // Load default strategy implementations from properties file. // This is currently strictly internal and not meant to be customized // by application developers. try &#123; ClassPathResource resource = new ClassPathResource(DEFAULT_STRATEGIES_PATH, DispatcherServlet.class); defaultStrategies = PropertiesLoaderUtils.loadProperties(resource); &#125; catch (IOException ex) &#123; throw new IllegalStateException(&quot;Could not load &apos;DispatcherServlet.properties&apos;: &quot; + ex.getMessage()); &#125; &#125; 1private static final String DEFAULT_STRATEGIES_PATH = &quot;DispatcherServlet.properties&quot;; 这个DispatcherServlet.properties里面，以键值对的方式，记录了SpringMVC默认实现类，它在spring-webmvc-3.1.3.RELEASE.jar这个jar包内，在org.springframework.web.servlet包里面。 123456789101112131415161718192021222324# Default implementation classes for DispatcherServlet&apos;s strategy interfaces.# Used as fallback when no matching beans are found in the DispatcherServlet context.# Not meant to be customized by application developers.org.springframework.web.servlet.LocaleResolver=org.springframework.web.servlet.i18n.AcceptHeaderLocaleResolverorg.springframework.web.servlet.ThemeResolver=org.springframework.web.servlet.theme.FixedThemeResolverorg.springframework.web.servlet.HandlerMapping=org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping,\ org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMappingorg.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter,\ org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter,\ org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapterorg.springframework.web.servlet.HandlerExceptionResolver=org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerExceptionResolver,\ org.springframework.web.servlet.mvc.annotation.ResponseStatusExceptionResolver,\ org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolverorg.springframework.web.servlet.RequestToViewNameTranslator=org.springframework.web.servlet.view.DefaultRequestToViewNameTranslatororg.springframework.web.servlet.ViewResolver=org.springframework.web.servlet.view.InternalResourceViewResolverorg.springframework.web.servlet.FlashMapManager=org.springframework.web.servlet.support.SessionFlashMapManager 至此，我们分析完了initHandlerMappings(context)方法的执行过程，其他的初始化过程与这个方法非常类似。所有初始化方法执行完后，SpringMVC正式完成初始化，静静等待Web请求的到来。 总结回顾整个SpringMVC的初始化流程，我们看到，通过HttpServletBean、FrameworkServlet、DispatcherServlet三个不同的类层次，SpringMVC的设计者将三种不同的职责分别抽象，运用模版方法设计模式分别固定在三个类层次中。其中HttpServletBean完成的是配置元素的依赖注入，FrameworkServlet完成的是容器上下文的建立，DispatcherServlet完成的是SpringMVC具体编程元素的初始化策略。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析2：SpringMVC设计理念与DispatcherServlet]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%EF%BC%9ASpringMVC%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5%E4%B8%8EDispatcherServlet%2F</url>
    <content type="text"><![CDATA[转自：https://my.oschina.net/lichhao/blog 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 SpringMVC简介SpringMVC作为Struts2之后异军突起的一个表现层框架，正越来越流行，相信javaee的开发者们就算没使用过SpringMVC，也应该对其略有耳闻。我试图通过对SpringMVC的设计思想和源码实现的剖析，从抽象意义上的设计层面和实现意义上的代码层面两个方面，逐一揭开SpringMVC神秘的面纱，本文的代码，都是基于Spring的 3.1.3RELEASE版本。 任何一个框架，都有自己特定的适用领域，框架的设计和实现，必定是为了应付该领域内许多通用的，烦琐的、基础的工作而生。SpringMVC作为一个表现层框架，也必须直面Web开发领域中表现层中的几大课题，并给出自己的回答： URL到框架的映射。 http请求参数绑定 http响应的生成和输出 这三大课题，组成一个完整的web请求流程，每一个部分都具有非常广阔的外延。SpringMVC框架对这些课题的回答又是什么呢？ 学习一个框架，首要的是要先领会它的设计思想。从抽象、从全局上来审视这个框架。其中最具有参考价值的，就是这个框架所定义的核心接口。核心接口定义了框架的骨架，也在最抽象的意义上表达了框架的设计思想。 下面我以一个web请求流程为载体，依次介绍SpringMVC的核心接口和类。 用户在浏览器中，输入了http://www.xxxx.com/aaa/bbb.ccc的地址，回车后，浏览器发起一个http请求。请求到达你的服务器后，首先会被SpringMVC注册在web.xml中的前端转发器DispatcherServlet接收，DispatcherServlet是一个标准的Servlet，它的作用是接受和转发web请求到内部框架处理单元。 HandlerMapping接口下面看一下第一个出现在你面前的核心接口，它是在org.springframework.web.servlet包中定义的HandlerMapping接口： 12345678910111213141516171819package org.springframework.web.servlet;import javax.servlet.http.HttpServletRequest;public interface HandlerMapping &#123; String PATH_WITHIN_HANDLER_MAPPING_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.pathWithinHandlerMapping&quot;; String BEST_MATCHING_PATTERN_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.bestMatchingPattern&quot;; String INTROSPECT_TYPE_LEVEL_MAPPING = HandlerMapping.class.getName() + &quot;.introspectTypeLevelMapping&quot;; String URI_TEMPLATE_VARIABLES_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.uriTemplateVariables&quot;; String PRODUCIBLE_MEDIA_TYPES_ATTRIBUTE = HandlerMapping.class.getName() + &quot;.producibleMediaTypes&quot;; HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception;&#125; 为了阅读方便，我去掉了源码中的注释，但是我强烈建议你一定要记得去阅读它，这样你才能从框架的设计者口中得到最准确的关于这个类或者接口的设计说明。类中定义的几个常量，我们先不去管它。关键在于这个接口中唯一的方法： 1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; 这个方法就算对于一个java初学者来说，也很容易理解：它只有一个类型为HttpServletRequest的参数，throws Exception的声明表示它不处理任何类型的异常，HandlerExecutionChain是它的返回类型。 DispatcherServlet接受请求并找到对应Handler回到DispatcherServlet的处理流程，当DispatcherServlet接收到web请求后，由标准Servlet类处理方法doGet或者doPost，经过几次转发后，最终注册在DispatcherServlet类中的HandlerMapping实现类组成的一个List（有点拗口）会在一个循环中被遍历。以该web请求的HttpServletRequest对象为参数，依次调用其getHandler方法，第一个不为null的调用结果，将被返回。DispatcherServlet类中的这个遍历方法不长，贴一下，让大家有更直观的了解。 12345678910111213141516171819/** * Return the HandlerExecutionChain for this request. * &lt;p&gt;Tries all handler mappings in order. * @param request current HTTP request * @return the HandlerExecutionChain, or &lt;code&gt;null&lt;/code&gt; if no handler could be found */ protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123; for (HandlerMapping hm : this.handlerMappings) &#123; if (logger.isTraceEnabled()) &#123; logger.trace( &quot;Testing handler map [&quot; + hm + &quot;] in DispatcherServlet with name &apos;&quot; + getServletName() + &quot;&apos;&quot;); &#125; HandlerExecutionChain handler = hm.getHandler(request); if (handler != null) &#123; return handler; &#125; &#125; return null; &#125; 是的，第一步处理就这么简单的完成了。一个web请求经过处理后，会得到一个HandlerExecutionChain对象，这就是SpringMVC对URl映射给出的回答。需要留意的是，HandlerMapping接口的getHandler方法参数是HttpServletRequest，这意味着，HandlerMapping的实现类可以利用HttpServletRequest中的 所有信息来做出这个HandlerExecutionChain对象的生成”决策“。这包括，请求头、url路径、cookie、session、参数等等一切你从一个web请求中可以得到的任何东西（最常用的是url路径）。 SpirngMVC的第一个扩展点，就出现在这里。我们可以编写任意的HandlerMapping实现类，依据任何策略来决定一个web请求到HandlerExecutionChain对象的生成。可以说，从第一个核心接口的声明开始，SpringMVC就把自己的灵活性和野心暴露无疑：哥玩的就是”Open-Closed“。 HandlerExecutionChain这个类，就是我们下一个要了解的核心类。从名字可以直观的看得出，这个对象是一个执行链的封装。熟悉Struts2的都知道，Action对象也是被层层拦截器包装，这里可以做个类比，说明SpringMVC确实是吸收了Struts2的部分设计思想。 HandlerExecutionChain类的代码不长，它定义在org.springframework.web.servlet包中，为了更直观的理解，先上代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package org.springframework.web.servlet;import java.util.ArrayList;import java.util.Arrays;import java.util.List;import org.springframework.util.CollectionUtils;public class HandlerExecutionChain &#123; private final Object handler; private HandlerInterceptor[] interceptors; private List&lt;HandlerInterceptor&gt; interceptorList; public HandlerExecutionChain(Object handler) &#123; this(handler, null); &#125; public HandlerExecutionChain(Object handler, HandlerInterceptor[] interceptors) &#123; if (handler instanceof HandlerExecutionChain) &#123; HandlerExecutionChain originalChain = (HandlerExecutionChain) handler; this.handler = originalChain.getHandler(); this.interceptorList = new ArrayList&lt;HandlerInterceptor&gt;(); CollectionUtils.mergeArrayIntoCollection(originalChain.getInterceptors(), this.interceptorList); CollectionUtils.mergeArrayIntoCollection(interceptors, this.interceptorList); &#125; else &#123; this.handler = handler; this.interceptors = interceptors; &#125; &#125; public Object getHandler() &#123; return this.handler; &#125; public void addInterceptor(HandlerInterceptor interceptor) &#123; initInterceptorList(); this.interceptorList.add(interceptor); &#125; public void addInterceptors(HandlerInterceptor[] interceptors) &#123; if (interceptors != null) &#123; initInterceptorList(); this.interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; private void initInterceptorList() &#123; if (this.interceptorList == null) &#123; this.interceptorList = new ArrayList&lt;HandlerInterceptor&gt;(); &#125; if (this.interceptors != null) &#123; this.interceptorList.addAll(Arrays.asList(this.interceptors)); this.interceptors = null; &#125; &#125; public HandlerInterceptor[] getInterceptors() &#123; if (this.interceptors == null &amp;&amp; this.interceptorList != null) &#123; this.interceptors = this.interceptorList.toArray(new HandlerInterceptor[this.interceptorList.size()]); &#125; return this.interceptors; &#125; @Override public String toString() &#123; if (this.handler == null) &#123; return &quot;HandlerExecutionChain with no handler&quot;; &#125; StringBuilder sb = new StringBuilder(); sb.append(&quot;HandlerExecutionChain with handler [&quot;).append(this.handler).append(&quot;]&quot;); if (!CollectionUtils.isEmpty(this.interceptorList)) &#123; sb.append(&quot; and &quot;).append(this.interceptorList.size()).append(&quot; interceptor&quot;); if (this.interceptorList.size() &gt; 1) &#123; sb.append(&quot;s&quot;); &#125; &#125; return sb.toString(); &#125;&#125; 乱七八糟一大堆，相信你也没全看完，也没必要全看。其实只需要看两行足矣。 123private final Object handler; private HandlerInterceptor[] interceptors; 不出我们所料，一个实质执行对象，还有一堆拦截器。这不就是Struts2中的实现么，SpringMVC没有避嫌，还是采用了这种封装。得到HandlerExecutionChain这个执行链（execution chain）之后，下一步的处理将围绕其展开。 HandlerInterceptor接口HandlerInterceptor也是SpringMVC的核心接口，定义如下： 12345678910111213141516171819package org.springframework.web.servlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public interface HandlerInterceptor &#123; boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; void postHandle( HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception; void afterCompletion( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception;&#125; 至此，HandlerExecutionChain整个执行脉络也就清楚了：在真正调用其handler对象前，HandlerInterceptor接口实现类组成的数组将会被遍历，其preHandle方法会被依次调用，然后真正的handler对象将被调用。 handler对象被调用后，就生成了需要的响应数据，在将处理结果写到HttpServletResponse对象之前（SpringMVC称为渲染视图），其postHandle方法会被依次调用。视图渲染完成后，最后afterCompletion方法会被依次调用，整个web请求的处理过程就结束了。 在一个处理对象执行之前，之后利用拦截器做文章，这已经成为一种经典的框架设计套路。Struts2中的拦截器会做诸如参数绑定这类复杂的工作，那么SpringMVC的拦截器具体做些什么呢？我们暂且不关心，虽然这是很重要的细节，但细节毕竟是细节，我们先来理解更重要的东西。 HandlerInterceptor，是SpringMVC的第二个扩展点的暴露，通过自定义拦截器，我们可以在一个请求被真正处理之前、请求被处理但还没输出到响应中、请求已经被输出到响应中之后这三个时间点去做任何我们想要做的事情。Struts2框架的成功，就是源于这种拦截器的设计，SpringMVC吸收了这种设计思想，并推陈出新，更合理的划分了三个不同的时间点，从而给web请求处理这个流程，提供了更大的扩展性。 这个HandlerExecutionChain类中以Object引用所声明的handler对象，到底是个什么东东？它是怎么被调用的？ HandlerAdapter回答这些问题之前，先看SpringMVC中的又一个核心接口，HandlerAdapter： 1234567891011121314package org.springframework.web.servlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public interface HandlerAdapter &#123; boolean supports(Object handler); ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; long getLastModified(HttpServletRequest request, Object handler);&#125; 在DispatcherServlet中，除了HandlerMapping实现类的列表，同样也注册了一个HandlerAdapter实现类组成的列表，有代码为证。 12345/** List of HandlerMappings used by this servlet */ private List&lt;HandlerMapping&gt; handlerMappings; /** List of HandlerAdapters used by this servlet */ private List&lt;HandlerAdapter&gt; handlerAdapters; 接下来，我们再以DispatcherServlet类中另外一段代码来回答上述的问题： 1234567891011121314151617/** * Return the HandlerAdapter for this handler object. * @param handler the handler object to find an adapter for * @throws ServletException if no HandlerAdapter can be found for the handler. This is a fatal error. */ protected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123; for (HandlerAdapter ha : this.handlerAdapters) &#123; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Testing handler adapter [&quot; + ha + &quot;]&quot;); &#125; if (ha.supports(handler)) &#123; return ha; &#125; &#125; throw new ServletException(&quot;No adapter for handler [&quot; + handler + &quot;]: Does your handler implement a supported interface like Controller?&quot;); &#125; 请求流程总结这段代码已经很明显了，HandlerExecutionChain中的handler对象会被作为参数传递进去，在DispatcherServlet类中注册的HandlerAdapter实现类列表会被遍历，然后返回第一个supports方法返回true的HandlerAdapter对象，用这个HandlerAdapter实现类中的handle方法处理handler对象，并返回ModelAndView这个包含了视图和数据的对象。HandlerAdapter就是SpringMVC提供的第三个扩展点，你可以提供自己的实现类来处理handler对象。 ModelAndView对象的代码就不贴了，它是SpringMVC中对视图和数据的一个聚合类。其中的视图，就是由SpringMVC的最后一个核心接口View所抽象： 123456789101112131415161718package org.springframework.web.servlet;import java.util.Map;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public interface View &#123; String RESPONSE_STATUS_ATTRIBUTE = View.class.getName() + &quot;.responseStatus&quot;; String PATH_VARIABLES = View.class.getName() + &quot;.pathVariables&quot;; String getContentType(); void render(Map&lt;String, ?&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception;&#125; 所有的数据，最后会作为一个Map对象传递到View实现类中的render方法，调用这个render方法，就完成了视图到响应的渲染。这个View实现类，就是来自HandlerAdapter中的handle方法的返回结果。当然从ModelAndView到真正的View实现类有一个解析的过程，ModelAndView中可以有真正的视图对象，也可以只是有一个视图的名字，SpringMVC会负责将视图名称解析为真正的视图对象。 至此，我们了解了一个典型的完整的web请求在SpringMVC中的处理过程和其中涉及到的核心类和接口。 在一个典型的SpringMVC调用中，HandlerExecutionChain中封装handler对象就是用@Controller注解标识的类的一个实例，根据类级别和方法级别的@RequestMapping注解，由默认注册的DefaultAnnotationHandlerMapping（3.1.3中更新为RequestMappingHandlerMapping类，但是为了向后兼容，DefaultAnnotationHandlerMapping也可以使用）生成HandlerExecutionChain对象，再由AnnotationMethodHandlerAdapter（3.1.3中更新为RequestMappingHandlerAdapter类，但是为了向后兼容，AnnotationMethodHandlerAdapter也可以使用）来执行这个HandlerExecutionChain对象，生成最终的ModelAndView对象后，再由具体的View对象的render方法渲染视图。 可以看到，作为一个表现层框架，SpringMVC没有像Struts2那样激进，并没有采用和Web容器完全解耦的设计思想，而是以原生的Servlet框架对象为依托，通过合理的抽象，制定了严谨的的处理流程。这样做的结果是，执行效率比Struts2要高，灵活性也上升了一个层次。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析1：SpringMVC概述]]></title>
    <url>%2F2019%2F09%2F14%2Fspringmvc%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%EF%BC%9ASpringMVC%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[转自：跟开涛学SpringMVC 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Web MVC简介Web开发中的请求-响应模型： 在Web世界里，具体步骤如下： 1、 Web浏览器（如IE）发起请求，如访问http://sishuok.com 2、 Web服务器（如Tomcat）接收请求，处理请求（比如用户新增，则将把用户保存一下），最后产生响应（一般为html）。 3、web服务器处理完成后，返回内容给web客户端（一般就是我们的浏览器），客户端对接收的内容进行处理（如web浏览器将会对接收到的html内容进行渲染以展示给客户）。 因此，在Web世界里： 都是Web客户端发起请求，Web服务器接收、处理并产生响应。 一般Web服务器是不能主动通知Web客户端更新内容。虽然现在有些技术如服务器推（如Comet）、还有现在的HTML5 websocket可以实现Web服务器主动通知Web客户端。 到此我们了解了在web开发时的请求/响应模型，接下来我们看一下标准的MVC模型是什么。 标准MVC模型概述MVC模型：是一种架构型的模式，本身不引入新功能，只是帮助我们将开发的结构组织的更加合理，使展示与模型分离、流程控制逻辑、业务逻辑调用与展示逻辑分离。如图1-2 图1-2 首先让我们了解下MVC（Model-View-Controller）三元组的概念： Model（模型）：数据模型，提供要展示的数据，因此包含数据和行为，可以认为是领域模型或JavaBean组件（包含数据和行为），不过现在一般都分离开来：Value Object（数据） 和 服务层（行为）。也就是模型提供了模型数据查询和模型数据的状态更新等功能，包括数据和业务。 View（视图）：负责进行模型的展示，一般就是我们见到的用户界面，客户想看到的东西。 Controller（控制器）：接收用户请求，委托给模型进行处理（状态改变），处理完毕后把返回的模型数据返回给视图，由视图负责展示。 也就是说控制器做了个调度员的工作，。 从图1-1我们还看到，在标准的MVC中模型能主动推数据给视图进行更新（观察者设计模式，在模型上注册视图，当模型更新时自动更新视图），但在Web开发中模型是无法主动推给视图（无法主动更新用户界面），因为在Web开发是请求-响应模型。 那接下来我们看一下在Web里MVC是什么样子，我们称其为 Web MVC 来区别标准的MVC。 Web MVC概述模型-视图-控制器概念和标准MVC概念一样，请参考1.2，我们再看一下Web MVC标准架构，如图1-3： 如图1-3 在Web MVC模式下，模型无法主动推数据给视图，如果用户想要视图更新，需要再发送一次请求（即请求-响应模型）。 概念差不多了，我们接下来了解下Web端开发的发展历程，和使用代码来演示一下Web MVC是如何实现的，还有为什么要使用MVC这个模式呢？ Web端开发发展历程此处我们只是简单的叙述比较核心的历程，如图1-4 图1-4 1.4.1、CGI：（Common Gateway Interface）公共网关接口，一种在web服务端使用的脚本技术，使用C或Perl语言编写，用于接收web用户请求并处理，最后动态产生响应给用户，但每次请求将产生一个进程，重量级。 1.4.2、Servlet：一种JavaEE web组件技术，是一种在服务器端执行的web组件，用于接收web用户请求并处理，最后动态产生响应给用户。但每次请求只产生一个线程（而且有线程池），轻量级。而且能利用许多JavaEE技术（如JDBC等）。本质就是在java代码里面 输出 html流。但表现逻辑、控制逻辑、业务逻辑调用混杂。如图1-5 图1-5 如图1-5，这种做法是绝对不可取的，控制逻辑、表现代码、业务逻辑对象调用混杂在一起，最大的问题是直接在Java代码里面输出Html，这样前端开发人员无法进行页面风格等的设计与修改，即使修改也是很麻烦，因此实际项目这种做法不可取。 1.4.3、JSP：（Java Server Page）：一种在服务器端执行的web组件，是一种运行在标准的HTML页面中嵌入脚本语言（现在只支持Java）的模板页面技术。本质就是在html代码中嵌入java代码。JSP最终还是会被编译为Servlet，只不过比纯Servlet开发页面更简单、方便。但表现逻辑、控制逻辑、业务逻辑调用还是混杂。如图1-6 图1-6 如图1-6，这种做法也是绝对不可取的，控制逻辑、表现代码、业务逻辑对象调用混杂在一起，但比直接在servlet里输出html要好一点，前端开发人员可以进行简单的页面风格等的设计与修改（但如果嵌入的java脚本太多也是很难修改的），因此实际项目这种做法不可取。 JSP本质还是Servlet，最终在运行时会生成一个Servlet（如tomcat，将在tomcat\work\Catalina\web应用名\org\apache\jsp下生成），但这种使得写html简单点，但仍是控制逻辑、表现代码、业务逻辑对象调用混杂在一起。 1.4.4、Model1：可以认为是JSP的增强版，可以认为是jsp+javabean如图1-7 特点：使用jsp:useBean标准动作，自动将请求参数封装为JavaBean组件；还必须使用java脚本执行控制逻辑。 图1-7 此处我们可以看出，使用jsp:useBean标准动作可以简化javabean的获取/创建，及将请求参数封装到javabean，再看一下Model1架构，如图1-8。 图1-8 Model1架构 Model1架构中，JSP负责控制逻辑、表现逻辑、业务对象（javabean）的调用，只是比纯JSP简化了获取请求参数和封装请求参数。同样是不好的，在项目中应该严禁使用（或最多再demo里使用）。 1.4.5、Model2：在JavaEE世界里，它可以认为就是Web MVC模型 Model2架构其实可以认为就是我们所说的Web MVC模型，只是控制器采用Servlet、模型采用JavaBean、视图采用JSP，如图1-9 图1-9 Model2架构 具体代码事例如下： 从Model2架构可以看出，视图和模型分离了，控制逻辑和展示逻辑分离了。 但我们也看到严重的缺点： 1． 1、控制器： 1．1．1、控制逻辑可能比较复杂，其实我们可以按照规约，如请求参数submitFlag=toAdd，我们其实可以直接调用toAdd方法，来简化控制逻辑；而且每个模块基本需要一个控制器，造成控制逻辑可能很复杂； 1．1．2、请求参数到模型的封装比较麻烦，如果能交给框架来做这件事情，我们可以从中得到解放； 1．1．3、选择下一个视图，严重依赖Servlet API，这样很难或基本不可能更换视图； 1．1．4、给视图传输要展示的模型数据，使用Servlet API，更换视图技术也要一起更换，很麻烦。 1.2、模型： 1．2．1、此处模型使用JavaBean，可能造成JavaBean组件类很庞大，一般现在项目都是采用三层架构，而不采用JavaBean。 1.3、视图 1．3．1、现在被绑定在JSP，很难更换视图，比如Velocity、FreeMarker；比如我要支持Excel、PDF视图等等。 1.4.5、服务到工作者：Front Controller + Application Controller + Page Controller + Context 即，前端控制器+应用控制器+页面控制器（也有称其为动作）+上下文，也是Web MVC，只是责任更加明确，详情请参考《核心J2EE设计模式》和《企业应用架构模式》如图1-10： 图1-10 运行流程如下： 职责： Front Controller：前端控制器，负责为表现层提供统一访问点，从而避免Model2中出现的重复的控制逻辑（由前端控制器统一回调相应的功能方法，如前边的根据submitFlag=login转调login方法）；并且可以为多个请求提供共用的逻辑（如准备上下文等等），将选择具体视图和具体的功能处理（如login里边封装请求参数到模型，并调用业务逻辑对象）分离。 Application Controller：应用控制器，前端控制器分离选择具体视图和具体的功能处理之后，需要有人来管理，应用控制器就是用来选择具体视图技术（视图的管理）和具体的功能处理（页面控制器/命令对象/动作管理），一种策略设计模式的应用，可以很容易的切换视图/页面控制器，相互不产生影响。 Page Controller(Command)：页面控制器/动作/处理器：功能处理代码，收集参数、封装参数到模型，转调业务对象处理模型，返回逻辑视图名交给前端控制器（和具体的视图技术解耦），由前端控制器委托给应用控制器选择具体的视图来展示，可以是命令设计模式的实现。页面控制器也被称为处理器或动作。 Context：上下文，还记得Model2中为视图准备要展示的模型数据吗，我们直接放在request中（Servlet API相关），有了上下文之后，我们就可以将相关数据放置在上下文，从而与协议无关（如Servlet API）的访问/设置模型数据，一般通过ThreadLocal模式实现。 到此，我们回顾了整个web开发架构的发展历程，可能不同的web层框架在细节处理方面不同，但的目的是一样的： 干净的web表现层： 模型和视图的分离； 控制器中的控制逻辑与功能处理分离（收集并封装参数到模型对象、业务对象调用）； 控制器中的视图选择与具体视图技术分离。 轻薄的web表现层： 做的事情越少越好，薄薄的，不应该包含无关代码； 只负责收集并组织参数到模型对象，启动业务对象的调用； 控制器只返回逻辑视图名并由相应的应用控制器来选择具体使用的视图策略； 尽量少使用框架特定API，保证容易测试。 到此我们了解Web MVC的发展历程，接下来让我们了解下Spring MVC到底是什么、架构及来个HelloWorld了解下具体怎么使用吧。 本章具体代码请参考 springmvc-chapter1工程。 Spring Web MVC是什么Spring Web MVC是一种基于Java的实现了Web MVC设计模式的请求驱动类型的轻量级Web框架，即使用了MVC架构模式的思想，将web层进行职责解耦，基于请求驱动指的就是使用请求-响应模型，框架的目的就是帮助我们简化开发，Spring Web MVC也是要简化我们日常Web开发的。 另外还有一种基于组件的、事件驱动的Web框架在此就不介绍了，如Tapestry、JSF等。 Spring Web MVC也是服务到工作者模式的实现，但进行可优化。前端控制器是DispatcherServlet；应用控制器其实拆为处理器映射器(Handler Mapping)进行处理器管理和视图解析器(View Resolver)进行视图管理；页面控制器/动作/处理器为Controller接口（仅包含ModelAndView handleRequest(request, response) 方法）的实现（也可以是任何的POJO类）；支持本地化（Locale）解析、主题（Theme）解析及文件上传等；提供了非常灵活的数据验证、格式化和数据绑定机制；提供了强大的约定大于配置（惯例优先原则）的契约式编程支持。 Spring Web MVC能帮我们做什么√让我们能非常简单的设计出干净的Web层和薄薄的Web层； √进行更简洁的Web层的开发； √天生与Spring框架集成（如IoC容器、AOP等）； √提供强大的约定大于配置的契约式编程支持； √能简单的进行Web层的单元测试； √支持灵活的URL到页面控制器的映射； √非常容易与其他视图技术集成，如Velocity、FreeMarker等等，因为模型数据不放在特定的API里，而是放在一个Model里（Map数据结构实现，因此很容易被其他框架使用）； √非常灵活的数据验证、格式化和数据绑定机制，能使用任何对象进行数据绑定，不必实现特定框架的API； √提供一套强大的JSP标签库，简化JSP开发； √支持灵活的本地化、主题等解析； √更加简单的异常处理； √对静态资源的支持； √支持Restful风格。 Spring Web MVC架构Spring Web MVC框架也是一个基于请求驱动的Web框架，并且也使用了前端控制器模式来进行设计，再根据请求映射规则分发给相应的页面控制器（动作/处理器）进行处理。首先让我们整体看一下Spring Web MVC处理请求的流程： Spring Web MVC处理请求的流程如图2-1 图2-1 具体执行步骤如下： 1、 首先用户发送请求————&gt;前端控制器，前端控制器根据请求信息（如URL）来决定选择哪一个页面控制器进行处理并把请求委托给它，即以前的控制器的控制逻辑部分；图2-1中的1、2步骤； 2、 页面控制器接收到请求后，进行功能处理，首先需要收集和绑定请求参数到一个对象，这个对象在Spring Web MVC中叫命令对象，并进行验证，然后将命令对象委托给业务对象进行处理；处理完毕后返回一个ModelAndView（模型数据和逻辑视图名）；图2-1中的3、4、5步骤； 3、 前端控制器收回控制权，然后根据返回的逻辑视图名，选择相应的视图进行渲染，并把模型数据传入以便视图渲染；图2-1中的步骤6、7； 4、 前端控制器再次收回控制权，将响应返回给用户，图2-1中的步骤8；至此整个结束。 问题： 1、 请求如何给前端控制器？ 2、 前端控制器如何根据请求信息选择页面控制器进行功能处理？ 3、 如何支持多种页面控制器呢？ 4、 如何页面控制器如何使用业务对象？ 5、 页面控制器如何返回模型数据？ 6、 前端控制器如何根据页面控制器返回的逻辑视图名选择具体的视图进行渲染？ 7、 不同的视图技术如何使用相应的模型数据？ 首先我们知道有如上问题，那这些问题如何解决呢？请让我们先继续，在后边依次回答。 Spring Web MVC架构1、Spring Web MVC核心架构图，如图2-2 图2-2 架构图对应的DispatcherServlet核心代码如下： java代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687//前端控制器分派方法 protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; int interceptorIndex = -1; try &#123; ModelAndView mv; boolean errorView = false; try &#123; //检查是否是请求是否是multipart（如文件上传），如果是将通过MultipartResolver解析 processedRequest = checkMultipart(request); //步骤2、请求到处理器（页面控制器）的映射，通过HandlerMapping进行映射 mappedHandler = getHandler(processedRequest, false); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; //步骤3、处理器适配，即将我们的处理器包装成相应的适配器（从而支持多种类型的处理器） HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 304 Not Modified缓存支持 //此处省略具体代码 // 执行处理器相关的拦截器的预处理（HandlerInterceptor.preHandle） //此处省略具体代码 // 步骤4、由适配器执行处理器（调用处理器相应功能处理方法） mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); // Do we need view name translation? if (mv != null &amp;&amp; !mv.hasView()) &#123; mv.setViewName(getDefaultViewName(request)); &#125; // 执行处理器相关的拦截器的后处理（HandlerInterceptor.postHandle） //此处省略具体代码 &#125; catch (ModelAndViewDefiningException ex) &#123; logger.debug(&quot;ModelAndViewDefiningException encountered&quot;, ex); mv = ex.getModelAndView(); &#125; catch (Exception ex) &#123; Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); mv = processHandlerException(processedRequest, response, handler, ex); errorView = (mv != null); &#125; //步骤5 步骤6、解析视图并进行视图的渲染 //步骤5 由ViewResolver解析View（viewResolver.resolveViewName(viewName, locale)） //步骤6 视图在渲染时会把Model传入（view.render(mv.getModelInternal(), request, response);） if (mv != null &amp;&amp; !mv.wasCleared()) &#123; render(mv, processedRequest, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Null ModelAndView returned to DispatcherServlet with name &apos;&quot; + getServletName() + &quot;&apos;: assuming HandlerAdapter completed request handling&quot;); &#125; &#125; // 执行处理器相关的拦截器的完成后处理（HandlerInterceptor.afterCompletion） //此处省略具体代码 catch (Exception ex) &#123; // Trigger after-completion for thrown exception. triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, ex); throw ex; &#125; catch (Error err) &#123; ServletException ex = new NestedServletException(&quot;Handler processing failed&quot;, err); // Trigger after-completion for thrown exception. triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, ex); throw ex; &#125; finally &#123; // Clean up any resources used by a multipart request. if (processedRequest != request) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; 核心架构的具体流程步骤如下： 1、 首先用户发送请求——&gt;DispatcherServlet，前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理，作为统一访问点，进行全局的流程控制； 2、 DispatcherServlet——&gt;HandlerMapping， HandlerMapping将会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象、多个HandlerInterceptor拦截器）对象，通过这种策略模式，很容易添加新的映射策略； 3、 DispatcherServlet——&gt;HandlerAdapter，HandlerAdapter将会把处理器包装为适配器，从而支持多种类型的处理器，即适配器设计模式的应用，从而很容易支持很多类型的处理器； 4、 HandlerAdapter——&gt;处理器功能处理方法的调用，HandlerAdapter将会根据适配的结果调用真正的处理器的功能处理方法，完成功能处理；并返回一个ModelAndView对象（包含模型数据、逻辑视图名）； 5、 ModelAndView的逻辑视图名——&gt; ViewResolver， ViewResolver将把逻辑视图名解析为具体的View，通过这种策略模式，很容易更换其他视图技术； 6、 View——&gt;渲染，View会根据传进来的Model模型数据进行渲染，此处的Model实际是一个Map数据结构，因此很容易支持其他视图技术； 7、返回控制权给DispatcherServlet，由DispatcherServlet返回响应给用户，到此一个流程结束。 此处我们只是讲了核心流程，没有考虑拦截器、本地解析、文件上传解析等，后边再细述。 到此，再来看我们前边提出的问题： 1、 请求如何给前端控制器？这个应该在web.xml中进行部署描述，在HelloWorld中详细讲解。 2、 前端控制器如何根据请求信息选择页面控制器进行功能处理？ 我们需要配置HandlerMapping进行映射 3、 如何支持多种页面控制器呢？配置HandlerAdapter从而支持多种类型的页面控制器 4、 如何页面控制器如何使用业务对象？可以预料到，肯定利用Spring IoC容器的依赖注入功能 5、 页面控制器如何返回模型数据？使用ModelAndView返回 6、 前端控制器如何根据页面控制器返回的逻辑视图名选择具体的视图进行渲染？ 使用ViewResolver进行解析 7、 不同的视图技术如何使用相应的模型数据？ 因为Model是一个Map数据结构，很容易支持其他视图技术 在此我们可以看出具体的核心开发步骤： 1、 DispatcherServlet在web.xml中的部署描述，从而拦截请求到Spring Web MVC 2、 HandlerMapping的配置，从而将请求映射到处理器 3、 HandlerAdapter的配置，从而支持多种类型的处理器 4、 ViewResolver的配置，从而将逻辑视图名解析为具体视图技术 5、处理器（页面控制器）的配置，从而进行功能处理 上边的开发步骤我们会在Hello World中详细验证。 Spring Web MVC优势1、清晰的角色划分：前端控制器（DispatcherServlet）、请求到处理器映射（HandlerMapping）、处理器适配器（HandlerAdapter）、视图解析器（ViewResolver）、处理器或页面控制器（Controller）、验证器（ Validator）、命令对象（Command 请求参数绑定到的对象就叫命令对象）、表单对象（Form Object 提供给表单展示和提交到的对象就叫表单对象）。 2、分工明确，而且扩展点相当灵活，可以很容易扩展，虽然几乎不需要； 3、由于命令对象就是一个POJO，无需继承框架特定API，可以使用命令对象直接作为业务对象； 4、和Spring 其他框架无缝集成，是其它Web框架所不具备的； 5、可适配，通过HandlerAdapter可以支持任意的类作为处理器； 6、可定制性，HandlerMapping、ViewResolver等能够非常简单的定制； 7、功能强大的数据验证、格式化、绑定机制； 8、利用Spring提供的Mock对象能够非常简单的进行Web层单元测试； 9、本地化、主题的解析的支持，使我们更容易进行国际化和主题的切换。 10、强大的JSP标签库，使JSP编写更容易。 ………………还有比如RESTful风格的支持、简单的文件上传、约定大于配置的契约式编程支持、基于注解的零配置支持等等。 DispatcherServlet作用DispatcherServlet是前端控制器设计模式的实现，提供Spring Web MVC的集中访问点，而且负责职责的分派，而且与Spring IoC容器无缝集成，从而可以获得Spring的所有好处。 具体请参考第二章的图2-1。 DispatcherServlet主要用作职责调度工作，本身主要用于控制流程，主要职责如下： 1、文件上传解析，如果请求类型是multipart将通过MultipartResolver进行文件上传解析； 2、通过HandlerMapping，将请求映射到处理器（返回一个HandlerExecutionChain，它包括一个处理器、多个HandlerInterceptor拦截器）； 3、通过HandlerAdapter支持多种类型的处理器(HandlerExecutionChain中的处理器)； 4、通过ViewResolver解析逻辑视图名到具体视图实现； 5、本地化解析； 6、渲染具体的视图等； 7、如果执行过程中遇到异常将交给HandlerExceptionResolver来解析。 从以上我们可以看出DispatcherServlet主要负责流程的控制（而且在流程中的每个关键点都是很容易扩展的）。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析9：Spring事务源码剖析]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%909%EF%BC%9ASpring%E4%BA%8B%E5%8A%A1%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[转自：http://www.linkedkeeper.com/detail/blog.action?bid=1045 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 声明式事务使用Spring事务是我们日常工作中经常使用的一项技术，Spring提供了编程、注解、aop切面三种方式供我们使用Spring事务，其中编程式事务因为对代码入侵较大所以不被推荐使用，注解和aop切面的方式可以基于需求自行选择，我们以注解的方式为例来分析Spring事务的原理和源码实现。 首先我们简单看一下Spring事务的使用方式，配置： 12345&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; 在需要开启事务的方法上加上@Transactional注解即可，这里需要注意的是，当tx:annotation-driven标签在不指定transaction-manager属性的时候，会默认寻找id固定名为transactionManager的bean作为事务管理器，如果没有id为transactionManager的bean并且在使用@Transactional注解时也没有指定value（事务管理器），程序就会报错。当我们在配置两个以上的tx:annotation-driven标签时，如下： 12345678910 &lt;tx:annotation-driven transaction-manager=&quot;transactionManager1&quot;/&gt;&lt;bean id=&quot;transactionManager1&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource1&quot;/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager2&quot;/&gt;&lt;bean id=&quot;transactionManager2&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource2&quot;/&gt;&lt;/bean&gt; 这时第一个tx:annotation-driven会生效，也就是当我们使用@Transactional注解时不指定事务管理器，默认使用的事务管理器是transactionManager1，后文分析源码时会具体提到这些注意点。 下面我们开始分析Spring的相关源码，首先看一下对tx:annotation-driven标签的解析，这里需要读者对Spring自定义标签解析的过程有一定的了解，笔者后续也会出相关的文章。锁定TxNamespaceHandler： TxNamespaceHandler(右键可查看大图) 注册事务功能bean这个方法比较长，关键的部分做了标记，最外围的if判断限制了tx:annotation-driven标签只能被解析一次，所以只有第一次被解析的标签会生效。蓝色框的部分分别注册了三个BeanDefinition，分别为AnnotationTransactionAttributeSource、TransactionInterceptor和BeanFactoryTransactionAttributeSourceAdvisor，并将前两个BeanDefinition添加到第三个BeanDefinition的属性当中，这三个bean支撑了整个事务功能，后面会详细说明。我们先来看红色框的第个方法： 还记得当tx:annotation-driven标签在不指定transaction-manager属性的时候，会默认寻找id固定名为transactionManager的bean作为事务管理器这个注意事项么，就是在这里实现的。下面我们来看红色框的第二个方法： 这两个方法的主要目的是注册InfrastructureAdvisorAutoProxyCreator，注册这个类的目的是什么呢？我们看下这个类的层次： 使用bean的后处理方法获取增强器我们发现这个类间接实现了BeanPostProcessor接口，我们知道，Spring会保证所有bean在实例化的时候都会调用其postProcessAfterInitialization方法，我们可以使用这个方法包装和改变bean，而真正实现这个方法是在其父类AbstractAutoProxyCreator类中： 上面这个方法相信大家已经看出了它的目的，先找出所有对应Advisor的类的beanName，再通过beanFactory.getBean方法获取这些bean并返回。不知道大家还是否记得在文章开始的时候提到的三个类，其中BeanFactoryTransactionAttributeSourceAdvisor实现了Advisor接口，所以这个bean就会在此被提取出来，而另外两个bean被织入了BeanFactoryTransactionAttributeSourceAdvisor当中，所以也会一起被提取出来，下图为BeanFactoryTransactionAttributeSourceAdvisor类的层次： Spring获取匹配的增强器下面让我们来看Spring如何在所有候选的增强器中获取匹配的增强器： 上面的方法中提到引介增强的概念，在此做简要说明，引介增强是一种比较特殊的增强类型，它不是在目标方法周围织入增强，而是为目标类创建新的方法和属性，所以引介增强的连接点是类级别的，而非方法级别的。通过引介增强，我们可以为目标类添加一个接口的实现，即原来目标类未实现某个接口，通过引介增强可以为目标类创建实现该接口的代理，使用方法可以参考文末的引用链接。另外这个方法用两个重载的canApply方法为目标类寻找匹配的增强器，其中第一个canApply方法会调用第二个canApply方法并将第三个参数传为false： 在上面BeanFactoryTransactionAttributeSourceAdvisor类的层次中我们看到它实现了PointcutAdvisor接口，所以会调用红框中的canApply方法进行判断，第一个参数pca.getPointcut()也就是调用BeanFactoryTransactionAttributeSourceAdvisor的getPointcut方法： 这里的transactionAttributeSource也就是我们在文章开始看到的为BeanFactoryTransactionAttributeSourceAdvisor织入的两个bean中的AnnotationTransactionAttributeSource，我们以TransactionAttributeSourcePointcut作为第一个参数继续跟踪canApply方法： 我们跟踪pc.getMethodMatcher()方法也就是TransactionAttributeSourcePointcut的getMethodMatcher方法是在它的父类中实现： 发现方法直接返回this，也就是下面methodMatcher.matches方法就是调用TransactionAttributeSourcePointcut的matches方法： 在上面我们看到其实这个tas就是AnnotationTransactionAttributeSource，这里的目的其实也就是判断我们的业务方法或者类上是否有@Transactional注解，跟踪AnnotationTransactionAttributeSource的getTransactionAttribute方法： 方法中的事务声明优先级最高，如果方法上没有声明则在类上寻找： this.annotationParsers是在AnnotationTransactionAttributeSource类初始化的时候初始化的： 所以annotationParser.parseTransactionAnnotation就是调用SpringTransactionAnnotationParser的parseTransactionAnnotation方法： 至此，我们终于看到的Transactional注解，下面无疑就是解析注解当中声明的属性了： Transactional注解 在这个方法中我们看到了在Transactional注解中声明的各种常用或者不常用的属性的解析，至此，事务的初始化工作算是完成了，下面开始真正的进入执行阶段。 在上文AbstractAutoProxyCreator类的wrapIfNecessary方法中，获取到目标bean匹配的增强器之后，会为bean创建代理，这部分内容我们会在Spring AOP的文章中进行详细说明，在此简要说明方便大家理解，在执行代理类的目标方法时，会调用Advisor的getAdvice获取MethodInterceptor并执行其invoke方法，而我们本文的主角BeanFactoryTransactionAttributeSourceAdvisor的getAdvice方法会返回我们在文章开始看到的为其织入的另外一个bean，也就是TransactionInterceptor，它实现了MethodInterceptor，所以我们分析其invoke方法： 这个方法很长，但是整体逻辑还是非常清晰的，首选获取事务属性，这里的getTransactionAttrubuteSource()方法的返回值同样是在文章开始我们看到的被织入到TransactionInterceptor中的AnnotationTransactionAttributeSource，在事务准备阶段已经解析过事务属性并保存到缓存中，所以这里会直接从缓存中获取，接下来获取配置的TransactionManager，也就是determineTransactionManager方法，这里如果配置没有指定transaction-manager并且也没有默认id名为transactionManager的bean，就会报错，然后是针对声明式事务和编程式事务的不同处理，创建事务信息，执行目标方法，最后根据执行结果进行回滚或提交操作，我们先分析创建事务的过程。在分析之前希望大家能先去了解一下Spring的事务传播行为，有助于理解下面的源码，这里做一个简要的介绍，更详细的信息请大家自行查阅Spring官方文档，里面有更新详细的介绍。 Spring的事务传播行为定义在Propagation这个枚举类中，一共有七种，分别为： REQUIRED：业务方法需要在一个容器里运行。如果方法运行时，已经处在一个事务中，那么加入到这个事务，否则自己新建一个新的事务，是默认的事务传播行为。 NOT_SUPPORTED：声明方法不需要事务。如果方法没有关联到一个事务，容器不会为他开启事务，如果方法在一个事务中被调用，该事务会被挂起，调用结束后，原先的事务会恢复执行。 REQUIRESNEW：不管是否存在事务，该方法总汇为自己发起一个新的事务。如果方法已经运行在一个事务中，则原有事务挂起，新的事务被创建。 MANDATORY：该方法只能在一个已经存在的事务中执行，业务方法不能发起自己的事务。如果在没有事务的环境下被调用，容器抛出例外。 SUPPORTS：该方法在某个事务范围内被调用，则方法成为该事务的一部分。如果方法在该事务范围外被调用，该方法就在没有事务的环境下执行。 NEVER：该方法绝对不能在事务范围内执行。如果在就抛例外。只有该方法没有关联到任何事务，才正常执行。 NESTED：如果一个活动的事务存在，则运行在一个嵌套的事务中。如果没有活动事务，则按REQUIRED属性执行。它使用了一个单独的事务，这个事务拥有多个可以回滚的保存点。内部事务的回滚不会对外部事务造成影响。它只对DataSourceTransactionManager事务管理器起效。 开启事务过程 判断当前线程是否存在事务就是判断记录的数据库连接是否为空并且transactionActive状态为true。 REQUIRESNEW会开启一个新事务并挂起原事务，当然开启一个新事务就需要一个新的数据库连接： suspend挂起操作主要目的是将当前connectionHolder置为null，保存原有事务信息，以便于后续恢复原有事务，并将当前正在进行的事务信息进行重置。下面我们看Spring如何开启一个新事务： 这里我们看到了数据库连接的获取，如果是新事务需要获取新一个新的数据库连接，并为其设置了隔离级别、是否只读等属性，下面就是将事务信息记录到当前线程中： 接下来就是记录事务状态并返回事务信息： 然后就是我们目标业务方法的执行了，根据执行结果的不同做提交或回滚操作，我们先看一下回滚操作： 其中回滚条件默认为RuntimeException或Error，我们也可以自行配置。 保存点一般用于嵌入式事务，内嵌事务的回滚不会引起外部事务的回滚。下面我们来看新事务的回滚： 很简单，就是获取当前线程的数据库连接并调用其rollback方法进行回滚，使用的是底层数据库连接提供的API。最后还有一个清理和恢复挂起事务的操作： 如果事务执行前有事务挂起，那么当前事务执行结束后需要将挂起的事务恢复，挂起事务时保存了原事务信息，重置了当前事务信息，所以恢复操作就是将当前的事务信息设置为之前保存的原事务信息。到这里事务的回滚操作就结束了，下面让我们来看事务的提交操作： 在上文分析回滚流程中我们提到了如果当前事务不是独立的事务，也没有保存点，在回滚的时候只是设置一个回滚标记，由外部事务提交时统一进行整体事务的回滚。 提交操作也是很简单的调用数据库连接底层API的commit方法。 参考链接： http://blog.163.com/asd_wll/blog/static/2103104020124801348674/ https://docs.spring.io/spring/docs/current/spring-framework-reference/data-access.html#spring-data-tier 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析8：Spring事务概述]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%908%EF%BC%9ASpring%E4%BA%8B%E5%8A%A1%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[原文出处： 张开涛 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 数据库事务概述事务首先是一系列操作组成的工作单元，该工作单元内的操作是不可分割的，即要么所有操作都做，要么所有操作都不做，这就是事务。 事务必需满足ACID（原子性、一致性、隔离性和持久性）特性，缺一不可： 原子性（Atomicity）：即事务是不可分割的最小工作单元，事务内的操作要么全做，要么全不做； 一致性（Consistency）：在事务执行前数据库的数据处于正确的状态，而事务执行完成后数据库的数据还是处于正确的状态，即数据完整性约束没有被破坏；如银行转帐，A转帐给B，必须保证A的钱一定转给B，一定不会出现A的钱转了但B没收到，否则数据库的数据就处于不一致（不正确）的状态。 隔离性（Isolation）：并发事务执行之间无影响，在一个事务内部的操作对其他事务是不产生影响，这需要事务隔离级别来指定隔离性； 持久性（Durability）：事务一旦执行成功，它对数据库的数据的改变必须是永久的，不会因比如遇到系统故障或断电造成数据不一致或丢失。 在实际项目开发中数据库操作一般都是并发执行的，即有多个事务并发执行，并发执行就可能遇到问题，目前常见的问题如下： 丢失更新：两个事务同时更新一行数据，最后一个事务的更新会覆盖掉第一个事务的更新，从而导致第一个事务更新的数据丢失，这是由于没有加锁造成的； 脏读：一个事务看到了另一个事务未提交的更新数据； 不可重复读：在同一事务中，多次读取同一数据却返回不同的结果；也就是有其他事务更改了这些数据； 幻读：一个事务在执行过程中读取到了另一个事务已提交的插入数据；即在第一个事务开始时读取到一批数据，但此后另一个事务又插入了新数据并提交，此时第一个事务又读取这批数据但发现多了一条，即好像发生幻觉一样。 为了解决这些并发问题，需要通过数据库隔离级别来解决，在标准SQL规范中定义了四种隔离级别： 未提交读（Read Uncommitted）：最低隔离级别，一个事务能读取到别的事务未提交的更新数据，很不安全，可能出现丢失更新、脏读、不可重复读、幻读； 提交读（Read Committed）：一个事务能读取到别的事务提交的更新数据，不能看到未提交的更新数据，不可能可能出现丢失更新、脏读，但可能出现不可重复读、幻读； 可重复读（Repeatable Read）：保证同一事务中先后执行的多次查询将返回同一结果，不受其他事务影响，可能可能出现丢失更新、脏读、不可重复读，但可能出现幻读； 序列化（Serializable）：最高隔离级别，不允许事务并发执行，而必须串行化执行，最安全，不可能出现更新、脏读、不可重复读、幻读。 隔离级别越高，数据库事务并发执行性能越差，能处理的操作越少。因此在实际项目开发中为了考虑并发性能一般使用提交读隔离级别，它能避免丢失更新和脏读，尽管不可重复读和幻读不能避免，但可以在可能出现的场合使用悲观锁或乐观锁来解决这些问题。 事务类型数据库事务类型有本地事务和分布式事务： 本地事务：就是普通事务，能保证单台数据库上的操作的ACID，被限定在一台数据库上； 分布式事务：涉及两个或多个数据库源的事务，即跨越多台同类或异类数据库的事务（由每台数据库的本地事务组成的），分布式事务旨在保证这些本地事务的所有操作的ACID，使事务可以跨越多台数据库； Java事务类型有JDBC事务和JTA事务： JDBC事务：就是数据库事务类型中的本地事务，通过Connection对象的控制来管理事务； JTA事务：JTA指Java事务API(Java Transaction API)，是Java EE数据库事务规范， JTA只提供了事务管理接口，由应用程序服务器厂商（如WebSphere Application Server）提供实现，JTA事务比JDBC更强大，支持分布式事务。 Java EE事务类型有本地事务和全局事务： 本地事务：使用JDBC编程实现事务； 全局事务：由应用程序服务器提供，使用JTA事务； 按是否通过编程实现事务有声明式事务和编程式事务； 声明式事务： 通过注解或XML配置文件指定事务信息； 编程式事务：通过编写代码实现事务。 Spring提供的事务管理Spring框架最核心功能之一就是事务管理，而且提供一致的事务管理抽象，这能帮助我们： 提供一致的编程式事务管理API，不管使用Spring JDBC框架还是集成第三方框架使用该API进行事务编程； 无侵入式的声明式事务支持。 Spring支持声明式事务和编程式事务事务类型。 spring事务特性spring所有的事务管理策略类都继承自org.springframework.transaction.PlatformTransactionManager接口 其中TransactionDefinition接口定义以下特性： 事务隔离级别 隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务传播行为所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。 事务只读属性只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。 概述Spring框架支持事务管理的核心是事务管理器抽象，对于不同的数据访问框架（如Hibernate）通过实现策略接口PlatformTransactionManager，从而能支持各种数据访问框架的事务管理，PlatformTransactionManager接口定义如下： java代码： 123456public interface PlatformTransactionManager &#123; TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; void commit(TransactionStatus status) throws TransactionException; void rollback(TransactionStatus status) throws TransactionException;&#125; getTransaction()：返回一个已经激活的事务或创建一个新的事务（根据给定的TransactionDefinition类型参数定义的事务属性），返回的是TransactionStatus对象代表了当前事务的状态，其中该方法抛出TransactionException（未检查异常）表示事务由于某种原因失败。 commit()：用于提交TransactionStatus参数代表的事务，具体语义请参考Spring Javadoc； rollback()：用于回滚TransactionStatus参数代表的事务，具体语义请参考Spring Javadoc。 TransactionDefinition接口定义如下： java代码： 12345678public interface TransactionDefinition &#123; int getPropagationBehavior(); int getIsolationLevel(); int getTimeout(); boolean isReadOnly(); String getName();&#125; getPropagationBehavior()：返回定义的事务传播行为； getIsolationLevel()：返回定义的事务隔离级别； getTimeout()：返回定义的事务超时时间； isReadOnly()：返回定义的事务是否是只读的； getName()：返回定义的事务名字。 TransactionStatus接口定义如下： java代码： 12345678public interface TransactionStatus extends SavepointManager &#123; boolean isNewTransaction(); boolean hasSavepoint(); void setRollbackOnly(); boolean isRollbackOnly(); void flush(); boolean isCompleted();&#125; isNewTransaction()：返回当前事务状态是否是新事务； hasSavepoint()：返回当前事务是否有保存点； setRollbackOnly()：设置当前事务应该回滚； isRollbackOnly(()：返回当前事务是否应该回滚； flush()：用于刷新底层会话中的修改到数据库，一般用于刷新如Hibernate/JPA的会话，可能对如JDBC类型的事务无任何影响； isCompleted():当前事务否已经完成。 内置事务管理器实现Spring提供了许多内置事务管理器实现： DataSourceTransactionManager：位于org.springframework.jdbc.datasource包中，数据源事务管理器，提供对单个javax.sql.DataSource事务管理，用于Spring JDBC抽象框架、iBATIS或MyBatis框架的事务管理； JdoTransactionManager：位于org.springframework.orm.jdo包中，提供对单个javax.jdo.PersistenceManagerFactory事务管理，用于集成JDO框架时的事务管理； JpaTransactionManager：位于org.springframework.orm.jpa包中，提供对单个javax.persistence.EntityManagerFactory事务支持，用于集成JPA实现框架时的事务管理； HibernateTransactionManager：位于org.springframework.orm.hibernate3包中，提供对单个org.hibernate.SessionFactory事务支持，用于集成Hibernate框架时的事务管理；该事务管理器只支持Hibernate3+版本，且Spring3.0+版本只支持Hibernate 3.2+版本； JtaTransactionManager：位于org.springframework.transaction.jta包中，提供对分布式事务管理的支持，并将事务管理委托给Java EE应用服务器事务管理器； OC4JjtaTransactionManager：位于org.springframework.transaction.jta包中，Spring提供的对OC4J10.1.3+应用服务器事务管理器的适配器，此适配器用于对应用服务器提供的高级事务的支持； WebSphereUowTransactionManager：位于org.springframework.transaction.jta包中，Spring提供的对WebSphere 6.0+应用服务器事务管理器的适配器，此适配器用于对应用服务器提供的高级事务的支持； WebLogicJtaTransactionManager：位于org.springframework.transaction.jta包中，Spring提供的对WebLogic 8.1+应用服务器事务管理器的适配器，此适配器用于对应用服务器提供的高级事务的支持。 Spring不仅提供这些事务管理器，还提供对如JMS事务管理的管理器等，Spring提供一致的事务抽象如图9-1所示。 图9-1 Spring事务管理器 接下来让我们学习一下如何在Spring配置文件中定义事务管理器： 一、声明对本地事务的支持： a)JDBC及iBATIS、MyBatis框架事务管理器 java代码： 1234&lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt; 通过dataSource属性指定需要事务管理的单个javax.sql.DataSource对象。 b)Jdo事务管理器 java代码： 1234&lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.orm.jdo.JdoTransactionManager&quot;&gt; &lt;property name=&quot;persistenceManagerFactory&quot; ref=&quot;persistenceManagerFactory&quot;/&gt;&lt;/bean&gt; 通过persistenceManagerFactory属性指定需要事务管理的javax.jdo.PersistenceManagerFactory对象。 c)Jpa事务管理器 java代码： 123bean id=&quot;txManager&quot; class=&quot;org.springframework.orm.jpa.JpaTransactionManager&quot;&gt; &lt;property name=&quot;entityManagerFactory&quot; ref=&quot;entityManagerFactory&quot;/&gt;&lt;/bean&gt; 通过entityManagerFactory属性指定需要事务管理的javax.persistence.EntityManagerFactory对象。 还需要为entityManagerFactory对象指定jpaDialect属性，该属性所对应的对象指定了如何获取连接对象、开启事务、关闭事务等事务管理相关的行为。 java代码： 12345&lt;bean id=&quot;entityManagerFactory&quot; class=&quot;org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean&quot;&gt; …… &lt;property name=&quot;jpaDialect&quot; ref=&quot;jpaDialect&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;jpaDialect&quot; class=&quot;org.springframework.orm.jpa.vendor.HibernateJpaDialect&quot;/&gt; d)Hibernate事务管理器 java代码： 123&lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;/&gt;&lt;/bean&gt; 通过entityManagerFactory属性指定需要事务管理的org.hibernate.SessionFactory对象。 声明式事务声明式事务概述从上节编程式实现事务管理可以深刻体会到编程式事务的痛苦，即使通过代理配置方式也是不小的工作量。 本节将介绍声明式事务支持，使用该方式后最大的获益是简单，事务管理不再是令人痛苦的，而且此方式属于无侵入式，对业务逻辑实现无影响。 接下来先来看看声明式事务如何实现吧。 声明式实现事务管理1、定义业务逻辑实现，此处使用ConfigUserServiceImpl和ConfigAddressServiceImpl： 2、定义配置文件（chapter9/service/ applicationContext-service-declare.xml）： 2.1、XML命名空间定义，定义用于事务支持的tx命名空间和AOP支持的aop命名空间： 123456789101112131415161718java代码：&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.0.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx-3.0.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-3.0.xsd&quot;&gt; 2.2、业务实现配置，非常简单，使用以前定义的非侵入式业务实现： 12345678java代码：&lt;bean id=&quot;userService&quot; class=&quot;cn.javass.spring.chapter9.service.impl.ConfigUserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;addressService&quot; ref=&quot;addressService&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;addressService&quot; class=&quot;cn.javass.spring.chapter9.service.impl.ConfigAddressServiceImpl&quot;&gt; &lt;property name=&quot;addressDao&quot; ref=&quot;addressDao&quot;/&gt;&lt;/bean&gt; 2.3、事务相关配置： 1234567java代码：&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot; isolation=&quot;READ_COMMITTED&quot;/&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;REQUIRED&quot; isolation=&quot;READ_COMMITTED&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; java代码： 12345678&lt;tx:advice&gt;：事务通知定义，用于指定事务属性，其中“transaction-manager”属性指定事务管理器，并通过&lt; tx:attributes &gt;指定具体需要拦截的方法； &lt;tx:method name=”save*”&gt;：表示将拦截以save开头的方法，被拦截的方法将应用配置的事务属性：propagation=”REQUIRED”表示传播行为是Required，isolation=”READ_COMMITTED”表示隔离级别是提交读；&lt;tx:method name=”*”&gt;：表示将拦截其他所有方法，被拦截的方法将应用配置的事务属性：propagation=”REQUIRED”表示传播行为是Required，isolation=”READ_COMMITTED”表示隔离级别是提交读，read-only=”true”表示事务只读；：AOP相关配置：：切入点定义，定义名为”serviceMethod”的aspectj切入点，切入点表达式为”execution(* cn..chapter9.service..*.*(..))”表示拦截cn包及子包下的chapter9\. service包及子包下的任何类的任何方法；：Advisor定义，其中切入点为serviceMethod，通知为txAdvice。从配置中可以看出，将对cn包及子包下的chapter9\. service包及子包下的任何类的任何方法应用“txAdvice”通知指定的事务属性。 3、修改测试方法并测试该配置方式是否好用： 将TransactionTest 类的testServiceTransaction测试方法拷贝一份命名为testDeclareTransaction： 并在testDeclareTransaction测试方法内将： 4、执行测试，测试正常通过，说明该方式能正常工作，当调用save方法时将匹配到事务通知中定义的“&lt;tx:method name=”save_”&gt;”中指定的事务属性，而调用countAll方法时将匹配到事务通知中定义的“&lt;tx:method name=”_”&gt;”中指定的事务属性。 声明式事务是如何实现事务管理的呢？还记不记得TransactionProxyFactoryBean实现配置式事务管理，配置式事务管理是通过代理方式实现，而声明式事务管理同样是通过AOP代理方式实现。 声明式事务通过AOP代理方式实现事务管理，利用环绕通知TransactionInterceptor实现事务的开启及关闭，而TransactionProxyFactoryBean内部也是通过该环绕通知实现的，因此可以认为是tx:tags/帮你定义了TransactionProxyFactoryBean，从而简化事务管理。 了解了实现方式后，接下来详细学习一下配置吧： 9.4.4 tx:advice/配置详解声明式事务管理通过配置tx:advice/来定义事务属性，配置方式如下所示： 123456789101112131415java代码：&lt;tx:advice id=&quot;……&quot; transaction-manager=&quot;……&quot;&gt;&lt;tx:attributes&gt; &lt;tx:method name=&quot;……&quot; propagation=&quot; REQUIRED&quot; isolation=&quot;READ_COMMITTED&quot; timeout=&quot;-1&quot; read-only=&quot;false&quot; no-rollback-for=&quot;&quot; rollback-for=&quot;&quot;/&gt; …… &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;tx:advice&gt;：id用于指定此通知的名字， transaction-manager用于指定事务管理器，默认的事务管理器名字为“transactionManager”；&lt;tx:method&gt;：用于定义事务属性即相关联的方法名； name：定义与事务属性相关联的方法名，将对匹配的方法应用定义的事务属性，可以使用“_”通配符来匹配一组或所有方法，如“save_”将匹配以save开头的方法，而“*”将匹配所有方法； propagation：事务传播行为定义，默认为“REQUIRED”，表示Required，其值可以通过TransactionDefinition的静态传播行为变量的“PROPAGATION_”后边部分指定，如“TransactionDefinition.PROPAGATION_REQUIRED”可以使用“REQUIRED”指定； isolation：事务隔离级别定义；默认为“DEFAULT”，其值可以通过TransactionDefinition的静态隔离级别变量的“ISOLATION_”后边部分指定，如“TransactionDefinition. ISOLATION_DEFAULT”可以使用“DEFAULT”指定： timeout：事务超时时间设置，单位为秒，默认-1，表示事务超时将依赖于底层事务系统； read-only：事务只读设置，默认为false，表示不是只读； rollback-for：需要触发回滚的异常定义，以“，”分割，默认任何RuntimeException 将导致事务回滚，而任何Checked Exception 将不导致事务回滚；异常名字定义和TransactionProxyFactoryBean中含义一样 no-rollback-for：不被触发进行回滚的 Exception(s)；以“，”分割；异常名字定义和TransactionProxyFactoryBean中含义一样； 记不记得在配置方式中为了解决“自我调用”而导致的不能设置正确的事务属性问题，使用“((IUserService)AopContext.currentProxy()).otherTransactionMethod()”方式解决，在声明式事务要得到支持需要使用来开启。 9.4.5 多事务语义配置及最佳实践什么是多事务语义？说白了就是为不同的Bean配置不同的事务属性，因为我们项目中不可能就几个Bean，而可能很多，这可能需要为Bean分组，为不同组的Bean配置不同的事务语义。在Spring中，可以通过配置多切入点和多事务通知并通过不同方式组合使用即可。 1234567891011121314151617181920 1、首先看下声明式事务配置的最佳实践吧：&lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;txManager&quot;&gt;&lt;tx:attributes&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;add*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;create*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;merge*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;del*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;remove*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;put*&quot; propagation=&quot;REQUIRED&quot; /&gt; &lt;tx:method name=&quot;get*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;count*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;list*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot; /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 该声明式事务配置可以应付常见的CRUD接口定义，并实现事务管理，我们只需修改切入点表达式来拦截我们的业务实现从而对其应用事务属性就可以了，如果还有更复杂的事务属性直接添加即可，即 如果我们有一个batchSaveOrUpdate方法需要“REQUIRES_NEW”事务传播行为，则直接添加如下配置即可： java代码：1&lt;tx:method name=”batchSaveOrUpdate” propagation=”REQUIRES_NEW” /&gt;2、接下来看一下多事务语义配置吧，声明式事务最佳实践中已经配置了通用事务属性，因此可以针对需要其他事务属性的业务方法进行特例化配置： 123456java代码：&lt;tx:advice id=&quot;noTxAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method name=&quot;*&quot; propagation=&quot;NEVER&quot; /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt; 该声明将对切入点匹配的方法所在事务应用“Never”传播行为。 多事务语义配置时，切入点一定不要叠加，否则将应用两次事务属性，造成不必要的错误及麻烦。 @Transactional实现事务管理对声明式事务管理，Spring提供基于@Transactional注解方式来实现，但需要Java 5+。 注解方式是最简单的事务配置方式，可以直接在Java源代码中声明事务属性，且对于每一个业务类或方法如果需要事务都必须使用此注解。 接下来学习一下注解事务的使用吧： 1、定义业务逻辑实现： 123456789101112131415161718192021222324package cn.javass.spring.chapter9.service.impl;//省略importpublic class AnnotationUserServiceImpl implements IUserService &#123; private IUserDao userDao; private IAddressService addressService; public void setUserDao(IUserDao userDao) &#123; this.userDao = userDao; &#125; public void setAddressService(IAddressService addressService) &#123; this.addressService = addressService; &#125; @Transactional(propagation=Propagation.REQUIRED, isolation=Isolation.READ_COMMITTED) @Override public void save(final UserModel user) &#123; userDao.save(user); user.getAddress().setUserId(user.getId()); addressService.save(user.getAddress()); &#125; @Transactional(propagation=Propagation.REQUIRED, isolation=Isolation.READ_COMMITTED, readOnly=true) @Override public int countAll() &#123; return userDao.countAll(); &#125;&#125; 2、定义配置文件（chapter9/service/ applicationContext-service-annotation.xml）： 2.1、XML命名空间定义，定义用于事务支持的tx命名空间和AOP支持的aop命名空间： java代码： 1234567891011121314151617&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.0.xsdhttp://www.springframework.org/schema/txhttp://www.springframework.org/schema/tx/spring-tx-3.0.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-3.0.xsd&quot;&gt; 2.2、业务实现配置，非常简单，使用以前定义的非侵入式业务实现： 12345678java代码：&lt;bean id=&quot;userService&quot; class=&quot;cn.javass.spring.chapter9.service.impl.ConfigUserServiceImpl&quot;&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;addressService&quot; ref=&quot;addressService&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;addressService&quot; class=&quot;cn.javass.spring.chapter9.service.impl.ConfigAddressServiceImpl&quot;&gt; &lt;property name=&quot;addressDao&quot; ref=&quot;addressDao&quot;/&gt;&lt;/bean&gt; 2.3、事务相关配置： 1234567891011121314151617181920java代码：1&lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt;使用如上配置已支持声明式事务。3、修改测试方法并测试该配置方式是否好用：将TransactionTest 类的testServiceTransaction测试方法拷贝一份命名为testAnntationTransactionTest：classpath:chapter9/service/applicationContext-service-annotation.xml&quot;userService.save(user);try &#123; userService.save(user); Assert.fail();&#125; catch (RuntimeException e) &#123;&#125;Assert.assertEquals(0, userService.countAll());Assert.assertEquals(0, addressService.countAll()); 4、执行测试，测试正常通过，说明该方式能正常工作，因为在AnnotationAddressServiceImpl类的save方法中抛出异常，因此事务需要回滚，所以两个countAll操作都返回0。 9.4.7 @Transactional配置详解Spring提供的tx:annotation-driven/用于开启对注解事务管理的支持，从而能识别Bean类上的@Transactional注解元数据，其具有以下属性： transaction-manager：指定事务管理器名字，默认为transactionManager，当使用其他名字时需要明确指定；proxy-target-class：表示将使用的代码机制，默认false表示使用JDK代理，如果为true将使用CGLIB代理order：定义事务通知顺序，默认Ordered.LOWEST_PRECEDENCE，表示将顺序决定权交给AOP来处理。Spring使用@Transactional 来指定事务属性，可以在接口、类或方法上指定，如果类和方法上都指定了@Transactional ，则方法上的事务属性被优先使用，具体属性如下： value：指定事务管理器名字，默认使用tx:annotation-driven/指定的事务管理器，用于支持多事务管理器环境；propagation：指定事务传播行为，默认为Required，使用Propagation.REQUIRED指定；isolation：指定事务隔离级别，默认为“DEFAULT”，使用Isolation.DEFAULT指定；readOnly：指定事务是否只读，默认false表示事务非只读；timeout：指定事务超时时间，以秒为单位，默认-1表示事务超时将依赖于底层事务系统；rollbackFor：指定一组异常类，遇到该类异常将回滚事务；rollbackForClassname：指定一组异常类名字，其含义与tx:method中的rollback-for属性语义完全一样；noRollbackFor：指定一组异常类，即使遇到该类异常也将提交事务，即不回滚事务；noRollbackForClassname：指定一组异常类名字，其含义与tx:method中的no-rollback-for属性语义完全一样；Spring提供的@Transactional 注解事务管理内部同样利用环绕通知TransactionInterceptor实现事务的开启及关闭。 使用@Transactional注解事务管理需要特别注意以下几点： 如果在接口、实现类或方法上都指定了@Transactional 注解，则优先级顺序为方法&gt;实现类&gt;接口；建议只在实现类或实现类的方法上使用@Transactional，而不要在接口上使用，这是因为如果使用JDK代理机制是没问题，因为其使用基于接口的代理；而使用使用CGLIB代理机制时就会遇到问题，因为其使用基于类的代理而不是接口，这是因为接口上的@Transactional注解是“不能继承的”； 1具体请参考基于JDK动态代理和CGLIB动态代理的实现Spring注解管理事务（@Trasactional）到底有什么区别。 在Spring代理机制下(不管是JDK动态代理还是CGLIB代理)，“自我调用”同样不会应用相应的事务属性，其语义和tx:tags中一样； 默认只对RuntimeException异常回滚； 在使用Spring代理时，默认只有在public可见度的方法的@Transactional 注解才是有效的，其它可见度（protected、private、包可见）的方法上即使有@Transactional注解也不会应用这些事务属性的，Spring也不会报错，如果你非要使用非公共方法注解事务管理的话，可考虑使用AspectJ。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析7：AOP实现原理详解]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%907%EF%BC%9AAOP%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[本文转自五月的仓颉 https://www.cnblogs.com/xrq730 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言 前面写了六篇文章详细地分析了Spring Bean加载流程，这部分完了之后就要进入一个比较困难的部分了，就是AOP的实现原理分析。为了探究AOP实现原理，首先定义几个类，一个Dao接口： public interface Dao {public void select();public void insert();}Dao接口的实现类DaoImpl： 12345678910111213public class DaoImpl implements Dao &#123; @Override public void select() &#123; System.out.println(&quot;Enter DaoImpl.select()&quot;); &#125; @Override public void insert() &#123; System.out.println(&quot;Enter DaoImpl.insert()&quot;); &#125;&#125; 定义一个TimeHandler，用于方法调用前后打印时间，在AOP中，这扮演的是横切关注点的角色： 1234567public class TimeHandler &#123; public void printTime() &#123; System.out.println(&quot;CurrentTime:&quot; + System.currentTimeMillis()); &#125;&#125; 定义一个XML文件aop.xml： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.0.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-3.0.xsd&quot;&gt; &lt;bean id=&quot;daoImpl&quot; class=&quot;org.xrq.action.aop.DaoImpl&quot; /&gt; &lt;bean id=&quot;timeHandler&quot; class=&quot;org.xrq.action.aop.TimeHandler&quot; /&gt; &lt;/beans&gt; 写一段测试代码TestAop.java： 1234567891011public class TestAop &#123; @Test public void testAop() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;spring/aop.xml&quot;); Dao dao = (Dao)ac.getBean(&quot;daoImpl&quot;); dao.select(); &#125;&#125; 代码运行结果就不看了，有了以上的内容，我们就可以根据这些跟一下代码，看看Spring到底是如何实现AOP的。 AOP实现原理——找到Spring处理AOP的源头有很多朋友不愿意去看AOP源码的一个很大原因是因为找不到AOP源码实现的入口在哪里，这个确实是。不过我们可以看一下上面的测试代码，就普通Bean也好、AOP也好，最终都是通过getBean方法获取到Bean并调用方法的，getBean之后的对象已经前后都打印了TimeHandler类printTime()方法里面的内容，可以想见它们已经是被Spring容器处理过了。 既然如此，那无非就两个地方处理： 加载Bean定义的时候应该有过特殊的处理getBean的时候应该有过特殊的处理因此，本文围绕【1.加载Bean定义的时候应该有过特殊的处理】展开，先找一下到底是哪里Spring对AOP做了特殊的处理。代码直接定位到DefaultBeanDefinitionDocumentReader的parseBeanDefinitions方法： 1234567891011121314151617181920protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 正常来说，遇到、这两个标签的时候，都会执行第9行的代码，因为标签是默认的Namespace。但是在遇到后面的标签的时候就不一样了，并不是默认的Namespace，因此会执行第12行的代码，看一下： 123456789public BeanDefinition parseCustomElement(Element ele, BeanDefinition containingBd) &#123; String namespaceUri = getNamespaceURI(ele); NamespaceHandler handler = this.readerContext.getNamespaceHandlerResolver().resolve(namespaceUri); if (handler == null) &#123; error(&quot;Unable to locate Spring NamespaceHandler for XML schema namespace [&quot; + namespaceUri + &quot;]&quot;, ele); return null; &#125; return handler.parse(ele, new ParserContext(this.readerContext, this, containingBd));&#125; 因为之前把整个XML解析为了org.w3c.dom.Document，org.w3c.dom.Document以树的形式表示整个XML，具体到每一个节点就是一个Node。 首先第2行从这个Node（参数Element是Node接口的子接口）中拿到Namespace=”http://www.springframework.org/schema/aop“，第3行的代码根据这个Namespace获取对应的NamespaceHandler即Namespace处理器，具体到aop这个Namespace的NamespaceHandler是org.springframework.aop.config.AopNamespaceHandler类，也就是第3行代码获取到的结果。具体到AopNamespaceHandler里面，有几个Parser，是用于具体标签转换的，分别为： config–&gt;ConfigBeanDefinitionParseraspectj-autoproxy–&gt;AspectJAutoProxyBeanDefinitionParserscoped-proxy–&gt;ScopedProxyBeanDefinitionDecoratorspring-configured–&gt;SpringConfiguredBeanDefinitionParser接着，就是第8行的代码，利用AopNamespaceHandler的parse方法，解析下的内容了。 解析增强器advisorAOP Bean定义加载——根据织入方式将、转换成名为adviceDef的RootBeanDefinition上面经过分析，已经找到了Spring是通过AopNamespaceHandler处理的AOP，那么接着进入AopNamespaceHandler的parse方法源代码： 123public BeanDefinition parse(Element element, ParserContext parserContext) &#123; return findParserForElement(element, parserContext).parse(element, parserContext);&#125; 首先获取具体的Parser，因为当前节点是，上一部分最后有列，config是通过ConfigBeanDefinitionParser来处理的，因此findParserForElement(element, parserContext)这一部分代码获取到的是ConfigBeanDefinitionParser，接着看ConfigBeanDefinitionParser的parse方法： 123456789101112131415161718192021222324public BeanDefinition parse(Element element, ParserContext parserContext) &#123; CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), parserContext.extractSource(element)); parserContext.pushContainingComponent(compositeDef); configureAutoProxyCreator(parserContext, element); List&lt;Element&gt; childElts = DomUtils.getChildElements(element); for (Element elt: childElts) &#123; String localName = parserContext.getDelegate().getLocalName(elt); if (POINTCUT.equals(localName)) &#123; parsePointcut(elt, parserContext); &#125; else if (ADVISOR.equals(localName)) &#123; parseAdvisor(elt, parserContext); &#125; else if (ASPECT.equals(localName)) &#123; parseAspect(elt, parserContext); &#125; &#125; parserContext.popAndRegisterContainingComponent(); return null;&#125; 重点先提一下第6行的代码，该行代码的具体实现不跟了但它非常重要，configureAutoProxyCreator方法的作用我用几句话说一下： 向Spring容器注册了一个BeanName为org.springframework.aop.config.internalAutoProxyCreator的Bean定义，可以自定义也可以使用Spring提供的（根据优先级来）Spring默认提供的是org.springframework.aop.aspectj.autoproxy.AspectJAwareAdvisorAutoProxyCreator，这个类是AOP的核心类，留在下篇讲解在这个方法里面也会根据配置proxy-target-class和expose-proxy，设置是否使用CGLIB进行代理以及是否暴露最终的代理。下的节点为，想见必然是执行第18行的代码parseAspect，跟进去： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private void parseAspect(Element aspectElement, ParserContext parserContext) &#123; String aspectId = aspectElement.getAttribute(ID); String aspectName = aspectElement.getAttribute(REF); try &#123; this.parseState.push(new AspectEntry(aspectId, aspectName)); List&lt;BeanDefinition&gt; beanDefinitions = new ArrayList&lt;BeanDefinition&gt;(); List&lt;BeanReference&gt; beanReferences = new ArrayList&lt;BeanReference&gt;(); List&lt;Element&gt; declareParents = DomUtils.getChildElementsByTagName(aspectElement, DECLARE_PARENTS); for (int i = METHOD_INDEX; i &lt; declareParents.size(); i++) &#123; Element declareParentsElement = declareParents.get(i); beanDefinitions.add(parseDeclareParents(declareParentsElement, parserContext)); &#125; // We have to parse &quot;advice&quot; and all the advice kinds in one loop, to get the // ordering semantics right. NodeList nodeList = aspectElement.getChildNodes(); boolean adviceFoundAlready = false; for (int i = 0; i &lt; nodeList.getLength(); i++) &#123; Node node = nodeList.item(i); if (isAdviceNode(node, parserContext)) &#123; if (!adviceFoundAlready) &#123; adviceFoundAlready = true; if (!StringUtils.hasText(aspectName)) &#123; parserContext.getReaderContext().error( &quot; tag needs aspect bean reference via &apos;ref&apos; attribute when declaring advices.&quot;, aspectElement, this.parseState.snapshot()); return; &#125; beanReferences.add(new RuntimeBeanReference(aspectName)); &#125; AbstractBeanDefinition advisorDefinition = parseAdvice( aspectName, i, aspectElement, (Element) node, parserContext, beanDefinitions, beanReferences); beanDefinitions.add(advisorDefinition); &#125; &#125; AspectComponentDefinition aspectComponentDefinition = createAspectComponentDefinition( aspectElement, aspectId, beanDefinitions, beanReferences, parserContext); parserContext.pushContainingComponent(aspectComponentDefinition); List&lt;Element&gt; pointcuts = DomUtils.getChildElementsByTagName(aspectElement, POINTCUT); for (Element pointcutElement : pointcuts) &#123; parsePointcut(pointcutElement, parserContext); &#125; parserContext.popAndRegisterContainingComponent(); &#125; finally &#123; this.parseState.pop(); &#125;&#125; 从第20行~第37行的循环开始关注这个方法。这个for循环有一个关键的判断就是第22行的ifAdviceNode判断，看下ifAdviceNode方法做了什么： 12345678910private boolean isAdviceNode(Node aNode, ParserContext parserContext) &#123; if (!(aNode instanceof Element)) &#123; return false; &#125; else &#123; String name = parserContext.getDelegate().getLocalName(aNode); return (BEFORE.equals(name) || AFTER.equals(name) || AFTER_RETURNING_ELEMENT.equals(name) || AFTER_THROWING_ELEMENT.equals(name) || AROUND.equals(name)); &#125;&#125; 即这个for循环只用来处理标签下的、、、、这五个标签的。 接着，如果是上述五种标签之一，那么进入第33行~第34行的parseAdvice方法： 123456789101112131415161718192021222324252627282930313233343536373839private AbstractBeanDefinition parseAdvice( String aspectName, int order, Element aspectElement, Element adviceElement, ParserContext parserContext, List&lt;BeanDefinition&gt; beanDefinitions, List&lt;BeanReference&gt; beanReferences) &#123; try &#123; this.parseState.push(new AdviceEntry(parserContext.getDelegate().getLocalName(adviceElement))); // create the method factory bean RootBeanDefinition methodDefinition = new RootBeanDefinition(MethodLocatingFactoryBean.class); methodDefinition.getPropertyValues().add(&quot;targetBeanName&quot;, aspectName); methodDefinition.getPropertyValues().add(&quot;methodName&quot;, adviceElement.getAttribute(&quot;method&quot;)); methodDefinition.setSynthetic(true); // create instance factory definition RootBeanDefinition aspectFactoryDef = new RootBeanDefinition(SimpleBeanFactoryAwareAspectInstanceFactory.class); aspectFactoryDef.getPropertyValues().add(&quot;aspectBeanName&quot;, aspectName); aspectFactoryDef.setSynthetic(true); // register the pointcut AbstractBeanDefinition adviceDef = createAdviceDefinition( adviceElement, parserContext, aspectName, order, methodDefinition, aspectFactoryDef, beanDefinitions, beanReferences); // configure the advisor RootBeanDefinition advisorDefinition = new RootBeanDefinition(AspectJPointcutAdvisor.class); advisorDefinition.setSource(parserContext.extractSource(adviceElement)); advisorDefinition.getConstructorArgumentValues().addGenericArgumentValue(adviceDef); if (aspectElement.hasAttribute(ORDER_PROPERTY)) &#123; advisorDefinition.getPropertyValues().add( ORDER_PROPERTY, aspectElement.getAttribute(ORDER_PROPERTY)); &#125; // register the final advisor parserContext.getReaderContext().registerWithGeneratedName(advisorDefinition); return advisorDefinition; &#125; finally &#123; this.parseState.pop(); &#125;&#125; 方法主要做了三件事： 根据织入方式（before、after这些）创建RootBeanDefinition，名为adviceDef即advice定义 将上一步创建的RootBeanDefinition写入一个新的RootBeanDefinition，构造一个新的对象，名为advisorDefinition，即advisor定义将advisorDefinition注册到DefaultListableBeanFactory中下面来看做的第一件事createAdviceDefinition方法定义： 1234567891011121314151617181920212223242526272829303132333435363738394041private AbstractBeanDefinition createAdviceDefinition( Element adviceElement, ParserContext parserContext, String aspectName, int order, RootBeanDefinition methodDef, RootBeanDefinition aspectFactoryDef, List&lt;BeanDefinition&gt; beanDefinitions, List&lt;BeanReference&gt; beanReferences) &#123; RootBeanDefinition adviceDefinition = new RootBeanDefinition(getAdviceClass(adviceElement, parserContext)); adviceDefinition.setSource(parserContext.extractSource(adviceElement)); adviceDefinition.getPropertyValues().add(ASPECT_NAME_PROPERTY, aspectName); adviceDefinition.getPropertyValues().add(DECLARATION_ORDER_PROPERTY, order); if (adviceElement.hasAttribute(RETURNING)) &#123; adviceDefinition.getPropertyValues().add( RETURNING_PROPERTY, adviceElement.getAttribute(RETURNING)); &#125; if (adviceElement.hasAttribute(THROWING)) &#123; adviceDefinition.getPropertyValues().add( THROWING_PROPERTY, adviceElement.getAttribute(THROWING)); &#125; if (adviceElement.hasAttribute(ARG_NAMES)) &#123; adviceDefinition.getPropertyValues().add( ARG_NAMES_PROPERTY, adviceElement.getAttribute(ARG_NAMES)); &#125; ConstructorArgumentValues cav = adviceDefinition.getConstructorArgumentValues(); cav.addIndexedArgumentValue(METHOD_INDEX, methodDef); Object pointcut = parsePointcutProperty(adviceElement, parserContext); if (pointcut instanceof BeanDefinition) &#123; cav.addIndexedArgumentValue(POINTCUT_INDEX, pointcut); beanDefinitions.add((BeanDefinition) pointcut); &#125; else if (pointcut instanceof String) &#123; RuntimeBeanReference pointcutRef = new RuntimeBeanReference((String) pointcut); cav.addIndexedArgumentValue(POINTCUT_INDEX, pointcutRef); beanReferences.add(pointcutRef); &#125; cav.addIndexedArgumentValue(ASPECT_INSTANCE_FACTORY_INDEX, aspectFactoryDef); return adviceDefinition;&#125; 首先可以看到，创建的AbstractBeanDefinition实例是RootBeanDefinition，这和普通Bean创建的实例为GenericBeanDefinition不同。然后进入第6行的getAdviceClass方法看一下： 123456789101112131415161718192021private Class getAdviceClass(Element adviceElement, ParserContext parserContext) &#123; String elementName = parserContext.getDelegate().getLocalName(adviceElement); if (BEFORE.equals(elementName)) &#123; return AspectJMethodBeforeAdvice.class; &#125; else if (AFTER.equals(elementName)) &#123; return AspectJAfterAdvice.class; &#125; else if (AFTER_RETURNING_ELEMENT.equals(elementName)) &#123; return AspectJAfterReturningAdvice.class; &#125; else if (AFTER_THROWING_ELEMENT.equals(elementName)) &#123; return AspectJAfterThrowingAdvice.class; &#125; else if (AROUND.equals(elementName)) &#123; return AspectJAroundAdvice.class; &#125; else &#123; throw new IllegalArgumentException(&quot;Unknown advice kind [&quot; + elementName + &quot;].&quot;); &#125;&#125; 既然创建Bean定义，必然该Bean定义中要对应一个具体的Class，不同的切入方式对应不同的Class： before对应AspectJMethodBeforeAdviceAfter对应AspectJAfterAdviceafter-returning对应AspectJAfterReturningAdviceafter-throwing对应AspectJAfterThrowingAdvicearound对应AspectJAroundAdvice createAdviceDefinition方法剩余逻辑没什么，就是判断一下标签里面的属性并设置一下相应的值而已，至此、两个标签对应的AbstractBeanDefinition就创建出来了。 AOP Bean定义加载——将名为adviceDef的RootBeanDefinition转换成名为advisorDefinition的RootBeanDefinition下面我们看一下第二步的操作，将名为adviceDef的RootBeanD转换成名为advisorDefinition的RootBeanDefinition，跟一下上面一部分ConfigBeanDefinitionParser类parseAdvice方法的第26行~32行的代码： 1234567RootBeanDefinition advisorDefinition = new RootBeanDefinition(AspectJPointcutAdvisor.class);advisorDefinition.setSource(parserContext.extractSource(adviceElement));advisorDefinition.getConstructorArgumentValues().addGenericArgumentValue(adviceDef);if (aspectElement.hasAttribute(ORDER_PROPERTY)) &#123; advisorDefinition.getPropertyValues().add( ORDER_PROPERTY, aspectElement.getAttribute(ORDER_PROPERTY));&#125; 这里相当于将上一步生成的RootBeanDefinition包装了一下，new一个新的RootBeanDefinition出来，Class类型是org.springframework.aop.aspectj.AspectJPointcutAdvisor。 第4行~第7行的代码是用于判断标签中有没有”order”属性的，有就设置一下，”order”属性是用来控制切入方法优先级的。 AOP Bean定义加载——将BeanDefinition注册到DefaultListableBeanFactory中 最后一步就是将BeanDefinition注册到DefaultListableBeanFactory中了，代码就是前面ConfigBeanDefinitionParser的parseAdvice方法的最后一部分了： 1234// register the final advisorparserContext.getReaderContext().registerWithGeneratedName(advisorDefinition);...跟一下registerWithGeneratedName方法的实现： public String registerWithGeneratedName(BeanDefinition beanDefinition) { String generatedName = generateBeanName(beanDefinition); getRegistry().registerBeanDefinition(generatedName, beanDefinition); return generatedName;} 1234第2行获取注册的名字BeanName，和&lt;bean&gt;的注册差不多，使用的是Class全路径+”#”+全局计数器的方式，其中的Class全路径为org.springframework.aop.aspectj.AspectJPointcutAdvisor，依次类推，每一个BeanName应当为org.springframework.aop.aspectj.AspectJPointcutAdvisor#0、org.springframework.aop.aspectj.AspectJPointcutAdvisor#1、org.springframework.aop.aspectj.AspectJPointcutAdvisor#2这样下去。第3行向DefaultListableBeanFactory中注册，BeanName已经有了，剩下的就是Bean定义，Bean定义的解析流程之前已经看过了，就不说了。 解析切面的过程AOP Bean定义加载——AopNamespaceHandler处理流程回到ConfigBeanDefinitionParser的parseAspect方法： private void parseAspect(Element aspectElement, ParserContext parserContext) { ... AspectComponentDefinition aspectComponentDefinition = createAspectComponentDefinition( aspectElement, aspectId, beanDefinitions, beanReferences, parserContext); parserContext.pushContainingComponent(aspectComponentDefinition); List&lt;Element&gt; pointcuts = DomUtils.getChildElementsByTagName(aspectElement, POINTCUT); for (Element pointcutElement : pointcuts) { parsePointcut(pointcutElement, parserContext); } parserContext.popAndRegisterContainingComponent(); } finally { this.parseState.pop(); } }省略号部分表示是解析的是、这种标签，上部分已经说过了，就不说了，下面看一下解析部分的源码。 第5行~第7行的代码构建了一个Aspect标签组件定义，并将Apsect标签组件定义推到ParseContext即解析工具上下文中，这部分代码不是关键。 第9行的代码拿到所有下的pointcut标签，进行遍历，由parsePointcut方法进行处理： 12345678910111213141516171819202122232425262728private AbstractBeanDefinition parsePointcut(Element pointcutElement, ParserContext parserContext) &#123; String id = pointcutElement.getAttribute(ID); String expression = pointcutElement.getAttribute(EXPRESSION); AbstractBeanDefinition pointcutDefinition = null; try &#123; this.parseState.push(new PointcutEntry(id)); pointcutDefinition = createPointcutDefinition(expression); pointcutDefinition.setSource(parserContext.extractSource(pointcutElement)); String pointcutBeanName = id; if (StringUtils.hasText(pointcutBeanName)) &#123; parserContext.getRegistry().registerBeanDefinition(pointcutBeanName, pointcutDefinition); &#125; else &#123; pointcutBeanName = parserContext.getReaderContext().registerWithGeneratedName(pointcutDefinition); &#125; parserContext.registerComponent( new PointcutComponentDefinition(pointcutBeanName, pointcutDefinition, expression)); &#125; finally &#123; this.parseState.pop(); &#125; return pointcutDefinition;&#125; 第2行~第3行的代码获取标签下的”id”属性与”expression”属性。 第8行的代码推送一个PointcutEntry，表示当前Spring上下文正在解析Pointcut标签。 第9行的代码创建Pointcut的Bean定义，之后再看，先把其他方法都看一下。 第10行的代码不管它，最终从NullSourceExtractor的extractSource方法获取Source，就是个null。 第12行~第18行的代码用于注册获取到的Bean定义，默认pointcutBeanName为标签中定义的id属性： 如果标签中配置了id属性就执行的是第13行第15行的代码，pointcutBeanName=id如果标签中没有配置id属性就执行的是第16行第18行的代码，和Bean不配置id属性一样的规则，pointcutBeanName=org.springframework.aop.aspectj.AspectJExpressionPointcut#序号（从0开始累加）第20行~第21行的代码向解析工具上下文中注册一个Pointcut组件定义 第23行~第25行的代码，finally块在标签解析完毕后，让之前推送至栈顶的PointcutEntry出栈，表示此次标签解析完毕。 最后回头来一下第9行代码createPointcutDefinition的实现，比较简单： 1234567protected AbstractBeanDefinition createPointcutDefinition(String expression) &#123; RootBeanDefinition beanDefinition = new RootBeanDefinition(AspectJExpressionPointcut.class); beanDefinition.setScope(BeanDefinition.SCOPE_PROTOTYPE); beanDefinition.setSynthetic(true); beanDefinition.getPropertyValues().add(EXPRESSION, expression); return beanDefinition;&#125; 关键就是注意一下两点： 标签对应解析出来的BeanDefinition是RootBeanDefinition，且RootBenaDefinitoin中的Class是org.springframework.aop.aspectj.AspectJExpressionPointcut标签对应的Bean是prototype即原型的这样一个流程下来，就解析了标签中的内容并将之转换为RootBeanDefintion存储在Spring容器中。 AOP为Bean生成代理的时机分析上篇文章说了，org.springframework.aop.aspectj.autoproxy.AspectJAwareAdvisorAutoProxyCreator这个类是Spring提供给开发者的AOP的核心类，就是AspectJAwareAdvisorAutoProxyCreator完成了【类/接口–&gt;代理】的转换过程，首先我们看一下AspectJAwareAdvisorAutoProxyCreator的层次结构： 这里最值得注意的一点是最左下角的那个方框，我用几句话总结一下： AspectJAwareAdvisorAutoProxyCreator是BeanPostProcessor接口的实现类postProcessBeforeInitialization方法与postProcessAfterInitialization方法实现在父类AbstractAutoProxyCreator中postProcessBeforeInitialization方法是一个空实现逻辑代码在postProcessAfterInitialization方法中基于以上的分析，将Bean生成代理的时机已经一目了然了：在每个Bean初始化之后，如果需要，调用AspectJAwareAdvisorAutoProxyCreator中的postProcessBeforeInitialization为Bean生成代理。 代理对象实例化—-判断是否为生成代理上文分析了Bean生成代理的时机是在每个Bean初始化之后，下面把代码定位到Bean初始化之后，先是AbstractAutowireCapableBeanFactory的initializeBean方法进行初始化： 1234567891011121314151617181920212223242526272829303132protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; invokeAwareMethods(beanName, bean); return null; &#125; &#125;, getAccessControlContext()); &#125; else &#123; invokeAwareMethods(beanName, bean); &#125; Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; invokeInitMethods(beanName, wrappedBean, mbd); &#125; catch (Throwable ex) &#123; throw new BeanCreationException( (mbd != null ? mbd.getResourceDescription() : null), beanName, &quot;Invocation of init method failed&quot;, ex); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 初始化之前是第16行的applyBeanPostProcessorsBeforeInitialization方法，初始化之后即29行的applyBeanPostProcessorsAfterInitialization方法： 123456789101112public Object applyBeanPostProcessorsAfterInitialization(Object existingBean, String beanName) throws BeansException &#123; Object result = existingBean; for (BeanPostProcessor beanProcessor : getBeanPostProcessors()) &#123; result = beanProcessor.postProcessAfterInitialization(result, beanName); if (result == null) &#123; return result; &#125; &#125; return result;&#125; 这里调用每个BeanPostProcessor的postProcessBeforeInitialization方法。按照之前的分析，看一下AbstractAutoProxyCreator的postProcessAfterInitialization方法实现： 123456789public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 跟一下第5行的方法wrapIfNecessary： 123456789101112131415161718192021222324protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (this.nonAdvisedBeans.contains(cacheKey)) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.nonAdvisedBeans.add(cacheKey); return bean; &#125; // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.add(cacheKey); Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.nonAdvisedBeans.add(cacheKey); return bean;&#125; 第2行~第11行是一些不需要生成代理的场景判断，这里略过。首先我们要思考的第一个问题是：哪些目标对象需要生成代理？因为配置文件里面有很多Bean，肯定不能对每个Bean都生成代理，因此需要一套规则判断Bean是不是需要生成代理，这套规则就是第14行的代码getAdvicesAndAdvisorsForBean： 123456789protected List&lt;Advisor&gt; findEligibleAdvisors(Class beanClass, String beanName) &#123; List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; 顾名思义，方法的意思是为指定class寻找合适的Advisor。 第2行代码，寻找候选Advisors，根据上文的配置文件，有两个候选Advisor，分别是节点下的和这两个，这两个在XML解析的时候已经被转换生成了RootBeanDefinition。 跳过第3行的代码，先看下第4行的代码extendAdvisors方法，之后再重点看一下第3行的代码。第4行的代码extendAdvisors方法作用是向候选Advisor链的开头（也就是List.get(0)的位置）添加一个org.springframework.aop.support.DefaultPointcutAdvisor。 第3行代码，根据候选Advisors，寻找可以使用的Advisor，跟一下方法实现： 12345678910111213141516171819202122public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; if (candidateAdvisors.isEmpty()) &#123; return candidateAdvisors; &#125; List&lt;Advisor&gt; eligibleAdvisors = new LinkedList&lt;Advisor&gt;(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor &amp;&amp; canApply(candidate, clazz)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; boolean hasIntroductions = !eligibleAdvisors.isEmpty(); for (Advisor candidate : candidateAdvisors) &#123; if (candidate instanceof IntroductionAdvisor) &#123; // already processed continue; &#125; if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; return eligibleAdvisors;&#125; 整个方法的主要判断都围绕canApply展开方法： 12345678910111213public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (advisor instanceof IntroductionAdvisor) &#123; return ((IntroductionAdvisor) advisor).getClassFilter().matches(targetClass); &#125; else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn&apos;t have a pointcut so we assume it applies. return true; &#125;&#125; 第一个参数advisor的实际类型是AspectJPointcutAdvisor，它是PointcutAdvisor的子类，因此执行第7行的方法： 12345678910111213141516171819202122232425public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; MethodMatcher methodMatcher = pc.getMethodMatcher(); IntroductionAwareMethodMatcher introductionAwareMethodMatcher = null; if (methodMatcher instanceof IntroductionAwareMethodMatcher) &#123; introductionAwareMethodMatcher = (IntroductionAwareMethodMatcher) methodMatcher; &#125; Set&lt;Class&gt; classes = new HashSet&lt;Class&gt;(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); classes.add(targetClass); for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = clazz.getMethods(); for (Method method : methods) &#123; if ((introductionAwareMethodMatcher != null &amp;&amp; introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions)) || methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; return false;&#125; 这个方法其实就是拿当前Advisor对应的expression做了两层判断： 目标类必须满足expression的匹配规则目标类中的方法必须满足expression的匹配规则，当然这里方法不是全部需要满足expression的匹配规则，有一个方法满足即可如果以上两条都满足，那么容器则会判断该满足条件，需要被生成代理对象，具体方式为返回一个数组对象，该数组对象中存储的是对应的Advisor。 代理对象实例化过程代理对象实例化—-为生成代理代码上下文梳理上文分析了为生成代理的条件，现在就正式看一下Spring上下文是如何为生成代理的。回到AbstractAutoProxyCreator的wrapIfNecessary方法： 123456789101112131415161718192021222324protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (this.nonAdvisedBeans.contains(cacheKey)) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.nonAdvisedBeans.add(cacheKey); return bean; &#125; // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.add(cacheKey); Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.nonAdvisedBeans.add(cacheKey); return bean;&#125; 第14行拿到对应的Advisor数组，第15行判断只要Advisor数组不为空，那么就会通过第17行的代码为创建代理： 12345678910111213141516171819202122232425262728293031protected Object createProxy( Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) &#123; ProxyFactory proxyFactory = new ProxyFactory(); // Copy our properties (proxyTargetClass etc) inherited from ProxyConfig. proxyFactory.copyFrom(this); if (!shouldProxyTargetClass(beanClass, beanName)) &#123; // Must allow for introductions; can&apos;t just set interfaces to // the target&apos;s interfaces only. Class&lt;?&gt;[] targetInterfaces = ClassUtils.getAllInterfacesForClass(beanClass, this.proxyClassLoader); for (Class&lt;?&gt; targetInterface : targetInterfaces) &#123; proxyFactory.addInterface(targetInterface); &#125; &#125; Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) &#123; proxyFactory.addAdvisor(advisor); &#125; proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; return proxyFactory.getProxy(this.proxyClassLoader);&#125; 第4行~第6行new出了一个ProxyFactory，Proxy，顾名思义，代理工厂的意思，提供了简单的方式使用代码获取和配置AOP代理。 第8行的代码做了一个判断，判断的内容是这个节点中proxy-target-class=”false”或者proxy-target-class不配置，即不使用CGLIB生成代理。如果满足条件，进判断，获取当前Bean实现的所有接口，讲这些接口Class对象都添加到ProxyFactory中。 第17行~第28行的代码没什么看的必要，向ProxyFactory中添加一些参数而已。重点看第30行proxyFactory.getProxy(this.proxyClassLoader)这句： 123public Object getProxy(ClassLoader classLoader) &#123;return createAopProxy().getProxy(classLoader);&#125; 实现代码就一行，但是却明确告诉我们做了两件事情： 创建AopProxy接口实现类通过AopProxy接口的实现类的getProxy方法获取对应的代理就从这两个点出发，分两部分分析一下。 代理对象实例化—-创建AopProxy接口实现类看一下createAopProxy()方法的实现，它位于DefaultAopProxyFactory类中： 123456protected final synchronized AopProxy createAopProxy() &#123;if (!this.active) &#123;activate();&#125;return getAopProxyFactory().createAopProxy(this);&#125; 前面的部分没什么必要看，直接进入重点即createAopProxy方法： 123456789101112131415161718192021public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; if (targetClass.isInterface()) &#123; return new JdkDynamicAopProxy(config); &#125; if (!cglibAvailable) &#123; throw new AopConfigException( &quot;Cannot proxy target class because CGLIB2 is not available. &quot; + &quot;Add CGLIB to the class path or specify proxy interfaces.&quot;); &#125; return CglibProxyFactory.createCglibProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&#125; 平时我们说AOP原理三句话就能概括：对类生成代理使用CGLIB对接口生成代理使用JDK原生的Proxy可以通过配置文件指定对接口使用CGLIB生成代理这三句话的出处就是createAopProxy方法。看到默认是第19行的代码使用JDK自带的Proxy生成代理，碰到以下三种情况例外： ProxyConfig的isOptimize方法为true，这表示让Spring自己去优化而不是用户指定ProxyConfig的isProxyTargetClass方法为true，这表示配置了proxy-target-class=”true”ProxyConfig满足hasNoUserSuppliedProxyInterfaces方法执行结果为true，这表示对象没有实现任何接口或者实现的接口是SpringProxy接口在进入第2行的if判断之后再根据目标的类型决定返回哪种AopProxy。简单总结起来就是： proxy-target-class没有配置或者proxy-target-class=”false”，返回JdkDynamicAopProxyproxy-target-class=”true”或者对象没有实现任何接口或者只实现了SpringProxy接口，返回Cglib2AopProxy当然，不管是JdkDynamicAopProxy还是Cglib2AopProxy，AdvisedSupport都是作为构造函数参数传入的，里面存储了具体的Advisor。 代理对象实例化—-通过getProxy方法获取对应的代理其实代码已经分析到了JdkDynamicAopProxy和Cglib2AopProxy，剩下的就没什么好讲的了，无非就是看对这两种方式生成代理的熟悉程度而已。 Cglib2AopProxy生成代理的代码就不看了，对Cglib不熟悉的朋友可以看Cglib及其基本使用一文。 JdkDynamicAopProxy生成代理的方式稍微看一下： public Object getProxy(ClassLoader classLoader) { if (logger.isDebugEnabled()) { logger.debug(&quot;Creating JDK dynamic proxy: target source is &quot; + this.advised.getTargetSource()); } Class[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); }这边解释一下第5行和第6行的代码，第5行代码的作用是拿到所有要代理的接口，第6行代码的作用是尝试寻找这些接口方法里面有没有equals方法和hashCode方法，同时都有的话打个标记，寻找结束，equals方法和hashCode方法有特殊处理。 最终通过第7行的Proxy.newProxyInstance方法获取接口/类对应的代理对象，Proxy是JDK原生支持的生成代理的方式。 代理方法调用原理前面已经详细分析了为接口/类生成代理的原理，生成代理之后就要调用方法了，这里看一下使用JdkDynamicAopProxy调用方法的原理。 由于JdkDynamicAopProxy本身实现了InvocationHandler接口，因此具体代理前后处理的逻辑在invoke方法中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class targetClass = null; Object target = null; try &#123; if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; // The target does not implement the equals(Object) method itself. return equals(args[0]); &#125; if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; // The target does not implement the hashCode() method itself. return hashCode(); &#125; if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; Object retVal; if (this.advised.exposeProxy) &#123; // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // May be null. Get as late as possible to minimize the time we &quot;own&quot; the target, // in case it comes from a pool. target = targetSource.getTarget(); if (target != null) &#123; targetClass = target.getClass(); &#125; // Get the interception chain for this method. List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don&apos;t, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. if (chain.isEmpty()) &#123; // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. retVal = AopUtils.invokeJoinpointUsingReflection(target, method, args); &#125; else &#123; // We need to create a method invocation... invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed(); &#125; // Massage return value if necessary. if (retVal != null &amp;&amp; retVal == target &amp;&amp; method.getReturnType().isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // Special case: it returned &quot;this&quot; and the return type of the method // is type-compatible. Note that we can&apos;t help if the target sets // a reference to itself in another returned object. retVal = proxy; &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // Restore old proxy. AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 第11行~第18行的代码，表示equals方法与hashCode方法即使满足expression规则，也不会为之产生代理内容，调用的是JdkDynamicAopProxy的equals方法与hashCode方法。至于这两个方法是什么作用，可以自己查看一下源代码。 第19行~第23行的代码，表示方法所属的Class是一个接口并且方法所属的Class是AdvisedSupport的父类或者父接口，直接通过反射调用该方法。 第27行~第30行的代码，是用于判断是否将代理暴露出去的，由标签中的expose-proxy=”true/false”配置。 第41行的代码，获取AdvisedSupport中的所有拦截器和动态拦截器列表，用于拦截方法，具体到我们的实际代码，列表中有三个Object，分别是： chain.get(0)：ExposeInvocationInterceptor，这是一个默认的拦截器，对应的原Advisor为DefaultPointcutAdvisorchain.get(1)：MethodBeforeAdviceInterceptor，用于在实际方法调用之前的拦截，对应的原Advisor为AspectJMethodBeforeAdvicechain.get(2)：AspectJAfterAdvice，用于在实际方法调用之后的处理第45行~第50行的代码，如果拦截器列表为空，很正常，因为某个类/接口下的某个方法可能不满足expression的匹配规则，因此此时通过反射直接调用该方法。 第51行~第56行的代码，如果拦截器列表不为空，按照注释的意思，需要一个ReflectiveMethodInvocation，并通过proceed方法对原方法进行拦截，proceed方法感兴趣的朋友可以去看一下，里面使用到了递归的思想对chain中的Object进行了层层的调用。 CGLIB代理实现下面我们来看一下CGLIB代理的方式，这里需要读者去了解一下CGLIB以及其创建代理的方式： 这里将拦截器链封装到了DynamicAdvisedInterceptor中，并加入了Callback，DynamicAdvisedInterceptor实现了CGLIB的MethodInterceptor，所以其核心逻辑在intercept方法中： 这里我们看到了与JDK动态代理同样的获取拦截器链的过程，并且CglibMethodInvokcation继承了我们在JDK动态代理看到的ReflectiveMethodInvocation，但是并没有重写其proceed方法，只是重写了执行目标方法的逻辑，所以整体上是大同小异的。 到这里，整个Spring 动态AOP的源码就分析完了，Spring还支持静态AOP，这里就不过多赘述了，有兴趣的读者可以查阅相关资料来学习。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析6：Spring AOP概述]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%906%EF%BC%9ASpring%20AOP%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文转自五月的仓颉 https://www.cnblogs.com/xrq730 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 我们为什么要使用 AOP一年半前写了一篇文章Spring3：AOP，是当时学习如何使用Spring AOP的时候写的，比较基础。这篇文章最后的推荐以及回复认为我写的对大家有帮助的评论有很多，但是现在从我个人的角度来看，这篇文章写得并不好，甚至可以说是没有太多实质性的内容，因此这些推荐和评论让我觉得受之有愧。 基于以上原因，更新一篇文章，从最基础的原始代码–&gt;使用设计模式（装饰器模式与代理）–&gt;使用AOP三个层次来讲解一下为什么我们要使用AOP，希望这篇文章可以对网友朋友们有益。 原始代码的写法既然要通过代码来演示，那必须要有例子，这里我的例子为： 有一个接口Dao有insert、delete、update三个方法，在insert与update被调用的前后，打印调用前的毫秒数与调用后的毫秒数首先定义一个Dao接口： 123456789101112/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public interface Dao &#123; public void insert(); public void delete(); public void update();&#125; 然后定义一个实现类DaoImpl： 123456789101112131415161718192021/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class DaoImpl implements Dao &#123; @Override public void insert() &#123; System.out.println(&quot;DaoImpl.insert()&quot;); &#125; @Override public void delete() &#123; System.out.println(&quot;DaoImpl.delete()&quot;); &#125; @Override public void update() &#123; System.out.println(&quot;DaoImpl.update()&quot;); &#125;&#125; 最原始的写法，我要在调用insert()与update()方法前后分别打印时间，就只能定义一个新的类包一层，在调用insert()方法与update()方法前后分别处理一下，新的类我命名为ServiceImpl，其实现为： 123456789101112131415161718192021222324/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class ServiceImpl &#123; private Dao dao = new DaoImpl(); public void insert() &#123; System.out.println(&quot;insert()方法开始时间：&quot; + System.currentTimeMillis()); dao.insert(); System.out.println(&quot;insert()方法结束时间：&quot; + System.currentTimeMillis()); &#125; public void delete() &#123; dao.delete(); &#125; public void update() &#123; System.out.println(&quot;update()方法开始时间：&quot; + System.currentTimeMillis()); dao.update(); System.out.println(&quot;update()方法结束时间：&quot; + System.currentTimeMillis()); &#125;&#125; 这是最原始的写法，这种写法的缺点也是一目了然： 方法调用前后输出时间的逻辑无法复用，如果有别的地方要增加这段逻辑就得再写一遍 如果Dao有其它实现类，那么必须新增一个类去包装该实现类，这将导致类数量不断膨胀 使用装饰器模式接着我们使用上设计模式，先用装饰器模式，看看能解决多少问题。装饰器模式的核心就是实现Dao接口并持有Dao接口的引用，我将新增的类命名为LogDao，其实现为： 12345678910111213141516171819202122232425262728293031/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class LogDao implements Dao &#123; private Dao dao; public LogDao(Dao dao) &#123; this.dao = dao; &#125; @Override public void insert() &#123; System.out.println(&quot;insert()方法开始时间：&quot; + System.currentTimeMillis()); dao.insert(); System.out.println(&quot;insert()方法结束时间：&quot; + System.currentTimeMillis()); &#125; @Override public void delete() &#123; dao.delete(); &#125; @Override public void update() &#123; System.out.println(&quot;update()方法开始时间：&quot; + System.currentTimeMillis()); dao.update(); System.out.println(&quot;update()方法结束时间：&quot; + System.currentTimeMillis()); &#125;&#125; 在使用的时候，可以使用”Dao dao = new LogDao(new DaoImpl())”的方式，这种方式的优点为： 透明，对调用方来说，它只知道Dao，而不知道加上了日志功能类不会无限膨胀，如果Dao的其它实现类需要输出日志，只需要向LogDao的构造函数中传入不同的Dao实现类即可不过这种方式同样有明显的缺点，缺点为： 输出日志的逻辑还是无法复用输出日志的逻辑与代码有耦合，如果我要对delete()方法前后同样输出时间，需要修改LogDao但是，这种做法相比最原始的代码写法，已经有了很大的改进。 使用代理模式接着我们使用代理模式尝试去实现最原始的功能，使用代理模式，那么我们就要定义一个InvocationHandler，我将它命名为LogInvocationHandler，其实现为： 1234567891011121314151617181920212223242526/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class LogInvocationHandler implements InvocationHandler &#123; private Object obj; public LogInvocationHandler(Object obj) &#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String methodName = method.getName(); if (&quot;insert&quot;.equals(methodName) || &quot;update&quot;.equals(methodName)) &#123; System.out.println(methodName + &quot;()方法开始时间：&quot; + System.currentTimeMillis()); Object result = method.invoke(obj, args); System.out.println(methodName + &quot;()方法结束时间：&quot; + System.currentTimeMillis()); return result; &#125; return method.invoke(obj, args); &#125;&#125; 其调用方式很简单，我写一个main函数： 1234567891011121314/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public static void main(String[] args) &#123; Dao dao = new DaoImpl(); Dao proxyDao = (Dao)Proxy.newProxyInstance(LogInvocationHandler.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Dao.class&#125;, new LogInvocationHandler(dao)); proxyDao.insert(); System.out.println(&quot;----------分割线----------&quot;); proxyDao.delete(); System.out.println(&quot;----------分割线----------&quot;); proxyDao.update();&#125; 结果就不演示了，这种方式的优点为： 输出日志的逻辑被复用起来，如果要针对其他接口用上输出日志的逻辑，只要在newProxyInstance的时候的第二个参数增加Class&lt;?&gt;数组中的内容即可 这种方式的缺点为： JDK提供的动态代理只能针对接口做代理，不能针对类做代理代码依然有耦合，如果要对delete方法调用前后打印时间，得在LogInvocationHandler中增加delete方法的判断 使用CGLIB接着看一下使用CGLIB的方式，使用CGLIB只需要实现MethodInterceptor接口即可： 12345678910111213141516171819202122/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class DaoProxy implements MethodInterceptor &#123; @Override public Object intercept(Object object, Method method, Object[] objects, MethodProxy proxy) throws Throwable &#123; String methodName = method.getName(); if (&quot;insert&quot;.equals(methodName) || &quot;update&quot;.equals(methodName)) &#123; System.out.println(methodName + &quot;()方法开始时间：&quot; + System.currentTimeMillis()); proxy.invokeSuper(object, objects); System.out.println(methodName + &quot;()方法结束时间：&quot; + System.currentTimeMillis()); return object; &#125; proxy.invokeSuper(object, objects); return object; &#125;&#125; 代码调用方式为： 1234567891011121314151617/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public static void main(String[] args) &#123; DaoProxy daoProxy = new DaoProxy(); Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(DaoImpl.class); enhancer.setCallback(daoProxy); Dao dao = (DaoImpl)enhancer.create(); dao.insert(); System.out.println(&quot;----------分割线----------&quot;); dao.delete(); System.out.println(&quot;----------分割线----------&quot;); dao.update();&#125; 使用CGLIB解决了JDK的Proxy无法针对类做代理的问题，但是这里要专门说明一个问题：使用装饰器模式可以说是对使用原生代码的一种改进，使用Java代理可以说是对于使用装饰器模式的一种改进，但是使用CGLIB并不是对于使用Java代理的一种改进。 前面的可以说改进是因为使用装饰器模式比使用原生代码更好，使用Java代理又比使用装饰器模式更好，但是Java代理与CGLIb的对比并不能说改进，因为使用CGLIB并不一定比使用Java代理更好，这两种各有优缺点，像Spring框架就同时支持Java Proxy与CGLIB两种方式。 从目前看来代码又更好了一些，但是我认为还有两个缺点： 无论使用Java代理还是使用CGLIB，编写这部分代码都稍显麻烦代码之间的耦合还是没有解决，像要针对delete()方法加上这部分逻辑就必须修改代码 使用AOP最后来看一下使用AOP的方式，首先定义一个时间处理类，我将它命名为TimeHandler： 12345678910111213141516171819202122/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class TimeHandler &#123; public void printTime(ProceedingJoinPoint pjp) &#123; Signature signature = pjp.getSignature(); if (signature instanceof MethodSignature) &#123; MethodSignature methodSignature = (MethodSignature)signature; Method method = methodSignature.getMethod(); System.out.println(method.getName() + &quot;()方法开始时间：&quot; + System.currentTimeMillis()); try &#123; pjp.proceed(); System.out.println(method.getName() + &quot;()方法结束时间：&quot; + System.currentTimeMillis()); &#125; catch (Throwable e) &#123; &#125; &#125; &#125;&#125; 到第8行的代码与第12行的代码分别打印方法开始执行时间与方法结束执行时间。我这里写得稍微复杂点，使用了的写法，其实也可以拆分为与两种，这个看个人喜好。 这里多说一句，切面方法printTime本身可以不用定义任何的参数，但是有些场景下需要获取调用方法的类、方法签名等信息，此时可以在printTime方法中定义JointPoint，Spring会自动将参数注入，可以通过JoinPoint获取调用方法的类、方法签名等信息。由于这里我用的，要保证方法的调用，这样才能在方法调用前后输出时间，因此不能直接使用JoinPoint，因为JoinPoint没法保证方法调用。此时可以使用ProceedingJoinPoint，ProceedingPointPoint的proceed()方法可以保证方法调用，但是要注意一点，ProceedingJoinPoint只能和搭配，换句话说，如果aop.xml中配置的是，然后printTime的方法参数又是ProceedingJoinPoint的话，Spring容器启动将报错。 接着看一下aop.xml的配置： 123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beanshttp://www.springframework.org/schema/beans/spring-beans-3.0.xsdhttp://www.springframework.org/schema/aophttp://www.springframework.org/schema/aop/spring-aop-3.0.xsd&quot;&gt; &lt;bean id=&quot;daoImpl&quot; class=&quot;org.xrq.spring.action.aop.DaoImpl&quot; /&gt; &lt;bean id=&quot;timeHandler&quot; class=&quot;org.xrq.spring.action.aop.TimeHandler&quot; /&gt;&lt;/beans&gt; 我不大会写expression，也懒得去百度了，因此这里就拦截Dao下的所有方法了。测试代码很简单： /** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */ public class AopTest { 1234567891011121314 @Test @SuppressWarnings(&quot;resource&quot;) public void testAop() &#123; ApplicationContext ac = new ClassPathXmlApplicationContext(&quot;spring/aop.xml&quot;); Dao dao = (Dao)ac.getBean(&quot;daoImpl&quot;); dao.insert(); System.out.println(&quot;----------分割线----------&quot;); dao.delete(); System.out.println(&quot;----------分割线----------&quot;); dao.update(); &#125;&#125;AOP总结结果就不演示了。到此我总结一下使用AOP的几个优点： 切面的内容可以复用，比如TimeHandler的printTime方法，任何地方需要打印方法执行前的时间与方法执行后的时间，都可以使用TimeHandler的printTime方法避免使用Proxy、CGLIB生成代理，这方面的工作全部框架去实现，开发者可以专注于切面内容本身代码与代码之间没有耦合，如果拦截的方法有变化修改配置文件即可下面用一张图来表示一下AOP的作用： 我们传统的编程方式是垂直化的编程，即A–&gt;B–&gt;C–&gt;D这么下去，一个逻辑完毕之后执行另外一段逻辑。但是AOP提供了另外一种思路，它的作用是在业务逻辑不知情（即业务逻辑不需要做任何的改动）的情况下对业务代码的功能进行增强，这种编程思想的使用场景有很多，例如事务提交、方法执行之前的权限检测、日志打印、方法调用事件等等。 AOP使用场景举例上面的例子纯粹为了演示使用，为了让大家更加理解AOP的作用，这里以实际场景作为例子。 第一个例子，我们知道MyBatis的事务默认是不会自动提交的，因此在编程的时候我们必须在增删改完毕之后调用SqlSession的commit()方法进行事务提交，这非常麻烦，下面利用AOP简单写一段代码帮助我们自动提交事务（这段代码我个人测试过可用）： 1234567891011121314151617181920212223242526/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class TransactionHandler &#123; public void commit(JoinPoint jp) &#123; Object obj = jp.getTarget(); if (obj instanceof MailDao) &#123; Signature signature = jp.getSignature(); if (signature instanceof MethodSignature) &#123; SqlSession sqlSession = SqlSessionThrealLocalUtil.getSqlSession(); MethodSignature methodSignature = (MethodSignature)signature; Method method = methodSignature.getMethod(); String methodName = method.getName(); if (methodName.startsWith(&quot;insert&quot;) || methodName.startsWith(&quot;update&quot;) || methodName.startsWith(&quot;delete&quot;)) &#123; sqlSession.commit(); &#125; sqlSession.close(); &#125; &#125; &#125;&#125; 这种场景下我们要使用的aop标签为，即切在方法调用之后。 这里我做了一个SqlSessionThreadLocalUtil，每次打开会话的时候，都通过SqlSessionThreadLocalUtil把当前会话SqlSession放到ThreadLocal中，看到通过TransactionHandler，可以实现两个功能： insert、update、delete操作事务自动提交对SqlSession进行close()，这样就不需要在业务代码里面关闭会话了，因为有些时候我们写业务代码的时候会忘记关闭SqlSession，这样可能会造成内存句柄的膨胀，因此这部分切面也一并做了整个过程，业务代码是不知道的，而TransactionHandler的内容可以充分再多处场景下进行复用。 第二个例子是权限控制的例子，不管是从安全角度考虑还是从业务角度考虑，我们在开发一个Web系统的时候不可能所有请求都对所有用户开放，因此这里就需要做一层权限控制了，大家看AOP作用的时候想必也肯定会看到AOP可以做权限控制，这里我就演示一下如何使用AOP做权限控制。我们知道原生的Spring MVC，Java类是实现Controller接口的，基于此，利用AOP做权限控制的大致代码如下（这段代码纯粹就是一段示例，我构建的Maven工程是一个普通的Java工程，因此没有验证过）： 1234567891011121314151617181920212223242526272829303132333435363738/** * @author 五月的仓颉http://www.cnblogs.com/xrq730/p/7003082.html */public class PermissionHandler &#123; public void hasPermission(JoinPoint jp) throws Exception &#123; Object obj = jp.getTarget(); if (obj instanceof Controller) &#123; Signature signature = jp.getSignature(); MethodSignature methodSignature = (MethodSignature)signature; // 获取方法签名 Method method = methodSignature.getMethod(); // 获取方法参数 Object[] args = jp.getArgs(); // Controller中唯一一个方法的方法签名ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception; // 这里对这个方法做一层判断 if (&quot;handleRequest&quot;.equals(method.getName()) &amp;&amp; args.length == 2) &#123; Object firstArg = args[0]; if (obj instanceof HttpServletRequest) &#123; HttpServletRequest request = (HttpServletRequest)firstArg; // 获取用户id long userId = Long.parseLong(request.getParameter(&quot;userId&quot;)); // 获取当前请求路径 String requestUri = request.getRequestURI(); if(!PermissionUtil.hasPermission(userId, requestUri)) &#123; throw new Exception(&quot;没有权限&quot;); &#125; &#125; &#125; &#125; &#125;&#125; 毫无疑问这种场景下我们要使用的aop标签为。这里我写得很简单，获取当前用户id与请求路径，根据这两者，判断该用户是否有权限访问该请求，大家明白意思即可。 后记文章演示了从原生代码到使用AOP的过程，一点一点地介绍了每次演化的优缺点，最后以实际例子分析了AOP可以做什么事情。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析5：JDK和cglib动态代理原理详解]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%905%EF%BC%9AJDK%E5%92%8Ccglib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[转自 https://www.jianshu.com/u/668d0795a95b 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言AOP的基础是Java动态代理，了解和使用两种动态代理能让我们更好地理解 AOP，在讲解AOP之前，让我们先来看看Java动态代理的使用方式以及底层实现原理。 本文是基于jdk1.8来对动态代理的底层机制进行探究的 Java代理介绍Java中代理的实现一般分为三种：JDK静态代理、JDK动态代理以及CGLIB动态代理。在Spring的AOP实现中，主要应用了JDK动态代理以及CGLIB动态代理。但是本文着重介绍JDK动态代理机制，CGLIB动态代理后面会接着探究。 代理一般实现的模式为JDK静态代理：创建一个接口，然后创建被代理的类实现该接口并且实现该接口中的抽象方法。之后再创建一个代理类，同时使其也实现这个接口。在代理类中持有一个被代理对象的引用，而后在代理类方法中调用该对象的方法。 其实就是代理类为被代理类预处理消息、过滤消息并在此之后将消息转发给被代理类，之后还能进行消息的后置处理。代理类和被代理类通常会存在关联关系(即上面提到的持有的被带离对象的引用)，代理类本身不实现服务，而是通过调用被代理类中的方法来提供服务。 静态代理 接口 被代理类 代理类 测试类以及输出结果 我们可以看出，使用JDK静态代理很容易就完成了对一个类的代理操作。但是JDK静态代理的缺点也暴露了出来：由于代理只能为一个类服务，如果需要代理的类很多，那么就需要编写大量的代理类，比较繁琐。 下面我们使用JDK动态代理来做同样的事情 JDK动态代理 接口 被代理类 代理类 测试类以及输出结果 JDK动态代理实现原理JDK动态代理其实也是基本接口实现的。因为通过接口指向实现类实例的多态方式，可以有效地将具体实现与调用解耦，便于后期的修改和维护。 通过上面的介绍，我们可以发现JDK静态代理与JDK动态代理之间有些许相似，比如说都要创建代理类，以及代理类都要实现接口等。但是不同之处也非常明显—-在静态代理中我们需要对哪个接口和哪个被代理类创建代理类，所以我们在编译前就需要代理类实现与被代理类相同的接口，并且直接在实现的方法中调用被代理类相应的方法；但是动态代理则不同，我们不知道要针对哪个接口、哪个被代理类创建代理类，因为它是在运行时被创建的。 让我们用一句话来总结一下JDK静态代理和JDK动态代理的区别，然后开始探究JDK动态代理的底层实现机制：JDK静态代理是通过直接编码创建的，而JDK动态代理是利用反射机制在运行时创建代理类的。其实在动态代理中，核心是InvocationHandler。每一个代理的实例都会有一个关联的调用处理程序(InvocationHandler)。对待代理实例进行调用时，将对方法的调用进行编码并指派到它的调用处理器(InvocationHandler)的invoke方法。所以对代理对象实例方法的调用都是通过InvocationHandler中的invoke方法来完成的，而invoke方法会根据传入的代理对象、方法名称以及参数决定调用代理的哪个方法。 我们从JDK动态代理的测试类中可以发现代理类生成是通过Proxy类中的newProxyInstance来完成的，下面我们进入这个函数看一看： Proxy类中的newProxyInstance123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException &#123; //如果h为空将抛出异常 Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone();//拷贝被代理类实现的一些接口，用于后面权限方面的一些检查 final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; //在这里对某些安全权限进行检查，确保我们有权限对预期的被代理类进行代理 checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * 下面这个方法将产生代理类 */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * 使用指定的调用处理程序获取代理类的构造函数对象 */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; //假如代理类的构造函数是private的，就使用反射来set accessible if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; //根据代理类的构造函数来生成代理类的对象并返回 return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125; &#125; 所以代理类其实是通过getProxyClass方法来生成的： 1234567891011121314/** * 生成一个代理类，但是在调用本方法之前必须进行权限检查 */ private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; //如果接口数量大于65535，抛出非法参数错误 if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); &#125; // 如果在缓存中有对应的代理类，那么直接返回 // 否则代理类将有 ProxyClassFactory 来创建 return proxyClassCache.get(loader, interfaces); &#125; 那么ProxyClassFactory是什么呢？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * 里面有一个根据给定ClassLoader和Interface来创建代理类的工厂函数 * */ private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; &#123; // 代理类的名字的前缀统一为“$Proxy” private static final String proxyClassNamePrefix = &quot;$Proxy&quot;; // 每个代理类前缀后面都会跟着一个唯一的编号，如$Proxy0、$Proxy1、$Proxy2 private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); for (Class&lt;?&gt; intf : interfaces) &#123; /* * 验证类加载器加载接口得到对象是否与由apply函数参数传入的对象相同 */ Class&lt;?&gt; interfaceClass = null; try &#123; interfaceClass = Class.forName(intf.getName(), false, loader); &#125; catch (ClassNotFoundException e) &#123; &#125; if (interfaceClass != intf) &#123; throw new IllegalArgumentException( intf + &quot; is not visible from class loader&quot;); &#125; /* * 验证这个Class对象是不是接口 */ if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( interfaceClass.getName() + &quot; is not an interface&quot;); &#125; /* * 验证这个接口是否重复 */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) &#123; throw new IllegalArgumentException( &quot;repeated interface: &quot; + interfaceClass.getName()); &#125; &#125; String proxyPkg = null; // 声明代理类所在的package int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * 记录一个非公共代理接口的包，以便在同一个包中定义代理类。同时验证所有非公共 * 代理接口都在同一个包中 */ for (Class&lt;?&gt; intf : interfaces) &#123; int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) &#123; accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf(&apos;.&apos;); String pkg = ((n == -1) ? &quot;&quot; : name.substring(0, n + 1)); if (proxyPkg == null) &#123; proxyPkg = pkg; &#125; else if (!pkg.equals(proxyPkg)) &#123; throw new IllegalArgumentException( &quot;non-public interfaces from different packages&quot;); &#125; &#125; &#125; if (proxyPkg == null) &#123; // 如果全是公共代理接口，那么生成的代理类就在com.sun.proxy package下 proxyPkg = ReflectUtil.PROXY_PACKAGE + &quot;.&quot;; &#125; /* * 为代理类生成一个name package name + 前缀+唯一编号 * 如 com.sun.proxy.$Proxy0.class */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * 生成指定代理类的字节码文件 */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try &#123; return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); &#125; &#125; &#125; 字节码生成由上方代码byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags);可以看到，其实生成代理类字节码文件的工作是通过 ProxyGenerate类中的generateProxyClass方法来完成的。 12345678910111213141516171819202122232425262728293031public static byte[] generateProxyClass(final String var0, Class&lt;?&gt;[] var1, int var2) &#123; ProxyGenerator var3 = new ProxyGenerator(var0, var1, var2); // 真正用来生成代理类字节码文件的方法在这里 final byte[] var4 = var3.generateClassFile(); // 保存代理类的字节码文件 if(saveGeneratedFiles) &#123; AccessController.doPrivileged(new PrivilegedAction() &#123; public Void run() &#123; try &#123; int var1 = var0.lastIndexOf(46); Path var2; if(var1 &gt; 0) &#123; Path var3 = Paths.get(var0.substring(0, var1).replace(&apos;.&apos;, File.separatorChar), new String[0]); Files.createDirectories(var3, new FileAttribute[0]); var2 = var3.resolve(var0.substring(var1 + 1, var0.length()) + &quot;.class&quot;); &#125; else &#123; var2 = Paths.get(var0 + &quot;.class&quot;, new String[0]); &#125; Files.write(var2, var4, new OpenOption[0]); return null; &#125; catch (IOException var4x) &#123; throw new InternalError(&quot;I/O exception saving generated file: &quot; + var4x); &#125; &#125; &#125;); &#125; return var4; &#125; 下面来看看真正用于生成代理类字节码文件的generateClassFile方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117private byte[] generateClassFile() &#123; //下面一系列的addProxyMethod方法是将接口中的方法和Object中的方法添加到代理方法中(proxyMethod) this.addProxyMethod(hashCodeMethod, Object.class); this.addProxyMethod(equalsMethod, Object.class); this.addProxyMethod(toStringMethod, Object.class); Class[] var1 = this.interfaces; int var2 = var1.length; int var3; Class var4; //获得接口中所有方法并添加到代理方法中 for(var3 = 0; var3 &lt; var2; ++var3) &#123; var4 = var1[var3]; Method[] var5 = var4.getMethods(); int var6 = var5.length; for(int var7 = 0; var7 &lt; var6; ++var7) &#123; Method var8 = var5[var7]; this.addProxyMethod(var8, var4); &#125; &#125; Iterator var11 = this.proxyMethods.values().iterator(); //验证具有相同方法签名的方法的返回类型是否一致 List var12; while(var11.hasNext()) &#123; var12 = (List)var11.next(); checkReturnTypes(var12); &#125; //后面一系列的步骤用于写代理类Class文件 Iterator var15; try &#123; //生成代理类的构造函数 this.methods.add(this.generateConstructor()); var11 = this.proxyMethods.values().iterator(); while(var11.hasNext()) &#123; var12 = (List)var11.next(); var15 = var12.iterator(); while(var15.hasNext()) &#123; ProxyGenerator.ProxyMethod var16 = (ProxyGenerator.ProxyMethod)var15.next(); //将代理类字段声明为Method，并且字段修饰符为 private static. //因为 10 是 ACC_PRIVATE和ACC_STATIC的与运算 故代理类的字段都是 private static Method *** this.fields.add(new ProxyGenerator.FieldInfo(var16.methodFieldName, &quot;Ljava/lang/reflect/Method;&quot;, 10)); //生成代理类的方法 this.methods.add(var16.generateMethod()); &#125; &#125; //为代理类生成静态代码块对某些字段进行初始化 this.methods.add(this.generateStaticInitializer()); &#125; catch (IOException var10) &#123; throw new InternalError(&quot;unexpected I/O Exception&quot;, var10); &#125; if(this.methods.size() &gt; &apos;\uffff&apos;) &#123; //代理类中的方法数量超过65535就抛异常 throw new IllegalArgumentException(&quot;method limit exceeded&quot;); &#125; else if(this.fields.size() &gt; &apos;\uffff&apos;) &#123;// 代理类中字段数量超过65535也抛异常 throw new IllegalArgumentException(&quot;field limit exceeded&quot;); &#125; else &#123; // 后面是对文件进行处理的过程 this.cp.getClass(dotToSlash(this.className)); this.cp.getClass(&quot;java/lang/reflect/Proxy&quot;); var1 = this.interfaces; var2 = var1.length; for(var3 = 0; var3 &lt; var2; ++var3) &#123; var4 = var1[var3]; this.cp.getClass(dotToSlash(var4.getName())); &#125; this.cp.setReadOnly(); ByteArrayOutputStream var13 = new ByteArrayOutputStream(); DataOutputStream var14 = new DataOutputStream(var13); try &#123; var14.writeInt(-889275714); var14.writeShort(0); var14.writeShort(49); this.cp.write(var14); var14.writeShort(this.accessFlags); var14.writeShort(this.cp.getClass(dotToSlash(this.className))); var14.writeShort(this.cp.getClass(&quot;java/lang/reflect/Proxy&quot;)); var14.writeShort(this.interfaces.length); Class[] var17 = this.interfaces; int var18 = var17.length; for(int var19 = 0; var19 &lt; var18; ++var19) &#123; Class var22 = var17[var19]; var14.writeShort(this.cp.getClass(dotToSlash(var22.getName()))); &#125; var14.writeShort(this.fields.size()); var15 = this.fields.iterator(); while(var15.hasNext()) &#123; ProxyGenerator.FieldInfo var20 = (ProxyGenerator.FieldInfo)var15.next(); var20.write(var14); &#125; var14.writeShort(this.methods.size()); var15 = this.methods.iterator(); while(var15.hasNext()) &#123; ProxyGenerator.MethodInfo var21 = (ProxyGenerator.MethodInfo)var15.next(); var21.write(var14); &#125; var14.writeShort(0); return var13.toByteArray(); &#125; catch (IOException var9) &#123; throw new InternalError(&quot;unexpected I/O Exception&quot;, var9); &#125; &#125; &#125; 代理类的方法调用下面是将接口与Object中一些方法添加到代理类中的addProxyMethod方法： 12345678910111213141516171819202122232425262728private void addProxyMethod(Method var1, Class&lt;?&gt; var2) &#123; String var3 = var1.getName();//获得方法名称 Class[] var4 = var1.getParameterTypes();//获得方法参数类型 Class var5 = var1.getReturnType();//获得方法返回类型 Class[] var6 = var1.getExceptionTypes();//异常类型 String var7 = var3 + getParameterDescriptors(var4);//获得方法签名 Object var8 = (List)this.proxyMethods.get(var7);//根据方法前面获得proxyMethod的value if(var8 != null) &#123;//处理多个代理接口中方法重复的情况 Iterator var9 = ((List)var8).iterator(); while(var9.hasNext()) &#123; ProxyGenerator.ProxyMethod var10 = (ProxyGenerator.ProxyMethod)var9.next(); if(var5 == var10.returnType) &#123; ArrayList var11 = new ArrayList(); collectCompatibleTypes(var6, var10.exceptionTypes, var11); collectCompatibleTypes(var10.exceptionTypes, var6, var11); var10.exceptionTypes = new Class[var11.size()]; var10.exceptionTypes = (Class[])var11.toArray(var10.exceptionTypes); return; &#125; &#125; &#125; else &#123; var8 = new ArrayList(3); this.proxyMethods.put(var7, var8); &#125; ((List)var8).add(new ProxyGenerator.ProxyMethod(var3, var4, var5, var6, var2, null)); &#125; 这就是最终真正的代理类，它继承自Proxy并实现了我们定义的Subject接口。我们通过 1HelloInterface helloInterface = (HelloInterface ) Proxy.newProxyInstance(loader, interfaces, handler); 1 得到的最终代理类对象就是上面这个类的实例。那么我们执行如下语句： 1helloInterface.hello(&quot;Tom&quot;); 1 实际上就是执行上面类的相应方法，也就是： 1234567891011121314151617public final void hello(String paramString) &#123; try &#123; this.h.invoke(this, m3, new Object[] &#123; paramString &#125;); //就是调用我们自定义的InvocationHandlerImpl的 invoke方法： return; &#125; catch (Error|RuntimeException localError) &#123; throw localError; &#125; catch (Throwable localThrowable) &#123; throw new UndeclaredThrowableException(localThrowable); &#125; &#125; 注意这里的this.h.invoke中的h，它是类Proxy中的一个属性 1protected InvocationHandler h; 因为这个代理类继承了Proxy，所以也就继承了这个属性，而这个属性值就是我们定义的 1InvocationHandler handler = new InvocationHandlerImpl(hello); 1 同时我们还发现，invoke方法的第一参数在底层调用的时候传入的是this，也就是最终生成的代理对象ProxySubject，这是JVM自己动态生成的，而不是我们自己定义的代理对象。 深入理解CGLIB动态代理机制Cglib是什么 Cglib是一个强大的、高性能的代码生成包，它广泛被许多AOP框架使用，为他们提供方法的拦截。下图是我网上找到的一张Cglib与一些框架和语言的关系： 对此图总结一下： 最底层的是字节码Bytecode，字节码是Java为了保证“一次编译、到处运行”而产生的一种虚拟指令格式，例如iload_0、iconst_1、if_icmpne、dup等 位于字节码之上的是ASM，这是一种直接操作字节码的框架，应用ASM需要对Java字节码、Class结构比较熟悉 位于ASM之上的是CGLIB、Groovy、BeanShell，后两种并不是Java体系中的内容而是脚本语言，它们通过ASM框架生成字节码变相执行Java代码，这说明在JVM中执行程序并不一定非要写Java代码—-只要你能生成Java字节码，JVM并不关心字节码的来源，当然通过Java代码生成的JVM字节码是通过编译器直接生成的，算是最“正统”的JVM字节码 位于CGLIB、Groovy、BeanShell之上的就是Hibernate、Spring AOP这些框架了，这一层大家都比较熟悉 最上层的是Applications，即具体应用，一般都是一个Web项目或者本地跑一个程序 本文是基于CGLIB 3.1进行探究的 cglib is a powerful, high performance and quality Code Generation Library, It is used to extend JAVA classes and implements interfaces at runtime. 在Spring AOP中，通常会用它来生成AopProxy对象。不仅如此，在Hibernate中PO(Persistant Object 持久化对象)字节码的生成工作也要靠它来完成。 本文将深入探究CGLIB动态代理的实现机制，配合下面这篇文章一起食用口味更佳：深入理解JDK动态代理机制 CGLIB动态代理示例下面由一个简单的示例开始我们对CGLIB动态代理的介绍： 为了后续编码的顺利进行，我们需要使用Maven引入CGLIB的包 图1.1 被代理类 图1.2 实现MethodInterceptor接口生成方法拦截器 图1.3 生成代理类对象并打印在代理类对象调用方法之后的执行结果 JDK代理要求被代理的类必须实现接口，有很强的局限性。而CGLIB动态代理则没有此类强制性要求。简单的说，CGLIB会让生成的代理类继承被代理类，并在代理类中对代理方法进行强化处理(前置处理、后置处理等)。在CGLIB底层，其实是借助了ASM这个非常强大的Java字节码生成框架。 生成代理类对象从图1.3中我们看到，代理类对象是由Enhancer类创建的。Enhancer是CGLIB的字节码增强器，可以很方便的对类进行拓展，如图1.3中的为类设置Superclass。 创建代理对象的几个步骤: 生成代理类的二进制字节码文件； 加载二进制字节码，生成Class对象( 例如使用Class.forName()方法 )； 通过反射机制获得实例构造，并创建代理类对象 我们来看看将代理类Class文件反编译之后的Java代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230package proxy;import java.lang.reflect.Method;import net.sf.cglib.core.ReflectUtils;import net.sf.cglib.core.Signature;import net.sf.cglib.proxy.Callback;import net.sf.cglib.proxy.Factory;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;public class HelloServiceImpl$EnhancerByCGLIB$82ef2d06 extends HelloServiceImpl implements Factory&#123; private boolean CGLIB$BOUND; private static final ThreadLocal CGLIB$THREAD_CALLBACKS; private static final Callback[] CGLIB$STATIC_CALLBACKS; private MethodInterceptor CGLIB$CALLBACK_0; private static final Method CGLIB$sayHello$0$Method; private static final MethodProxy CGLIB$sayHello$0$Proxy; private static final Object[] CGLIB$emptyArgs; private static final Method CGLIB$finalize$1$Method; private static final MethodProxy CGLIB$finalize$1$Proxy; private static final Method CGLIB$equals$2$Method; private static final MethodProxy CGLIB$equals$2$Proxy; private static final Method CGLIB$toString$3$Method; private static final MethodProxy CGLIB$toString$3$Proxy; private static final Method CGLIB$hashCode$4$Method; private static final MethodProxy CGLIB$hashCode$4$Proxy; private static final Method CGLIB$clone$5$Method; private static final MethodProxy CGLIB$clone$5$Proxy; static void CGLIB$STATICHOOK1() &#123; CGLIB$THREAD_CALLBACKS = new ThreadLocal(); CGLIB$emptyArgs = new Object[0]; Class localClass1 = Class.forName(&quot;proxy.HelloServiceImpl$EnhancerByCGLIB$82ef2d06&quot;); Class localClass2; Method[] tmp95_92 = ReflectUtils.findMethods(new String[] &#123; &quot;finalize&quot;, &quot;()V&quot;, &quot;equals&quot;, &quot;(Ljava/lang/Object;)Z&quot;, &quot;toString&quot;, &quot;()Ljava/lang/String;&quot;, &quot;hashCode&quot;, &quot;()I&quot;, &quot;clone&quot;, &quot;()Ljava/lang/Object;&quot; &#125;, (localClass2 = Class.forName(&quot;java.lang.Object&quot;)).getDeclaredMethods()); CGLIB$finalize$1$Method = tmp95_92[0]; CGLIB$finalize$1$Proxy = MethodProxy.create(localClass2, localClass1, &quot;()V&quot;, &quot;finalize&quot;, &quot;CGLIB$finalize$1&quot;); Method[] tmp115_95 = tmp95_92; CGLIB$equals$2$Method = tmp115_95[1]; CGLIB$equals$2$Proxy = MethodProxy.create(localClass2, localClass1, &quot;(Ljava/lang/Object;)Z&quot;, &quot;equals&quot;, &quot;CGLIB$equals$2&quot;); Method[] tmp135_115 = tmp115_95; CGLIB$toString$3$Method = tmp135_115[2]; CGLIB$toString$3$Proxy = MethodProxy.create(localClass2, localClass1, &quot;()Ljava/lang/String;&quot;, &quot;toString&quot;, &quot;CGLIB$toString$3&quot;); Method[] tmp155_135 = tmp135_115; CGLIB$hashCode$4$Method = tmp155_135[3]; CGLIB$hashCode$4$Proxy = MethodProxy.create(localClass2, localClass1, &quot;()I&quot;, &quot;hashCode&quot;, &quot;CGLIB$hashCode$4&quot;); Method[] tmp175_155 = tmp155_135; CGLIB$clone$5$Method = tmp175_155[4]; CGLIB$clone$5$Proxy = MethodProxy.create(localClass2, localClass1, &quot;()Ljava/lang/Object;&quot;, &quot;clone&quot;, &quot;CGLIB$clone$5&quot;); tmp175_155; Method[] tmp223_220 = ReflectUtils.findMethods(new String[] &#123; &quot;sayHello&quot;, &quot;()V&quot; &#125;, (localClass2 = Class.forName(&quot;proxy.HelloServiceImpl&quot;)).getDeclaredMethods()); CGLIB$sayHello$0$Method = tmp223_220[0]; CGLIB$sayHello$0$Proxy = MethodProxy.create(localClass2, localClass1, &quot;()V&quot;, &quot;sayHello&quot;, &quot;CGLIB$sayHello$0&quot;); tmp223_220; return; &#125; final void CGLIB$sayHello$0() &#123; super.sayHello(); &#125; public final void sayHello() &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; if (this.CGLIB$CALLBACK_0 != null) &#123; return; &#125; super.sayHello(); &#125; final void CGLIB$finalize$1() throws Throwable &#123; super.finalize(); &#125; protected final void finalize() throws Throwable &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; if (this.CGLIB$CALLBACK_0 != null) &#123; return; &#125; super.finalize(); &#125; final boolean CGLIB$equals$2(Object paramObject) &#123; return super.equals(paramObject); &#125; public final boolean equals(Object paramObject) &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; MethodInterceptor tmp17_14 = this.CGLIB$CALLBACK_0; if (tmp17_14 != null) &#123; Object tmp41_36 = tmp17_14.intercept(this, CGLIB$equals$2$Method, new Object[] &#123; paramObject &#125;, CGLIB$equals$2$Proxy); tmp41_36; return tmp41_36 == null ? false : ((Boolean)tmp41_36).booleanValue(); &#125; return super.equals(paramObject); &#125; final String CGLIB$toString$3() &#123; return super.toString(); &#125; public final String toString() &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; MethodInterceptor tmp17_14 = this.CGLIB$CALLBACK_0; if (tmp17_14 != null) &#123; return (String)tmp17_14.intercept(this, CGLIB$toString$3$Method, CGLIB$emptyArgs, CGLIB$toString$3$Proxy); &#125; return super.toString(); &#125; final int CGLIB$hashCode$4() &#123; return super.hashCode(); &#125; public final int hashCode() &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; MethodInterceptor tmp17_14 = this.CGLIB$CALLBACK_0; if (tmp17_14 != null) &#123; Object tmp36_31 = tmp17_14.intercept(this, CGLIB$hashCode$4$Method, CGLIB$emptyArgs, CGLIB$hashCode$4$Proxy); tmp36_31; return tmp36_31 == null ? 0 : ((Number)tmp36_31).intValue(); &#125; return super.hashCode(); &#125; final Object CGLIB$clone$5() throws CloneNotSupportedException &#123; return super.clone(); &#125; protected final Object clone() throws CloneNotSupportedException &#123; MethodInterceptor tmp4_1 = this.CGLIB$CALLBACK_0; if (tmp4_1 == null) &#123; tmp4_1; CGLIB$BIND_CALLBACKS(this); &#125; MethodInterceptor tmp17_14 = this.CGLIB$CALLBACK_0; if (tmp17_14 != null) &#123; return tmp17_14.intercept(this, CGLIB$clone$5$Method, CGLIB$emptyArgs, CGLIB$clone$5$Proxy); &#125; return super.clone(); &#125; public static MethodProxy CGLIB$findMethodProxy(Signature paramSignature) &#123; String tmp4_1 = paramSignature.toString(); switch (tmp4_1.hashCode()) &#123; case -1574182249: if (tmp4_1.equals(&quot;finalize()V&quot;)) &#123; return CGLIB$finalize$1$Proxy; &#125; break; &#125; &#125; public HelloServiceImpl$EnhancerByCGLIB$82ef2d06() &#123; CGLIB$BIND_CALLBACKS(this); &#125; public static void CGLIB$SET_THREAD_CALLBACKS(Callback[] paramArrayOfCallback) &#123; CGLIB$THREAD_CALLBACKS.set(paramArrayOfCallback); &#125; public static void CGLIB$SET_STATIC_CALLBACKS(Callback[] paramArrayOfCallback) &#123; CGLIB$STATIC_CALLBACKS = paramArrayOfCallback; &#125; private static final void CGLIB$BIND_CALLBACKS(Object paramObject) &#123; 82ef2d06 local82ef2d06 = (82ef2d06)paramObject; if (!local82ef2d06.CGLIB$BOUND) &#123; local82ef2d06.CGLIB$BOUND = true; Object tmp23_20 = CGLIB$THREAD_CALLBACKS.get(); if (tmp23_20 == null) &#123; tmp23_20; CGLIB$STATIC_CALLBACKS; &#125; local82ef2d06.CGLIB$CALLBACK_0 = (// INTERNAL ERROR // 对委托类进行代理我们上面贴出了生成的代理类源码。以我们上面的例子为参考，下面我们总结一下CGLIB在进行代理的时候都进行了哪些工作呢 生成的代理类HelloServiceImpl$EnhancerByCGLIB$82ef2d06继承被代理类HelloServiceImpl。在这里我们需要注意一点：如果委托类被final修饰，那么它不可被继承，即不可被代理；同样，如果委托类中存在final修饰的方法，那么该方法也不可被代理； 代理类会为委托方法生成两个方法，一个是重写的sayHello方法，另一个是CGLIB$sayHello$0方法，我们可以看到它是直接调用父类的sayHello方法； 当执行代理对象的sayHello方法时，会首先判断一下是否存在实现了MethodInterceptor接口的CGLIB$CALLBACK_0;，如果存在，则将调用MethodInterceptor中的intercept方法，如图2.1。 图2.1 intercept方法 图2.2 代理类为每个委托方法都会生成两个方法 在intercept方法中，我们除了会调用委托方法，还会进行一些增强操作。在Spring AOP中，典型的应用场景就是在某些敏感方法执行前后进行操作日志记录。 我们从图2.1中看到，调用委托方法是通过代理方法的MethodProxy对象调用invokeSuper方法来执行的，下面我们看看invokeSuper方法中的玄机： 图2.3 invokeSuper方法 在这里好像不能直接看出代理方法的调用。没关系，我会慢慢介绍。我们知道，在JDK动态代理中方法的调用是通过反射来完成的。如果有对此不太了解的同学，可以看下我之前的博客—-深入理解JDK动态代理机制。但是在CGLIB中，方法的调用并不是通过反射来完成的，而是直接对方法进行调用：FastClass对Class对象进行特别的处理，比如将会用数组保存method的引用，每次调用方法的时候都是通过一个index下标来保持对方法的引用。比如下面的getIndex方法就是通过方法签名来获得方法在存储了Class信息的数组中的下标。 图2.4 getIndex方法 图2.5 FastClassInfo类中持有两个FastClass对象的引用.png 以我们上面的sayHello方法为例，f1指向委托类对象，f2指向代理类对象，i1和i2分别代表着sayHello方法以及CGLIB$sayHello$0方法在对象信息数组中的下标。 到此为止CGLIB动态代理机制就介绍完了，下面给出三种代理方式之间对比。 代理方式 实现 优点 缺点 特点 JDK静态代理 代理类与委托类实现同一接口，并且在代理类中需要硬编码接口 实现简单，容易理解 代理类需要硬编码接口，在实际应用中可能会导致重复编码，浪费存储空间并且效率很低 好像没啥特点 JDK动态代理 代理类与委托类实现同一接口，主要是通过代理类实现InvocationHandler并重写invoke方法来进行动态代理的，在invoke方法中将对方法进行增强处理 不需要硬编码接口，代码复用率高 只能够代理实现了接口的委托类 底层使用反射机制进行方法的调用 CGLIB动态代理 代理类将委托类作为自己的父类并为其中的非final委托方法创建两个方法，一个是与委托方法签名相同的方法，它在方法中会通过super调用委托方法；另一个是代理类独有的方法。在代理方法中，它会判断是否存在实现了MethodInterceptor接口的对象，若存在则将调用intercept方法对委托方法进行代理 可以在运行时对类或者是接口进行增强操作，且委托类无需实现接口 不能对final类以及final方法进行代理 底层将方法全部存入一个数组中，通过数组索引直接进行方法调用 ## 微信公众号 个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析4：懒加载的单例Bean获取过程分析]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%904%EF%BC%9A%E6%87%92%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%8D%95%E4%BE%8BBean%E8%8E%B7%E5%8F%96%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文转自五月的仓颉 https://www.cnblogs.com/xrq730 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言xml的读取应该是Spring的重要功能，因为Spring的大部分功能都是以配置做为切入点的。 我们在静态代码块中读取配置文件可以这样做：12//这样来加载配置文件 XmlBeanFactory factory = new XmlBeanFactory(new ClassPathResource(&quot;beans.xml&quot;)); （1）XmlBeanFactory 继承 AbstractBeanDefinitionReader ，使用ResourceLoader 将资源文件路径转换为对应的Resource文件。 （2）通过DocumentLoader 对 Resource 文件进行转换，将 Resource 文件转换为 Document 文件。 （3）通过实现接口 BeanDefinitionDocumentReader 的 DefaultBeanDefinitionDocumentReader 类对Document 进行解析，并且使用 BeanDefinitionParserDelegate对Element进行解析。step1: 在平常开发中，我们也可以使用Resource 获取 资源文件：12Resource resource = new ClassPathResource(&quot;application.xml&quot;);InputStream in = resource.getInputStream(); step2: 在资源实现加载之前，调用了 super(parentBeanFactory) -- /**Ignore the given dependency interface for autowiring.(忽略接口的自动装配功能)*/ 调用XmlBeanDefinitionReader 的 loadBeanDefinitions（）方法进行加载资源： （1） 对Resource资源进行编码 （2） 通过SAX读取XML文件来创建InputSource对象 （3） 核心处理 可以很直观的看出来是这个function是在解析xml文件从而获得对应的Document对象。 在doLoadDocument方法里面还存一个方法getValidationModeForResource（）用来读取xml的验证模式。（和我关心的没什么关系，暂时不看了~） 转换成document也是最常用的方法： ![869effccb2e4f7b69e0b53d17fe0a2b50044d61b](https://oss-cn-hangzhou.aliyuncs.com/yqfiles/869effccb2e4f7b69e0b53d17fe0a2b50044d61b.png)step3 : 我们已经step by step 的看到了如何将xml文件转换成Document的，现在就要分析是如何提取和注册bean的。/**Register the bean definitions contained in the given DOM document*/ 参数doc是doLoadBeanDefinitions（）方法传进来的 loadDocument 加载过来的。这边就很好的体现出了面向对象的单一全责原则，将逻辑处理委托給单一的类去处理。 在这边单一逻辑处理类是： BeanDefinitionDocumentReader 核心方法： documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); 开始解析： 在Spring的xml配置中有两种方式来声明bean: 一种是默认的： &lt;bean id = &quot; &quot; class = &quot; &quot; /&gt; 还有一种是自定义的： &lt; tx : annotation-driven / &gt; 通过xml配置文件的默认配置空间来判断：http://www.springframework.org/schema/beans 对于默认标签的解析： 对Bean 配置的解析： BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); 返回BeanDefinitionHolder 这边代码大致看下来： 提取元素中的id和name属性 进一步解析将其他属性封装到 BeanDefinition 的实现类中 如果没有指定beanName 变使用默认规则生成beanName 封装类BeanDefinitionHolder 可以先了解一下 BeanDefinition 这个类的作用。 BeanDefinition是一个接口，对应着配置文件中&lt;bean&gt;里面的所有配置，在Spring中存在着三个实现类： 在配置文件中，可以定义父&lt;bean&gt;和子&lt;bean&gt;，父&lt;bean&gt;是用RootDefinition来表示，子&lt;bean&gt;是用ChildBeanDefinition来表示。 Spring 通过BeanDefiniton将配置文件中的&lt;bean&gt;配置信息转换为容器内部表示，并且将这些BeanDefinition注册到BeanDefinitonRegistry中。Spring容器的BeanDefinitonRegistry就像是Spring配置信息的内存数据库，主要是以map的形式保存的。 因此解析属性首先要创建用于承载属性的实例： 然后就是各种对属性的解析的具体方法： 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析3：Spring IOC容器的加载过程]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%903%EF%BC%9ASpring%20IOC%E5%AE%B9%E5%99%A8%E7%9A%84%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文转自五月的仓颉 https://www.cnblogs.com/xrq730 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 spring ioc 容器的加载流程1.目标：熟练使用spring，并分析其源码，了解其中的思想。这篇主要介绍spring ioc 容器的加载 2.前提条件：会使用debug 3.源码分析方法：Intellj idea debug 模式下源码追溯通过ClassPathXmlApplicationContext 进行xml 件的读取，从每个堆栈中读取程序的运行信息 4.注意：由于Spring的类继承体系比较复杂,不能全部贴图，所以只将分析源码之后发现的最主要的类继承结构类图贴在下方。 5.关于Spring IocDemo：我们从demo入手一步步进行代码追溯。 Spring Ioc Demo 1.定义数据访问接口IUserDao.java 123public interface IUserDao &#123; public void InsertUser(String username,String password);&#125; 2.定义IUserDao.java实现类IUserDaoImpl.java 123456public class UserDaoImpl implements IUserDao &#123; @Override public void InsertUser(String username, String password) &#123; System.out.println(&quot;----UserDaoImpl --addUser----&quot;); &#125;&#125; 3.定义业务逻辑接口UserService.java 123public interface UserService &#123; public void addUser(String username,String password);&#125; 4.定义UserService.java实现类UserServiceImpl.java 12345678910public class UserServiceImpl implements UserService &#123; private IUserDao userDao; //set方法 public void setUserDao(IUserDao userDao) &#123; this.userDao = userDao; &#125; @Override public void addUser(String username,String password) &#123; userDao.InsertUser(username,password); &#125;&#125; bean.xml配置文件 1234567891011&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd &quot;&gt; &lt;!--id名字自己取，class表示他代表的类，如果在包里的话需要加上包名--&gt; &lt;bean id=&quot;userService&quot; class=&quot;UserServiceImpl&quot; &gt; &lt;!--property代表是通过set方法注入,ref的值表示注入的内容--&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;userDao&quot; class=&quot;UserDaoImpl&quot;/&gt;&lt;/beans&gt; ApplicationContext 继承结构 1.顶层接口：ApplicationContext2.ClassPathXmlApplicationContext实现类继承AbstractXmlApplication 抽象类3.AbstractXmlApplication 继承AbstractRefreshableConfigApplicationContext4.AbstractRefreshableConfigApplicationContext抽象类继承AbstractRefreshableApplicationContext5.AbstractRefreshableApplicationContext 继承 AbstractApplicationContext6.AbstractApplicationContext 实现ConfigurableApplicationContext 接口7.ConfigurableApplicationContext 接口继承ApplicationContext接口总体来说继承实现结构较深，内部使用了大量适配器模式。以ClassPathXmlApplicationContext为例，继承类图如下图所示： Spring Ioc容器加载过程源码详解 在开始之前，先介绍一个整体的概念。即spring ioc容器的加载，大体上经过以下几个过程：资源文件定位、解析、注册、实例化 1.资源文件定位其中资源文件定位，一般是在ApplicationContext的实现类里完成的，因为ApplicationContext接口继承ResourcePatternResolver 接口，ResourcePatternResolver接口继承ResourceLoader接口，ResourceLoader其中的getResource()方法，可以将外部的资源，读取为Resource类。 2.解析DefaultBeanDefinitionDocumentReader，解析主要是在BeanDefinitionReader中完成的，最常用的实现类是XmlBeanDefinitionReader，其中的loadBeanDefinitions()方法，负责读取Resource，并完成后续的步骤。ApplicationContext完成资源文件定位之后，是将解析工作委托给XmlBeanDefinitionReader来完成的解析这里涉及到很多步骤，最常见的情况，资源文件来自一个XML配置文件。首先是BeanDefinitionReader，将XML文件读取成w3c的Document文档。 DefaultBeanDefinitionDocumentReader对Document进行进一步解析。然后DefaultBeanDefinitionDocumentReader又委托给BeanDefinitionParserDelegate进行解析。如果是标准的xml namespace元素，会在Delegate内部完成解析，如果是非标准的xml namespace元素，则会委托合适的NamespaceHandler进行解析最终解析的结果都封装为BeanDefinitionHolder，至此解析就算完成。后续会进行细致讲解。 3.注册然后bean的注册是在BeanFactory里完成的，BeanFactory接口最常见的一个实现类是DefaultListableBeanFactory，它实现了BeanDefinitionRegistry接口，所以其中的registerBeanDefinition()方法，可以对BeanDefinition进行注册这里附带一提，最常见的XmlWebApplicationContext不是自己持有BeanDefinition的，它继承自AbstractRefreshableApplicationContext，其持有一个DefaultListableBeanFactory的字段，就是用它来保存BeanDefinition所谓的注册，其实就是将BeanDefinition的name和实例，保存到一个Map中。 刚才说到，最常用的实现DefaultListableBeanFactory，其中的字段就是beanDefinitionMap，是一个ConcurrentHashMap。代码如下：&gt;1.DefaultListableBeanFactory继承实现关系 1234567891011121314public class DefaultListableBeanFactoryextends AbstractAutowireCapableBeanFactory implementsConfigurableListableBeanFactory, BeanDefinitionRegistry,Serializable &#123; // DefaultListableBeanFactory的实例中最终保存了所有注册的bean beanDefinitionMap /** Map of bean definition objects, keyed by bean name */ private final Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String, BeanDefinition&gt;(64); //实现BeanDefinitionRegistry中定义的registerBeanDefinition()抽象方法 public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; &#125; &gt;2.BeanDefinitionRegistry接口 123public interface BeanDefinitionRegistry extends AliasRegistry &#123; //定义注册BeanDefinition实例的抽象方法 void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException; 4.实例化 注册也完成之后，在BeanFactory的getBean()方法之中，会完成初始化，也就是依赖注入的过程大体上的流程就是这样。 refresh()方法 1.目标：这篇记录debug 追溯源码的过程，大概分三个篇幅，这是第一篇，现整体了解一下运行流程，定位资源加载，资源解析，bean 注册发生的位置。2.记录结构：1.调试栈截图2.整体流程3.bean.xml的处理每段代码下面有相应的讲解 调试栈截图 每个栈帧中方法的行号都有标明，按照行号追溯源码，然后配合教程能够快速学习。 整体流程 ioc容器实例化代码 1ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); 进入代码中一步步追溯，发现重要方法：refresh();如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); //beanFactory实例化方法 单步调试入口 // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &apos;active&apos; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; &#125; &#125; 首先这个方法是同步的，以避免重复刷新。然后刷新的每个步骤，都放在单独的方法里，比较清晰，可以按顺序一个个看 首先是prepareRefresh()方法 123456789101112131415161718protected void prepareRefresh() &#123; this.startupDate = System.currentTimeMillis(); synchronized (this.activeMonitor) &#123; this.active = true; &#125; if (logger.isInfoEnabled()) &#123; logger.info(&quot;Refreshing &quot; + this); &#125; // Initialize any placeholder property sources in the context environment initPropertySources(); // Validate that all properties marked as required are resolvable // see ConfigurablePropertyResolver#setRequiredProperties this.environment.validateRequiredProperties(); &#125; 这个方法里做的事情不多，记录了开始时间，输出日志，另外initPropertySources()方法和validateRequiredProperties()方法一般都没有做什么事。 然后是核心的obtainFreshBeanFactory()方法，这个方法是初始化BeanFactory，是整个refresh()方法的核心，其中完成了配置文件的加载、解析、注册，后面会专门详细说 。 这里要说明一下，ApplicationContext实现了BeanFactory接口，并实现了ResourceLoader、MessageSource等接口，可以认为是增强的BeanFactory。但是ApplicationContext并不自己重复实现BeanFactory定义的方法，而是委托给DefaultListableBeanFactory来实现。这种设计思路也是值得学习的。后面的 prepareBeanFactory()、postProcessBeanFactory()、invokeBeanFactoryPostProcessors()、registerBeanPostProcessors()、initMessageSource()、initApplicationEventMulticaster()、onRefresh()、registerListeners()、finishBeanFactoryInitialization()、finishRefresh()等方法，是添加一些后处理器、广播、拦截器等，就不一个个细说了 其中的关键方法是finishBeanFactoryInitialization()，在这个方法中，会对刚才注册的Bean（不延迟加载的），进行实例化，所以也是一个核心方法。 bean.xml的处理 从整体上介绍完了流程，接下来就重点看obtainFreshBeanFactory()方法，上文说到，在这个方法里，完成了配置文件的加载、解析、注册 12345678protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; refreshBeanFactory(); ConfigurableListableBeanFactory beanFactory = getBeanFactory(); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Bean factory for &quot; + getDisplayName() + &quot;: &quot; + beanFactory); &#125; return beanFactory; &#125; 这个方法做了2件事，首先通过refreshBeanFactory()方法，创建了DefaultListableBeanFactory的实例，并进行初始化。 123456789101112131415161718protected final void refreshBeanFactory() throws BeansException &#123; if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125; &#125; 首先如果已经有BeanFactory实例，就先清空。然后通过createBeanFactory()方法，创建一个DefaultListableBeanFactory的实例 123protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory()); &#125; 接下来设置ID唯一标识 1beanFactory.setSerializationId(getId()); 然后允许用户进行一些自定义的配置 123456789protected void customizeBeanFactory(DefaultListableBeanFactory beanFactory) &#123; if (this.allowBeanDefinitionOverriding != null) &#123; beanFactory.setAllowBeanDefinitionOverriding(this.allowBeanDefinitionOverriding); &#125; if (this.allowCircularReferences != null) &#123; beanFactory.setAllowCircularReferences(this.allowCircularReferences); &#125; beanFactory.setAutowireCandidateResolver(new QualifierAnnotationAutowireCandidateResolver()); &#125; 最后，就是核心的loadBeanDefinitions()方法 123456789101112131415protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&apos;s // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader); &#125; 这里首先会创建一个XmlBeanDefinitionReader的实例，然后进行初始化。这个XmlBeanDefinitionReader中其实传递的BeanDefinitionRegistry类型的实例，为什么可以传递一个beanFactory呢，因为DefaultListableBeanFactory实现了BeanDefinitionRegistry接口，这里是多态的使用。 1234567891011121314protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context&apos;s // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader);&#125; 这里要说明一下，ApplicationContext并不自己负责配置文件的加载、解析、注册，而是将这些工作委托给XmlBeanDefinitionReader来做。 1loadBeanDefinitions(beanDefinitionReader); 这行代码，就是Bean定义读取实际发生的地方。这里的工作，主要是XmlBeanDefinitionReader来完成的，下一篇博客会详细介绍这个过程。 loadBeanDefinitionsloadBeanDefinitions: 源码阅读 入口是loadBeanDefinitions方法 123456789protected void loadBeanDefinitions(XmlBeanDefinitionReader reader) throws IOException &#123; String[] configLocations = getConfigLocations(); if (configLocations != null) &#123; for (String configLocation : configLocations) &#123; reader.loadBeanDefinitions(configLocation); &#125; &#125;&#125; 这是解析过程最外围的代码，首先要获取到配置文件的路径，这在之前已经完成了。然后将每个配置文件的路径，作为参数传给BeanDefinitionReader的loadBeanDefinitions方法里 123public int loadBeanDefinitions(String location) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(location, null);&#125; 这个方法又调用了重载方法 1234567891011121314151617181920212223242526272829303132333435363738394041public int loadBeanDefinitions(String location, Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader == null) &#123; throw new BeanDefinitionStoreException( &quot;Cannot import bean definitions from location [&quot; + location + &quot;]: no ResourceLoader available&quot;); &#125; if (resourceLoader instanceof ResourcePatternResolver) &#123; // Resource pattern matching available. try &#123; Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); if (actualResources != null) &#123; for (Resource resource : resources) &#123; actualResources.add(resource); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Loaded &quot; + loadCount + &quot; bean definitions from location pattern [&quot; + location + &quot;]&quot;); &#125; return loadCount; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;Could not resolve bean definition resource pattern [&quot; + location + &quot;]&quot;, ex); &#125; &#125; else &#123; // Can only load single resources by absolute URL. Resource resource = resourceLoader.getResource(location); int loadCount = loadBeanDefinitions(resource); if (actualResources != null) &#123; actualResources.add(resource); &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Loaded &quot; + loadCount + &quot; bean definitions from location [&quot; + location + &quot;]&quot;); &#125; return loadCount; &#125; &#125; 首先getResourceLoader()的实现的前提条件是因为XmlBeanDefinitionReader在实例化的时候已经确定了创建了实例ResourceLoader实例, 代码位于 AbstractBeanDefinitionReader 12345678910111213141516protected AbstractBeanDefinitionReader(BeanDefinitionRegistry registry) &#123; Assert.notNull(registry, &quot;BeanDefinitionRegistry must not be null&quot;); this.registry = registry; // Determine ResourceLoader to use. if (this.registry instanceof ResourceLoader) &#123; this.resourceLoader = (ResourceLoader) this.registry; &#125; else &#123; this.resourceLoader = new PathMatchingResourcePatternResolver(); &#125; // Inherit Environment if possible if (this.registry instanceof EnvironmentCapable) &#123; this.environment = ((EnvironmentCapable)this.registry).getEnvironment(); &#125; else &#123; this.environment = new StandardEnvironment(); &#125;&#125; 这个方法比较长，BeanDefinitionReader不能直接加载配置文件，需要把配置文件封装成Resource，然后才能调用重载方法loadBeanDefinitions()。所以这个方法其实就是2段，第一部分是委托ResourceLoader将配置文件封装成Resource，第二部分是调用loadBeanDefinitions()，对Resource进行解析 而这里的ResourceLoader，就是前面的XmlWebApplicationContext，因为ApplicationContext接口，是继承自ResourceLoader接口的 Resource也是一个接口体系，在web环境下，这里就是ServletContextResource 接下来进入重载方法loadBeanDefinitions() 12345678public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, &quot;Resource array must not be null&quot;); int counter = 0; for (Resource resource : resources) &#123; counter += loadBeanDefinitions(resource); &#125; return counter; &#125; 这里就不用说了，就是把每一个Resource作为参数，继续调用重载方法。读spring源码，会发现重载方法特别多。 1234public int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException &#123; return loadBeanDefinitions(new EncodedResource(resource));&#125; 还是重载方法，不过这里对传进来的Resource又进行了一次封装，变成了编码后的Resource。 12345678910111213141516171819202122232425262728293031323334353637383940public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException &#123; Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Loading XML bean definitions from &quot; + encodedResource.getResource()); &#125; Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;EncodedResource&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException( &quot;IOException parsing XML document from &quot; + encodedResource.getResource(), ex); &#125; finally &#123; currentResources.remove(encodedResource); if (currentResources.isEmpty()) &#123; this.resourcesCurrentlyBeingLoaded.remove(); &#125; &#125; &#125; 这个就是loadBeanDefinitions()的最后一个重载方法，比较长，可以拆看来看。 1234567891011121314Assert.notNull(encodedResource, &quot;EncodedResource must not be null&quot;); if (logger.isInfoEnabled()) &#123; logger.info(&quot;Loading XML bean definitions from &quot; + encodedResource.getResource()); &#125; Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) &#123; currentResources = new HashSet&lt;EncodedResource&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); &#125; if (!currentResources.add(encodedResource)) &#123; throw new BeanDefinitionStoreException( &quot;Detected cyclic loading of &quot; + encodedResource + &quot; - check your import definitions!&quot;); &#125; 这第一部分，是处理线程相关的工作，把当前正在解析的Resource，设置为当前Resource。 12345678910111213try &#123; InputStream inputStream = encodedResource.getResource().getInputStream(); try &#123; InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) &#123; inputSource.setEncoding(encodedResource.getEncoding()); &#125; return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); &#125; finally &#123; inputStream.close(); &#125; &#125; 这里是第二部分，是核心，首先把Resource还原为InputStream，然后调用实际解析的方法doLoadBeanDefinitions()。可以看到，这种命名方式是很值得学习的，一种业务方法，比如parse()，可能需要做一些外围的工作，然后实际解析的方法，可以命名为doParse()。这种doXXX()的命名方法，在很多开源框架中都有应用，比如logback等。接下来就看一下这个doLoadBeanDefinitions()方法 123456789101112131415161718192021222324252627282930protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; try &#123; Document doc = doLoadDocument(inputSource, resource);return registerBeanDefinitions(doc, resource); return registerBeanDefinitions(doc, resource); &#125; catch (BeanDefinitionStoreException ex) &#123; throw ex; &#125; catch (SAXParseException ex) &#123; throw new XmlBeanDefinitionStoreException(resource.getDescription(), &quot;Line &quot; + ex.getLineNumber() + &quot; in XML document from &quot; + resource + &quot; is invalid&quot;, ex); &#125; catch (SAXException ex) &#123; throw new XmlBeanDefinitionStoreException(resource.getDescription(), &quot;XML document from &quot; + resource + &quot; is invalid&quot;, ex); &#125; catch (ParserConfigurationException ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), &quot;Parser configuration exception parsing XML from &quot; + resource, ex); &#125; catch (IOException ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), &quot;IOException parsing XML document from &quot; + resource, ex); &#125; catch (Throwable ex) &#123; throw new BeanDefinitionStoreException(resource.getDescription(), &quot;Unexpected exception parsing XML document from &quot; + resource, ex); &#125; &#125; 抛开异常处理：核心代码如下： 12Document doc = doLoadDocument(inputSource, resource);return registerBeanDefinitions(doc, resource); doLoadDocument方法将InputStream读取成标准的Document对象，然后调用registerBeanDefinitions()，进行解析工作。 123456protected Document doLoadDocument(InputSource inputSource, Resource resource) throws Exception &#123; return this.documentLoader.loadDocument(inputSource, getEntityResolver(), this.errorHandler, getValidationModeForResource(resource), isNamespaceAware());&#125; 接下来就看一下这个核心方法registerBeanDefinitions 12345678public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; //创建的其实是DefaultBeanDefinitionDocumentReader 的实例，利用反射创建的。 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); documentReader.setEnvironment(this.getEnvironment()); int countBefore = getRegistry().getBeanDefinitionCount(); documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125; 这里注意两点 : 1.Document对象首先这个Document对象，是W3C定义的标准XML对象，跟spring无关。其次这个registerBeanDefinitions方法，我觉得命名有点误导性。因为这个时候实际上解析还没有开始，怎么直接就注册了呢。比较好的命名，我觉得可以是parseAndRegisterBeanDefinitions()。2.documentReader的创建时使用反射创建的，代码如下 12345protected BeanDefinitionDocumentReader createBeanDefinitionDocumentReader() &#123; return BeanDefinitionDocumentReader.class.cast(BeanUtils. instantiateClass(this.documentReaderClass));&#125; instantiateClass方法中传入了一个Class类型的参数。追溯发现下述代码： 12private Class&lt;?&gt; documentReaderClass = DefaultBeanDefinitionDocumentReader.class; 所以创建的documentReaderClass是DefaultBeanDefinitionDocumentReader类的实例。接下来就进入BeanDefinitionDocumentReader 中定义的registerBeanDefinitions()方法看看 123456public void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug(&quot;Loading bean definitions&quot;); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root); &#125; 处理完外围事务之后，进入doRegisterBeanDefinitions()方法，这种命名规范，上文已经介绍过了 12345678910111213141516171819202122protected void doRegisterBeanDefinitions(Element root) &#123; String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) &#123; Assert.state(this.environment != null, &quot;environment property must not be null&quot;); String[] specifiedProfiles = StringUtils.tokenizeToStringArray(profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!this.environment.acceptsProfiles(specifiedProfiles)) &#123; return; &#125; &#125; // any nested &lt;beans&gt; elements will cause recursion in this method. In // order to propagate and preserve &lt;beans&gt; default-* attributes correctly, // keep track of the current (parent) delegate, which may be null. Create // the new (child) delegate with a reference to the parent for fallback purposes, // then ultimately reset this.delegate back to its original (parent) reference. // this behavior emulates a stack of delegates without actually necessitating one. BeanDefinitionParserDelegate parent = this.delegate; this.delegate = createHelper(readerContext, root, parent); preProcessXml(root); parseBeanDefinitions(root, this.delegate); postProcessXml(root); this.delegate = parent;&#125; 这个方法也比较长，拆开来看 12345678String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) &#123; Assert.state(this.environment != null, &quot;environment property must not be null&quot;); String[] specifiedProfiles = StringUtils.tokenizeToStringArray(profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); if (!this.environment.acceptsProfiles(specifiedProfiles)) &#123; return; &#125;&#125; 如果配置文件中元素，配有profile属性，就会进入这一段，不过一般都是不会的 123456BeanDefinitionParserDelegate parent = this.delegate;this.delegate = createHelper(readerContext, root, parent);preProcessXml(root);parseBeanDefinitions(root, this.delegate);postProcessXml(root);this.delegate = parent; 然后这里创建了BeanDefinitionParserDelegate对象，preProcessXml()和postProcessXml()都是空方法，核心就是parseBeanDefinitions()方法。这里又把BeanDefinition解析和注册的工作，委托给了BeanDefinitionParserDelegate对象，在parseBeanDefinitions()方法中完成总的来说，解析工作的委托链是这样的：ClassPathXmlApplicationContext，XmlBeanDefinitionReader，DefaultBeanDefinitionDocumentReader，BeanDefinitionParserDelegateClassPathXmlApplicationContext作为最外围的组件，发起解析的请求XmlBeanDefinitionReader将配置文件路径封装为Resource，读取出w3c定义的Document对象，然后委托给DefaultBeanDefinitionDocumentReaderDefaultBeanDefinitionDocumentReader就开始做实际的解析工作了，但是涉及到bean的具体解析，它还是会继续委托给BeanDefinitionParserDelegate来做。接下来在parseBeanDefinitions()方法中发生了什么，以及BeanDefinitionParserDelegate类完成的工作，在下一篇博客中继续介绍。 loadBeanDefinitions BeanDefinition的解析,已经走到了DefaultBeanDefinitionDocumentReader里，这时候配置文件已经被加载，并解析成w3c的Document对象。这篇博客就接着介绍，DefaultBeanDefinitionDocumentReader和BeanDefinitionParserDelegate类，是怎么协同完成bean的解析和注册的。 123456BeanDefinitionParserDelegate parent = this.delegate;this.delegate = createHelper(readerContext, root, parent);preProcessXml(root);parseBeanDefinitions(root, this.delegate);postProcessXml(root);this.delegate = parent; 这段代码，创建了一个BeanDefinitionParserDelegate组件，然后就是preProcessXml()、parseBeanDefinitions()、postProcessXml()方法其中preProcessXml()和postProcessXml()默认是空方法，接下来就看下parseBeanDefinitions()方法 1234567891011121314151617181920protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125; &#125; 从这个方法开始，BeanDefinitionParserDelegate就开始发挥作用了，判断当前解析元素是否属于默认的命名空间，如果是的话，就调用parseDefaultElement()方法，否则调用delegate上parseCustomElement()方法 123456public boolean isDefaultNamespace(String namespaceUri) &#123; return (!StringUtils.hasLength(namespaceUri) || BEANS_NAMESPACE_URI.equals(namespaceUri)); &#125; public boolean isDefaultNamespace(Node node) &#123; return isDefaultNamespace(getNamespaceURI(node)); &#125; 只有http://www.springframework.org/schema/beans，会被认为是默认的命名空间。也就是说，beans、bean这些元素，会认为属于默认的命名空间，而像task:scheduled这些，就认为不属于默认命名空间。根节点beans的一个子节点bean，是属于默认命名空间的，所以会进入parseDefaultElement()方法 123456789101112131415private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // recurse doRegisterBeanDefinitions(ele); &#125; &#125; 这里可能会有4种情况，import、alias、bean、beans，分别有一个方法与之对应，这里解析的是bean元素，所以会进入processBeanDefinition()方法 12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &apos;&quot; + bdHolder.getBeanName() + &quot;&apos;&quot;, ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125; &#125; 这里主要有3个步骤，先是委托delegate对bean进行解析，然后委托delegate对bean进行装饰，最后由一个工具类来完成BeanDefinition的注册可以看出来，DefaultBeanDefinitionDocumentReader不负责任何具体的bean解析，它面向的是xml Document对象，根据其元素的命名空间和名称，起一个类似路由的作用（不过，命名空间的判断，也是委托给delegate来做的）。所以这个类的命名，是比较贴切的，突出了其面向Document的特性。具体的工作，是由BeanDefinitionParserDelegate来完成的下面就看下parseBeanDefinitionElement()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public BeanDefinitionHolder parseBeanDefinitionElement(Element ele, BeanDefinition containingBean) &#123; String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;No XML &apos;id&apos; specified - using &apos;&quot; + beanName + &quot;&apos; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Neither XML &apos;id&apos; nor &apos;name&apos; specified - &quot; + &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; return null; &#125; 这个方法很长，可以分成三段来看 123456789101112131415161718String id = ele.getAttribute(ID_ATTRIBUTE); String nameAttr = ele.getAttribute(NAME_ATTRIBUTE); List&lt;String&gt; aliases = new ArrayList&lt;String&gt;(); if (StringUtils.hasLength(nameAttr)) &#123; String[] nameArr = StringUtils.tokenizeToStringArray(nameAttr, MULTI_VALUE_ATTRIBUTE_DELIMITERS); aliases.addAll(Arrays.asList(nameArr)); &#125; String beanName = id; if (!StringUtils.hasText(beanName) &amp;&amp; !aliases.isEmpty()) &#123; beanName = aliases.remove(0); if (logger.isDebugEnabled()) &#123; logger.debug(&quot;No XML &apos;id&apos; specified - using &apos;&quot; + beanName + &quot;&apos; as bean name and &quot; + aliases + &quot; as aliases&quot;); &#125; &#125; if (containingBean == null) &#123; checkNameUniqueness(beanName, aliases, ele); &#125; 这一段，主要是处理一些跟alias，id等标识相关的东西 1AbstractBeanDefinition beanDefinition = parseBeanDefinitionElement(ele, beanName, containingBean); 这一行是核心，进行实际的解析 1234567891011121314151617181920212223242526272829303132if (beanDefinition != null) &#123; if (!StringUtils.hasText(beanName)) &#123; try &#123; if (containingBean != null) &#123; beanName = BeanDefinitionReaderUtils.generateBeanName( beanDefinition, this.readerContext.getRegistry(), true); &#125; else &#123; beanName = this.readerContext.generateBeanName(beanDefinition); // Register an alias for the plain bean class name, if still possible, // if the generator returned the class name plus a suffix. // This is expected for Spring 1.2/2.0 backwards compatibility. String beanClassName = beanDefinition.getBeanClassName(); if (beanClassName != null &amp;&amp; beanName.startsWith(beanClassName) &amp;&amp; beanName.length() &gt; beanClassName.length() &amp;&amp; !this.readerContext.getRegistry().isBeanNameInUse(beanClassName)) &#123; aliases.add(beanClassName); &#125; &#125; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;Neither XML &apos;id&apos; nor &apos;name&apos; specified - &quot; + &quot;using generated bean name [&quot; + beanName + &quot;]&quot;); &#125; &#125; catch (Exception ex) &#123; error(ex.getMessage(), ele); return null; &#125; &#125; String[] aliasesArray = StringUtils.toStringArray(aliases); return new BeanDefinitionHolder(beanDefinition, beanName, aliasesArray); &#125; 这段是后置处理，对beanName进行处理前置处理和后置处理，不是核心，就不细看了，重点看下核心的那一行调用 123456789101112131415161718192021222324252627282930313233343536373839public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; try &#123; String parent = null; if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE); &#125; AbstractBeanDefinition bd = createBeanDefinition(className, parent); parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); parseMetaElements(ele, bd); parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); parseConstructorArgElements(ele, bd); parsePropertyElements(ele, bd); parseQualifierElements(ele, bd); bd.setResource(this.readerContext.getResource()); bd.setSource(extractSource(ele)); return bd; &#125; catch (ClassNotFoundException ex) &#123; error(&quot;Bean class [&quot; + className + &quot;] not found&quot;, ele, ex); &#125; catch (NoClassDefFoundError err) &#123; error(&quot;Class that bean class [&quot; + className + &quot;] depends on not found&quot;, ele, err); &#125; catch (Throwable ex) &#123; error(&quot;Unexpected failure during bean definition parsing&quot;, ele, ex); &#125; finally &#123; this.parseState.pop(); &#125; return null; &#125; 这个方法也挺长的，拆开看看 12345this.parseState.push(new BeanEntry(beanName)); String className = null; if (ele.hasAttribute(CLASS_ATTRIBUTE)) &#123; className = ele.getAttribute(CLASS_ATTRIBUTE).trim(); &#125; 这段是从配置中抽取出类名。接下来的长长一段，把异常处理先抛开，看看实际的业务 12345678910111213141516String parent = null;if (ele.hasAttribute(PARENT_ATTRIBUTE)) &#123; parent = ele.getAttribute(PARENT_ATTRIBUTE);&#125;AbstractBeanDefinition bd = createBeanDefinition(className, parent);parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT));parseMetaElements(ele, bd);parseLookupOverrideSubElements(ele, bd.getMethodOverrides());parseReplacedMethodSubElements(ele, bd.getMethodOverrides());parseConstructorArgElements(ele, bd);parsePropertyElements(ele, bd);parseQualifierElements(ele, bd);bd.setResource(this.readerContext.getResource());bd.setSource(extractSource(ele));return bd; 这里每个方法的命名，就说明了是要干什么，可以一个个跟进去看，本文就不细说了。总之，经过这里的解析，就得到了一个完整的BeanDefinitionHolder。只是说明一下，如果在配置文件里，没有对一些属性进行设置，比如autowire-candidate等，那么这个解析生成的BeanDefinition，都会得到一个默认值然后，对这个Bean做一些必要的装饰 12345678910111213141516171819public BeanDefinitionHolder decorateBeanDefinitionIfRequired( Element ele, BeanDefinitionHolder definitionHolder, BeanDefinition containingBd) &#123; BeanDefinitionHolder finalDefinition = definitionHolder; // Decorate based on custom attributes first. NamedNodeMap attributes = ele.getAttributes(); for (int i = 0; i &lt; attributes.getLength(); i++) &#123; Node node = attributes.item(i); finalDefinition = decorateIfRequired(node, finalDefinition, containingBd); &#125; // Decorate based on custom nested elements. NodeList children = ele.getChildNodes(); for (int i = 0; i &lt; children.getLength(); i++) &#123; Node node = children.item(i); if (node.getNodeType() == Node.ELEMENT_NODE) &#123; finalDefinition = decorateIfRequired(node, finalDefinition, containingBd); &#125; &#125; return finalDefinition; &#125; 持续单步调试，代码继续运行到DefaultBeanDefinitionDocumentReader中的processBeanDefinition中的registerBeanDefinition() 12BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); 单步进入代码发现BeanDefinitionReaderUtils静态方法registerBeanDefinition() 123456789101112131415public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; // Register bean definition under primary name. String beanName = definitionHolder.getBeanName(); // 其实调用的是DefaultListableBeanFactory中的registerBeanDefinition方法 registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String aliase : aliases) &#123; registry.registerAlias(beanName, aliase); &#125; &#125; &#125; 解释一下其实调用的是DefaultListableBeanFactory中的registerBeanDefinition方法这句话，因为DefaultListableBeanFactory实现BeanDefinitionRegistry接口，BeanDefinitionRegistry接口中定义了registerBeanDefinition()方法看下DefaultListableBeanFactory中registerBeanDefinition()实例方法的具体实现： 123456789101112131415161718192021222324252627282930313233343536public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; Assert.hasText(beanName, &quot;Bean name must not be empty&quot;); Assert.notNull(beanDefinition, &quot;BeanDefinition must not be null&quot;); if (beanDefinition instanceof AbstractBeanDefinition) &#123; try &#123; ((AbstractBeanDefinition) beanDefinition).validate(); &#125; catch (BeanDefinitionValidationException ex) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, &quot;Validation of bean definition failed&quot;, ex); &#125; &#125; synchronized (this.beanDefinitionMap) &#123; Object oldBeanDefinition = this.beanDefinitionMap.get(beanName); if (oldBeanDefinition != null) &#123; if (!this.allowBeanDefinitionOverriding) &#123; throw new BeanDefinitionStoreException(beanDefinition.getResourceDescription(), beanName, &quot;Cannot register bean definition [&quot; + beanDefinition + &quot;] for bean &apos;&quot; + beanName + &quot;&apos;: There is already [&quot; + oldBeanDefinition + &quot;] bound.&quot;); &#125; else &#123; if (this.logger.isInfoEnabled()) &#123; this.logger.info(&quot;Overriding bean definition for bean &apos;&quot; + beanName + &quot;&apos;: replacing [&quot; + oldBeanDefinition + &quot;] with [&quot; + beanDefinition + &quot;]&quot;); &#125; &#125; &#125; else &#123; this.beanDefinitionNames.add(beanName); this.frozenBeanDefinitionNames = null; &#125; this.beanDefinitionMap.put(beanName, beanDefinition); resetBeanDefinition(beanName); &#125; &#125; 代码追溯之后发现这个方法里，最关键的是以下2行： 12this.beanDefinitionNames.add(beanName);this.beanDefinitionMap.put(beanName, beanDefinition); 前者是把beanName放到队列里，后者是把BeanDefinition放到map中，到此注册就完成了。在后面实例化的时候，就是把beanDefinitionMap中的BeanDefinition取出来，逐一实例化BeanFactory准备完毕之后，代码又回到了ClassPathXmlApplicationContext里 1234567891011121314151617181920212223242526272829303132333435363738public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &apos;active&apos; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; &#125; &#125; 也就是obtainFreshBeanFactory()方法执行之后，再进行下面的步骤。总结来说，ApplicationContext将解析配置文件的工作委托给BeanDefinitionReader，然后BeanDefinitionReader将配置文件读取为xml的Document文档之后，又委托给BeanDefinitionDocumentReaderBeanDefinitionDocumentReader这个组件是根据xml元素的命名空间和元素名，起到一个路由的作用，实际的解析工作，是委托给BeanDefinitionParserDelegate来完成的。 BeanDefinitionParserDelegate的解析工作完成以后，会返回BeanDefinitionHolder给BeanDefinitionDocumentReader，在这里，会委托给DefaultListableBeanFactory完成bean的注册XmlBeanDefinitionReader（计数、解析XML文档），BeanDefinitionDocumentReader（依赖xml文档，进行解析和注册），BeanDefinitionParserDelegate（实际的解析工作）。 可以看出，在解析bean的过程中，这3个组件的分工是比较清晰的，各司其职，这种设计思想值得学习到此为止，bean的解析、注册、spring ioc 容器的实例化过程就基本分析结束了。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析1：Spring概述]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%901%EF%BC%9ASpring%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[原文出处： 张开涛 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 在讲源码之前，先让我们回顾一下一下Spring的基本概念，当然，在看源码之前你需要使用过spring或者spirngmvc。 Spring是什么Spring是一个开源的轻量级Java SE（Java 标准版本）/Java EE（Java 企业版本）开发应用框架，其目的是用于简化企业级应用程序开发。应用程序是由一组相互协作的对象组成。而在传统应用程序开发中，一个完整的应用是由一组相互协作的对象组成。所以开发一个应用除了要开发业务逻辑之外，最多的是关注如何使这些对象协作来完成所需功能，而且要低耦合、高内聚。 业务逻辑开发是不可避免的，那如果有个框架出来帮我们来创建对象及管理这些对象之间的依赖关系。可能有人说了，比如“抽象工厂、工厂方法设计模式”不也可以帮我们创建对象，“生成器模式”帮我们处理对象间的依赖关系，不也能完成这些功能吗？ 可是这些又需要我们创建另一些工厂类、生成器类，我们又要而外管理这些类，增加了我们的负担，如果能有种通过配置方式来创建对象，管理对象之间依赖关系，我们不需要通过工厂和生成器来创建及管理对象之间的依赖关系，这样我们是不是减少了许多工作，加速了开发，能节省出很多时间来干其他事。Spring框架刚出来时主要就是来完成这个功能。 Spring框架除了帮我们管理对象及其依赖关系，还提供像通用日志记录、性能统计、安全控制、异常处理等面向切面的能力，还能帮我管理最头疼的数据库事务，本身提供了一套简单的JDBC访问实现，提供与第三方数据访问框架集成（如Hibernate、JPA），与各种Java EE技术整合（如Java Mail、任务调度等等），提供一套自己的web层框架Spring MVC、而且还能非常简单的与第三方web框架集成。 从这里我们可以认为Spring是一个超级粘合平台，除了自己提供功能外，还提供粘合其他技术和框架的能力，从而使我们可以更自由的选择到底使用什么技术进行开发。而且不管是JAVA SE（C/S架构）应用程序还是JAVA EE（B/S架构）应用程序都可以使用这个平台进行开发。让我们来深入看一下Spring到底能帮我们做些什么？ Spring能帮我们做什么Spring除了不能帮我们写业务逻辑，其余的几乎什么都能帮助我们简化开发： 一、传统程序开发，创建对象及组装对象间依赖关系由我们在程序内部进行控制，这样会加大各个对象间的耦合，如果我们要修改对象间的依赖关系就必须修改源代码，重新编译、部署；而如果采用Spring，则由Spring根据配置文件来进行创建及组装对象间依赖关系，只需要改配置文件即可，无需重新编译。所以，Spring能帮我们根据配置文件创建及组装对象之间的依赖关系。 二、当我们要进行一些日志记录、权限控制、性能统计等时，在传统应用程序当中我们可能在需要的对象或方法中进行，而且比如权限控制、性能统计大部分是重复的，这样代码中就存在大量重复代码，即使有人说我把通用部分提取出来，那必然存在调用还是存在重复，像性能统计我们可能只是在必要时才进行，在诊断完毕后要删除这些代码；还有日志记录，比如记录一些方法访问日志、数据访问日志等等，这些都会渗透到各个要访问方法中； 还有权限控制，必须在方法执行开始进行审核，想想这些是多么可怕而且是多么无聊的工作。如果采用Spring，这些日志记录、权限控制、性能统计从业务逻辑中分离出来，通过Spring支持的面向切面编程，在需要这些功能的地方动态添加这些功能，无需渗透到各个需要的方法或对象中； 有人可能说了，我们可以使用“代理设计模式”或“包装器设计模式”，你可以使用这些，但还是需要通过编程方式来创建代理对象，还是要耦合这些代理对象，而采用Spring 面向切面编程能提供一种更好的方式来完成上述功能，一般通过配置方式，而且不需要在现有代码中添加任何额外代码，现有代码专注业务逻辑。 所以，Spring 面向切面编程能帮助我们无耦合的实现日志记录，性能统计，安全控制。 三、在传统应用程序当中，我们如何来完成数据库事务管理？需要一系列“获取连接，执行SQL，提交或回滚事务，关闭连接”，而且还要保证在最后一定要关闭连接，多么可怕的事情，而且也很无聊；如果采用Spring，我们只需获取连接，执行SQL，其他的都交给Spring来管理了，简单吧。所以，Spring能非常简单的帮我们管理数据库事务。 四、Spring还提供了与第三方数据访问框架（如Hibernate、JPA）无缝集成，而且自己也提供了一套JDBC访问模板，来方便数据库访问。 五、Spring还提供与第三方Web（如Struts、JSF）框架无缝集成，而且自己也提供了一套Spring MVC框架，来方便web层搭建。 六、Spring能方便的与Java EE（如Java Mail、任务调度）整合，与更多技术整合（比如缓存框架）。 Spring能帮我们做这么多事情，提供这么多功能和与那么多主流技术整合，而且是帮我们做了开发中比较头疼和困难的事情，那可能有人会问，难道只有Spring这一个框架，没有其他选择？当然有，比如EJB需要依赖应用服务器、开发效率低、在开发中小型项目是宰鸡拿牛刀，虽然发展到现在EJB比较好用了，但还是比较笨重还需要依赖应用服务器等。那为何需要使用Spring，而不是其他框架呢？让我们接着往下看。 为何需要Spring一 首先阐述几个概念 1、应用程序：是能完成我们所需要功能的成品，比如购物网站、OA系统。 2、框架：是能完成一定功能的半成品，比如我们可以使用框架进行购物网站开发；框架做一部分功能，我们自己做一部分功能，这样应用程序就创建出来了。而且框架规定了你在开发应用程序时的整体架构，提供了一些基础功能，还规定了类和对象的如何创建、如何协作等，从而简化我们开发，让我们专注于业务逻辑开发。 3、非侵入式设计：从框架角度可以这样理解，无需继承框架提供的类，这种设计就可以看作是非侵入式设计，如果继承了这些框架类，就是侵入设计，如果以后想更换框架之前写过的代码几乎无法重用，如果非侵入式设计则之前写过的代码仍然可以继续使用。 4、轻量级及重量级：轻量级是相对于重量级而言的，轻量级一般就是非入侵性的、所依赖的东西非常少、资源占用非常少、部署简单等等，其实就是比较容易使用，而重量级正好相反。 5、POJO：POJO（Plain Old Java Objects）简单的Java对象，它可以包含业务逻辑或持久化逻辑，但不担当任何特殊角色且不继承或不实现任何其它Java框架的类或接口。 6、容器：在日常生活中容器就是一种盛放东西的器具，从程序设计角度看就是装对象的的对象，因为存在放入、拿出等操作，所以容器还要管理对象的生命周期。 7、控制反转：即Inversion of Control，缩写为IoC，控制反转还有一个名字叫做依赖注入（Dependency Injection），就是由容器控制程序之间的关系，而非传统实现中，由程序代码直接操控。 8、Bean：一般指容器管理对象，在Spring中指Spring IoC容器管理对象。 为什么需要Spring及Spring的优点●非常轻量级的容器：以集中的、自动化的方式进行应用程序对象创建和装配，负责对象创建和装配，管理对象生命周期，能组合成复杂的应用程序。Spring容器是非侵入式的（不需要依赖任何Spring特定类），而且完全采用POJOs进行开发，使应用程序更容易测试、更容易管理。而且核心JAR包非常小，Spring3.0.5不到1M，而且不需要依赖任何应用服务器，可以部署在任何环境（Java SE或Java EE）。 ●AOP：AOP是Aspect Oriented Programming的缩写，意思是面向切面编程，提供从另一个角度来考虑程序结构以完善面向对象编程（相对于OOP），即可以通过在编译期间、装载期间或运行期间实现在不修改源代码的情况下给程序动态添加功能的一种技术。通俗点说就是把可重用的功能提取出来，然后将这些通用功能在合适的时候织入到应用程序中；比如安全，日记记录，这些都是通用的功能，我们可以把它们提取出来，然后在程序执行的合适地方织入这些代码并执行它们，从而完成需要的功能并复用了这些功能。 ● 简单的数据库事务管理：在使用数据库的应用程序当中，自己管理数据库事务是一项很让人头疼的事，而且很容易出现错误，Spring支持可插入的事务管理支持，而且无需JEE环境支持，通过Spring管理事务可以把我们从事务管理中解放出来来专注业务逻辑。 ●JDBC抽象及ORM框架支持：Spring使JDBC更加容易使用；提供DAO（数据访问对象）支持，非常方便集成第三方ORM框架，比如Hibernate等；并且完全支持Spring事务和使用Spring提供的一致的异常体系。 ●灵活的Web层支持：Spring本身提供一套非常强大的MVC框架，而且可以非常容易的与第三方MVC框架集成，比如Struts等。 ●简化各种技术集成：提供对Java Mail、任务调度、JMX、JMS、JNDI、EJB、动态语言、远程访问、Web Service等的集成。 Spring能帮助我们简化应用程序开发，帮助我们创建和组装对象，为我们管理事务，简单的MVC框架，可以把Spring看作是一个超级粘合平台，能把很多技术整合在一起，形成一个整体，使系统结构更优良、性能更出众，从而加速我们程序开发，有如上优点，我们没有理由不考虑使用它。 如何学好Spring要学好Spring，首先要明确Spring是个什么东西，能帮我们做些什么事情，知道了这些然后做个简单的例子，这样就基本知道怎么使用Spring了。 Spring核心是IoC容器，所以一定要透彻理解什么是IoC容器，以及如何配置及使用容器，其他所有技术都是基于容器实现的； 理解好IoC后，接下来是面向切面编程，首先还是明确概念，基本配置，最后是实现原理，接下来就是数据库事务管理，其实Spring管理事务是通过面向切面编程实现的，所以基础很重要，IoC容器和面向切面编程搞定后，其余都是基于这俩东西的实现，学起来就更加轻松了。要学好Spring不能急，一定要把基础打牢，基础牢固了，这就是磨刀不误砍柴工。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码剖析1：Spring概述]]></title>
    <url>%2F2019%2F09%2F14%2Fspring%2FSpring%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%902%EF%BC%9A%E5%88%9D%E6%8E%A2Spring%20IOC%E6%A0%B8%E5%BF%83%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文转载自互联网，侵删 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章将同步到我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《Spring和SpringMVC源码分析》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从spring基础入手，一步步地学习spring基础和springmvc的框架知识，并上手进行项目实战，spring框架是每一个Java工程师必须要学习和理解的知识点，进一步来说，你还需要掌握spring甚至是springmvc的源码以及实现原理，才能更完整地了解整个spring技术体系，形成自己的知识框架。 后续还会有springboot和springcloud的技术专题，陆续为大家带来，敬请期待。 为了更好地总结和检验你的学习成果，本系列文章也会提供部分知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言本文大致地介绍了IOC容器的初始化过程，只列出了比较重要的过程和代码，可以从中看出IOC容器执行的大致流程。 接下来的文章会更加深入剖析Bean容器如何解析xml，注册和初始化bean，以及如何获取bean实例等详细的过程。 转自：http://www.importnew.com/19243.html 1. 初始化 大致单步跟了下Spring IOC的初始化过程，整个脉络很庞大，初始化的过程主要就是读取XML资源，并解析，最终注册到Bean Factory中： 在完成初始化的过程后，Bean们就在BeanFactory中蓄势以待地等调用了。下面通过一个具体的例子，来详细地学习一下初始化过程，例如当加载下面一个bean： 12345678&lt;bean id=&quot;XiaoWang&quot; class=&quot;com.springstudy.talentshow.SuperInstrumentalist&quot;&gt; &lt;property name=&quot;instruments&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;piano&quot;/&gt; &lt;ref bean=&quot;saxophone&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 加载时需要读取、解析、注册bean，这个过程具体的调用栈如下所示： 下面对每一步的关键的代码进行详细分析： 准备保存配置位置，并刷新在调用ClassPathXmlApplicationContext后，先会将配置位置信息保存到configLocations，供后面解析使用，之后，会调用AbstractApplicationContext的refresh方法进行刷新： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); // 保存位置信息，比如`com/springstudy/talentshow/talent-show.xml` setConfigLocations(configLocations); if (refresh) &#123; // 刷新 refresh(); &#125;&#125;public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset &apos;active&apos; flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; &#125;&#125; 创建载入BeanFactory 1234567protected final void refreshBeanFactory() throws BeansException &#123; // ... ... DefaultListableBeanFactory beanFactory = createBeanFactory(); // ... ... loadBeanDefinitions(beanFactory); // ... ...&#125; 创建XMLBeanDefinitionReader 123456789protected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // ... ... // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader); 读取创建处理每一个resource 123456789101112131415161718public int loadBeanDefinitions(String location, Set&lt;Resource&gt; actualResources) throws BeanDefinitionStoreException &#123; // ... ... // 通过Location来读取Resource Resource[] resources = ((ResourcePatternResolver) resourceLoader).getResources(location); int loadCount = loadBeanDefinitions(resources); // ... ...&#125;public int loadBeanDefinitions(Resource... resources) throws BeanDefinitionStoreException &#123; Assert.notNull(resources, &quot;Resource array must not be null&quot;); int counter = 0; for (Resource resource : resources) &#123; // 载入每一个resource counter += loadBeanDefinitions(resource); &#125; return counter;&#125; 处理XML每个元素 123456789101112131415161718protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) &#123; // ... ... NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; if (delegate.isDefaultNamespace(ele)) &#123; // 处理每个xml中的元素，可能是import、alias、bean parseDefaultElement(ele, delegate); &#125; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; // ... ...&#125; 解析和注册bean 12345678910111213141516171819protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; // 解析 BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // 注册 // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition( bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error(&quot;Failed to register bean definition with name &apos;&quot; + bdHolder.getBeanName() + &quot;&apos;&quot;, ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 本步骤中，通过parseBeanDefinitionElement将XML的元素解析为BeanDefinition，然后存在BeanDefinitionHolder中，然后再利用BeanDefinitionHolder将BeanDefinition注册，实质就是把BeanDefinition的实例put进BeanFactory中，和后面将详细的介绍解析和注册过程。 解析 处理每个Bean的元素 1234567891011121314151617181920public AbstractBeanDefinition parseBeanDefinitionElement( Element ele, String beanName, BeanDefinition containingBean) &#123; // ... ... // 创建beandefinition AbstractBeanDefinition bd = createBeanDefinition(className, parent); parseBeanDefinitionAttributes(ele, beanName, containingBean, bd); bd.setDescription(DomUtils.getChildElementValueByTagName(ele, DESCRIPTION_ELEMENT)); parseMetaElements(ele, bd); parseLookupOverrideSubElements(ele, bd.getMethodOverrides()); parseReplacedMethodSubElements(ele, bd.getMethodOverrides()); // 处理“Constructor” parseConstructorArgElements(ele, bd); // 处理“Preperty” parsePropertyElements(ele, bd); parseQualifierElements(ele, bd); // ... ...&#125; 处理属性的值 12345678910111213141516171819202122232425262728public Object parsePropertyValue(Element ele, BeanDefinition bd, String propertyName) &#123; String elementName = (propertyName != null) ? &quot;&lt;property&gt; element for property &apos;&quot; + propertyName + &quot;&apos;&quot; : &quot;&lt;constructor-arg&gt; element&quot;; // ... ... if (hasRefAttribute) &#123; // 处理引用 String refName = ele.getAttribute(REF_ATTRIBUTE); if (!StringUtils.hasText(refName)) &#123; error(elementName + &quot; contains empty &apos;ref&apos; attribute&quot;, ele); &#125; RuntimeBeanReference ref = new RuntimeBeanReference(refName); ref.setSource(extractSource(ele)); return ref; &#125; else if (hasValueAttribute) &#123; // 处理值 TypedStringValue valueHolder = new TypedStringValue(ele.getAttribute(VALUE_ATTRIBUTE)); valueHolder.setSource(extractSource(ele)); return valueHolder; &#125; else if (subElement != null) &#123; // 处理子类型（比如list、map等） return parsePropertySubElement(subElement, bd); &#125; // ... ...&#125; 1.4 注册 123456789101112131415161718192021222324252627public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; // Register bean definition under primary name. String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; registry.registerAlias(beanName, alias); &#125; &#125;&#125;public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; // ...... // 将beanDefinition注册 this.beanDefinitionMap.put(beanName, beanDefinition); // ......&#125; 注册过程中，最核心的一句就是：this.beanDefinitionMap.put(beanName, beanDefinition)，也就是说注册的实质就是以beanName为key，以beanDefinition为value，将其put到HashMap中。 注册1234567891011121314151617181920212223242526 public static void registerBeanDefinition( BeanDefinitionHolder definitionHolder, BeanDefinitionRegistry registry) throws BeanDefinitionStoreException &#123; // Register bean definition under primary name. String beanName = definitionHolder.getBeanName(); registry.registerBeanDefinition(beanName, definitionHolder.getBeanDefinition()); // Register aliases for bean name, if any. String[] aliases = definitionHolder.getAliases(); if (aliases != null) &#123; for (String alias : aliases) &#123; registry.registerAlias(beanName, alias); &#125; &#125;&#125;public void registerBeanDefinition(String beanName, BeanDefinition beanDefinition) throws BeanDefinitionStoreException &#123; // ...... // 将beanDefinition注册 this.beanDefinitionMap.put(beanName, beanDefinition); // ...... 理解了以上两个过程，我们就可以自己实现一个简单的Spring框架了。于是，我根据自己的理解实现了一个简单的IOC框架Simple Spring，有兴趣可以看看。 ​ 注册过程中，最核心的一句就是：this.beanDefinitionMap.put(beanName, beanDefinition)，也就是说注册的实质就是以beanName为key，以beanDefinition为value，将其put到HashMap中。 注入依赖当完成初始化IOC容器后，如果bean没有设置lazy-init(延迟加载)属性，那么bean的实例就会在初始化IOC完成之后，及时地进行初始化。初始化时会先建立实例，然后根据配置利用反射对实例进行进一步操作，具体流程如下所示： 创建bean的实例创建bean的实例过程函数调用栈如下所示： 注入bean的属性注入bean的属性过程函数调用栈如下所示： 在创建bean和注入bean的属性时，都是在doCreateBean函数中进行的，我们重点看下： 1234567891011121314151617181920212223242526protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; // Instantiate the bean. BeanWrapper instanceWrapper = null; if (mbd.isSingleton()) &#123; instanceWrapper = this.factoryBeanInstanceCache.remove(beanName); &#125; if (instanceWrapper == null) &#123; // 创建bean的实例 instanceWrapper = createBeanInstance(beanName, mbd, args); &#125; // ... ... // Initialize the bean instance. Object exposedObject = bean; try &#123; // 初始化bean的实例，如注入属性 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; // ... ... &#125; 理解了以上两个过程，我们就可以自己实现一个简单的Spring框架了。于是，我根据自己的理解实现了一个简单的IOC框架Simple Spring，有兴趣可以看看。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现15：Redis分布式锁进化史]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B015%EF%BC%9ARedis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E8%BF%9B%E5%8C%96%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Redis分布式锁进化史近两年来微服务变得越来越热门，越来越多的应用部署在分布式环境中，在分布式环境中，数据一致性是一直以来需要关注并且去解决的问题，分布式锁也就成为了一种广泛使用的技术，常用的分布式实现方式为Redis，Zookeeper，其中基于Redis的分布式锁的使用更加广泛。 但是在工作和网络上看到过各个版本的Redis分布式锁实现，每种实现都有一些不严谨的地方，甚至有可能是错误的实现，包括在代码中，如果不能正确的使用分布式锁，可能造成严重的生产环境故障，本文主要对目前遇到的各种分布式锁以及其缺陷做了一个整理，并对如何选择合适的Redis分布式锁给出建议。 各个版本的Redis分布式锁 V1.0 1234567tryLock()&#123; SETNX Key 1 EXPIRE Key Seconds&#125;release()&#123; DELETE Key&#125; 这个版本应该是最简单的版本，也是出现频率很高的一个版本，首先给锁加一个过期时间操作是为了避免应用在服务重启或者异常导致锁无法释放后，不会出现锁一直无法被释放的情况。 这个方案的一个问题在于每次提交一个Redis请求，如果执行完第一条命令后应用异常或者重启，锁将无法过期，一种改善方案就是使用Lua脚本（包含SETNX和EXPIRE两条命令），但是如果Redis仅执行了一条命令后crash或者发生主从切换，依然会出现锁没有过期时间，最终导致无法释放。 另外一个问题在于，很多同学在释放分布式锁的过程中，无论锁是否获取成功，都在finally中释放锁，这样是一个锁的错误使用，这个问题将在后续的V3.0版本中解决。 针对锁无法释放问题的一个解决方案基于GETSET命令来实现 V1.1 基于GETSET 123456789101112131415161718tryLock()&#123; NewExpireTime=CurrentTimestamp+ExpireSeconds if(SETNX Key NewExpireTime Seconds)&#123; oldExpireTime = GET(Key) if( oldExpireTime &lt; CurrentTimestamp)&#123; NewExpireTime=CurrentTimestamp+ExpireSeconds CurrentExpireTime=GETSET(Key,NewExpireTime) if(CurrentExpireTime == oldExpireTime)&#123; return 1; &#125;else&#123; return 0; &#125; &#125; &#125;&#125;release()&#123; DELETE key &#125; 思路： SETNX(Key,ExpireTime)获取锁 如果获取锁失败，通过GET(Key)返回的时间戳检查锁是否已经过期 GETSET(Key,ExpireTime)修改Value为NewExpireTime 检查GETSET返回的旧值，如果等于GET返回的值，则认为获取锁成功 注意：这个版本去掉了EXPIRE命令，改为通过Value时间戳值来判断过期 问题： 1231. 在锁竞争较高的情况下，会出现Value不断被覆盖，但是没有一个Client获取到锁 2. 在获取锁的过程中不断的修改原有锁的数据，设想一种场景C1，C2竞争锁，C1获取到了锁，C2锁执行了GETSET操作修改了C1锁的过期时间，如果C1没有正确释放锁，锁的过期时间被延长，其它Client需要等待更久的时间 V2.0 基于SETNX 1tryLock()&#123; SETNX Key 1 Seconds&#125;release()&#123; DELETE Key&#125; Redis 2.6.12版本后SETNX增加过期时间参数，这样就解决了两条命令无法保证原子性的问题。但是设想下面一个场景： 1. C1成功获取到了锁，之后C1因为GC进入等待或者未知原因导致任务执行过长，最后在锁失效前C1没有主动释放锁 2.C2在C1的锁超时后获取到锁，并且开始执行，这个时候C1和C2都同时在执行，会因重复执行造成数据不一致等未知情况 3. C1如果先执行完毕，则会释放C2的锁，此时可能导致另外一个C3进程获取到了锁 大致的流程图 存在问题： 1231\. 由于C1的停顿导致C1 和C2同都获得了锁并且同时在执行，在业务实现间接要求必须保证幂等性2\. C1释放了不属于C1的锁 V3.0 12345678910111213tryLock()&#123; SETNX Key UnixTimestamp Seconds&#125;release()&#123; EVAL( //LuaScript if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1]) else return 0 end )&#125; 这个方案通过指定Value为时间戳，并在释放锁的时候检查锁的Value是否为获取锁的Value，避免了V2.0版本中提到的C1释放了C2持有的锁的问题；另外在释放锁的时候因为涉及到多个Redis操作，并且考虑到Check And Set 模型的并发问题，所以使用Lua脚本来避免并发问题。 存在问题： 如果在并发极高的场景下，比如抢红包场景，可能存在UnixTimestamp重复问题，另外由于不能保证分布式环境下的物理时钟一致性，也可能存在UnixTimestamp重复问题，只不过极少情况下会遇到。 V3.1 12345678910111213tryLock()&#123; SET Key UniqId Seconds&#125;release()&#123; EVAL( //LuaScript if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1]) else return 0 end )&#125; Redis 2.6.12后SET同样提供了一个NX参数，等同于SETNX命令，官方文档上提醒后面的版本有可能去掉SETNX, SETEX, PSETEX,并用SET命令代替，另外一个优化是使用一个自增的唯一UniqId代替时间戳来规避V3.0提到的时钟问题。 这个方案是目前最优的分布式锁方案，但是如果在Redis集群环境下依然存在问题： 由于Redis集群数据同步为异步，假设在Master节点获取到锁后未完成数据同步情况下Master节点crash，此时在新的Master节点依然可以获取锁，所以多个Client同时获取到了锁 分布式Redis锁：RedlockV3.1的版本仅在单实例的场景下是安全的，针对如何实现分布式Redis的锁，国外的分布式专家有过激烈的讨论， antirez提出了分布式锁算法Redlock，在distlock话题下可以看到对Redlock的详细说明，下面是Redlock算法的一个中文说明（引用） 假设有N个独立的Redis节点 获取当前时间（毫秒数）。 按顺序依次向N个Redis节点执行获取锁的操作。这个获取操作跟前面基于单Redis节点的获取锁的过程相同，包含随机字符串_my_random_value_，也包含过期时间(比如_PX 30000_，即锁的有效时间)。为了保证在某个Redis节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间(time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个Redis节点获取锁失败以后，应该立即尝试下一个Redis节点。这里的失败，应该包含任何类型的失败，比如该Redis节点不可用，或者该Redis节点上的锁已经被其它客户端持有（注：Redlock原文中这里只提到了Redis节点不可用的情况，但也应该包含其它的失败情况）。 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。如果客户端从大多数Redis节点（&gt;= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间(lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间。 如果最终获取锁失败了（可能由于获取到锁的Redis节点个数少于N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有Redis节点发起释放锁的操作（即前面介绍的Redis Lua脚本）。 释放锁：对所有的Redis节点发起释放锁操作 然而Martin Kleppmann针对这个算法提出了质疑，提出应该基于fencing token机制（每次对资源进行操作都需要进行token验证） 11. Redlock在系统模型上尤其是在分布式时钟一致性问题上提出了假设，实际场景下存在时钟不一致和时钟跳跃问题，而Redlock恰恰是基于timing的分布式锁 2. 另外Redlock由于是基于自动过期机制，依然没有解决长时间的gc pause等问题带来的锁自动失效，从而带来的安全性问题。 接着antirez又回复了Martin Kleppmann的质疑，给出了过期机制的合理性，以及实际场景中如果出现停顿问题导致多个Client同时访问资源的情况下如何处理。 针对Redlock的问题，基于Redis的分布式锁到底安全吗给出了详细的中文说明，并对Redlock算法存在的问题提出了分析。 总结不论是基于SETNX版本的Redis单实例分布式锁，还是Redlock分布式锁，都是为了保证下特性 11\. 安全性：在同一时间不允许多个Client同时持有锁 2\. 活性 死锁：锁最终应该能够被释放，即使Client端crash或者出现网络分区（通常基于超时机制） 容错性：只要超过半数Redis节点可用，锁都能被正确获取和释放 所以在开发或者使用分布式锁的过程中要保证安全性和活性，避免出现不可预测的结果。 另外每个版本的分布式锁都存在一些问题，在锁的使用上要针对锁的实用场景选择合适的锁，通常情况下锁的使用场景包括： Efficiency(效率)：只需要一个Client来完成操作，不需要重复执行，这是一个对宽松的分布式锁，只需要保证锁的活性即可； Correctness(正确性)：多个Client保证严格的互斥性，不允许出现同时持有锁或者对同时操作同一资源，这种场景下需要在锁的选择和使用上更加严格，同时在业务代码上尽量做到幂等 在Redis分布式锁的实现上还有很多问题等待解决，我们需要认识到这些问题并清楚如何正确实现一个Redis 分布式锁，然后在工作中合理的选择和正确的使用分布式锁。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现14：Redis事务浅析与ACID特性介绍]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B014%EF%BC%9ARedis%E4%BA%8B%E5%8A%A1%E6%B5%85%E6%9E%90%E4%B8%8EACID%E7%89%B9%E6%80%A7%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 事务MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用redis-check-aof程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作，具体信息请参考文档的后半部分。 用法MULTI 命令用于开启一个事务，它总是返回 OK 。 MULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC命令被调用时， 所有队列中的命令才会被执行。 另一方面， 通过调用 DISCARD ， 客户端可以清空事务队列， 并放弃执行事务。 以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值： 1&gt; MULTIOK&gt; INCR fooQUEUED&gt; INCR barQUEUED&gt; EXEC1) (integer) 12) (integer) 1 EXEC 命令的回复是一个数组， 数组中的每个元素都是执行事务中的命令所产生的回复。 其中， 回复元素的先后顺序和命令发送的先后顺序一致。 当客户端处于事务状态时， 所有传入的命令都会返回一个内容为 QUEUED 的状态回复（status reply）， 这些被入队的命令将在 EXEC 命令被调用时执行。 事务中的错误使用事务时可能会遇上以下两种错误： 事务在执行 EXEC 之前，入队的命令可能会出错。比如说，命令可能会产生语法错误（参数数量错误，参数名错误，等等），或者其他更严重的错误，比如内存不足（如果服务器使用 maxmemory 设置了最大内存限制的话）。 命令可能在 EXEC 调用之后失败。举个例子，事务中的命令可能处理了错误类型的键，比如将列表命令用在了字符串键上面，诸如此类。 对于发生在 EXEC 执行之前的错误，客户端以前的做法是检查命令入队所得的返回值：如果命令入队时返回 QUEUED ，那么入队成功；否则，就是入队失败。如果有命令在入队时失败，那么大部分客户端都会停止并取消这个事务。 不过，从 Redis 2.6.5 开始，服务器会对命令入队失败的情况进行记录，并在客户端调用 EXEC 命令时，拒绝执行并自动放弃这个事务。 在 Redis 2.6.5 以前， Redis 只执行事务中那些入队成功的命令，而忽略那些入队失败的命令。 而新的处理方式则使得在流水线（pipeline）中包含事务变得简单，因为发送事务和读取事务的回复都只需要和服务器进行一次通讯。 至于那些在 EXEC 命令执行之后所产生的错误， 并没有对它们进行特别处理： 即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。 从协议的角度来看这个问题，会更容易理解一些。 以下例子中， LPOP 命令的执行将出错， 尽管调用它的语法是正确的： 1Trying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.MULTI+OKSET a 3abc+QUEUEDLPOP a+QUEUEDEXEC*2+OK-ERR Operation against a key holding the wrong kind of value EXEC 返回两条bulk-string-reply： 第一条是 OK ，而第二条是 -ERR 。 至于怎样用合适的方法来表示事务中的错误， 则是由客户端自己决定的。 最重要的是记住这样一条， 即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行 —— Redis 不会停止执行事务中的命令。 以下例子展示的是另一种情况， 当命令在入队时产生错误， 错误会立即被返回给客户端： 1MULTI+OKINCR a b c-ERR wrong number of arguments for &apos;incr&apos; command 因为调用 INCR 命令的参数格式不正确， 所以这个 INCR 命令入队失败。 为什么 Redis 不支持回滚（roll back）如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR， 回滚是没有办法处理这些情况的。 放弃事务当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出： 1&gt; SET foo 1OK&gt; MULTIOK&gt; INCR fooQUEUED&gt; DISCARDOK&gt; GET foo&quot;1&quot; 使用 check-and-set 操作实现乐观锁WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回nil-reply来表示事务已经失败。 举个例子， 假设我们需要原子性地为某个值进行增 1 操作（假设 INCR 不存在）。 首先我们可能会这样做： 1val = GET mykeyval = val + 1SET mykey $val 上面的这个实现在只有一个客户端的时候可以执行得很好。 但是， 当多个客户端同时对同一个键进行这样的操作时， 就会产生竞争条件。举个例子， 如果客户端 A 和 B 都读取了键原来的值， 比如 10 ， 那么两个客户端都会将键的值设为 11 ， 但正确的结果应该是 12 才对。 有了 WATCH ， 我们就可以轻松地解决这类问题了： 1WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。 这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。 了解 WATCHWATCH 使得 EXEC 命令需要有条件地执行： 事务只能在所有被监视键都没有被修改的前提下执行， 如果这个前提不能满足的话，事务就不会被执行。 了解更多-&gt; WATCH 命令可以被调用多次。 对键的监视从 WATCH 执行之后开始生效， 直到调用 EXEC 为止。 用户还可以在单个 WATCH 命令中监视任意多个键， 就像这样： 1redis&gt; WATCH key1 key2 key3OK 当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。 另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。 使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。 使用 WATCH 实现 ZPOPWATCH 可以用于创建 Redis 没有内置的原子操作。举个例子， 以下代码实现了原创的 ZPOP 命令， 它可以原子地弹出有序集合中分值（score）最小的元素： 1WATCH zsetelement = ZRANGE zset 0 0MULTIZREM zset elementEXEC 程序只要重复执行这段代码， 直到 EXEC 的返回值不是nil-reply回复即可。 Redis 脚本和事务从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。 因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。 不过我们并不打算在短时间内就移除事务功能， 因为事务提供了一种即使不使用脚本， 也可以避免竞争条件的方法， 而且事务本身的实现并不复杂。 不过在不远的将来， 可能所有用户都会只使用脚本来实现事务也说不定。 如果真的发生这种情况的话， 那么我们将废弃并最终移除事务功能。 redis事务的ACID特性1在传统的关系型数据库中,尝尝用ACID特质来检测事务功能的可靠性和安全性。 在redis中事务总是具有原子性(Atomicity),一致性(Consistency)和隔离性(Isolation),并且当redis运行在某种特定的持久化模式下,事务也具有耐久性(Durability). ①原子性 12&gt; 事务具有原子性指的是,数据库将事务中的多个操作当作一个整体来执行,服务器要么就执行事务中的所有操作,要么就一个操作也不执行。&gt; 但是对于redis的事务功能来说,事务队列中的命令要么就全部执行,要么就一个都不执行,因此redis的事务是具有原子性的。我们通常会知道 两种关于redis事务原子性的说法,一种是要么事务都执行,要么都不执行。另外一种说法是redis事务当事务中的命令执行失败后面的命令还会执行,错误之前的命令不会回滚。其实这个两个说法都是正确的。但是缺一不可。我们接下来具体分析下 1我们先看一个可以正确执行的事务例子 1redis &gt; MULTIOK redis &gt; SET username &quot;bugall&quot;QUEUED redis &gt; EXEC1) OK2) &quot;bugall&quot; 1与之相反,我们再来看一个事务执行失败的例子。这个事务因为命令在放入事务队列的时候被服务器拒绝,所以事务中的所有命令都不会执行,因为前面我们有介绍到,redis的事务命令是统一先放到事务队列里,在用户输入EXEC命令的时候再统一执行。但是我们错误的使用&quot;GET&quot;命令,在命令放入事务队列的时候被检测到事务,这时候还没有接收到EXEC命令,所以这个时候不牵扯到回滚的问题,在EXEC的时候发现事务队列里有命令存在错误,所以事务里的命令就全都不执行,这样就达到里事务的原子性,我们看下例子。 1redis &gt; MULTIOK redis &gt; GET(error) ERR wrong number of arguments for &apos;get&apos; command redis &gt; GET usernameQUEUED redis &gt; EXEC(error) EXECABORT Transaction discarded because of previous errors 1redis的事务和传统的关系型数据库事务的最大区别在于,redis不支持事务的回滚机制,即使事务队列中的某个命令在执行期间出现错误,整个事务也会继续执行下去,直到将事务队列中的所有命令都执行完毕为止,我们看下面的例子 1redis &gt; SET username &quot;bugall&quot;OK redis &gt; MULTIOK redis &gt; SADD member &quot;bugall&quot; &quot;litengfe&quot; &quot;yangyifang&quot;QUEUED redis &gt; RPUSH username &quot;b&quot; &quot;l&quot; &quot;y&quot; //错误对键username使用列表键命令QUEUED redis &gt; SADD password &quot;123456&quot; &quot;123456&quot; &quot;123456&quot;QUEUED redis &gt; EXEC1) (integer) 32) (error) WRONGTYPE Operation against a key holding the wrong kind of value3) (integer) 3 1redis的作者在十五功能的文档中解释说,不支持事务回滚是因为这种复杂的功能和redis追求的简单高效的设计主旨不符合,并且他认为,redis事务的执行时错误通常都是编程错误造成的,这种错误通常只会出现在开发环境中,而很少会在实际的生产环境中出现,所以他认为没有必要为redis开发事务回滚功能。所以我们在讨论redis事务回滚的时候,一定要区分命令发生错误的时候。 ②一致性 12&gt; 事务具有一致性指的是,如果数据库在执行事务之前是一致的,那么在事务执行之后,无论事务是否执行成功,数据库也应该仍然一致的。 ”一致“指的是数据符合数据库本身的定义和要求,没有包含非法或者无效的错误数据。redis通过谨慎的错误检测和简单的设计来保证事务一致性。&gt; ③隔离性 12&gt; 事务的隔离性指的是,即使数据库中有多个事务并发在执行,各个事务之间也不会互相影响,并且在并发状态下执行的事务和串行执行的事务产生的结果完全 相同。 因为redis使用单线程的方式来执行事务(以及事务队列中的命令),并且服务器保证,在执行事务期间不会对事物进行中断,因此,redis的事务总是以串行 的方式运行的,并且事务也总是具有隔离性的&gt; ④持久性 12&gt; 事务的耐久性指的是,当一个事务执行完毕时,执行这个事务所得的结果已经被保持到永久存储介质里面。 因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能,所以redis事务的耐久性由redis使用的模式 决定 - 当服务器在无持久化的内存模式下运行时,事务不具有耐久性,一旦服务器停机,包括事务数据在内的所有服务器数据都将丢失 - 当服务器在RDB持久化模式下运作的时候,服务器只会在特定的保存条件满足的时候才会执行BGSAVE命令,对数据库进行保存操作,并且异步执行的BGSAVE不 能保证事务数据被第一时间保存到硬盘里面,因此RDB持久化模式下的事务也不具有耐久性 - 当服务器运行在AOF持久化模式下,并且appedfsync的选项的值为always时,程序总会在执行命令之后调用同步函数,将命令数据真正的保存到硬盘里面,因此 这种配置下的事务是具有耐久性的。 - 当服务器运行在AOF持久化模式下,并且appedfsync的选项的值为everysec时,程序会每秒同步一次&gt; 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现13：Redis集群机制及一个Redis架构演进实例]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B013%EF%BC%9ARedis%E9%9B%86%E7%BE%A4%E6%9C%BA%E5%88%B6%E5%8F%8A%E4%B8%80%E4%B8%AARedis%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 转自http://blog.720ui.com/2016/redis_action_04_cluster/#Replication%EF%BC%88%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%EF%BC%89 下面介绍Redis的集群方案。 Replication（主从复制）Redis的replication机制允许slave从master那里通过网络传输拷贝到完整的数据备份，从而达到主从机制。为了实现主从复制，我们准备三个redis服务，依次命名为master，slave1，slave2。 配置主服务器为了测试效果，我们先修改主服务器的配置文件redis.conf的端口信息 1. port 6300 配置从服务器replication相关的配置比较简单，只需要把下面一行加到slave的配置文件中。你只需要把ip地址和端口号改一下。 1. slaveof 192.168.1.1 6379 我们先修改从服务器1的配置文件redis.conf的端口信息和从服务器配置。 1. port 6301 2. slaveof 127.0.0.1 6300 我们再修改从服务器2的配置文件redis.conf的端口信息和从服务器配置。 1. port 6302 2. slaveof 127.0.0.1 6300 值得注意的是，从redis2.6版本开始，slave支持只读模式，而且是默认的。可以通过配置项slave-read-only来进行配置。此外，如果master通过requirepass配置项设置了密码，slave每次同步操作都需要验证密码，可以通过在slave的配置文件中添加以下配置项 1. masterauth 测试分别启动主服务器，从服务器，我们来验证下主从复制。我们在主服务器写入一条消息，然后再其他从服务器查看是否成功复制了。 Sentinel（哨兵）主从机制，上面的方案中主服务器可能存在单点故障，万一主服务器宕机，这是个麻烦事情，所以Redis提供了Redis-Sentinel，以此来实现主从切换的功能，类似与zookeeper。 Redis-Sentinel是Redis官方推荐的高可用性(HA)解决方案，当用Redis做master-slave的高可用方案时，假如master宕机了，Redis本身(包括它的很多客户端)都没有实现自动进行主备切换，而Redis-Sentinel本身也是一个独立运行的进程，它能监控多个master-slave集群，发现master宕机后能进行自动切换。 它的主要功能有以下几点 监控（Monitoring）：不断地检查redis的主服务器和从服务器是否运作正常。 提醒（Notification）：如果发现某个redis服务器运行出现状况，可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）：能够进行自动切换。当一个主服务器不能正常工作时，会将失效主服务器的其中一个从服务器升级为新的主服务器，并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 兼容 Redis 2.4.16 或以上版本， 推荐使用 Redis 2.8.0 或以上的版本。 配置Sentinel必须指定一个sentinel的配置文件sentinel.conf，如果不指定将无法启动sentinel。首先，我们先创建一个配置文件sentinel.conf 1. port 26379 2. sentinel monitor mymaster 127.0.0.1 6300 2 官方典型的配置如下 1. sentinel monitor mymaster 127.0.0.1 6379 2 2. sentinel down-after-milliseconds mymaster 60000 3. sentinel failover-timeout mymaster 180000 4. sentinel parallel-syncs mymaster 1 5. 6. sentinel monitor resque 192.168.1.3 6380 4 7. sentinel down-after-milliseconds resque 10000 8. sentinel failover-timeout resque 180000 9. sentinel parallel-syncs resque 5 配置文件只需要配置master的信息就好啦，不用配置slave的信息，因为slave能够被自动检测到(master节点会有关于slave的消息)。 需要注意的是，配置文件在sentinel运行期间是会被动态修改的，例如当发生主备切换时候，配置文件中的master会被修改为另外一个slave。这样，之后sentinel如果重启时，就可以根据这个配置来恢复其之前所监控的redis集群的状态。 接下来我们将一行一行地解释上面的配置项： 1. sentinel monitor mymaster 127.0.0.1 6379 2 这行配置指示 Sentinel 去监视一个名为 mymaster 的主服务器， 这个主服务器的 IP 地址为 127.0.0.1 ， 端口号为 6300， 而将这个主服务器判断为失效至少需要 2 个 Sentinel 同意，只要同意 Sentinel 的数量不达标，自动故障迁移就不会执行。 不过要注意， 无论你设置要多少个 Sentinel 同意才能判断一个服务器失效， 一个 Sentinel 都需要获得系统中多数（majority） Sentinel 的支持， 才能发起一次自动故障迁移， 并预留一个给定的配置纪元 （configuration Epoch ，一个配置纪元就是一个新主服务器配置的版本号）。换句话说， 在只有少数（minority） Sentinel 进程正常运作的情况下， Sentinel 是不能执行自动故障迁移的。sentinel集群中各个sentinel也有互相通信，通过gossip协议。 除了第一行配置，我们发现剩下的配置都有一个统一的格式: 1. sentinel 接下来我们根据上面格式中的option_name一个一个来解释这些配置项： down-after-milliseconds 选项指定了 Sentinel 认为服务器已经断线所需的毫秒数。 parallel-syncs 选项指定了在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长。 启动 Sentinel对于 redis-sentinel 程序， 你可以用以下命令来启动 Sentinel 系统 1. redis-sentinel sentinel.conf 对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器 1. redis-server sentinel.conf --sentinel 以上两种方式，都必须指定一个sentinel的配置文件sentinel.conf， 如果不指定将无法启动sentinel。sentinel默认监听26379端口，所以运行前必须确定该端口没有被别的进程占用。 测试此时，我们开启两个Sentinel，关闭主服务器，我们来验证下Sentinel。发现，服务器发生切换了。当6300端口的这个服务重启的时候，他会变成6301端口服务的slave。 TwemproxyTwemproxy是由Twitter开源的Redis代理， Redis客户端把请求发送到Twemproxy，Twemproxy根据路由规则发送到正确的Redis实例，最后Twemproxy把结果汇集返回给客户端。 Twemproxy通过引入一个代理层，将多个Redis实例进行统一管理，使Redis客户端只需要在Twemproxy上进行操作，而不需要关心后面有多少个Redis实例，从而实现了Redis集群。Twemproxy本身也是单点，需要用Keepalived做高可用方案。 这么些年来，Twenproxy作为应用范围最广、稳定性最高、最久经考验的分布式中间件，在业界广泛使用。 但是，Twemproxy存在诸多不方便之处，最主要的是，Twemproxy无法平滑地增加Redis实例，业务量突增，需增加Redis服务器；业务量萎缩，需要减少Redis服务器。但对Twemproxy而言，基本上都很难操作。其次，没有友好的监控管理后台界面，不利于运维监控。 CodisCodis解决了Twemproxy的这两大痛点，由豌豆荚于2014年11月开源，基于Go和C开发、现已广泛用于豌豆荚的各种Redis业务场景。 Codis 3.x 由以下组件组成： Codis Server：基于 redis-2.8.21 分支开发。增加了额外的数据结构，以支持 slot 有关的操作以及数据迁移指令。具体的修改可以参考文档 redis 的修改。 Codis Proxy：客户端连接的 Redis 代理服务, 实现了 Redis 协议。 除部分命令不支持以外(不支持的命令列表)，表现的和原生的 Redis 没有区别（就像 Twemproxy）。对于同一个业务集群而言，可以同时部署多个 codis-proxy 实例；不同 codis-proxy 之间由 codis-dashboard 保证状态同步。 Codis Dashboard：集群管理工具，支持 codis-proxy、codis-server 的添加、删除，以及据迁移等操作。在集群状态发生改变时，codis-dashboard 维护集群下所有 codis-proxy 的状态的一致性。对于同一个业务集群而言，同一个时刻 codis-dashboard 只能有 0个或者1个；所有对集群的修改都必须通过 codis-dashboard 完成。 Codis Admin：集群管理的命令行工具。可用于控制 codis-proxy、codis-dashboard 状态以及访问外部存储。 Codis FE：集群管理界面。多个集群实例共享可以共享同一个前端展示页面；通过配置文件管理后端 codis-dashboard 列表，配置文件可自动更新。 Codis HA：为集群提供高可用。依赖 codis-dashboard 实例，自动抓取集群各个组件的状态；会根据当前集群状态自动生成主从切换策略，并在需要时通过 codis-dashboard 完成主从切换。 Storage：为集群状态提供外部存储。提供 Namespace 概念，不同集群的会按照不同 product name 进行组织；目前仅提供了 Zookeeper 和 Etcd 两种实现，但是提供了抽象的 interface 可自行扩展。 Codis引入了Group的概念，每个Group包括1个Redis Master及一个或多个Redis Slave，这是和Twemproxy的区别之一，实现了Redis集群的高可用。当1个Redis Master挂掉时，Codis不会自动把一个Slave提升为Master，这涉及数据的一致性问题，Redis本身的数据同步是采用主从异步复制，当数据在Maste写入成功时，Slave是否已读入这个数据是没法保证的，需要管理员在管理界面上手动把Slave提升为Master。 Codis使用，可以参考官方文档https://github.com/CodisLabs/codis/blob/release3.0/doc/tutorial_zh.md Redis 3.0集群Redis 3.0集群采用了P2P的模式，完全去中心化。支持多节点数据集自动分片，提供一定程度的分区可用性，部分节点挂掉或者无法连接其他节点后，服务可以正常运行。Redis 3.0集群采用Hash Slot方案，而不是一致性哈希。Redis把所有的Key分成了16384个slot，每个Redis实例负责其中一部分slot。集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新。 Redis客户端在任意一个Redis实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。 Redis 3.0集群，目前支持的cluster特性 节点自动发现 slave-&gt;master 选举,集群容错 Hot resharding:在线分片 集群管理:cluster xxx 基于配置(nodes-port.conf)的集群管理 ASK 转向/MOVED 转向机制 如上图所示，所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。节点的fail是通过集群中超过半数的节点检测失效时才生效。客户端与redis节点直连，不需要中间proxy层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。redis-cluster把所有的物理节点映射到[0-16383]slot上cluster负责维护node&lt;-&gt;slot&lt;-&gt;value。 选举过程是集群中所有master参与，如果半数以上master节点与master节点通信超时，认为当前master节点挂掉。 当集群不可用时，所有对集群的操作做都不可用，收到((error) CLUSTERDOWN The cluster is down)错误。如果集群任意master挂掉，且当前master没有slave，集群进入fail状态，也可以理解成进群的slot映射[0-16383]不完成时进入fail状态。如果进群超过半数以上master挂掉，无论是否有slave集群进入fail状态。 环境搭建现在，我们进行集群环境搭建。集群环境至少需要3个主服务器节点。本次测试，使用另外3个节点作为从服务器的节点，即3个主服务器，3个从服务器。 修改配置文件，其它的保持默认即可。 1. # 根据实际情况修改 2. port 7000 3. # 允许redis支持集群模式 4. cluster-enabled yes 5. # 节点配置文件，由redis自动维护 6. cluster-config-file nodes.conf 7. # 节点超时毫秒 8. cluster-node-timeout 5000 9. # 开启AOF同步模式 10. appendonly yes 创建集群目前这些实例虽然都开启了cluster模式，但是彼此还不认识对方，接下来可以通过Redis集群的命令行工具redis-trib.rb来完成集群创建。首先，下载 https://raw.githubusercontent.com/antirez/redis/unstable/src/redis-trib.rb。 然后，搭建Redis 的 Ruby 支持环境。这里，不进行扩展，参考相关文档。 现在，接下来运行以下命令。这个命令在这里用于创建一个新的集群, 选项–replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 1. redis-trib.rb create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 5.3、测试 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现12：浅析Redis主从复制]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B012%EF%BC%9A%E6%B5%85%E6%9E%90Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 转自网络，侵删 早期的RDBMS被设计为运行在单个CPU之上，读写操作都由经单个数据库实例完成，复制技术使得数据库的读写操作可以分散在运行于不同CPU之上的独立服务器上，Redis作为一个开源的、优秀的key-value缓存及持久化存储解决方案，也提供了复制功能，本文主要介绍Redis的复制原理及特性。 Redis复制概论数据库复制指的是发生在不同数据库实例之间，单向的信息传播的行为，通常由被复制方和复制方组成，被复制方和复制方之间建立网络连接，复制方式通常为被复制方主动将数据发送到复制方，复制方接收到数据存储在当前实例，最终目的是为了保证双方的数据一致、同步。 复制示意图 Redis复制方式Redis的复制方式有两种，一种是主（master）-从（slave）模式，一种是从（slave）-从（slave）模式，因此Redis的复制拓扑图会丰富一些，可以像星型拓扑，也可以像个有向无环： Redis集群复制结构图 通过配置多个Redis实例独立运行、定向复制，形成Redis集群，master负责写、slave负责读。 复制优点通过配置多个Redis实例，数据备份在不同的实例上，主库专注写请求，从库负责读请求，这样的好处主要体现在下面几个方面： 1、高可用性在一个Redis集群中，如果master宕机，slave可以介入并取代master的位置，因此对于整个Redis服务来说不至于提供不了服务，这样使得整个Redis服务足够安全。 2、高性能在一个Redis集群中，master负责写请求，slave负责读请求，这么做一方面通过将读请求分散到其他机器从而大大减少了master服务器的压力，另一方面slave专注于提供读服务从而提高了响应和读取速度。 3、水平扩展性通过增加slave机器可以横向（水平）扩展Redis服务的整个查询服务的能力。 复制缺点复制提供了高可用性的解决方案，但同时引入了分布式计算的复杂度问题，认为有两个核心问题： 数据一致性问题，如何保证master服务器写入的数据能够及时同步到slave机器上。 编程复杂，如何在客户端提供读写分离的实现方案，通过客户端实现将读写请求分别路由到master和slave实例上。 上面两个问题，尤其是第一个问题是Redis服务实现一直在演变，致力于解决的一个问题。 复制实时性和数据一致性矛盾Redis提供了提高数据一致性的解决方案，本文后面会进行介绍，一致性程度的增加虽然使得我能够更信任数据，但是更好的一致性方案通常伴随着性能的损失，从而减少了吞吐量和服务能力。然而我们希望系统的性能达到最优，则必须要牺牲一致性的程度，因此Redis的复制实时性和数据一致性是存在矛盾的。 Redis复制原理及特性slave指向master举个例子，我们有四台redis实例，M1，R1、R2、R3，其中M1为master，R1、R2、R3分别为三台slave redis实例。在M1启动如下： 1./redis-server ../redis8000.conf --port 8000 下面分别为R1、R2、R3的启动命令： 1./redis-server ../redis8001.conf --port 8001 --slaveof 127.0.0.1 8000 ./redis-server ../redis8002.conf --port 8002 --slaveof 127.0.0.1 8000 ./redis-server ../redis8003.conf --port 8003 --slaveof 127.0.0.1 8000 这样，我们就成功的启动了四台Redis实例，master实例的服务端口为8000，R1、R2、R3的服务端口分别为8001、8002、8003，集群图如下： Redis集群复制拓扑 上面的命令在slave启动的时候就指定了master机器，我们也可以在slave运行的时候通过slaveof命令来指定master机器。 复制过程Redis复制主要由SYNC命令实现，复制过程如下图： Redis复制过程 上图为Redis复制工作过程： slave向master发送sync命令。 master开启子进程来讲dataset写入rdb文件，同时将子进程完成之前接收到的写命令缓存起来。 子进程写完，父进程得知，开始将RDB文件发送给slave。 master发送完RDB文件，将缓存的命令也发给slave。 master增量的把写命令发给slave。 值得注意的是，当slave跟master的连接断开时，slave可以自动的重新连接master，在redis2.8版本之前，每当slave进程挂掉重新连接master的时候都会开始新的一轮全量复制。如果master同时接收到多个slave的同步请求，则master只需要备份一次RDB文件。 增量复制上面复制过程介绍的最后提到，slave和master断开了、当slave和master重新连接上之后需要全量复制，这个策略是很不友好的，从Redis2.8开始，Redis提供了增量复制的机制： 增量复制机制 master除了备份RDB文件之外还会维护者一个环形队列，以及环形队列的写索引和slave同步的全局offset，环形队列用于存储最新的操作数据，当slave和maste断开重连之后，会把slave维护的offset，也就是上一次同步到哪里的这个值告诉master，同时会告诉master上次和当前slave连接的master的runid，满足下面两个条件，Redis不会全量复制： slave传递的run id和master的run id一致。 master在环形队列上可以找到对呀offset的值。 满足上面两个条件，Redis就不会全量复制，这样的好处是大大的提高的性能，不做无效的功。 增量复制是由psync命令实现的，slave可以通过psync命令来让Redis进行增量复制，当然最终是否能够增量复制取决于环形队列的大小和slave的断线时间长短和重连的这个master是否是之前的master。 环形队列大小配置参数： 1repl-backlog-size 1mb Redis同时也提供了当没有slave需要同步的时候，多久可以释放环形队列： 1repl-backlog-ttl 3600 免持久化复制免持久化机制官方叫做Diskless Replication，前面基于RDB文件写磁盘的方式可以看出，Redis必须要先将RDB文件写入磁盘，才进行网络传输，那么为什么不能直接通过网络把RDB文件传送给slave呢？免持久化复制就是做这个事情的，而且在Redis2.8.18版本开始支持，当然目前还是实验阶段。 值得注意的是，一旦基于Diskless Replication的复制传送开始，新的slave请求需要等待这次传输完毕才能够得到服务。 是否开启Diskless Replication的开关配置为： 1repo-diskless-sync no 为了让后续的slave能够尽量赶上本次复制，Redis提供了一个参数配置指定复制开始的时间延迟： 1repl-diskless-sync-delay 5 slave只读模式自从Redis2.6版本开始，支持对slave的只读模式的配置，默认对slave的配置也是只读。只读模式的slave将会拒绝客户端的写请求，从而避免因为从slave写入而导致的数据不一致问题。 半同步复制和MySQL复制策略有点类似，Redis复制本身是异步的，但也提供了半同步的复制策略，半同步复制策略在Redis复制中的语义是这样的： 1允许用户给出这样的配置：在maste接受写操作的时候，只有当一定时间间隔内，至少有N台slave在线，否则写入无效。 上面功能的实现基于Redis下面特性： Redis slaves每秒钟会ping一次master，告诉master当前slave复制到哪里了。 Redis master会记住每个slave复制到哪里了。 我们可以通过下面配置来指定时间间隔和N这个值： 1min-slaves-to-write &lt;number of slaves&gt;min-slaves-max-lag &lt;number of seconds&gt; 当配置了上面两个参数之后，一旦对于一个写操作没有满足上面的两个条件，则master会报错，并且将本次写操作视为无效。这有点像CAP理论中的“C”，即一致性实现，虽然半同步策略不能够完全保证master和slave的数据一致性，但是相对减少了不一致性的窗口期。 总结本文在理解Redis复制概念和复制的优缺点的基础之上介绍了当前Redis复制工作原理以及主要特性，希望能够帮助大家。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现11：使用快照和AOF将Redis数据持久化到硬盘中]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B011%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%BF%AB%E7%85%A7%E5%92%8CAOF%E5%B0%86Redis%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96%E5%88%B0%E7%A1%AC%E7%9B%98%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 转自https://blog.csdn.net/xlgen157387/article/details/61925524前言我们知道Redis是一款内存服务器，就算我们对自己的服务器足够的信任，不会出现任何软件或者硬件的故障，但也会有可能出现突然断电等情况，造成Redis服务器中的数据失效。因此，我们需要向传统的关系型数据库一样对数据进行备份，将Redis在内存中的数据持久化到硬盘等非易失性介质中，来保证数据的可靠性。 将Redis内存服务器中的数据持久化到硬盘等介质中的一个好处就是，使得我们的服务器在重启之后还可以重用以前的数据，或者是为了防止系统出现故障而将数据备份到一个远程的位置。 还有一些场景，例如： 1对于一些需要进行大量计算而得到的数据，放置在Redis服务器，我们就有必要对其进行数据的持久化，如果需要对数据进行恢复的时候，我们就不需进行重新的计算，只需要简单的将这台机器上的数据复制到另一台需要恢复的Redis服务器就可以了。 Redis给我们提供了两种不同方式的持久化方法：快照（Snapshotting） 和 只追加文件（append-only-file）。 （1）名词简介 快照（RDB）：就是我们俗称的备份，他可以在定期内对数据进行备份，将Redis服务器中的数据持久化到硬盘中； 只追加文件（AOF）：他会在执行写命令的时候，将执行的写命令复制到硬盘里面，后期恢复的时候，只需要重新执行一下这个写命令就可以了。类似于我们的MySQL数据库在进行主从复制的时候，使用的是binlog二进制文件，同样的是执行一遍写命令； （2）快照持久化通用的配置： 1save 60 1000 #60秒时间内有1000次写入操作的时候执行快照的创建stop-writes-on-bgsave-error no #创建快照失败的时候是否仍然继续执行写命令rdbcompression yes #是否对快照文件进行压缩dbfilename dump.rdb #如何命名硬盘上的快照文件dir ./ #快照所保存的位置 （3）AOP持久化配置： 1appendonly no #是否使用AOF持久化appendfsync everysec #多久执行一次将写入内容同步到硬盘上no-appendfsync-on-rewrite no #对AOF进行压缩的时候能否执行同步操作auto-aof-rewrite-percentage 100 #多久执行一次AOF压缩auto-aof-rewrite-min-size 64mb #多久执行一次AOF压缩dir ./ #AOF所保存的位置 需要注意的是：这两种持久化的方式既可以单独的使用，也可以同时使用，具体选择哪种方式需要根据具体的情况进行选择。 快照持久化快照就是我们所说的备份。用户可以将Redis内存中的数据在某一个时间点进行备份，在创建快照之后，用户可以对快照进行备份。通常情况下，为了防止单台服务器出现故障造成所有数据的丢失，我们还可以将快照复制到其他服务器，创建具有相同数据的数据副本，这样的话，数据恢复的时候或者服务器重启的时候就可以使用这些快照信息进行数据的恢复，也可以防止单台服务器出现故障的时候造成数据的丢失。 但是，没我们还需要注意的是，创建快照的方式，并不能完全保证我们的数据不丢失，这个大家可以很好的理解，因为快照的创建时定时的，并不是每一次更新操作都会创建一个快照的。系统发生崩溃的时候，用户将丢失最近一次生成快照之后更改的所有数据。因此，快照持久化的方式只适合于数据不经常修改或者丢失部分数据影响不大的场景。 一、创建快照的方式： （1）客户端通过向Redis发送BGSAVE 命令来创建快照。 使用BGSAVE的时候，Redis会调用fork来创建一个子进程，然后子进程负责将快照写到硬盘中，而父进程则继续处理命令请求。 使用场景： 如果用户使用了save设置，例如：save 60 1000 ,那么从Redis最近一次创建快照之后开始计算，当“60秒之内有1000次写入操作”这个条件满足的时候，Redis就会自动触发BGSAVE命令。 如果用户使用了多个save设置，那么当任意一个save配置满足条件的时候，Redis都会触发一次BGSAVE命令。 （2）客户端通过向Redis发送SAVE 命令来创建快照。 接收到SAVE命令的Redis服务器在快照创建完毕之前将不再响应任何其他命令的请求。SAVE命令并不常用，我们通常只在没有足够的内存去执行BGSAVE命令的时候才会使用SAVE命令，或者即使等待持久化操作执行完毕也无所谓的情况下，才会使用这个命令； 使用场景： 当Redis通过SHUTDOWN命令接收到关闭服务器的请求时，或者接收到标准的TERM信号时，会执行一次SAVE命令，阻塞所有的客户端，不再执行客户端发送的任何命令，并且在执行完SAVE命令之后关闭服务器。 二、使用快照持久化注意事项： 我们在使用快照的方式来保存数据的时候，如果Redis服务器中的数据量比较小的话，例如只有几个GB的时候。Redis会创建子进程并将数据保存到硬盘里边，生成快照所需的时间比读取数据所需要的时间还要短。 但是，随着数据的增大，Redis占用的内存越来越大的时候，BGSAVE在创建子进程的时候消耗的时间也会越来越多，如果Redis服务器所剩下的内存不多的时候，这行BGSAVE命令会使得系统长时间地停顿，还有可能导致服务器无法使用。 各虚拟机类别，创建子线程所耗时间： 因此，为了防止Redis因为创建子进程的时候出现停顿，我们可以考虑关闭自动保存，转而通过手动的方式发送BGSAVE或者SAVE来进行持久化， 手动的方式发送BGSAVE也会出现停顿的现象，但是我们可以控制发送该命令的时间来控制出现停顿的时候不影响具体的业务请求。 另外，值得注意的是，在使用SAVE命令的时候，虽然会一直阻塞Redis直到快照生成完毕，但是其不需要创建子进程，所以不会向BGSAVE一样，因为创建子进程而导致Redis停顿。也正因为如此，SAVE创建快照的速度要比BGSAVE创建快照的速度更快一些。 创建快照的时候，我们可以在业务请求，比较少的时候，比如凌晨三、四点，通过手写脚本的方式，定时执行。 AOF持久化AOF持久化会将被执行的写命令写到AOF文件的末尾，以此来记录数据发生的变化。这样，我们在恢复数据的时候，只需要从头到尾的执行一下AOF文件即可恢复数据。 一、打开AOF持久化选项 我们可以通过使用如下命令打开AOF： 1appendonly yes 我们，通过如下命令来配置AOF文件的同步频率： 1appendfsync everysec/always/no 二、appendfsync同步频率的区别 appendfsync同步频率的区别如下图： （1）always的方式固然可以对没一条数据进行很好的保存，但是这种同步策略需要对硬盘进行大量的写操作，所以Redis处理命令的速度会受到硬盘性能的限制。 普通的硬盘每秒钟只能处理大约200个写命令，使用固态硬盘SSD每秒可以处理几万个写命令，但是每次只写一个命令，这种只能怪不断地写入很少量的数据的做法有可能引发严重的写入放大问题，这种情况下降严重影响固态硬盘的使用寿命。 （2）everysec的方式，Redis以每秒一次的频率大队AOF文件进行同步。这样的话既可以兼顾数据安全也可以兼顾写入性能。 Redis以每秒同步一次AOF文件的性能和不使用任何持久化特性时的性能相差无几，使用每秒更新一次 的方式，可以保证，即使出现故障，丢失的数据也在一秒之内产生的数据。 （3）no的方式，Redis将不对AOF文件执行任何显示的同步操作，而是由操作系统来决定应该何时对AOF文件进行同步。 这个命令一般不会对Redis的性能造成多大的影响，但是当系统出现故障的时候使用这种选项的Redis服务器丢失不定数量的数据。 另外，当用户的硬盘处理写入操作的速度不够快的话，那么缓冲区被等待写入硬盘的数据填满时，Redis的写入操作将被阻塞，并导致Redis处理命令请求的速度变慢，因为这个原因，一般不推荐使用这个选项。 三、重写/压缩AOF文件 随着数据量的增大，AOF的文件可能会很大，这样在每次进行数据恢复的时候就会进行很长的时间，为了解决日益增大的AOF文件，用户可以向Redis发送BGREWRITEAOF 命令，这个命令会通过移除AOF文件中的冗余命令来重写AOF文件，是AOF文件的体检变得尽可能的小。 BGREWRITEAOF的工作原理和BGSAVE的原理很像：Redis会创建一个子进程，然后由子进程负责对AOF文件的重写操作。 因为AOF文件重写的时候汇创建子进程，所以快照持久化因为创建子进程而导致的性能和内存占用问题同样会出现在AOF文件重写的 时候。 四、触发重写/压缩AOF文件条件设定 AOF通过设置auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size 选项来自动执行BGREWRITEAOF。 其具体含义，通过实例可以看出，如下配置： 1auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 表示当前AOF的文件体积大于64MB，并且AOF文件的体积比上一次重写之后的体积变大了至少一倍（100%）的时候，Redis将执行重写BGREWRITEAOF命令。 如果AOF重写执行的过于频繁的话，可以将auto-aof-rewrite-percentage 选项的值设置为100以上，这种最偶发就可以让Redis在AOF文件的体积变得更大之后才执行重写操作，不过，这也使得在进行数据恢复的时候执行的时间变得更加长一些。 验证快照文件和AOF文件无论使用哪种方式进行持久化，我们在进行恢复数据的时候，Redis提供了两个命令行程序： 1redis-check-aofredis-check-dump 他们可以再系统发生故障的时候，检查快照和AOF文件的状态，并对有需要的情况对文件进行修复。 如果用户在运行redis-check-aof命令的时候，指定了--fix 参数，那么程序将对AOF文件进行修复。 程序修复AOF文件的方法很简单：他会扫描给定的AOF文件，寻找不正确或者不完整的命令，当发现第一个出现错误命令的时候，程序会删除出错命令以及出错命令之后的所有命令，只保留那些位于出错命令之前的正确命令。大部分情况，被删除的都是AOF文件末尾的不完整的写命令。 总结上述，一起学习了两种支持持久化的方式，一方面我们需要通过快照或者AOF的方式对数据进行持久化，另一方面，我们还需要将持久化所得到的文件进行备份，备份到不同的服务器上，这样才可以尽可能的减少数据丢失的损失。 参考文章： 1、Redis in Action - [美] Josiah L.Carlsono 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现10：Redis的事件驱动模型与命令执行过程]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B010%EF%BC%9ARedis%E7%9A%84%E4%BA%8B%E4%BB%B6%E9%A9%B1%E5%8A%A8%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 原文地址：https://www.xilidou.com/2018/03/22/redis-event/ Redis 是一个事件驱动的内存数据库，服务器需要处理两种类型的事件。 文件事件时间事件下面就会介绍这两种事件的实现原理。 文件事件Redis 服务器通过 socket 实现与客户端（或其他redis服务器）的交互,文件事件就是服务器对 socket 操作的抽象。 Redis 服务器，通过监听这些 socket 产生的文件事件并处理这些事件，实现对客户端调用的响应。 ReactorRedis 基于 Reactor 模式开发了自己的事件处理器。 这里就先展开讲一讲 Reactor 模式。看下图：reactor “I/O 多路复用模块”会监听多个 FD ，当这些FD产生，accept，read，write 或 close 的文件事件。会向“文件事件分发器（dispatcher）”传送事件。 文件事件分发器（dispatcher）在收到事件之后，会根据事件的类型将事件分发给对应的 handler。 我们顺着图，从上到下的逐一讲解 Redis 是怎么实现这个 Reactor 模型的。 I/O 多路复用模块Redis 的 I/O 多路复用模块，其实是封装了操作系统提供的 select，epoll，avport 和 kqueue 这些基础函数。向上层提供了一个统一的接口，屏蔽了底层实现的细节。 一般而言 Redis 都是部署到 Linux 系统上，所以我们就看看使用 Redis 是怎么利用 linux 提供的 epoll 实现I/O 多路复用。 首先看看 epoll 提供的三个方法： /* * 创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大 */ int epoll_create(int size)； /* * 可以理解为，增删改 fd 需要监听的事件 * epfd 是 epoll_create() 创建的句柄。 * op 表示 增删改 * epoll_event 表示需要监听的事件，Redis 只用到了可读，可写，错误，挂断 四个状态 */ int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； /* * 可以理解为查询符合条件的事件 * epfd 是 epoll_create() 创建的句柄。 * epoll_event 用来存放从内核得到事件的集合 * maxevents 获取的最大事件数 * timeout 等待超时时间 */ int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 再看 Redis 对文件事件，封装epoll向上提供的接口： /* * 事件状态 */ typedef struct aeApiState { // epoll_event 实例描述符 int epfd; // 事件槽 struct epoll_event *events; } aeApiState; /* * 创建一个新的 epoll */ static int aeApiCreate(aeEventLoop *eventLoop) /* * 调整事件槽的大小 */ static int aeApiResize(aeEventLoop *eventLoop, int setsize) /* * 释放 epoll 实例和事件槽 */ static void aeApiFree(aeEventLoop *eventLoop) /* * 关联给定事件到 fd */ static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) /* * 从 fd 中删除给定事件 */ static void aeApiDelEvent(aeEventLoop *eventLoop, int fd, int mask) /* * 获取可执行事件 */ static int aeApiPoll(aeEventLoop *eventLoop, struct timeval *tvp)所以看看这个ae_peoll.c 如何对 epoll 进行封装的： aeApiCreate() 是对 epoll.epoll_create() 的封装。aeApiAddEvent()和aeApiDelEvent() 是对 epoll.epoll_ctl()的封装。aeApiPoll() 是对 epoll_wait()的封装。这样 Redis 的利用 epoll 实现的 I/O 复用器就比较清晰了。 再往上一层次我们需要看看 ea.c 是怎么封装的？ 首先需要关注的是事件处理器的数据结构： typedef struct aeFileEvent { // 监听事件类型掩码， // 值可以是 AE_READABLE 或 AE_WRITABLE ， // 或者 AE_READABLE | AE_WRITABLE int mask; /* one of AE_(READABLE|WRITABLE) */ // 读事件处理器 aeFileProc *rfileProc; // 写事件处理器 aeFileProc *wfileProc; // 多路复用库的私有数据 void *clientData; } aeFileEvent;mask 就是可以理解为事件的类型。 除了使用 ae_peoll.c 提供的方法外,ae.c 还增加 “增删查” 的几个 API。 增:aeCreateFileEvent删:aeDeleteFileEvent查: 查包括两个维度 aeGetFileEvents 获取某个 fd 的监听类型和aeWait等待某个fd 直到超时或者达到某个状态。事件分发器（dispatcher）Redis 的事件分发器 ae.c/aeProcessEvents 不但处理文件事件还处理时间事件，所以这里只贴与文件分发相关的出部分代码，dispather 根据 mask 调用不同的事件处理器。 //从 epoll 中获关注的事件 numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j &lt; numevents; j++) { // 从已就绪数组中获取事件 aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd]; int mask = eventLoop-&gt;fired[j].mask; int fd = eventLoop-&gt;fired[j].fd; int rfired = 0; // 读事件 if (fe-&gt;mask &amp; mask &amp; AE_READABLE) { // rfired 确保读/写事件只能执行其中一个 rfired = 1; fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask); } // 写事件 if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) { if (!rfired || fe-&gt;wfileProc != fe-&gt;rfileProc) fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask); } processed++; }可以看到这个分发器，根据 mask 的不同将事件分别分发给了读事件和写事件。 文件事件处理器的类型Redis 有大量的事件处理器类型，我们就讲解处理一个简单命令涉及到的三个处理器： acceptTcpHandler 连接应答处理器，负责处理连接相关的事件，当有client 连接到Redis的时候们就会产生 AE_READABLE 事件。引发它执行。readQueryFromClinet 命令请求处理器，负责读取通过 sokect 发送来的命令。sendReplyToClient 命令回复处理器，当Redis处理完命令，就会产生 AE_WRITEABLE 事件，将数据回复给 client。文件事件实现总结我们按照开始给出的 Reactor 模型，从上到下讲解了文件事件处理器的实现，下面将会介绍时间时间的实现。 时间事件Reids 有很多操作需要在给定的时间点进行处理，时间事件就是对这类定时任务的抽象。 先看时间事件的数据结构： /* Time event structure * * 时间事件结构 */ typedef struct aeTimeEvent { // 时间事件的唯一标识符 long long id; /* time event identifier. */ // 事件的到达时间 long when_sec; /* seconds */ long when_ms; /* milliseconds */ // 事件处理函数 aeTimeProc *timeProc; // 事件释放函数 aeEventFinalizerProc *finalizerProc; // 多路复用库的私有数据 void *clientData; // 指向下个时间事件结构，形成链表 struct aeTimeEvent *next; } aeTimeEvent;看见 next 我们就知道这个 aeTimeEvent 是一个链表结构。看图： timeEvent 注意这是一个按照id倒序排列的链表，并没有按照事件顺序排序。 processTimeEventRedis 使用这个函数处理所有的时间事件，我们整理一下执行思路： 记录最新一次执行这个函数的时间，用于处理系统时间被修改产生的问题。遍历链表找出所有 when_sec 和 when_ms 小于现在时间的事件。执行事件对应的处理函数。检查事件类型，如果是周期事件则刷新该事件下一次的执行事件。否则从列表中删除事件。综合调度器（aeProcessEvents）综合调度器是 Redis 统一处理所有事件的地方。我们梳理一下这个函数的简单逻辑： // 1. 获取离当前时间最近的时间事件 shortest = aeSearchNearestTimer(eventLoop); // 2. 获取间隔时间 timeval = shortest - nowTime; // 如果timeval 小于 0，说明已经有需要执行的时间事件了。 if(timeval &lt; 0){ timeval = 0 } // 3. 在 timeval 时间内，取出文件事件。 numevents = aeApiPoll(eventLoop, timeval); // 4.根据文件事件的类型指定不同的文件处理器 if (AE_READABLE) { // 读事件 rfileProc(eventLoop,fd,fe-&gt;clientData,mask); } // 写事件 if (AE_WRITABLE) { wfileProc(eventLoop,fd,fe-&gt;clientData,mask); }以上的伪代码就是整个 Redis 事件处理器的逻辑。 我们可以再看看谁执行了这个 aeProcessEvents: void aeMain(aeEventLoop *eventLoop) { eventLoop-&gt;stop = 0; while (!eventLoop-&gt;stop) { // 如果有需要在事件处理前执行的函数，那么运行它 if (eventLoop-&gt;beforesleep != NULL) eventLoop-&gt;beforesleep(eventLoop); // 开始处理事件 aeProcessEvents(eventLoop, AE_ALL_EVENTS); } }然后我们再看看是谁调用了 eaMain: int main(int argc, char **argv) { //一些配置和准备 ... aeMain(server.el); //结束后的回收工作 ... }我们在 Redis 的 main 方法中找个了它。 这个时候我们整理出的思路就是: Redis 的 main() 方法执行了一些配置和准备以后就调用 eaMain() 方法。 eaMain() while(true) 的调用 aeProcessEvents()。 所以我们说 Redis 是一个事件驱动的程序，期间我们发现，Redis 没有 fork 过任何线程。所以也可以说 Redis 是一个基于事件驱动的单线程应用。 总结在后端的面试中 Redis 总是一个或多或少会问到的问题。 读完这篇文章你也许就能回答这几个问题： 为什么 Redis 是一个单线程应用？为什么 Redis 是一个单线程应用，却有如此高的性能？如果你用本文提供的知识点回答这两个问题，一定会在面试官心中留下一个高大的形象。 大家还可以阅读我的 Redis 相关的文章： 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现9：数据库redisDb与键过期删除策略]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B09%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93redisDb%E4%B8%8E%E9%94%AE%E8%BF%87%E6%9C%9F%E5%88%A0%E9%99%A4%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 一. 数据库Redis的数据库使用字典作为底层实现，数据库的增、删、查、改都是构建在字典的操作之上的。redis服务器将所有数据库都保存在服务器状态结构redisServer(redis.h/redisServer)的db数组（应该是一个链表）里： struct redisServer { //.. // 数据库数组，保存着服务器中所有的数据库 redisDb *db; //.. }在初始化服务器时，程序会根据服务器状态的dbnum属性来决定应该创建多少个数据库： struct redisServer { // .. //服务器中数据库的数量 int dbnum; //.. }dbnum属性的值是由服务器配置的database选项决定的，默认值为16； 二、切换数据库原理每个Redis客户端都有自己的目标数据库，每当客户端执行数据库的读写命令时，目标数据库就会成为这些命令的操作对象。 127.0.0.1:6379&gt; set msg &apos;Hello world&apos; OK 127.0.0.1:6379&gt; get msg &quot;Hello world&quot; 127.0.0.1:6379&gt; select 2 OK 127.0.0.1:6379[2]&gt; get msg (nil) 127.0.0.1:6379[2]&gt;在服务器内部，客户端状态redisClient结构(redis.h/redisClient)的db属性记录了客户端当前的目标数据库，这个属性是一个指向redisDb结构(redis.h/redisDb)的指针： typedef struct redisClient { //.. // 客户端当前正在使用的数据库 redisDb *db; //.. } redisClient;redisClient.db指针指向redisServer.db数组中的一个元素，而被指向的元素就是当前客户端的目标数据库。我们就可以通过修改redisClient指针，让他指向服务器中的不同数据库，从而实现切换数据库的功能–这就是select命令的实现原理。实现代码： int selectDb(redisClient *c, int id) { // 确保 id 在正确范围内 if (id &lt; 0 || id &gt;= server.dbnum) return REDIS_ERR; // 切换数据库（更新指针） c-&gt;db = &amp;server.db[id]; return REDIS_OK; }三、数据库的键空间1、数据库的结构（我们只分析键空间和键过期时间） typedef struct redisDb { // 数据库键空间，保存着数据库中的所有键值对 dict dict; / The keyspace for this DB / // 键的过期时间，字典的键为键，字典的值为过期事件 UNIX 时间戳 dict *expires; / Timeout of keys with a timeout set / // 数据库号码 int id; / Database ID / // 数据库的键的平均 TTL ，统计信息 long long avg_ttl; / Average TTL, just for stats */ //.. } redisDb 上图是一个RedisDb的示例，该数据库存放有五个键值对，分别是sRedis，INums，hBooks，SortNum和sNums，它们各自都有自己的值对象，另外，其中有三个键设置了过期时间，当前数据库是服务器的第0号数据库。现在，我们就从源码角度分析这个数据库结构：我们知道，Redis是一个键值对数据库服务器，服务器中的每一个数据库都是一个redis.h/redisDb结构，其中，结构中的dict字典保存了数据库中所有的键值对，我们就将这个字典成为键空间。Redis数据库的数据都是以键值对的形式存在，其充分利用了字典高效索引的特点。a、键空间的键就是数据库中的键，一般都是字符串对象；b、键空间的值就是数据库中的值，可以是5种类型对象（字符串、列表、哈希、集合和有序集合）之一。数据库的键空间结构分析完了，我们先看看数据库的初始化。 2、键空间的初始化在redis.c中，我们可以找到键空间的初始化操作： //创建并初始化数据库结构 for (j = 0; j &lt; server.dbnum; j++) { // 创建每个数据库的键空间 server.db[j].dict = dictCreate(&amp;dbDictType,NULL); // ... // 设定当前数据库的编号 server.db[j].id = j; }初始化之后就是对键空间的操作了。 3、键空间的操作我先把一些常见的键空间操作函数列出来： // 从数据库中取出键key的值对象，若不存在就返回NULL robj *lookupKey(redisDb *db, robj *key); /* 先删除过期键，以读操作的方式从数据库中取出指定键对应的值对象 * 并根据是否成功找到值，更新服务器的命中或不命中信息, * 如不存在则返回NULL，底层调用lookupKey函数 */ robj *lookupKeyRead(redisDb *db, robj *key); /* 先删除过期键，以写操作的方式从数据库中取出指定键对应的值对象 * 如不存在则返回NULL，底层调用lookupKey函数， * 不会更新服务器的命中或不命中信息 */ robj *lookupKeyWrite(redisDb *db, robj *key); /* 先删除过期键，以读操作的方式从数据库中取出指定键对应的值对象 * 如不存在则返回NULL，底层调用lookupKeyRead函数 * 此操作需要向客户端回复 */ robj *lookupKeyReadOrReply(redisClient *c, robj *key, robj *reply); /* 先删除过期键，以写操作的方式从数据库中取出指定键对应的值对象 * 如不存在则返回NULL，底层调用lookupKeyWrite函数 * 此操作需要向客户端回复 */ robj *lookupKeyWriteOrReply(redisClient *c, robj *key, robj *reply); /* 添加元素到指定数据库 */ void dbAdd(redisDb *db, robj *key, robj *val); /* 重写指定键的值 */ void dbOverwrite(redisDb *db, robj *key, robj *val); /* 设定指定键的值 */ void setKey(redisDb *db, robj *key, robj *val); /* 判断指定键是否存在 */ int dbExists(redisDb *db, robj *key); /* 随机返回数据库中的键 */ robj *dbRandomKey(redisDb *db); /* 删除指定键 */ int dbDelete(redisDb *db, robj *key); /* 清空所有数据库，返回键值对的个数 */ long long emptyDb(void(callback)(void*));下面我选取几个比较典型的操作函数分析一下： 查找键值对函数–lookupKeyrobj *lookupKey(redisDb *db, robj *key) { // 查找键空间 dictEntry *de = dictFind(db-&gt;dict,key-&gt;ptr); // 节点存在 if (de) { // 取出该键对应的值 robj *val = dictGetVal(de); // 更新时间信息 if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1) val-&gt;lru = LRU_CLOCK(); // 返回值 return val; } else { // 节点不存在 return NULL; }} 添加键值对–dbAdd添加键值对使我们经常使用到的函数，底层由dbAdd()函数实现，传入的参数是待添加的数据库，键对象和值对象，源码如下： void dbAdd(redisDb *db, robj *key, robj *val) { // 复制键名 sds copy = sdsdup(key-&gt;ptr); // 尝试添加键值对 int retval = dictAdd(db-&gt;dict, copy, val); // 如果键已经存在，那么停止 redisAssertWithInfo(NULL,key,retval == REDIS_OK); // 如果开启了集群模式，那么将键保存到槽里面 if (server.cluster_enabled) slotToKeyAdd(key); } 好了，关于键空间操作函数就分析到这，其他函数(在文件db.c中)大家可以自己去分析，有问题的话可以回帖，我们可以一起讨论！ 四、数据库的过期键操作在前面我们说到，redisDb结构中有一个expires指针（概况图可以看上图），该指针指向一个字典结构，字典中保存了所有键的过期时间，该字典称为过期字典。过期字典的初始化： // 创建并初始化数据库结构 for (j = 0; j &lt; server.dbnum; j++) { // 创建每个数据库的过期时间字典 server.db[j].expires = dictCreate(&amp;keyptrDictType,NULL); // 设定当前数据库的编号 server.db[j].id = j; // .. } a、过期字典的键是一个指针，指向键空间中的某一个键对象（就是某一个数据库键）；b、过期字典的值是一个long long类型的整数，这个整数保存了键所指向的数据库键的时间戳–一个毫秒精度的unix时间戳。下面我们就来分析过期键的处理函数： 1、过期键处理函数设置键的过期时间–setExpire()/* 将键 key 的过期时间设为 when /void setExpire(redisDb *db, robj *key, long long when) { dictEntry *kde, *de; // 从键空间中取出键key kde = dictFind(db-&gt;dict,key-&gt;ptr); // 如果键空间找不到该键，报错 redisAssertWithInfo(NULL,key,kde != NULL); // 向过期字典中添加该键 de = dictReplaceRaw(db-&gt;expires,dictGetKey(kde)); // 设置键的过期时间 // 这里是直接使用整数值来保存过期时间，不是用 INT 编码的 String 对象 dictSetSignedIntegerVal(de,when);} 获取键的过期时间–getExpire()long long getExpire(redisDb *db, robj *key) { dictEntry *de; // 如果过期键不存在，那么直接返回 if (dictSize(db-&gt;expires) == 0 || (de = dictFind(db-&gt;expires,key-&gt;ptr)) == NULL) return -1; redisAssertWithInfo(NULL,key,dictFind(db-&gt;dict,key-&gt;ptr) != NULL); // 返回过期时间 return dictGetSignedIntegerVal(de);} 删除键的过期时间–removeExpire()// 移除键 key 的过期时间int removeExpire(redisDb *db, robj *key) { // 确保键带有过期时间 redisAssertWithInfo(NULL,key,dictFind(db-&gt;dict,key-&gt;ptr) != NULL); // 删除过期时间 return dictDelete(db-&gt;expires,key-&gt;ptr) == DICT_OK;} 2、过期键删除策略通过前面的介绍，大家应该都知道数据库键的过期时间都保存在过期字典里，那假如一个键过期了，那么这个过期键是什么时候被删除的呢？现在来看看redis的过期键的删除策略：a、定时删除：在设置键的过期时间的同时，创建一个定时器，在定时结束的时候，将该键删除；b、惰性删除：放任键过期不管，在访问该键的时候，判断该键的过期时间是否已经到了，如果过期时间已经到了，就执行删除操作；c、定期删除：每隔一段时间，对数据库中的键进行一次遍历，删除过期的键。其中定时删除可以及时删除数据库中的过期键，并释放过期键所占用的内存，但是它为每一个设置了过期时间的键都开了一个定时器，使的cpu的负载变高，会对服务器的响应时间和吞吐量造成影响。惰性删除有效的克服了定时删除对CPU的影响，但是，如果一个过期键很长时间没有被访问到，且若存在大量这种过期键时，势必会占用很大的内存空间，导致内存消耗过大。定时删除可以算是上述两种策略的折中。设定一个定时器，每隔一段时间遍历数据库，删除其中的过期键，有效的缓解了定时删除对CPU的占用以及惰性删除对内存的占用。在实际应用中，Redis采用了惰性删除和定时删除两种策略来对过期键进行处理，上面提到的lookupKeyWrite等函数中就利用到了惰性删除策略，定时删除策略则是在根据服务器的例行处理程序serverCron来执行删除操作，该程序每100ms调用一次。 惰性删除函数–expireIfNeeded()源码如下： /* 检查key是否已经过期，如果是的话，将它从数据库中删除 并将删除命令写入AOF文件以及附属节点(主从复制和AOF持久化相关) 返回0代表该键还没有过期，或者没有设置过期时间 返回1代表该键因为过期而被删除 /int expireIfNeeded(redisDb *db, robj *key) { // 获取该键的过期时间 mstime_t when = getExpire(db,key); mstime_t now; // 该键没有设定过期时间 if (when &lt; 0) return 0; // 服务器正在加载数据的时候，不要处理 if (server.loading) return 0; // lua脚本相关 now = server.lua_caller ? server.lua_time_start : mstime(); // 主从复制相关，附属节点不主动删除key if (server.masterhost != NULL) return now &gt; when; // 该键还没有过期 if (now &lt;= when) return 0; // 删除过期键 server.stat_expiredkeys++; // 将删除命令传播到AOF文件和附属节点 propagateExpire(db,key); // 发送键空间操作时间通知 notifyKeyspaceEvent(NOTIFY_EXPIRED,&quot;expired&quot;,key,db-&gt;id); // 将该键从数据库中删除 return dbDelete(db,key);} 定期删除策略过期键的定期删除策略由redis.c/activeExpireCycle()函数实现，服务器周期性地操作redis.c/serverCron()（每隔100ms执行一次）时，会调用activeExpireCycle()函数，分多次遍历服务器中的各个数据库，从数据库中的expires字典中随机检查一部分键的过期时间，并删除其中的过期键。删除过期键的操作由activeExpireCycleTryExpire函数(activeExpireCycle()调用了该函数)执行，其源码如下： /* 检查键的过期时间，如过期直接删除*/int activeExpireCycleTryExpire(redisDb *db, dictEntry *de, long long now) { // 获取过期时间 long long t = dictGetSignedIntegerVal(de); if (now &gt; t) { // 执行到此说明过期 // 创建该键的副本 sds key = dictGetKey(de); robj *keyobj = createStringObject(key,sdslen(key)); // 将删除命令传播到AOF和附属节点 propagateExpire(db,keyobj); // 在数据库中删除该键 dbDelete(db,keyobj); // 发送事件通知 notifyKeyspaceEvent(NOTIFY_EXPIRED, “expired”,keyobj,db-&gt;id); // 临时键对象的引用计数减1 decrRefCount(keyobj); // 服务器的过期键计数加1 // 该参数影响每次处理的数据库个数 server.stat_expiredkeys++; return 1; } else { return 0; }} 删除过期键对AOF、RDB和主从复制都有影响，等到了介绍相关功能时再讨论。今天就先到这里~ 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现8：连接底层与表面的数据结构robj]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B08%EF%BC%9A%E8%BF%9E%E6%8E%A5%E5%BA%95%E5%B1%82%E4%B8%8E%E8%A1%A8%E9%9D%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84robj%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文是《Redis内部数据结构详解》系列的第三篇，讲述在Redis实现中的一个基础数据结构：robj。 那到底什么是robj呢？它有什么用呢？ 从Redis的使用者的角度来看，一个Redis节点包含多个database（非cluster模式下默认是16个，cluster模式下只能是1个），而一个database维护了从key space到object space的映射关系。这个映射关系的key是string类型，而value可以是多种数据类型，比如：string, list, hash等。我们可以看到，key的类型固定是string，而value可能的类型是多个。 而从Redis内部实现的角度来看，在前面第一篇文章中，我们已经提到过，一个database内的这个映射关系是用一个dict来维护的。dict的key固定用一种数据结构来表达就够了，这就是动态字符串sds。而value则比较复杂，为了在同一个dict内能够存储不同类型的value，这就需要一个通用的数据结构，这个通用的数据结构就是robj（全名是redisObject）。举个例子：如果value是一个list，那么它的内部存储结构是一个quicklist（quicklist的具体实现我们放在后面的文章讨论）；如果value是一个string，那么它的内部存储结构一般情况下是一个sds。当然实际情况更复杂一点，比如一个string类型的value，如果它的值是一个数字，那么Redis内部还会把它转成long型来存储，从而减小内存使用。而一个robj既能表示一个sds，也能表示一个quicklist，甚至还能表示一个long型。 robj的数据结构定义在server.h中我们找到跟robj定义相关的代码，如下（注意，本系列文章中的代码片段全部来源于Redis源码的3.2分支）： /* Object types */ #define OBJ_STRING 0 #define OBJ_LIST 1 #define OBJ_SET 2 #define OBJ_ZSET 3 #define OBJ_HASH 4 /* Objects encoding. Some kind of objects like Strings and Hashes can be * internally represented in multiple ways. The &apos;encoding&apos; field of the object * is set to one of this fields for this object. */ #define OBJ_ENCODING_RAW 0 /* Raw representation */ #define OBJ_ENCODING_INT 1 /* Encoded as integer */ #define OBJ_ENCODING_HT 2 /* Encoded as hash table */ #define OBJ_ENCODING_ZIPMAP 3 /* Encoded as zipmap */ #define OBJ_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */ #define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */ #define OBJ_ENCODING_INTSET 6 /* Encoded as intset */ #define OBJ_ENCODING_SKIPLIST 7 /* Encoded as skiplist */ #define OBJ_ENCODING_EMBSTR 8 /* Embedded sds string encoding */ #define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */ #define LRU_BITS 24 typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr; } robj;一个robj包含如下5个字段： type: 对象的数据类型。占4个bit。可能的取值有5种：OBJ_STRING, OBJ_LIST, OBJ_SET, OBJ_ZSET, OBJ_HASH，分别对应Redis对外暴露的5种数据结构（即我们在第一篇文章中提到的第一个层面的5种数据结构）。 encoding: 对象的内部表示方式（也可以称为编码）。占4个bit。可能的取值有10种，即前面代码中的10个OBJ_ENCODING_XXX常量。 lru: 做LRU替换算法用，占24个bit。这个不是我们这里讨论的重点，暂时忽略。 refcount: 引用计数。它允许robj对象在某些情况下被共享。 ptr: 数据指针。指向真正的数据。比如，一个代表string的robj，它的ptr可能指向一个sds结构；一个代表list的robj，它的ptr可能指向一个quicklist。 这里特别需要仔细察看的是encoding字段。对于同一个type，还可能对应不同的encoding，这说明同样的一个数据类型，可能存在不同的内部表示方式。而不同的内部表示，在内存占用和查找性能上会有所不同。 比如，当type = OBJ_STRING的时候，表示这个robj存储的是一个string，这时encoding可以是下面3种中的一种： OBJ_ENCODING_RAW: string采用原生的表示方式，即用sds来表示。 OBJ_ENCODING_INT: string采用数字的表示方式，实际上是一个long型。 OBJ_ENCODING_EMBSTR: string采用一种特殊的嵌入式的sds来表示。接下来我们会讨论到这个细节。 再举一个例子：当type = OBJ_HASH的时候，表示这个robj存储的是一个hash，这时encoding可以是下面2种中的一种： OBJ_ENCODING_HT: hash采用一个dict来表示。 OBJ_ENCODING_ZIPLIST: hash采用一个ziplist来表示（ziplist的具体实现我们放在后面的文章讨论）。 本文剩余主要部分将针对表示string的robj对象，围绕它的3种不同的encoding来深入讨论。前面代码段中出现的所有10种encoding，在这里我们先简单解释一下，在这个系列后面的文章中，我们应该还有机会碰到它们。 OBJ_ENCODING_RAW: 最原生的表示方式。其实只有string类型才会用这个encoding值（表示成sds）。 OBJ_ENCODING_INT: 表示成数字。实际用long表示。 OBJ_ENCODING_HT: 表示成dict。 OBJ_ENCODING_ZIPMAP: 是个旧的表示方式，已不再用。在小于Redis 2.6的版本中才有。 OBJ_ENCODING_LINKEDLIST: 也是个旧的表示方式，已不再用。 OBJ_ENCODING_ZIPLIST: 表示成ziplist。 OBJ_ENCODING_INTSET: 表示成intset。用于set数据结构。 OBJ_ENCODING_SKIPLIST: 表示成skiplist。用于sorted set数据结构。 OBJ_ENCODING_EMBSTR: 表示成一种特殊的嵌入式的sds。 OBJ_ENCODING_QUICKLIST: 表示成quicklist。用于list数据结构。 我们来总结一下robj的作用： 为多种数据类型提供一种统一的表示方式。 允许同一类型的数据采用不同的内部表示，从而在某些情况下尽量节省内存。 支持对象共享和引用计数。当对象被共享的时候，只占用一份内存拷贝，进一步节省内存。 string robj的编码过程当我们执行Redis的set命令的时候，Redis首先将接收到的value值（string类型）表示成一个type = OBJ_STRING并且encoding = OBJ_ENCODING_RAW的robj对象，然后在存入内部存储之前先执行一个编码过程，试图将它表示成另一种更节省内存的encoding方式。这一过程的核心代码，是object.c中的tryObjectEncoding函数。 robj *tryObjectEncoding(robj *o) { long value; sds s = o-&gt;ptr; size_t len; /* Make sure this is a string object, the only type we encode * in this function. Other types use encoded memory efficient * representations but are handled by the commands implementing * the type. */ serverAssertWithInfo(NULL,o,o-&gt;type == OBJ_STRING); /* We try some specialized encoding only for objects that are * RAW or EMBSTR encoded, in other words objects that are still * in represented by an actually array of chars. */ if (!sdsEncodedObject(o)) return o; /* It&apos;s not safe to encode shared objects: shared objects can be shared * everywhere in the &quot;object space&quot; of Redis and may end in places where * they are not handled. We handle them only as values in the keyspace. */ if (o-&gt;refcount &gt; 1) return o; /* Check if we can represent this string as a long integer. * Note that we are sure that a string larger than 21 chars is not * representable as a 32 nor 64 bit integer. */ len = sdslen(s); if (len &lt;= 21 &amp;&amp; string2l(s,len,&amp;value)) { /* This object is encodable as a long. Try to use a shared object. * Note that we avoid using shared integers when maxmemory is used * because every object needs to have a private LRU field for the LRU * algorithm to work well. */ if ((server.maxmemory == 0 || (server.maxmemory_policy != MAXMEMORY_VOLATILE_LRU &amp;&amp; server.maxmemory_policy != MAXMEMORY_ALLKEYS_LRU)) &amp;&amp; value &gt;= 0 &amp;&amp; value &lt; OBJ_SHARED_INTEGERS) { decrRefCount(o); incrRefCount(shared.integers[value]); return shared.integers[value]; } else { if (o-&gt;encoding == OBJ_ENCODING_RAW) sdsfree(o-&gt;ptr); o-&gt;encoding = OBJ_ENCODING_INT; o-&gt;ptr = (void*) value; return o; } } /* If the string is small and is still RAW encoded, * try the EMBSTR encoding which is more efficient. * In this representation the object and the SDS string are allocated * in the same chunk of memory to save space and cache misses. */ if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) { robj *emb; if (o-&gt;encoding == OBJ_ENCODING_EMBSTR) return o; emb = createEmbeddedStringObject(s,sdslen(s)); decrRefCount(o); return emb; } /* We can&apos;t encode the object... * * Do the last try, and at least optimize the SDS string inside * the string object to require little space, in case there * is more than 10% of free space at the end of the SDS string. * * We do that only for relatively large strings as this branch * is only entered if the length of the string is greater than * OBJ_ENCODING_EMBSTR_SIZE_LIMIT. */ if (o-&gt;encoding == OBJ_ENCODING_RAW &amp;&amp; sdsavail(s) &gt; len/10) { o-&gt;ptr = sdsRemoveFreeSpace(o-&gt;ptr); } /* Return the original object. */ return o; }这段代码执行的操作比较复杂，我们有必要仔细看一下每一步的操作： 第1步检查，检查type。确保只对string类型的对象进行操作。 第2步检查，检查encoding。sdsEncodedObject是定义在server.h中的一个宏，确保只对OBJ_ENCODING_RAW和OBJ_ENCODING_EMBSTR编码的string对象进行操作。这两种编码的string都采用sds来存储，可以尝试进一步编码处理。 #define sdsEncodedObject(objptr) (objptr-&gt;encoding == OBJ_ENCODING_RAW || objptr-&gt;encoding == OBJ_ENCODING_EMBSTR) 第3步检查，检查refcount。引用计数大于1的共享对象，在多处被引用。由于编码过程结束后robj的对象指针可能会变化（我们在前一篇介绍sdscatlen函数的时候提到过类似这种接口使用模式），这样对于引用计数大于1的对象，就需要更新所有地方的引用，这不容易做到。因此，对于计数大于1的对象不做编码处理。 试图将字符串转成64位的long。64位的long所能表达的数据范围是-2^63到2^63-1，用十进制表达出来最长是20位数（包括负号）。这里判断小于等于21，似乎是写多了，实际判断小于等于20就够了（如果我算错了请一定告诉我哦）。string2l如果将字符串转成long转成功了，那么会返回1并且将转好的long存到value变量里。 在转成long成功时，又分为两种情况。 第一种情况：如果Redis的配置不要求运行LRU替换算法，且转成的long型数字的值又比较小（小于OBJ_SHARED_INTEGERS，在目前的实现中这个值是10000），那么会使用共享数字对象来表示。之所以这里的判断跟LRU有关，是因为LRU算法要求每个robj有不同的lru字段值，所以用了LRU就不能共享robj。shared.integers是一个长度为10000的数组，里面预存了10000个小的数字对象。这些小数字对象都是encoding = OBJ_ENCODING_INT的string robj对象。 第二种情况：如果前一步不能使用共享小对象来表示，那么将原来的robj编码成encoding = OBJ_ENCODING_INT，这时ptr字段直接存成这个long型的值。注意ptr字段本来是一个void *指针（即存储的是内存地址），因此在64位机器上有64位宽度，正好能存储一个64位的long型值。这样，除了robj本身之外，它就不再需要额外的内存空间来存储字符串值。 接下来是对于那些不能转成64位long的字符串进行处理。最后再做两步处理： 如果字符串长度足够小（小于等于OBJ_ENCODING_EMBSTR_SIZE_LIMIT，定义为44），那么调用createEmbeddedStringObject编码成encoding = OBJ_ENCODING_EMBSTR； 如果前面所有的编码尝试都没有成功（仍然是OBJ_ENCODING_RAW），且sds里空余字节过多，那么做最后一次努力，调用sds的sdsRemoveFreeSpace接口来释放空余字节。 其中调用的createEmbeddedStringObject，我们有必要看一下它的代码： robj *createEmbeddedStringObject(const char *ptr, size_t len) { robj *o = zmalloc(sizeof(robj)+sizeof(struct sdshdr8)+len+1); struct sdshdr8 *sh = (void*)(o+1); o-&gt;type = OBJ_STRING; o-&gt;encoding = OBJ_ENCODING_EMBSTR; o-&gt;ptr = sh+1; o-&gt;refcount = 1; o-&gt;lru = LRU_CLOCK(); sh-&gt;len = len; sh-&gt;alloc = len; sh-&gt;flags = SDS_TYPE_8; if (ptr) { memcpy(sh-&gt;buf,ptr,len); sh-&gt;buf[len] = &apos;\0&apos;; } else { memset(sh-&gt;buf,0,len+1); } return o; }createEmbeddedStringObject对sds重新分配内存，将robj和sds放在一个连续的内存块中分配，这样对于短字符串的存储有利于减少内存碎片。这个连续的内存块包含如下几部分： 16个字节的robj结构。 3个字节的sdshdr8头。 最多44个字节的sds字符数组。 1个NULL结束符。 加起来一共不超过64字节（16+3+44+1），因此这样的一个短字符串可以完全分配在一个64字节长度的内存块中。 string robj的解码过程当我们需要获取字符串的值，比如执行get命令的时候，我们需要执行与前面讲的编码过程相反的操作——解码。 这一解码过程的核心代码，是object.c中的getDecodedObject函数。 robj *getDecodedObject(robj *o) { robj *dec; if (sdsEncodedObject(o)) { incrRefCount(o); return o; } if (o-&gt;type == OBJ_STRING &amp;&amp; o-&gt;encoding == OBJ_ENCODING_INT) { char buf[32]; ll2string(buf,32,(long)o-&gt;ptr); dec = createStringObject(buf,strlen(buf)); return dec; } else { serverPanic(&quot;Unknown encoding type&quot;); } }这个过程比较简单，需要我们注意的点有： 编码为OBJ_ENCODING_RAW和OBJ_ENCODING_EMBSTR的字符串robj对象，不做变化，原封不动返回。站在使用者的角度，这两种编码没有什么区别，内部都是封装的sds。 编码为数字的字符串robj对象，将long重新转为十进制字符串的形式，然后调用createStringObject转为sds的表示。注意：这里由long转成的sds字符串长度肯定不超过20，而根据createStringObject的实现，它们肯定会被编码成OBJ_ENCODING_EMBSTR的对象。createStringObject的代码如下： robj *createStringObject(const char *ptr, size_t len) { if (len &lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) return createEmbeddedStringObject(ptr,len); else return createRawStringObject(ptr,len);} 再谈sds与string的关系在上一篇文章中，我们简单地提到了sds与string的关系；在本文介绍了robj的概念之后，我们重新总结一下sds与string的关系。 确切地说，string在Redis中是用一个robj来表示的。 用来表示string的robj可能编码成3种内部表示：OBJ_ENCODING_RAW, OBJ_ENCODING_EMBSTR, OBJ_ENCODING_INT。其中前两种编码使用的是sds来存储，最后一种OBJ_ENCODING_INT编码直接把string存成了long型。 在对string进行incr, decr等操作的时候，如果它内部是OBJ_ENCODING_INT编码，那么可以直接进行加减操作；如果它内部是OBJ_ENCODING_RAW或OBJ_ENCODING_EMBSTR编码，那么Redis会先试图把sds存储的字符串转成long型，如果能转成功，再进行加减操作。 对一个内部表示成long型的string执行append, setbit, getrange这些命令，针对的仍然是string的值（即十进制表示的字符串），而不是针对内部表示的long型进行操作。比如字符串”32”，如果按照字符数组来解释，它包含两个字符，它们的ASCII码分别是0x33和0x32。当我们执行命令setbit key 7 0的时候，相当于把字符0x33变成了0x32，这样字符串的值就变成了”22”。而如果将字符串”32”按照内部的64位long型来解释，那么它是0x0000000000000020，在这个基础上执行setbit位操作，结果就完全不对了。因此，在这些命令的实现中，会把long型先转成字符串再进行相应的操作。由于篇幅原因，这三个命令的实现代码这里就不详细介绍了，有兴趣的读者可以参考Redis源码： t_string.c中的appendCommand函数； biops.c中的setbitCommand函数； t_string.c中的getrangeCommand函数。 值得一提的是，append和setbit命令的实现中，都会最终调用到db.c中的dbUnshareStringValue函数，将string对象的内部编码转成OBJ_ENCODING_RAW的（只有这种编码的robj对象，其内部的sds 才能在后面自由追加新的内容），并解除可能存在的对象共享状态。这里面调用了前面提到的getDecodedObject。 robj *dbUnshareStringValue(redisDb *db, robj *key, robj *o) { serverAssert(o-&gt;type == OBJ_STRING); if (o-&gt;refcount != 1 || o-&gt;encoding != OBJ_ENCODING_RAW) { robj *decoded = getDecodedObject(o); o = createRawStringObject(decoded-&gt;ptr, sdslen(decoded-&gt;ptr)); decrRefCount(decoded); dbOverwrite(db,key,o); } return o; }robj的引用计数操作将robj的引用计数加1和减1的操作，定义在object.c中： void incrRefCount(robj *o) { o-&gt;refcount++; } void decrRefCount(robj *o) { if (o-&gt;refcount &lt;= 0) serverPanic(&quot;decrRefCount against refcount &lt;= 0&quot;); if (o-&gt;refcount == 1) { switch(o-&gt;type) { case OBJ_STRING: freeStringObject(o); break; case OBJ_LIST: freeListObject(o); break; case OBJ_SET: freeSetObject(o); break; case OBJ_ZSET: freeZsetObject(o); break; case OBJ_HASH: freeHashObject(o); break; default: serverPanic(&quot;Unknown object type&quot;); break; } zfree(o); } else { o-&gt;refcount--; } }我们特别关注一下将引用计数减1的操作decrRefCount。如果只剩下最后一个引用了（refcount已经是1了），那么在decrRefCount被调用后，整个robj将被释放。 注意：Redis的del命令就依赖decrRefCount操作将value释放掉。 经过了本文的讨论，我们很容易看出，robj所表示的就是Redis对外暴露的第一层面的数据结构：string, list, hash, set, sorted set，而每一种数据结构的底层实现所对应的是哪个（或哪些）第二层面的数据结构（dict, sds, ziplist, quicklist, skiplist, 等），则通过不同的encoding来区分。可以说，robj是联结两个层面的数据结构的桥梁。 本文详细介绍了OBJ_STRING类型的字符串对象的底层实现，其编码和解码过程在Redis里非常重要，应用广泛，我们在后面的讨论中可能还会遇到。现在有了robj的概念基础，我们下一篇会讨论ziplist，以及它与hash的关系。 后记(追加于2016-07-09): 本文在解析“将string编码成long型”的代码时提到的判断21字节的问题，后来已经提交给@antirez并合并进了unstable分支，详见commit f648c5a。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现7：Redis内部数据结构详解——intset]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B07%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94intset%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文是《Redis内部数据结构详解》系列的第七篇。在本文中，我们围绕一个Redis的内部数据结构——intset展开讨论。 Redis里面使用intset是为了实现集合(set)这种对外的数据结构。set结构类似于数学上的集合的概念，它包含的元素无序，且不能重复。Redis里的set结构还实现了基础的集合并、交、差的操作。与Redis对外暴露的其它数据结构类似，set的底层实现，随着元素类型是否是整型以及添加的元素的数目多少，而有所变化。概括来讲，当set中添加的元素都是整型且元素数目较少时，set使用intset作为底层数据结构，否则，set使用dict作为底层数据结构。 在本文中我们将大体分成三个部分进行介绍： 集中介绍intset数据结构。 讨论set是如何在intset和dict基础上构建起来的。 集中讨论set的并、交、差的算法实现以及时间复杂度。注意，其中差集的计算在Redis中实现了两种算法。 我们在讨论中还会涉及到一个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 1set-max-intset-entries 512 注：本文讨论的代码实现基于Redis源码的3.2分支。 intset数据结构简介intset顾名思义，是由整数组成的集合。实际上，intset是一个由整数组成的有序集合，从而便于在上面进行二分查找，用于快速地判断一个元素是否属于这个集合。它在内存分配上与ziplist有些类似，是连续的一整块内存空间，而且对于大整数和小整数（按绝对值）采取了不同的编码，尽量对内存的使用进行了优化。 intset的数据结构定义如下（出自intset.h和intset.c）： 123456789typedef struct intset &#123; uint32_t encoding; uint32_t length; int8_t contents[];&#125; intset;#define INTSET_ENC_INT16 (sizeof(int16_t))#define INTSET_ENC_INT32 (sizeof(int32_t))#define INTSET_ENC_INT64 (sizeof(int64_t)) 各个字段含义如下： encoding: 数据编码，表示intset中的每个数据元素用几个字节来存储。它有三种可能的取值：INTSET_ENC_INT16表示每个元素用2个字节存储，INTSET_ENC_INT32表示每个元素用4个字节存储，INTSET_ENC_INT64表示每个元素用8个字节存储。因此，intset中存储的整数最多只能占用64bit。 length: 表示intset中的元素个数。encoding和length两个字段构成了intset的头部（header）。 contents: 是一个柔性数组（flexible array member），表示intset的header后面紧跟着数据元素。这个数组的总长度（即总字节数）等于encoding * length。柔性数组在Redis的很多数据结构的定义中都出现过（例如sds, quicklist, skiplist），用于表达一个偏移量。contents需要单独为其分配空间，这部分内存不包含在intset结构当中。 其中需要注意的是，intset可能会随着数据的添加而改变它的数据编码： 最开始，新创建的intset使用占内存最小的INTSET_ENC_INT16（值为2）作为数据编码。 每添加一个新元素，则根据元素大小决定是否对数据编码进行升级。 下图给出了一个添加数据的具体例子（点击看大图）。 在上图中： 新创建的intset只有一个header，总共8个字节。其中encoding = 2, length = 0。 添加13, 5两个元素之后，因为它们是比较小的整数，都能使用2个字节表示，所以encoding不变，值还是2。 当添加32768的时候，它不再能用2个字节来表示了（2个字节能表达的数据范围是-215~215-1，而32768等于215，超出范围了），因此encoding必须升级到INTSET_ENC_INT32（值为4），即用4个字节表示一个元素。 在添加每个元素的过程中，intset始终保持从小到大有序。 与ziplist类似，intset也是按小端（little endian）模式存储的（参见维基百科词条Endianness）。比如，在上图中intset添加完所有数据之后，表示encoding字段的4个字节应该解释成0x00000004，而第5个数据应该解释成0x000186A0 = 100000。 intset与ziplist相比： ziplist可以存储任意二进制串，而intset只能存储整数。 ziplist是无序的，而intset是从小到大有序的。因此，在ziplist上查找只能遍历，而在intset上可以进行二分查找，性能更高。 ziplist可以对每个数据项进行不同的变长编码（每个数据项前面都有数据长度字段len），而intset只能整体使用一个统一的编码（encoding）。 intset的查找和添加操作要理解intset的一些实现细节，只需要关注intset的两个关键操作基本就可以了：查找（intsetFind）和添加（intsetAdd）元素。 intsetFind的关键代码如下所示（出自intset.c）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445uint8_t intsetFind(intset *is, int64_t value) &#123; uint8_t valenc = _intsetValueEncoding(value); return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL);&#125;static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) &#123; int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1; int64_t cur = -1; /* The value can never be found when the set is empty */ if (intrev32ifbe(is-&gt;length) == 0) &#123; if (pos) *pos = 0; return 0; &#125; else &#123; /* Check for the case where we know we cannot find the value, * but do know the insert position. */ if (value &gt; _intsetGet(is,intrev32ifbe(is-&gt;length)-1)) &#123; if (pos) *pos = intrev32ifbe(is-&gt;length); return 0; &#125; else if (value &lt; _intsetGet(is,0)) &#123; if (pos) *pos = 0; return 0; &#125; &#125; while(max &gt;= min) &#123; mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1; cur = _intsetGet(is,mid); if (value &gt; cur) &#123; min = mid+1; &#125; else if (value &lt; cur) &#123; max = mid-1; &#125; else &#123; break; &#125; &#125; if (value == cur) &#123; if (pos) *pos = mid; return 1; &#125; else &#123; if (pos) *pos = min; return 0; &#125;&#125; 关于以上代码，我们需要注意的地方包括： intsetFind在指定的intset中查找指定的元素value，找到返回1，没找到返回0。 _intsetValueEncoding函数会根据要查找的value落在哪个范围而计算出相应的数据编码（即它应该用几个字节来存储）。 如果value所需的数据编码比当前intset的编码要大，则它肯定在当前intset所能存储的数据范围之外（特别大或特别小），所以这时会直接返回0；否则调用intsetSearch执行一个二分查找算法。 intsetSearch在指定的intset中查找指定的元素value，如果找到，则返回1并且将参数pos指向找到的元素位置；如果没找到，则返回0并且将参数pos指向能插入该元素的位置。 intsetSearch是对于二分查找算法的一个实现，它大致分为三个部分： 特殊处理intset为空的情况。 特殊处理两个边界情况：当要查找的value比最后一个元素还要大或者比第一个元素还要小的时候。实际上，这两部分的特殊处理，在二分查找中并不是必须的，但它们在这里提供了特殊情况下快速失败的可能。 真正执行二分查找过程。注意：如果最后没找到，插入位置在min指定的位置。 代码中出现的intrev32ifbe是为了在需要的时候做大小端转换的。前面我们提到过，intset里的数据是按小端（little endian）模式存储的，因此在大端（big endian）机器上运行时，这里的intrev32ifbe会做相应的转换。 这个查找算法的总的时间复杂度为O(log n)。 而intsetAdd的关键代码如下所示（出自intset.c）： 12345678910111213141516171819202122232425262728intset *intsetAdd(intset *is, int64_t value, uint8_t *success) &#123; uint8_t valenc = _intsetValueEncoding(value); uint32_t pos; if (success) *success = 1; /* Upgrade encoding if necessary. If we need to upgrade, we know that * this value should be either appended (if &gt; 0) or prepended (if &lt; 0), * because it lies outside the range of existing values. */ if (valenc &gt; intrev32ifbe(is-&gt;encoding)) &#123; /* This always succeeds, so we don&apos;t need to curry *success. */ return intsetUpgradeAndAdd(is,value); &#125; else &#123; /* Abort if the value is already present in the set. * This call will populate &quot;pos&quot; with the right position to insert * the value when it cannot be found. */ if (intsetSearch(is,value,&amp;pos)) &#123; if (success) *success = 0; return is; &#125; is = intsetResize(is,intrev32ifbe(is-&gt;length)+1); if (pos &lt; intrev32ifbe(is-&gt;length)) intsetMoveTail(is,pos,pos+1); &#125; _intsetSet(is,pos,value); is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1); return is;&#125; 关于以上代码，我们需要注意的地方包括： intsetAdd在intset中添加新元素value。如果value在添加前已经存在，则不会重复添加，这时参数success被置为0；如果value在原来intset中不存在，则将value插入到适当位置，这时参数success被置为0。 如果要添加的元素value所需的数据编码比当前intset的编码要大，那么则调用intsetUpgradeAndAdd将intset的编码进行升级后再插入value。 调用intsetSearch，如果能查到，则不会重复添加。 如果没查到，则调用intsetResize对intset进行内存扩充，使得它能够容纳新添加的元素。因为intset是一块连续空间，因此这个操作会引发内存的realloc（参见http://man.cx/realloc）。这有可能带来一次数据拷贝。同时调用intsetMoveTail将待插入位置后面的元素统一向后移动1个位置，这也涉及到一次数据拷贝。值得注意的是，在intsetMoveTail中是调用memmove完成这次数据拷贝的。memmove保证了在拷贝过程中不会造成数据重叠或覆盖，具体参见http://man.cx/memmove。 intsetUpgradeAndAdd的实现中也会调用intsetResize来完成内存扩充。在进行编码升级时，intsetUpgradeAndAdd的实现会把原来intset中的每个元素取出来，再用新的编码重新写入新的位置。 注意一下intsetAdd的返回值，它返回一个新的intset指针。它可能与传入的intset指针is相同，也可能不同。调用方必须用这里返回的新的intset，替换之前传进来的旧的intset变量。类似这种接口使用模式，在Redis的实现代码中是很常见的，比如我们之前在介绍sds和ziplist的时候都碰到过类似的情况。 显然，这个intsetAdd算法总的时间复杂度为O(n)。 Redis的set为了更好地理解Redis对外暴露的set数据结构，我们先看一下set的一些关键的命令。下面是一些命令举例： 上面这些命令的含义： sadd用于分别向集合s1和s2中添加元素。添加的元素既有数字，也有非数字（”a”和”b”）。 sismember用于判断指定的元素是否在集合内存在。 sinter, sunion和sdiff分别用于计算集合的交集、并集和差集。 我们前面提到过，set的底层实现，随着元素类型是否是整型以及添加的元素的数目多少，而有所变化。例如，具体到上述命令的执行过程中，集合s1的底层数据结构会发生如下变化： 在开始执行完sadd s1 13 5之后，由于添加的都是比较小的整数，所以s1底层是一个intset，其数据编码encoding = 2。 在执行完sadd s1 32768 10 100000之后，s1底层仍然是一个intset，但其数据编码encoding从2升级到了4。 在执行完sadd s1 a b之后，由于添加的元素不再是数字，s1底层的实现会转成一个dict。 我们知道，dict是一个用于维护key和value映射关系的数据结构，那么当set底层用dict表示的时候，它的key和value分别是什么呢？实际上，key就是要添加的集合元素，而value是NULL。 除了前面提到的由于添加非数字元素造成集合底层由intset转成dict之外，还有两种情况可能造成这种转换： 添加了一个数字，但它无法用64bit的有符号数来表达。intset能够表达的最大的整数范围为-264~264-1，因此，如果添加的数字超出了这个范围，这也会导致intset转成dict。 添加的集合元素个数超过了set-max-intset-entries配置的值的时候，也会导致intset转成dict（具体的触发条件参见t_set.c中的setTypeAdd相关代码）。 对于小集合使用intset来存储，主要的原因是节省内存。特别是当存储的元素个数较少的时候，dict所带来的内存开销要大得多（包含两个哈希表、链表指针以及大量的其它元数据）。所以，当存储大量的小集合而且集合元素都是数字的时候，用intset能节省下一笔可观的内存空间。 实际上，从时间复杂度上比较，intset的平均情况是没有dict性能高的。以查找为例，intset是O(log n)的，而dict可以认为是O(1)的。但是，由于使用intset的时候集合元素个数比较少，所以这个影响不大。 Redis set的并、交、差算法Redis set的并、交、差算法的实现代码，在t_set.c中。其中计算交集调用的是sinterGenericCommand，计算并集和差集调用的是sunionDiffGenericCommand。它们都能同时对多个（可以多于2个）集合进行运算。当对多个集合进行差集运算时，它表达的含义是：用第一个集合与第二个集合做差集，所得结果再与第三个集合做差集，依次向后类推。 我们在这里简要介绍一下三个算法的实现思路。 交集计算交集的过程大概可以分为三部分： 检查各个集合，对于不存在的集合当做空集来处理。一旦出现空集，则不用继续计算了，最终的交集就是空集。 对各个集合按照元素个数由少到多进行排序。这个排序有利于后面计算的时候从最小的集合开始，需要处理的元素个数较少。 对排序后第一个集合（也就是最小集合）进行遍历，对于它的每一个元素，依次在后面的所有集合中进行查找。只有在所有集合中都能找到的元素，才加入到最后的结果集合中。 需要注意的是，上述第3步在集合中进行查找，对于intset和dict的存储来说时间复杂度分别是O(log n)和O(1)。但由于只有小集合才使用intset，所以可以粗略地认为intset的查找也是常数时间复杂度的。因此，如Redis官方文档上所说（http://redis.io/commands/sinter），sinter命令的时间复杂度为： O(N*M) worst case where N is the cardinality of the smallest set and M is the number of sets. 并集计算并集最简单，只需要遍历所有集合，将每一个元素都添加到最后的结果集合中。向集合中添加元素会自动去重。 由于要遍历所有集合的每个元素，所以Redis官方文档给出的sunion命令的时间复杂度为（http://redis.io/commands/sunion）： O(N) where N is the total number of elements in all given sets. 注意，这里同前面讨论交集计算一样，将元素插入到结果集合的过程，忽略intset的情况，认为时间复杂度为O(1)。 差集计算差集有两种可能的算法，它们的时间复杂度有所区别。 第一种算法： 对第一个集合进行遍历，对于它的每一个元素，依次在后面的所有集合中进行查找。只有在所有集合中都找不到的元素，才加入到最后的结果集合中。 这种算法的时间复杂度为O(N*M)，其中N是第一个集合的元素个数，M是集合数目。 第二种算法： 将第一个集合的所有元素都加入到一个中间集合中。 遍历后面所有的集合，对于碰到的每一个元素，从中间集合中删掉它。 最后中间集合剩下的元素就构成了差集。 这种算法的时间复杂度为O(N)，其中N是所有集合的元素个数总和。 在计算差集的开始部分，会先分别估算一下两种算法预期的时间复杂度，然后选择复杂度低的算法来进行运算。还有两点需要注意： 在一定程度上优先选择第一种算法，因为它涉及到的操作比较少，只用添加，而第二种算法要先添加再删除。 如果选择了第一种算法，那么在执行该算法之前，Redis的实现中对于第二个集合之后的所有集合，按照元素个数由多到少进行了排序。这个排序有利于以更大的概率查找到元素，从而更快地结束查找。 对于sdiff的时间复杂度，Redis官方文档（http://redis.io/commands/sdiff）只给出了第二种算法的结果，是不准确的。 系列下一篇待续，敬请期待。 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！本文链接：http://zhangtielei.com/posts/blog-redis-intset.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现6：Redis内部数据结构详解——skiplist]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B06%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94skiplist%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文是《Redis内部数据结构详解》系列的第六篇。在本文中，我们围绕一个Redis的内部数据结构——skiplist展开讨论。 Redis里面使用skiplist是为了实现sorted set这种对外的数据结构。sorted set提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。同时，skiplist这种数据结构对于很多人来说都比较陌生，因为大部分学校里的算法课都没有对这种数据结构进行过详细的介绍。因此，为了介绍得足够清楚，本文会比这个系列的其它几篇花费更多的篇幅。 我们将大体分成三个部分进行介绍： 介绍经典的skiplist数据结构，并进行简单的算法分析。这一部分的介绍，与Redis没有直接关系。我会尝试尽量使用通俗易懂的语言进行描述。 讨论Redis里的skiplist的具体实现。为了支持sorted set本身的一些要求，在经典的skiplist基础上，Redis里的相应实现做了若干改动。 讨论sorted set是如何在skiplist, dict和ziplist基础上构建起来的。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的代码实现基于Redis源码的3.2分支。 skiplist数据结构简介skiplist本质上也是一种查找结构，用于解决算法中的查找问题（Searching），即根据给定的key，快速查到它所在的位置（或者对应的value）。 我们在《Redis内部数据结构详解》系列的第一篇中介绍dict的时候，曾经讨论过：一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 这种数据结构是由William Pugh发明的，最早出现于他在1990年发表的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。对细节感兴趣的同学可以下载论文原文来阅读。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程： 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist的算法性能分析在这一部分，我们来简单分析一下skiplist的时间复杂度和空间复杂度，以便对于skiplist的性能有一个直观的了解。如果你不是特别偏执于算法的性能分析，那么可以暂时跳过这一小节的内容。 我们先来计算一下每个节点所包含的平均指针数目（概率期望）。节点包含的指针数目，相当于这个算法在空间上的额外开销(overhead)，可以用来度量空间复杂度。 根据前面randomLevel()的伪码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下： 节点层数至少为1。而大于1的节点层数，满足一个概率分布。 节点层数恰好等于1的概率为1-p。 节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。 节点层数大于等于3的概率为p2，而节点层数恰好等于3的概率为p2(1-p)。 节点层数大于等于4的概率为p3，而节点层数恰好等于4的概率为p3(1-p)。 …… 因此，一个节点的平均层数（也即包含的平均指针数目），计算如下： 现在很容易计算出： 当p=1/2时，每个节点所包含的平均指针数目为2； 当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。 接下来，为了分析时间复杂度，我们计算一下skiplist的平均查找长度。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。以前面图中标出的查找23的查找路径为例，从左上角的头结点开始，一直到结点22，查找长度为6。 为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数randomLevel()计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以，从统计上来说，一个skiplist结构的形成与节点的插入顺序无关。 这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个skiplist的形成结构。 现在假设我们从一个层数为i的节点x出发，需要向左向上攀爬k层。这时我们有两种可能： 如果节点x有第(i+1)层指针，那么我们需要向上走。这种情况概率为p。 如果节点x没有第(i+1)层指针，那么我们需要向左走。这种情况概率为(1-p)。 这两种情形如下图所示： 用C(k)表示向上攀爬k个层级所需要走过的平均查找路径长度（概率期望），那么： 12C(0)=0C(k)=(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度) 代入，得到一个差分方程并化简： 123C(k)=(1-p)(C(k)+1) + p(C(k-1)+1)C(k)=1/p+C(k-1)C(k)=k/p 这个结果的意思是，我们每爬升1个层级，需要在查找路径上走1/p步。而我们总共需要攀爬的层级数等于整个skiplist的总层数-1。 那么接下来我们需要分析一下当skiplist中有n个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出： 第1层链表固定有n个节点； 第2层链表平均有n*p个节点； 第3层链表平均有n*p2个节点； … 所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为log1/pn，而最高层的平均节点数为1/p。 综上，粗略来计算的话，平均查找长度约等于： C(log1/pn-1)=(log1/pn-1)/p 即，平均时间复杂度为O(log n)。 当然，这里的时间复杂度分析还是比较粗略的。比如，沿着查找路径向左向上回溯的时候，可能先到达左侧头结点，然后沿头结点一路向上；还可能先到达最高层的节点，然后沿着最高层链表一路向左。但这些细节不影响平均时间复杂度的最后结果。另外，这里给出的时间复杂度只是一个概率平均值，但实际上计算一个精细的概率分布也是有可能的。详情还请参见William Pugh的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis中的skiplist实现在这一部分，我们讨论Redis中的skiplist实现。 在Redis中，skiplist被用于实现暴露给外部的一个数据结构：sorted set。准确地说，sorted set底层不仅仅使用了skiplist，还使用了ziplist和dict。这几个数据结构的关系，我们下一章再讨论。现在，我们先花点时间把sorted set的关键命令看一下。这些命令对于Redis里skiplist的实现，有重要的影响。 sorted set的命令举例sorted set是一个有序的数据集合，对于像类似排行榜这样的应用场景特别适合。 现在我们来看一个例子，用sorted set来存储代数课（algebra）的成绩表。原始数据如下： Alice 87.5 Bob 89.0 Charles 65.5 David 78.0 Emily 93.5 Fred 87.5 这份数据给出了每位同学的名字和分数。下面我们将这份数据存储到sorted set里面去： 对于上面的这些命令，我们需要的注意的地方包括： 前面的6个zadd命令，将6位同学的名字和分数(score)都输入到一个key值为algebra的sorted set里面了。注意Alice和Fred的分数相同，都是87.5分。 zrevrank命令查询Alice的排名（命令中的rev表示按照倒序排列，也就是从大到小），返回3。排在Alice前面的分别是Emily、Bob、Fred，而排名(rank)从0开始计数，所以Alice的排名是3。注意，其实Alice和Fred的分数相同，这种情况下sorted set会把分数相同的元素，按照字典顺序来排列。按照倒序，Fred排在了Alice的前面。 zscore命令查询了Charles对应的分数。 zrevrange命令查询了从大到小排名为0~3的4位同学。 zrevrangebyscore命令查询了分数在80.0和90.0之间的所有同学，并按分数从大到小排列。 总结一下，sorted set中的每个元素主要表现出3个属性： 数据本身（在前面的例子中我们把名字存成了数据）。 每个数据对应一个分数(score)。 根据分数大小和数据本身的字典排序，每个数据会产生一个排名(rank)。可以按正序或倒序。 Redis中skiplist实现的特殊性我们简单分析一下前面出现的几个查询命令： zrevrank由数据查询它对应的排名，这在前面介绍的skiplist中并不支持。 zscore由数据查询它对应的分数，这也不是skiplist所支持的。 zrevrange根据一个排名范围，查询排名在这个范围内的数据。这在前面介绍的skiplist中也不支持。 zrevrangebyscore根据分数区间查询数据集合，是一个skiplist所支持的典型的范围查找（score相当于key）。 实际上，Redis中sorted set的实现是这样的： 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个dict + 一个skiplist来实现的。简单来讲，dict用来查询数据到分数的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 这里sorted set的构成我们在下一章还会再详细地讨论。现在我们集中精力来看一下sorted set与skiplist的关系，： zscore的查询，不是由skiplist来提供的，而是由那个dict来提供的。 为了支持排名(rank)，Redis里对skiplist做了扩展，使得根据排名能够快速查到数据，或者根据分数查到数据之后，也同时很容易获得排名。而且，根据排名的查找，时间复杂度也为O(log n)。 zrevrange的查询，是根据排名查数据，由扩展后的skiplist来提供。 zrevrank是先在dict中由数据查到分数，再拿分数到skiplist中去查找，查到后也同时获得了排名。 前述的查询过程，也暗示了各个操作的时间复杂度： zscore只用查询一个dict，所以时间复杂度为O(1) zrevrank, zrevrange, zrevrangebyscore由于要查询skiplist，所以zrevrank的时间复杂度为O(log n)，而zrevrange, zrevrangebyscore的时间复杂度为O(log(n)+M)，其中M是当前查询返回的元素个数。 总结起来，Redis中的skiplist跟前面介绍的经典的skiplist相比，有如下不同： 分数(score)允许重复，即skiplist的key允许重复。这在最开始介绍的经典skiplist中是不允许的。 在比较时，不仅比较分数（相当于skiplist的key），还比较数据本身。在Redis的skiplist实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。 第1层链表不是一个单向链表，而是一个双向链表。这是为了方便以倒序方式获取一个范围内的元素。 在skiplist中可以很方便地计算出每个元素的排名(rank)。 skiplist的数据结构定义123456789101112131415161718#define ZSKIPLIST_MAXLEVEL 32#define ZSKIPLIST_P 0.25typedef struct zskiplistNode &#123; robj *obj; double score; struct zskiplistNode *backward; struct zskiplistLevel &#123; struct zskiplistNode *forward; unsigned int span; &#125; level[];&#125; zskiplistNode;typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; unsigned long length; int level;&#125; zskiplist; 这段代码出自server.h，我们来简要分析一下： 开头定义了两个常量，ZSKIPLIST_MAXLEVEL和ZSKIPLIST_P，分别对应我们前面讲到的skiplist的两个参数：一个是MaxLevel，一个是p。 zskiplistNode定义了skiplist的节点结构。 obj字段存放的是节点数据，它的类型是一个string robj。本来一个string robj可能存放的不是sds，而是long型，但zadd命令在将数据插入到skiplist里面之前先进行了解码，所以这里的obj字段里存储的一定是一个sds。有关robj的详情可以参见系列文章的第三篇：《Redis内部数据结构详解(3)——robj》。这样做的目的应该是为了方便在查找的时候对数据进行字典序的比较，而且，skiplist里的数据部分是数字的可能性也比较小。 score字段是数据对应的分数。 backward字段是指向链表前一个节点的指针（前向指针）。节点只有1个前向指针，所以只有第1层链表是一个双向链表。 level[]存放指向各层链表后一个节点的指针（后向指针）。每层对应1个后向指针，用forward字段表示。另外，每个后向指针还对应了一个span值，它表示当前的指针跨越了多少个节点。span用于计算元素排名(rank)，这正是前面我们提到的Redis对于skiplist所做的一个扩展。需要注意的是，level[]是一个柔性数组（flexible array member），因此它占用的内存不在zskiplistNode结构里面，而需要插入节点的时候单独为它分配。也正因为如此，skiplist的每个节点所包含的指针数目才是不固定的，我们前面分析过的结论——skiplist每个节点包含的指针数目平均为1/(1-p)——才能有意义。 zskiplist定义了真正的skiplist结构，它包含： 头指针header和尾指针tail。 链表长度length，即链表包含的节点总数。注意，新创建的skiplist包含一个空的头指针，这个头指针不包含在length计数中。 level表示skiplist的总层数，即所有节点层数的最大值。 下图以前面插入的代数课成绩表为例，展示了Redis中一个skiplist的可能结构： 注意：图中前向指针上面括号中的数字，表示对应的span的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。 假设我们在这个skiplist中查找score=89.0的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的span值累加起来，就得到了Bob的排名(2+2+1)-1=4（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用skiplist长度减去查找路径上的span累加值，即6-(2+2+1)=1。 可见，在查找skiplist的过程中，通过累加span值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似zrange和zrevrange那样），也可以不断累加span并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条O(log n)的查找路径。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。 当sorted set中插入的任意一个数据的长度超过了64的时候。 最后，zset结构的代码定义如下： 1234typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: 1) They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. 2) A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. 3) They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423 这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因，我们在前面其实也都涉及到了。 系列下一篇我们将介绍intset，以及它与Redis对外暴露的数据类型set的关系，敬请期待。 （完） 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！本文链接：http://zhangtielei.com/posts/blog-redis-skiplist.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现5：Redis内部数据结构详解——quicklist]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B05%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94quicklist%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文是《Redis内部数据结构详解》系列的第五篇。在本文中，我们介绍一个Redis内部数据结构——quicklist。Redis对外暴露的list数据类型，它底层实现所依赖的内部数据结构就是quicklist。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12list-max-ziplist-size -2list-compress-depth 0 我们在讨论中会详细解释这两个配置的含义。 注：本文讨论的quicklist实现基于Redis源码的3.2分支。 quicklist概述Redis对外暴露的上层list数据类型，经常被用作队列使用。比如它支持的如下一些操作： lpush: 在左侧（即列表头部）插入数据。 rpop: 在右侧（即列表尾部）删除数据。 rpush: 在右侧（即列表尾部）插入数据。 lpop: 在左侧（即列表头部）删除数据。 这些操作都是O(1)时间复杂度的。 当然，list也支持在任意中间位置的存取操作，比如lindex和linsert，但它们都需要对list进行遍历，所以时间复杂度较高，为O(N)。 概况起来，list具有这样的一些特点：它是一个能维持数据项先后顺序的列表（各个数据项的先后顺序由插入位置决定），便于在表的两端追加和删除数据，而对于中间位置的存取具有O(N)的时间复杂度。这不正是一个双向链表所具有的特点吗？ list的内部实现quicklist正是一个双向链表。在quicklist.c的文件头部注释中，是这样描述quicklist的： A doubly linked list of ziplists 它确实是一个双向链表，而且是一个ziplist的双向链表。 这是什么意思呢？ 我们知道，双向链表是由多个节点（Node）组成的。这个描述的意思是：quicklist的每个节点都是一个ziplist。ziplist我们已经在上一篇介绍过。 ziplist本身也是一个能维持数据项先后顺序的列表（按插入位置），而且是一个内存紧缩的列表（各个数据项在内存上前后相邻）。比如，一个包含3个节点的quicklist，如果每个节点的ziplist又包含4个数据项，那么对外表现上，这个list就总共包含12个数据项。 quicklist的结构为什么这样设计呢？总结起来，大概又是一个空间和时间的折中： 双向链表便于在表的两端进行push和pop操作，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。 ziplist由于是一整块连续内存，所以存储效率很高。但是，它不利于修改操作，每次数据变动都会引发一次内存的realloc。特别是当ziplist长度很长的时候，一次realloc可能会导致大批量的数据拷贝，进一步降低性能。 于是，结合了双向链表和ziplist的优点，quicklist就应运而生了。 不过，这也带来了一个新问题：到底一个quicklist节点包含多长的ziplist合适呢？比如，同样是存储12个数据项，既可以是一个quicklist包含3个节点，而每个节点的ziplist又包含4个数据项，也可以是一个quicklist包含6个节点，而每个节点的ziplist又包含2个数据项。 这又是一个需要找平衡点的难题。我们只从存储效率上分析一下： 每个quicklist节点上的ziplist越短，则内存碎片越多。内存碎片多了，有可能在内存中产生很多无法被利用的小碎片，从而降低存储效率。这种情况的极端是每个quicklist节点上的ziplist只包含一个数据项，这就蜕化成一个普通的双向链表了。 每个quicklist节点上的ziplist越长，则为ziplist分配大块连续内存空间的难度就越大。有可能出现内存里有很多小块的空闲空间（它们加起来很多），但却找不到一块足够大的空闲空间分配给ziplist的情况。这同样会降低存储效率。这种情况的极端是整个quicklist只有一个节点，所有的数据项都分配在这仅有的一个节点的ziplist里面。这其实蜕化成一个ziplist了。 可见，一个quicklist节点上的ziplist要保持一个合理的长度。那到底多长合理呢？这可能取决于具体应用场景。实际上，Redis提供了一个配置参数list-max-ziplist-size，就是为了让使用者可以来根据自己的情况进行调整。 1list-max-ziplist-size -2 我们来详细解释一下这个参数的含义。它可以取正值，也可以取负值。 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。 当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值，每个值含义如下： -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb =&gt; 1024 bytes） -4: 每个quicklist节点上的ziplist大小不能超过32 Kb。 -3: 每个quicklist节点上的ziplist大小不能超过16 Kb。 -2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值） -1: 每个quicklist节点上的ziplist大小不能超过4 Kb。 另外，list的设计目标是能够用来存储很长的数据列表的。比如，Redis官网给出的这个教程：Writing a simple Twitter clone with PHP and Redis，就是使用list来存储类似Twitter的timeline数据。 当列表很长的时候，最容易被访问的很可能是两端的数据，中间的数据被访问的频率比较低（访问起来性能也很低）。如果应用场景符合这个特点，那么list还提供了一个选项，能够把中间的数据节点进行压缩，从而进一步节省内存空间。Redis的配置参数list-compress-depth就是用来完成这个设置的。 1list-compress-depth 0 这个参数表示一个quicklist两端不被压缩的节点个数。注：这里的节点个数是指quicklist双向链表的节点个数，而不是指ziplist里面的数据项个数。实际上，一个quicklist节点上的ziplist，如果被压缩，就是整体被压缩的。 参数list-compress-depth的取值含义如下： 0: 是个特殊值，表示都不压缩。这是Redis的默认值。 1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。 2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。 3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。 依此类推… 由于0是个特殊值，很容易看出quicklist的头节点和尾节点总是不被压缩的，以便于在表的两端进行快速存取。 Redis对于quicklist内部节点的压缩算法，采用的LZF——一种无损压缩算法。 quicklist的数据结构定义quicklist相关的数据结构定义可以在quicklist.h中找到： 1234567891011121314151617181920212223242526typedef struct quicklistNode &#123; struct quicklistNode *prev; struct quicklistNode *next; unsigned char *zl; unsigned int sz; /* ziplist size in bytes */ unsigned int count : 16; /* count of items in ziplist */ unsigned int encoding : 2; /* RAW==1 or LZF==2 */ unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ unsigned int recompress : 1; /* was this node previous compressed? */ unsigned int attempted_compress : 1; /* node can&apos;t compress; too small */ unsigned int extra : 10; /* more bits to steal for future usage */&#125; quicklistNode;typedef struct quicklistLZF &#123; unsigned int sz; /* LZF size in bytes*/ char compressed[];&#125; quicklistLZF;typedef struct quicklist &#123; quicklistNode *head; quicklistNode *tail; unsigned long count; /* total count of all entries in all ziplists */ unsigned int len; /* number of quicklistNodes */ int fill : 16; /* fill factor for individual nodes */ unsigned int compress : 16; /* depth of end nodes not to compress;0=off */&#125; quicklist; quicklistNode结构代表quicklist的一个节点，其中各个字段的含义如下： prev: 指向链表前一个节点的指针。 next: 指向链表后一个节点的指针。 zl: 数据指针。如果当前节点的数据没有压缩，那么它指向一个ziplist结构；否则，它指向一个quicklistLZF结构。 sz: 表示zl指向的ziplist的总大小（包括zlbytes, zltail, zllen, zlend和各个数据项）。需要注意的是：如果ziplist被压缩了，那么这个sz的值仍然是压缩前的ziplist大小。 count: 表示ziplist里面包含的数据项个数。这个字段只有16bit。稍后我们会一起计算一下这16bit是否够用。 encoding: 表示ziplist是否压缩了（以及用了哪个压缩算法）。目前只有两种取值：2表示被压缩了（而且用的是LZF压缩算法），1表示没有压缩。 container: 是一个预留字段。本来设计是用来表明一个quicklist节点下面是直接存数据，还是使用ziplist存数据，或者用其它的结构来存数据（用作一个数据容器，所以叫container）。但是，在目前的实现中，这个值是一个固定的值2，表示使用ziplist作为数据容器。 recompress: 当我们使用类似lindex这样的命令查看了某一项本来压缩的数据时，需要把数据暂时解压，这时就设置recompress=1做一个标记，等有机会再把数据重新压缩。 attempted_compress: 这个值只对Redis的自动化测试程序有用。我们不用管它。 extra: 其它扩展字段。目前Redis的实现里也没用上。 quicklistLZF结构表示一个被压缩过的ziplist。其中： sz: 表示压缩后的ziplist大小。 compressed: 是个柔性数组（flexible array member），存放压缩后的ziplist字节数组。 真正表示quicklist的数据结构是同名的quicklist这个struct： head: 指向头节点（左侧第一个节点）的指针。 tail: 指向尾节点（右侧第一个节点）的指针。 count: 所有ziplist数据项的个数总和。 len: quicklist节点的个数。 fill: 16bit，ziplist大小设置，存放list-max-ziplist-size参数的值。 compress: 16bit，节点压缩深度设置，存放list-compress-depth参数的值。 上图是一个quicklist的结构图举例。图中例子对应的ziplist大小配置和节点压缩深度配置，如下： 12list-max-ziplist-size 3list-compress-depth 2 这个例子中我们需要注意的几点是： 两端各有2个橙黄色的节点，是没有被压缩的。它们的数据指针zl指向真正的ziplist。中间的其它节点是被压缩过的，它们的数据指针zl指向被压缩后的ziplist结构，即一个quicklistLZF结构。 左侧头节点上的ziplist里有2项数据，右侧尾节点上的ziplist里有1项数据，中间其它节点上的ziplist里都有3项数据（包括压缩的节点内部）。这表示在表的两端执行过多次push和pop操作后的一个状态。 现在我们来大概计算一下quicklistNode结构中的count字段这16bit是否够用。 我们已经知道，ziplist大小受到list-max-ziplist-size参数的限制。按照正值和负值有两种情况： 当这个参数取正值的时候，就是恰好表示一个quicklistNode结构中zl所指向的ziplist所包含的数据项的最大值。list-max-ziplist-size参数是由quicklist结构的fill字段来存储的，而fill字段是16bit，所以它所能表达的值能够用16bit来表示。 当这个参数取负值的时候，能够表示的ziplist最大长度是64 Kb。而ziplist中每一个数据项，最少需要2个字节来表示：1个字节的prevrawlen，1个字节的data（len字段和data合二为一；详见上一篇）。所以，ziplist中数据项的个数不会超过32 K，用16bit来表达足够了。 实际上，在目前的quicklist的实现中，ziplist的大小还会受到另外的限制，根本不会达到这里所分析的最大值。 下面进入代码分析阶段。 quicklist的创建当我们使用lpush或rpush命令第一次向一个不存在的list里面插入数据的时候，Redis会首先调用quicklistCreate接口创建一个空的quicklist。 1234567891011quicklist *quicklistCreate(void) &#123; struct quicklist *quicklist; quicklist = zmalloc(sizeof(*quicklist)); quicklist-&gt;head = quicklist-&gt;tail = NULL; quicklist-&gt;len = 0; quicklist-&gt;count = 0; quicklist-&gt;compress = 0; quicklist-&gt;fill = -2; return quicklist;&#125; 在很多介绍数据结构的书上，实现双向链表的时候经常会多增加一个空余的头节点，主要是为了插入和删除操作的方便。从上面quicklistCreate的代码可以看出，quicklist是一个不包含空余头节点的双向链表（head和tail都初始化为NULL）。 quicklist的push操作quicklist的push操作是调用quicklistPush来实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354void quicklistPush(quicklist *quicklist, void *value, const size_t sz, int where) &#123; if (where == QUICKLIST_HEAD) &#123; quicklistPushHead(quicklist, value, sz); &#125; else if (where == QUICKLIST_TAIL) &#123; quicklistPushTail(quicklist, value, sz); &#125;&#125;/* Add new entry to head node of quicklist. * * Returns 0 if used existing head. * Returns 1 if new head created. */int quicklistPushHead(quicklist *quicklist, void *value, size_t sz) &#123; quicklistNode *orig_head = quicklist-&gt;head; if (likely( _quicklistNodeAllowInsert(quicklist-&gt;head, quicklist-&gt;fill, sz))) &#123; quicklist-&gt;head-&gt;zl = ziplistPush(quicklist-&gt;head-&gt;zl, value, sz, ZIPLIST_HEAD); quicklistNodeUpdateSz(quicklist-&gt;head); &#125; else &#123; quicklistNode *node = quicklistCreateNode(); node-&gt;zl = ziplistPush(ziplistNew(), value, sz, ZIPLIST_HEAD); quicklistNodeUpdateSz(node); _quicklistInsertNodeBefore(quicklist, quicklist-&gt;head, node); &#125; quicklist-&gt;count++; quicklist-&gt;head-&gt;count++; return (orig_head != quicklist-&gt;head);&#125;/* Add new entry to tail node of quicklist. * * Returns 0 if used existing tail. * Returns 1 if new tail created. */int quicklistPushTail(quicklist *quicklist, void *value, size_t sz) &#123; quicklistNode *orig_tail = quicklist-&gt;tail; if (likely( _quicklistNodeAllowInsert(quicklist-&gt;tail, quicklist-&gt;fill, sz))) &#123; quicklist-&gt;tail-&gt;zl = ziplistPush(quicklist-&gt;tail-&gt;zl, value, sz, ZIPLIST_TAIL); quicklistNodeUpdateSz(quicklist-&gt;tail); &#125; else &#123; quicklistNode *node = quicklistCreateNode(); node-&gt;zl = ziplistPush(ziplistNew(), value, sz, ZIPLIST_TAIL); quicklistNodeUpdateSz(node); _quicklistInsertNodeAfter(quicklist, quicklist-&gt;tail, node); &#125; quicklist-&gt;count++; quicklist-&gt;tail-&gt;count++; return (orig_tail != quicklist-&gt;tail);&#125; 不管是在头部还是尾部插入数据，都包含两种情况： 如果头节点（或尾节点）上ziplist大小没有超过限制（即_quicklistNodeAllowInsert返回1），那么新数据被直接插入到ziplist中（调用ziplistPush）。 如果头节点（或尾节点）上ziplist太大了，那么新创建一个quicklistNode节点（对应地也会新创建一个ziplist），然后把这个新创建的节点插入到quicklist双向链表中（调用_quicklistInsertNodeAfter）。 在_quicklistInsertNodeAfter的实现中，还会根据list-compress-depth的配置将里面的节点进行压缩。它的实现比较繁琐，我们这里就不展开讨论了。 quicklist的其它操作quicklist的操作较多，且实现细节都比较繁杂，这里就不一一分析源码了，我们简单介绍一些比较重要的操作。 quicklist的pop操作是调用quicklistPopCustom来实现的。quicklistPopCustom的实现过程基本上跟quicklistPush相反，先从头部或尾部节点的ziplist中把对应的数据项删除，如果在删除后ziplist为空了，那么对应的头部或尾部节点也要删除。删除后还可能涉及到里面节点的解压缩问题。 quicklist不仅实现了从头部或尾部插入，也实现了从任意指定的位置插入。quicklistInsertAfter和quicklistInsertBefore就是分别在指定位置后面和前面插入数据项。这种在任意指定位置插入数据的操作，情况比较复杂，有众多的逻辑分支。 当插入位置所在的ziplist大小没有超过限制时，直接插入到ziplist中就好了； 当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小没有超过限制，那么就转而插入到相邻的那个quicklist链表节点的ziplist中； 当插入位置所在的ziplist大小超过了限制，但插入的位置位于ziplist两端，并且相邻的quicklist链表节点的ziplist大小也超过限制，这时需要新创建一个quicklist链表节点插入。 对于插入位置所在的ziplist大小超过了限制的其它情况（主要对应于在ziplist中间插入数据的情况），则需要把当前ziplist分裂为两个节点，然后再其中一个节点上插入数据。 quicklistSetOptions用于设置ziplist大小配置参数（list-max-ziplist-size）和节点压缩深度配置参数（list-compress-depth）。代码比较简单，就是将相应的值分别设置给quicklist结构的fill字段和compress字段。 下一篇我们将介绍skiplist和它所支撑的Redis数据类型sorted set，敬请期待。 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！本文链接：http://zhangtielei.com/posts/blog-redis-quicklist.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现3：Redis内部数据结构详解——sds]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B03%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94sds%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言本文是《Redis内部数据结构详解》系列的第二篇，讲述Redis中使用最多的一个基础数据结构：sds。 不管在哪门编程语言当中，字符串都几乎是使用最多的数据结构。sds正是在Redis中被广泛使用的字符串结构，它的全称是Simple Dynamic String。与其它语言环境中出现的字符串相比，它具有如下显著的特点： 可动态扩展内存。sds表示的字符串其内容可以修改，也可以追加。在很多语言中字符串会分为mutable和immutable两种，显然sds属于mutable类型的。二进制安全（Binary Safe）。sds能存储任意二进制数据，而不仅仅是可打印字符。与传统的C语言字符串类型兼容。这个的含义接下来马上会讨论。看到这里，很多对Redis有所了解的同学可能已经产生了一个疑问：Redis已经对外暴露了一个字符串结构，叫做string，那这里所说的sds到底和string是什么关系呢？可能有人会猜：string是基于sds实现的。这个猜想已经非常接近事实，但在描述上还不太准确。有关string和sds之间关系的详细分析，我们放在后面再讲。现在为了方便讨论，让我们先暂时简单地认为，string的底层实现就是sds。 在讨论sds的具体实现之前，我们先站在Redis使用者的角度，来观察一下string所支持的一些主要操作。下面是一个操作示例： Redis string操作示例 以上这些操作都比较简单，我们简单解释一下： 初始的字符串的值设为”tielei”。第3步通过append命令对字符串进行了追加，变成了”tielei zhang”。然后通过setbit命令将第53个bit设置成了1。bit的偏移量从左边开始算，从0开始。其中第48～55bit是中间的空格那个字符，它的ASCII码是0x20。将第53个bit设置成1之后，它的ASCII码变成了0x24，打印出来就是’$’。因此，现在字符串的值变成了”tielei$zhang”。最后通过getrange取从倒数第5个字节到倒数第1个字节的内容，得到”zhang”。这些命令的实现，有一部分是和sds的实现有关的。下面我们开始详细讨论。 sds的数据结构定义我们知道，在C语言中，字符串是以’\0’字符结尾（NULL结束符）的字符数组来存储的，通常表达为字符指针的形式（char *）。它不允许字节0出现在字符串中间，因此，它不能用来存储任意的二进制数据。 我们可以在sds.h中找到sds的类型定义： typedef char *sds;肯定有人感到困惑了，竟然sds就等同于char *？我们前面提到过，sds和传统的C语言字符串保持类型兼容，因此它们的类型定义是一样的，都是char *。在有些情况下，需要传入一个C语言字符串的地方，也确实可以传入一个sds。但是，sds和char *并不等同。sds是Binary Safe的，它可以存储任意二进制数据，不能像C语言字符串那样以字符’\0’来标识字符串的结束，因此它必然有个长度字段。但这个长度字段在哪里呢？实际上sds还包含一个header结构： struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; };sds一共有5种类型的header。之所以有5种，是为了能让不同长度的字符串可以使用不同大小的header。这样，短字符串就能使用较小的header，从而节省内存。 一个sds字符串的完整结构，由在内存地址上前后相邻的两部分组成： 一个header。通常包含字符串的长度(len)、最大容量(alloc)和flags。sdshdr5有所不同。一个字符数组。这个字符数组的长度等于最大容量+1。真正有效的字符串数据，其长度通常小于最大容量。在真正的字符串数据之后，是空余未用的字节（一般以字节0填充），允许在不重新分配内存的前提下让字符串数据向后做有限的扩展。在真正的字符串数据之后，还有一个NULL结束符，即ASCII码为0的’\0’字符。这是为了和传统C字符串兼容。之所以字符数组的长度比最大容量多1个字节，就是为了在字符串长度达到最大容量时仍然有1个字节存放NULL结束符。除了sdshdr5之外，其它4个header的结构都包含3个字段： len: 表示字符串的真正长度（不包含NULL结束符在内）。 alloc: 表示字符串的最大容量（不包含最后多余的那个字节）。 flags: 总是占用一个字节。其中的最低3个bit用来表示header的类型。header的类型共有5种，在sds.h中有常量定义。 #define SDS_TYPE_5 0 #define SDS_TYPE_8 1 #define SDS_TYPE_16 2 #define SDS_TYPE_32 3 #define SDS_TYPE_64 4sds的数据结构，我们有必要非常仔细地去解析它。 Redis dict结构举例上图是sds的一个内部结构的例子。图中展示了两个sds字符串s1和s2的内存结构，一个使用sdshdr8类型的header，另一个使用sdshdr16类型的header。但它们都表达了同样的一个长度为6的字符串的值：”tielei”。下面我们结合代码，来解释每一部分的组成。 sds的字符指针（s1和s2）就是指向真正的数据（字符数组）开始的位置，而header位于内存地址较低的方向。在sds.h中有一些跟解析header有关的宏定义： #define SDS_TYPE_MASK 7 #define SDS_TYPE_BITS 3 #define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T))); #define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T)))) #define SDS_TYPE_5_LEN(f) ((f)&gt;&gt;SDS_TYPE_BITS)其中SDS_HDR用来从sds字符串获得header起始位置的指针，比如SDS_HDR(8, s1)表示s1的header指针，SDS_HDR(16, s2)表示s2的header指针。 当然，使用SDS_HDR之前我们必须先知道到底是哪一种header，这样我们才知道SDS_HDR第1个参数应该传什么。由sds字符指针获得header类型的方法是，先向低地址方向偏移1个字节的位置，得到flags字段。比如，s1[-1]和s2[-1]分别获得了s1和s2的flags的值。然后取flags的最低3个bit得到header的类型。 由于s1[-1] == 0x01 == SDS_TYPE_8，因此s1的header类型是sdshdr8。由于s2[-1] == 0x02 == SDS_TYPE_16，因此s2的header类型是sdshdr16。有了header指针，就能很快定位到它的len和alloc字段： s1的header中，len的值为0x06，表示字符串数据长度为6；alloc的值为0x80，表示字符数组最大容量为128。s2的header中，len的值为0x0006，表示字符串数据长度为6；alloc的值为0x03E8，表示字符数组最大容量为1000。（注意：图中是按小端地址构成）在各个header的类型定义中，还有几个需要我们注意的地方： 在各个header的定义中使用了attribute ((packed))，是为了让编译器以紧凑模式来分配内存。如果没有这个属性，编译器可能会为struct的字段做优化对齐，在其中填充空字节。那样的话，就不能保证header和sds的数据部分紧紧前后相邻，也不能按照固定向低地址方向偏移1个字节的方式来获取flags字段了。 在各个header的定义中最后有一个char buf[]。我们注意到这是一个没有指明长度的字符数组，这是C语言中定义字符数组的一种特殊写法，称为柔性数组（flexible array member），只能定义在一个结构体的最后一个字段上。 它在这里只是起到一个标记的作用，表示在flags字段后面就是一个字符数组，或者说，它指明了紧跟在flags字段后面的这个字符数组在结构体中的偏移位置。而程序在为header分配的内存的时候，它并不占用内存空间。 如果计算sizeof(struct sdshdr16)的值，那么结果是5个字节，其中没有buf字段。sdshdr5与其它几个header结构不同，它不包含alloc字段，而长度使用flags的高5位来存储。 因此，它不能为字符串分配空余空间。如果字符串需要动态增长，那么它就必然要重新分配内存才行。所以说，这种类型的sds字符串更适合存储静态的短字符串（长度小于32）。 至此，我们非常清楚地看到了：sds字符串的header，其实隐藏在真正的字符串数据的前面（低地址方向）。这样的一个定义，有如下几个好处： header和数据相邻，而不用分成两块内存空间来单独分配。这有利于减少内存碎片，提高存储效率（memory efficiency）。 虽然header有多个类型，但sds可以用统一的char *来表达。且它与传统的C语言字符串保持类型兼容。 如果一个sds里面存储的是可打印字符串，那么我们可以直接把它传给C函数，比如使用strcmp比较字符串大小，或者使用printf进行打印。弄清了sds的数据结构，它的具体操作函数就比较好理解了。 sds的一些基础函数 sdslen(const sds s): 获取sds字符串长度。 sdssetlen(sds s, size_t newlen): 设置sds字符串长度。 sdsinclen(sds s, size_t inc): 增加sds字符串长度。 sdsalloc(const sds s): 获取sds字符串容量。 sdssetalloc(sds s, size_t newlen): 设置sds字符串容量。 sdsavail(const sds s): 获取sds字符串空余空间（即alloc - len）。 sdsHdrSize(char type): 根据header类型得到header大小。 sdsReqType(size_t string_size):根据字符串数据长度计算所需要的header类型。这里我们挑选sdslen和sdsReqType的代码，察看一下。 static inline size_t sdslen(const sds s) { unsigned char flags = s[-1]; switch(flags&amp;SDS_TYPE_MASK) { case SDS_TYPE_5: return SDS_TYPE_5_LEN(flags); case SDS_TYPE_8: return SDS_HDR(8,s)-&gt;len; case SDS_TYPE_16: return SDS_HDR(16,s)-&gt;len; case SDS_TYPE_32: return SDS_HDR(32,s)-&gt;len; case SDS_TYPE_64: return SDS_HDR(64,s)-&gt;len; } return 0; } static inline char sdsReqType(size_t string_size) { if (string_size &lt; 1&lt;&lt;5) return SDS_TYPE_5; if (string_size &lt; 1&lt;&lt;8) return SDS_TYPE_8; if (string_size &lt; 1&lt;&lt;16) return SDS_TYPE_16; if (string_size &lt; 1ll&lt;&lt;32) return SDS_TYPE_32; return SDS_TYPE_64; }跟前面的分析类似，sdslen先用s[-1]向低地址方向偏移1个字节，得到flags；然后与SDS_TYPE_MASK进行按位与，得到header类型；然后根据不同的header类型，调用SDS_HDR得到header起始指针，进而获得len字段。 通过sdsReqType的代码，很容易看到： 长度在0和2^5-1之间，选用SDS_TYPE_5类型的header。长度在2^5和2^8-1之间，选用SDS_TYPE_8类型的header。长度在2^8和2^16-1之间，选用SDS_TYPE_16类型的header。长度在2^16和2^32-1之间，选用SDS_TYPE_32类型的header。长度大于2^32的，选用SDS_TYPE_64类型的header。能表示的最大长度为2^64-1。注：sdsReqType的实现代码，直到3.2.0，它在长度边界值上都一直存在问题，直到最近3.2 branch上的commit 6032340才修复。 sds的创建和销毁sds sdsnewlen(const void *init, size_t initlen) { void *sh; sds s; char type = sdsReqType(initlen); /* Empty strings are usually created in order to append. Use type 8 * since type 5 is not good at this. */ if (type == SDS_TYPE_5 &amp;&amp; initlen == 0) type = SDS_TYPE_8; int hdrlen = sdsHdrSize(type); unsigned char *fp; /* flags pointer. */ sh = s_malloc(hdrlen+initlen+1); if (!init) memset(sh, 0, hdrlen+initlen+1); if (sh == NULL) return NULL; s = (char*)sh+hdrlen; fp = ((unsigned char*)s)-1; switch(type) { case SDS_TYPE_5: { *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS); break; } case SDS_TYPE_8: { SDS_HDR_VAR(8,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; } case SDS_TYPE_16: { SDS_HDR_VAR(16,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; } case SDS_TYPE_32: { SDS_HDR_VAR(32,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; } case SDS_TYPE_64: { SDS_HDR_VAR(64,s); sh-&gt;len = initlen; sh-&gt;alloc = initlen; *fp = type; break; } } if (initlen &amp;&amp; init) memcpy(s, init, initlen); s[initlen] = &apos;\0&apos;; return s; } sds sdsempty(void) { return sdsnewlen(&quot;&quot;,0); } sds sdsnew(const char *init) { size_t initlen = (init == NULL) ? 0 : strlen(init); return sdsnewlen(init, initlen); } void sdsfree(sds s) { if (s == NULL) return; s_free((char*)s-sdsHdrSize(s[-1])); }sdsnewlen创建一个长度为initlen的sds字符串，并使用init指向的字符数组（任意二进制数据）来初始化数据。如果init为NULL，那么使用全0来初始化数据。它的实现中，我们需要注意的是： 如果要创建一个长度为0的空字符串，那么不使用SDS_TYPE_5类型的header，而是转而使用SDS_TYPE_8类型的header。这是因为创建的空字符串一般接下来的操作很可能是追加数据，但SDS_TYPE_5类型的sds字符串不适合追加数据（会引发内存重新分配）。需要的内存空间一次性进行分配，其中包含三部分：header、数据、最后的多余字节（hdrlen+initlen+1）。初始化的sds字符串数据最后会追加一个NULL结束符（s[initlen] = ‘\0’）。关于sdsfree，需要注意的是：内存要整体释放，所以要先计算出header起始指针，把它传给s_free函数。这个指针也正是在sdsnewlen中调用s_malloc返回的那个地址。 sds的连接（追加）操作sds sdscatlen(sds s, const void *t, size_t len) { size_t curlen = sdslen(s); s = sdsMakeRoomFor(s,len); if (s == NULL) return NULL; memcpy(s+curlen, t, len); sdssetlen(s, curlen+len); s[curlen+len] = &apos;\0&apos;; return s; } sds sdscat(sds s, const char *t) { return sdscatlen(s, t, strlen(t)); } sds sdscatsds(sds s, const sds t) { return sdscatlen(s, t, sdslen(t)); } sds sdsMakeRoomFor(sds s, size_t addlen) { void *sh, *newsh; size_t avail = sdsavail(s); size_t len, newlen; char type, oldtype = s[-1] &amp; SDS_TYPE_MASK; int hdrlen; /* Return ASAP if there is enough space left. */ if (avail &gt;= addlen) return s; len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); if (newlen &lt; SDS_MAX_PREALLOC) newlen *= 2; else newlen += SDS_MAX_PREALLOC; type = sdsReqType(newlen); /* Don&apos;t use type 5: the user is appending to the string and type 5 is * not able to remember empty space, so sdsMakeRoomFor() must be called * at every appending operation. */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; hdrlen = sdsHdrSize(type); if (oldtype==type) { newsh = s_realloc(sh, hdrlen+newlen+1); if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; } else { /* Since the header size changes, need to move the string forward, * and can&apos;t use realloc */ newsh = s_malloc(hdrlen+newlen+1); if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); s_free(sh); s = (char*)newsh+hdrlen; s[-1] = type; sdssetlen(s, len); } sdssetalloc(s, newlen); return s; }sdscatlen将t指向的长度为len的任意二进制数据追加到sds字符串s的后面。本文开头演示的string的append命令，内部就是调用sdscatlen来实现的。 在sdscatlen的实现中，先调用sdsMakeRoomFor来保证字符串s有足够的空间来追加长度为len的数据。sdsMakeRoomFor可能会分配新的内存，也可能不会。 sdsMakeRoomFor是sds实现中很重要的一个函数。关于它的实现代码，我们需要注意的是： 如果原来字符串中的空余空间够用（avail &gt;= addlen），那么它什么也不做，直接返回。 如果需要分配空间，它会比实际请求的要多分配一些，以防备接下来继续追加。它在字符串已经比较长的情况下要至少多分配SDS_MAX_PREALLOC个字节，这个常量在sds.h中定义为(1024*1024)=1MB。 按分配后的空间大小，可能需要更换header类型（原来header的alloc字段太短，表达不了增加后的容量）。 如果需要更换header，那么整个字符串空间（包括header）都需要重新分配（s_malloc），并拷贝原来的数据到新的位置。 如果不需要更换header（原来的header够用），那么调用一个比较特殊的s_realloc，试图在原来的地址上重新分配空间。s_realloc的具体实现得看Redis编译的时候选用了哪个allocator（在Linux上默认使用jemalloc）。 但不管是哪个realloc的实现，它所表达的含义基本是相同的：它尽量在原来分配好的地址位置重新分配，如果原来的地址位置有足够的空余空间完成重新分配，那么它返回的新地址与传入的旧地址相同；否则，它分配新的地址块，并进行数据搬迁。参见http://man.cx/realloc。 从sdscatlen的函数接口，我们可以看到一种使用模式：调用它的时候，传入一个旧的sds变量，然后它返回一个新的sds变量。由于它的内部实现可能会造成地址变化，因此调用者在调用完之后，原来旧的变量就失效了，而都应该用新返回的变量来替换。不仅仅是sdscatlen函数，sds中的其它函数（比如sdscpy、sdstrim、sdsjoin等），还有Redis中其它一些能自动扩展内存的数据结构（如ziplist），也都是同样的使用模式。 浅谈sds与string的关系现在我们回过头来看看本文开头给出的string操作的例子。 append操作使用sds的sdscatlen来实现。前面已经提到。 setbit和getrange都是先根据key取到整个sds字符串，然后再从字符串选取或修改指定的部分。由于sds就是一个字符数组，所以对它的某一部分进行操作似乎都比较简单。 但是，string除了支持这些操作之外，当它存储的值是个数字的时候，它还支持incr、decr等操作。那么，当string存储数字值的时候，它的内部存储还是sds吗？ 实际上，不是了。而且，这种情况下，setbit和getrange的实现也会有所不同。这些细节，我们放在下一篇介绍robj的时候再进行系统地讨论。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现1：Redis 的基础数据结构概览]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B04%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94ziplist%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文是《Redis内部数据结构详解》系列的第四篇。在本文中，我们首先介绍一个新的Redis内部数据结构——ziplist，然后在文章后半部分我们会讨论一下在robj, dict和ziplist的基础上，Redis对外暴露的hash结构是怎样构建起来的。 我们在讨论中还会涉及到两个Redis配置（在redis.conf中的ADVANCED CONFIG部分）： 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 本文的后半部分会对这两个配置做详细的解释。 什么是ziplistRedis官方对于ziplist的定义是（出自ziplist.c的文件头部注释）： The ziplist is a specially encoded dually linked list that is designed to be very memory efficient. It stores both strings and integer values, where integers are encoded as actual integers instead of a series of characters. It allows push and pop operations on either side of the list in O(1) time. 翻译一下就是说：ziplist是一个经过特殊编码的双向链表，它的设计目标就是为了提高存储效率。ziplist可以用于存储字符串或整数，其中整数是按真正的二进制表示进行编码的，而不是编码成字符串序列。它能以O(1)的时间复杂度在表的两端提供push和pop操作。 实际上，ziplist充分体现了Redis对于存储效率的追求。一个普通的双向链表，链表中每一项都占用独立的一块内存，各项之间用地址指针（或引用）连接起来。这种方式会带来大量的内存碎片，而且地址指针也会占用额外的内存。而ziplist却是将表中每一项存放在前后连续的地址空间内，一个ziplist整体占用一大块内存。它是一个表（list），但其实不是一个链表（linked list）。 另外，ziplist为了在细节上节省内存，对于值的存储采用了变长的编码方式，大概意思是说，对于大的整数，就多用一些字节来存储，而对于小的整数，就少用一些字节来存储。我们接下来很快就会讨论到这些实现细节。 ziplist的数据结构定义ziplist的数据结构组成是本文要讨论的重点。实际上，ziplist还是稍微有点复杂的，它复杂的地方就在于它的数据结构定义。一旦理解了数据结构，它的一些操作也就比较容易理解了。 我们接下来先从总体上介绍一下ziplist的数据结构定义，然后举一个实际的例子，通过例子来解释ziplist的构成。如果你看懂了这一部分，本文的任务就算完成了一大半了。 从宏观上看，ziplist的内存结构如下： &lt;zlbytes&gt;&lt;zltail&gt;&lt;zllen&gt;&lt;entry&gt;...&lt;entry&gt;&lt;zlend&gt; 各个部分在内存上是前后相邻的，它们分别的含义如下： &lt;zlbytes&gt;: 32bit，表示ziplist占用的字节总数（也包括&lt;zlbytes&gt;本身占用的4个字节）。 &lt;zltail&gt;: 32bit，表示ziplist表中最后一项（entry）在ziplist中的偏移字节数。&lt;zltail&gt;的存在，使得我们可以很方便地找到最后一项（不用遍历整个ziplist），从而可以在ziplist尾端快速地执行push或pop操作。 &lt;zllen&gt;: 16bit， 表示ziplist中数据项（entry）的个数。zllen字段因为只有16bit，所以可以表达的最大值为2^16-1。这里需要特别注意的是，如果ziplist中数据项个数超过了16bit能表达的最大值，ziplist仍然可以来表示。那怎么表示呢？这里做了这样的规定：如果&lt;zllen&gt;小于等于2^16-2（也就是不等于2^16-1），那么&lt;zllen&gt;就表示ziplist中数据项的个数；否则，也就是&lt;zllen&gt;等于16bit全为1的情况，那么&lt;zllen&gt;就不表示数据项个数了，这时候要想知道ziplist中数据项总数，那么必须对ziplist从头到尾遍历各个数据项，才能计数出来。 &lt;entry&gt;: 表示真正存放数据的数据项，长度不定。一个数据项（entry）也有它自己的内部结构，这个稍后再解释。 &lt;zlend&gt;: ziplist最后1个字节，是一个结束标记，值固定等于255。 上面的定义中还值得注意的一点是：&lt;zlbytes&gt;, &lt;zltail&gt;, &lt;zllen&gt;既然占据多个字节，那么在存储的时候就有大端（big endian）和小端（little endian）的区别。ziplist采取的是小端模式来存储，这在下面我们介绍具体例子的时候还会再详细解释。 我们再来看一下每一个数据项&lt;entry&gt;的构成： &lt;prevrawlen&gt;&lt;len&gt;&lt;data&gt; 我们看到在真正的数据（&lt;data&gt;）前面，还有两个字段： &lt;prevrawlen&gt;: 表示前一个数据项占用的总字节数。这个字段的用处是为了让ziplist能够从后向前遍历（从后一项的位置，只需向前偏移prevrawlen个字节，就找到了前一项）。这个字段采用变长编码。 &lt;len&gt;: 表示当前数据项的数据长度（即&lt;data&gt;部分的长度）。也采用变长编码。 那么&lt;prevrawlen&gt;和&lt;len&gt;是怎么进行变长编码的呢？各位读者打起精神了，我们终于讲到了ziplist的定义中最繁琐的地方了。 先说&lt;prevrawlen&gt;。它有两种可能，或者是1个字节，或者是5个字节： 如果前一个数据项占用字节数小于254，那么&lt;prevrawlen&gt;就只用一个字节来表示，这个字节的值就是前一个数据项的占用字节数。 如果前一个数据项占用字节数大于等于254，那么&lt;prevrawlen&gt;就用5个字节来表示，其中第1个字节的值是254（作为这种情况的一个标记），而后面4个字节组成一个整型值，来真正存储前一个数据项的占用字节数。 有人会问了，为什么没有255的情况呢？ 这是因为：255已经定义为ziplist结束标记&lt;zlend&gt;的值了。在ziplist的很多操作的实现中，都会根据数据项的第1个字节是不是255来判断当前是不是到达ziplist的结尾了，因此一个正常的数据的第1个字节（也就是&lt;prevrawlen&gt;的第1个字节）是不能够取255这个值的，否则就冲突了。 而&lt;len&gt;字段就更加复杂了，它根据第1个字节的不同，总共分为9种情况（下面的表示法是按二进制表示）： |00pppppp| - 1 byte。第1个字节最高两个bit是00，那么&lt;len&gt;字段只有1个字节，剩余的6个bit用来表示长度值，最高可以表示63 (2^6-1)。 |01pppppp|qqqqqqqq| - 2 bytes。第1个字节最高两个bit是01，那么&lt;len&gt;字段占2个字节，总共有14个bit用来表示长度值，最高可以表示16383 (2^14-1)。 |10__|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt| - 5 bytes。第1个字节最高两个bit是10，那么len字段占5个字节，总共使用32个bit来表示长度值（6个bit舍弃不用），最高可以表示2^32-1。需要注意的是：在前三种情况下，&lt;data&gt;都是按字符串来存储的；从下面第4种情况开始，&lt;data&gt;开始变为按整数来存储了。 |11000000| - 1 byte。&lt;len&gt;字段占用1个字节，值为0xC0，后面的数据&lt;data&gt;存储为2个字节的int16_t类型。 |11010000| - 1 byte。&lt;len&gt;字段占用1个字节，值为0xD0，后面的数据&lt;data&gt;存储为4个字节的int32_t类型。 |11100000| - 1 byte。&lt;len&gt;字段占用1个字节，值为0xE0，后面的数据&lt;data&gt;存储为8个字节的int64_t类型。 |11110000| - 1 byte。&lt;len&gt;字段占用1个字节，值为0xF0，后面的数据&lt;data&gt;存储为3个字节长的整数。 |11111110| - 1 byte。&lt;len&gt;字段占用1个字节，值为0xFE，后面的数据&lt;data&gt;存储为1个字节的整数。 |1111xxxx| - - (xxxx的值在0001和1101之间)。这是一种特殊情况，xxxx从1到13一共13个值，这时就用这13个值来表示真正的数据。注意，这里是表示真正的数据，而不是数据长度了。也就是说，在这种情况下，后面不再需要一个单独的&lt;data&gt;字段来表示真正的数据了，而是&lt;len&gt;和&lt;data&gt;合二为一了。另外，由于xxxx只能取0001和1101这13个值了（其它可能的值和其它情况冲突了，比如0000和1110分别同前面第7种第8种情况冲突，1111跟结束标记冲突），而小数值应该从0开始，因此这13个值分别表示0到12，即xxxx的值减去1才是它所要表示的那个整数数据的值。 好了，ziplist的数据结构定义，我们介绍完了，现在我们看一个具体的例子。 上图是一份真实的ziplist数据。我们逐项解读一下： 这个ziplist一共包含33个字节。字节编号从byte[0]到byte[32]。图中每个字节的值使用16进制表示。 头4个字节（0x21000000）是按小端（little endian）模式存储的&lt;zlbytes&gt;字段。什么是小端呢？就是指数据的低字节保存在内存的低地址中（参见维基百科词条Endianness）。因此，这里&lt;zlbytes&gt;的值应该解析成0x00000021，用十进制表示正好就是33。 接下来4个字节（byte[4..7]）是&lt;zltail&gt;，用小端存储模式来解释，它的值是0x0000001D（值为29），表示最后一个数据项在byte[29]的位置（那个数据项为0x05FE14）。 再接下来2个字节（byte[8..9]），值为0x0004，表示这个ziplist里一共存有4项数据。 接下来6个字节（byte[10..15]）是第1个数据项。其中，prevrawlen=0，因为它前面没有数据项；len=4，相当于前面定义的9种情况中的第1种，表示后面4个字节按字符串存储数据，数据的值为”name”。 接下来8个字节（byte[16..23]）是第2个数据项，与前面数据项存储格式类似，存储1个字符串”tielei”。 接下来5个字节（byte[24..28]）是第3个数据项，与前面数据项存储格式类似，存储1个字符串”age”。 接下来3个字节（byte[29..31]）是最后一个数据项，它的格式与前面的数据项存储格式不太一样。其中，第1个字节prevrawlen=5，表示前一个数据项占用5个字节；第2个字节=FE，相当于前面定义的9种情况中的第8种，所以后面还有1个字节用来表示真正的数据，并且以整数表示。它的值是20（0x14）。 最后1个字节（byte[32]）表示&lt;zlend&gt;，是固定的值255（0xFF）。 总结一下，这个ziplist里存了4个数据项，分别为： 字符串: “name” 字符串: “tielei” 字符串: “age” 整数: 20 （好吧，被你发现了~~tielei实际上当然不是20岁，他哪有那么年轻啊……） 实际上，这个ziplist是通过两个hset命令创建出来的。这个我们后半部分会再提到。 好了，既然你已经阅读到这里了，说明你还是很有耐心的（其实我写到这里也已经累得不行了）。可以先把本文收藏，休息一下，回头再看后半部分。 接下来我要贴一些代码了。 ziplist的接口我们先不着急看实现，先来挑几个ziplist的重要的接口，看看它们长什么样子： 12345678910unsigned char *ziplistNew(void);unsigned char *ziplistMerge(unsigned char **first, unsigned char **second);unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where);unsigned char *ziplistIndex(unsigned char *zl, int index);unsigned char *ziplistNext(unsigned char *zl, unsigned char *p);unsigned char *ziplistPrev(unsigned char *zl, unsigned char *p);unsigned char *ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen);unsigned char *ziplistDelete(unsigned char *zl, unsigned char **p);unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip);unsigned int ziplistLen(unsigned char *zl); 我们从这些接口的名字就可以粗略猜出它们的功能，下面简单解释一下： ziplist的数据类型，没有用自定义的struct之类的来表达，而就是简单的unsigned char *。这是因为ziplist本质上就是一块连续内存，内部组成结构又是一个高度动态的设计（变长编码），也没法用一个固定的数据结构来表达。 ziplistNew: 创建一个空的ziplist（只包含&lt;zlbytes&gt;&lt;zltail&gt;&lt;zllen&gt;&lt;zlend&gt;）。 ziplistMerge: 将两个ziplist合并成一个新的ziplist。 ziplistPush: 在ziplist的头部或尾端插入一段数据（产生一个新的数据项）。注意一下这个接口的返回值，是一个新的ziplist。调用方必须用这里返回的新的ziplist，替换之前传进来的旧的ziplist变量，而经过这个函数处理之后，原来旧的ziplist变量就失效了。为什么一个简单的插入操作会导致产生一个新的ziplist呢？这是因为ziplist是一块连续空间，对它的追加操作，会引发内存的realloc，因此ziplist的内存位置可能会发生变化。实际上，我们在之前介绍sds的文章中提到过类似这种接口使用模式（参见sdscatlen函数的说明）。 ziplistIndex: 返回index参数指定的数据项的内存位置。index可以是负数，表示从尾端向前进行索引。 ziplistNext和ziplistPrev分别返回一个ziplist中指定数据项p的后一项和前一项。 ziplistInsert: 在ziplist的任意数据项前面插入一个新的数据项。 ziplistDelete: 删除指定的数据项。 ziplistFind: 查找给定的数据（由vstr和vlen指定）。注意它有一个skip参数，表示查找的时候每次比较之间要跳过几个数据项。为什么会有这么一个参数呢？其实这个参数的主要用途是当用ziplist表示hash结构的时候，是按照一个field，一个value来依次存入ziplist的。也就是说，偶数索引的数据项存field，奇数索引的数据项存value。当按照field的值进行查找的时候，就需要把奇数项跳过去。 ziplistLen: 计算ziplist的长度（即包含数据项的个数）。 ziplist的插入逻辑解析ziplist的相关接口的具体实现，还是有些复杂的，限于篇幅的原因，我们这里只结合代码来讲解插入的逻辑。插入是很有代表性的操作，通过这部分来一窥ziplist内部的实现，其它部分的实现我们也就会很容易理解了。 ziplistPush和ziplistInsert都是插入，只是对于插入位置的限定不同。它们在内部实现都依赖一个名为__ziplistInsert的内部函数，其代码如下（出自ziplist.c）: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889static unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) &#123; size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen; unsigned int prevlensize, prevlen = 0; size_t offset; int nextdiff = 0; unsigned char encoding = 0; long long value = 123456789; /* initialized to avoid warning. Using a value that is easy to see if for some reason we use it uninitialized. */ zlentry tail; /* Find out prevlen for the entry that is inserted. */ if (p[0] != ZIP_END) &#123; ZIP_DECODE_PREVLEN(p, prevlensize, prevlen); &#125; else &#123; unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl); if (ptail[0] != ZIP_END) &#123; prevlen = zipRawEntryLength(ptail); &#125; &#125; /* See if the entry can be encoded */ if (zipTryEncoding(s,slen,&amp;value,&amp;encoding)) &#123; /* &apos;encoding&apos; is set to the appropriate integer encoding */ reqlen = zipIntSize(encoding); &#125; else &#123; /* &apos;encoding&apos; is untouched, however zipEncodeLength will use the * string length to figure out how to encode it. */ reqlen = slen; &#125; /* We need space for both the length of the previous entry and * the length of the payload. */ reqlen += zipPrevEncodeLength(NULL,prevlen); reqlen += zipEncodeLength(NULL,encoding,slen); /* When the insert position is not equal to the tail, we need to * make sure that the next entry can hold this entry&apos;s length in * its prevlen field. */ nextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0; /* Store offset because a realloc may change the address of zl. */ offset = p-zl; zl = ziplistResize(zl,curlen+reqlen+nextdiff); p = zl+offset; /* Apply memory move when necessary and update tail offset. */ if (p[0] != ZIP_END) &#123; /* Subtract one because of the ZIP_END bytes */ memmove(p+reqlen,p-nextdiff,curlen-offset-1+nextdiff); /* Encode this entry&apos;s raw length in the next entry. */ zipPrevEncodeLength(p+reqlen,reqlen); /* Update offset for tail */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+reqlen); /* When the tail contains more than one entry, we need to take * &quot;nextdiff&quot; in account as well. Otherwise, a change in the * size of prevlen doesn&apos;t have an effect on the *tail* offset. */ zipEntry(p+reqlen, &amp;tail); if (p[reqlen+tail.headersize+tail.len] != ZIP_END) &#123; ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff); &#125; &#125; else &#123; /* This element will be the new tail. */ ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(p-zl); &#125; /* When nextdiff != 0, the raw length of the next entry has changed, so * we need to cascade the update throughout the ziplist */ if (nextdiff != 0) &#123; offset = p-zl; zl = __ziplistCascadeUpdate(zl,p+reqlen); p = zl+offset; &#125; /* Write the entry */ p += zipPrevEncodeLength(p,prevlen); p += zipEncodeLength(p,encoding,slen); if (ZIP_IS_STR(encoding)) &#123; memcpy(p,s,slen); &#125; else &#123; zipSaveInteger(p,value,encoding); &#125; ZIPLIST_INCR_LENGTH(zl,1); return zl;&#125; 我们来简单解析一下这段代码： 这个函数是在指定的位置p插入一段新的数据，待插入数据的地址指针是s，长度为slen。插入后形成一个新的数据项，占据原来p的配置，原来位于p位置的数据项以及后面的所有数据项，需要统一向后移动，给新插入的数据项留出空间。参数p指向的是ziplist中某一个数据项的起始位置，或者在向尾端插入的时候，它指向ziplist的结束标记&lt;zlend&gt;。 函数开始先计算出待插入位置前一个数据项的长度prevlen。这个长度要存入新插入的数据项的&lt;prevrawlen&gt;字段。 然后计算当前数据项占用的总字节数reqlen，它包含三部分：&lt;prevrawlen&gt;, &lt;len&gt;和真正的数据。其中的数据部分会通过调用zipTryEncoding先来尝试转成整数。 由于插入导致的ziplist对于内存的新增需求，除了待插入数据项占用的reqlen之外，还要考虑原来p位置的数据项（现在要排在待插入数据项之后）的&lt;prevrawlen&gt;字段的变化。本来它保存的是前一项的总长度，现在变成了保存当前插入的数据项的总长度。这样它的&lt;prevrawlen&gt;字段本身需要的存储空间也可能发生变化，这个变化可能是变大也可能是变小。这个变化了多少的值nextdiff，是调用zipPrevLenByteDiff计算出来的。如果变大了，nextdiff是正值，否则是负值。 现在很容易算出来插入后新的ziplist需要多少字节了，然后调用ziplistResize来重新调整大小。ziplistResize的实现里会调用allocator的zrealloc，它有可能会造成数据拷贝。 现在额外的空间有了，接下来就是将原来p位置的数据项以及后面的所有数据都向后挪动，并为它设置新的&lt;prevrawlen&gt;字段。此外，还可能需要调整ziplist的&lt;zltail&gt;字段。 最后，组装新的待插入数据项，放在位置p。 hash与ziplisthash是Redis中可以用来存储一个对象结构的比较理想的数据类型。一个对象的各个属性，正好对应一个hash结构的各个field。 我们在网上很容易找到这样一些技术文章，它们会说存储一个对象，使用hash比string要节省内存。实际上这么说是有前提的，具体取决于对象怎么来存储。如果你把对象的多个属性存储到多个key上（各个属性值存成string），当然占的内存要多。但如果你采用一些序列化方法，比如Protocol Buffers，或者Apache Thrift，先把对象序列化为字节数组，然后再存入到Redis的string中，那么跟hash相比，哪一种更省内存，就不一定了。 当然，hash比序列化后再存入string的方式，在支持的操作命令上，还是有优势的：它既支持多个field同时存取（hmset/hmget），也支持按照某个特定的field单独存取（hset/hget）。 实际上，hash随着数据的增大，其底层数据结构的实现是会发生变化的，当然存储效率也就不同。在field比较少，各个value值也比较小的时候，hash采用ziplist来实现；而随着field增多和value值增大，hash可能会变成dict来实现。当hash底层变成dict来实现的时候，它的存储效率就没法跟那些序列化方式相比了。 当我们为某个key第一次执行 hset key field value 命令的时候，Redis会创建一个hash结构，这个新创建的hash底层就是一个ziplist。 123456robj *createHashObject(void) &#123; unsigned char *zl = ziplistNew(); robj *o = createObject(OBJ_HASH, zl); o-&gt;encoding = OBJ_ENCODING_ZIPLIST; return o;&#125; 上面的createHashObject函数，出自object.c，它负责的任务就是创建一个新的hash结构。可以看出，它创建了一个type = OBJ_HASH但encoding = OBJ_ENCODING_ZIPLIST的robj对象。 实际上，本文前面给出的那个ziplist实例，就是由如下两个命令构建出来的。 12hset user:100 name tieleihset user:100 age 20 每执行一次hset命令，插入的field和value分别作为一个新的数据项插入到ziplist中（即每次hset产生两个数据项）。 当随着数据的插入，hash底层的这个ziplist就可能会转成dict。那么到底插入多少才会转呢？ 还记得本文开头提到的两个Redis配置吗？ 12hash-max-ziplist-entries 512hash-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成dict： 当hash中的数据项（即field-value对）的数目超过512的时候，也就是ziplist数据项超过1024的时候（请参考t_hash.c中的hashTypeSet函数）。 当hash中插入的任意一个value的长度超过了64的时候（请参考t_hash.c中的hashTypeTryConversion函数）。 Redis的hash之所以这样设计，是因为当ziplist变得很大的时候，它有如下几个缺点： 每次插入或修改引发的realloc操作会有更大的概率造成内存拷贝，从而降低性能。 一旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更大的一块数据。 当ziplist数据项过多的时候，在它上面查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历。 总之，ziplist本来就设计为各个数据项挨在一起组成连续的内存空间，这种结构并不擅长做修改操作。一旦数据发生改动，就会引发内存realloc，可能导致内存拷贝。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现2：Redis内部数据结构详解——dict]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B02%EF%BC%9ARedis%E5%86%85%E9%83%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%E2%80%94%E2%80%94dict%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 如果你使用过Redis，一定会像我一样对它的内部实现产生兴趣。《Redis内部数据结构详解》是我准备写的一个系列，也是我个人对于之前研究Redis的一个阶段性总结，着重讲解Redis在内存中的数据结构实现（暂不涉及持久化的话题）。Redis本质上是一个数据结构服务器（data structures server），以高效的方式实现了多种现成的数据结构，研究它的数据结构和基于其上的算法，对于我们自己提升局部算法的编程水平有很重要的参考意义。 当我们在本文中提到Redis的“数据结构”，可能是在两个不同的层面来讨论它。 第一个层面，是从使用者的角度。比如： string list hash set sorted set 这一层面也是Redis暴露给外部的调用接口。 第二个层面，是从内部实现的角度，属于更底层的实现。比如： dict sds ziplist quicklist skiplist 第一个层面的“数据结构”，Redis的官方文档(http://redis.io/topics/data-types-intro)有详细的介绍。本文的重点在于讨论第二个层面，Redis数据结构的内部实现，以及这两个层面的数据结构之间的关系：Redis如何通过组合第二个层面的各种基础数据结构来实现第一个层面的更高层的数据结构。 在讨论任何一个系统的内部实现的时候，我们都要先明确它的设计原则，这样我们才能更深刻地理解它为什么会进行如此设计的真正意图。在本文接下来的讨论中，我们主要关注以下几点： 存储效率（memory efficiency）。Redis是专用于存储数据的，它对于计算机资源的主要消耗就在于内存，因此节省内存是它非常非常重要的一个方面。这意味着Redis一定是非常精细地考虑了压缩数据、减少内存碎片等问题。 快速响应时间（fast response time）。与快速响应时间相对的，是高吞吐量（high throughput）。Redis是用于提供在线访问的，对于单个请求的响应时间要求很高，因此，快速响应时间是比高吞吐量更重要的目标。有时候，这两个目标是矛盾的。 单线程（single-threaded）。Redis的性能瓶颈不在于CPU资源，而在于内存访问和网络IO。而采用单线程的设计带来的好处是，极大简化了数据结构和算法的实现。相反，Redis通过异步IO和pipelining等机制来实现高速的并发访问。显然，单线程的设计，对于单个请求的快速响应时间也提出了更高的要求。 本文是《Redis内部数据结构详解》系列的第一篇，讲述Redis一个重要的基础数据结构：dict。 dict是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。Redis的一个database中所有key到value的映射，就是使用一个dict来维护的。不过，这只是它在Redis中的一个用途而已，它在Redis中被使用的地方还有很多。比如，一个Redis hash结构，当它的field较多时，便会采用dict来存储。再比如，Redis配合使用dict和skiplist来共同维护一个sorted set。这些细节我们后面再讨论，在本文中，我们集中精力讨论dict本身的实现。 dict本质上是为了解决算法中的查找问题（Searching），一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。我们平常使用的各种Map或dictionary，大都是基于哈希表实现的。在不要求数据有序存储，且能保持较低的哈希值冲突概率的前提下，基于哈希表的查找性能能做到非常高效，接近O(1)，而且实现简单。 在Redis中，dict也是一个基于哈希表的算法。和传统的哈希算法类似，它采用某个哈希函数从key计算得到在哈希表中的位置，采用拉链法解决冲突，并在装载因子（load factor）超过预定值时自动扩展内存，引发重哈希（rehashing）。Redis的dict实现最显著的一个特点，就在于它的重哈希。它采用了一种称为增量式重哈希（incremental rehashing）的方法，在需要扩展内存时避免一次性对所有key进行重哈希，而是将重哈希操作分散到对于dict的各个增删改查的操作中去。这种方法能做到每次只对一小部分key进行重哈希，而每次重哈希之间不影响dict的操作。dict之所以这样设计，是为了避免重哈希期间单个请求的响应时间剧烈增加，这与前面提到的“快速响应时间”的设计原则是相符的。 下面进行详细介绍。 dict的数据结构定义为了实现增量式重哈希（incremental rehashing），dict的数据结构里包含两个哈希表。在重哈希期间，数据从第一个哈希表向第二个哈希表迁移。 dict的C代码定义如下（出自Redis源码dict.h）： typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; } dictEntry; typedef struct dictType { unsigned int (*hashFunction)(const void *key); void *(*keyDup)(void *privdata, const void *key); void *(*valDup)(void *privdata, const void *obj); int (*keyCompare)(void *privdata, const void *key1, const void *key2); void (*keyDestructor)(void *privdata, void *key); void (*valDestructor)(void *privdata, void *obj); } dictType; /* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */ typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used; } dictht; typedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ int iterators; /* number of iterators currently running */ } dict;为了能更清楚地展示dict的数据结构定义，我们用一张结构图来表示它。如下。 结合上面的代码和结构图，可以很清楚地看出dict的结构。一个dict由如下若干项组成： 一个指向dictType结构的指针（type）。它通过自定义的方式使得dict的key和value能够存储任何类型的数据。 一个私有数据指针（privdata）。由调用者在创建dict的时候传进来。 两个哈希表（ht[2]）。只有在重哈希的过程中，ht[0]和ht[1]才都有效。而在平常情况下，只有ht[0]有效，ht[1]里面没有任何数据。上图表示的就是重哈希进行到中间某一步时的情况。 当前重哈希索引（rehashidx）。如果rehashidx = -1，表示当前没有在重哈希过程中；否则，表示当前正在进行重哈希，且它的值记录了当前重哈希进行到哪一步了。 当前正在进行遍历的iterator的个数。这不是我们现在讨论的重点，暂时忽略。 dictType结构包含若干函数指针，用于dict的调用者对涉及key和value的各种操作进行自定义。这些操作包含： hashFunction，对key进行哈希值计算的哈希算法。 keyDup和valDup，分别定义key和value的拷贝函数，用于在需要的时候对key和value进行深拷贝，而不仅仅是传递对象指针。 keyCompare，定义两个key的比较操作，在根据key进行查找时会用到。 keyDestructor和valDestructor，分别定义对key和value的析构函数。 私有数据指针（privdata）就是在dictType的某些操作被调用时会传回给调用者。 需要详细察看的是dictht结构。它定义一个哈希表的结构，由如下若干项组成： 一个dictEntry指针数组（table）。key的哈希值最终映射到这个数组的某个位置上（对应一个bucket）。如果多个key映射到同一个位置，就发生了冲突，那么就拉出一个dictEntry链表。 size：标识dictEntry指针数组的长度。它总是2的指数。 sizemask：用于将哈希值映射到table的位置索引。它的值等于(size-1)，比如7, 15, 31, 63，等等，也就是用二进制表示的各个bit全1的数字。每个key先经过hashFunction计算得到一个哈希值，然后计算(哈希值 &amp; sizemask)得到在table上的位置。相当于计算取余(哈希值 % size)。 used：记录dict中现有的数据个数。它与size的比值就是装载因子（load factor）。这个比值越大，哈希值冲突概率越高。 dictEntry结构中包含k, v和指向链表下一项的next指针。k是void指针，这意味着它可以指向任何类型。v是个union，当它的值是uint64_t、int64_t或double类型时，就不再需要额外的存储，这有利于减少内存碎片。当然，v也可以是void指针，以便能存储任何类型的数据。 dict的创建（dictCreate）dict *dictCreate(dictType *type, void *privDataPtr) { dict *d = zmalloc(sizeof(*d)); _dictInit(d,type,privDataPtr); return d; } int _dictInit(dict *d, dictType *type, void *privDataPtr) { _dictReset(&amp;d-&gt;ht[0]); _dictReset(&amp;d-&gt;ht[1]); d-&gt;type = type; d-&gt;privdata = privDataPtr; d-&gt;rehashidx = -1; d-&gt;iterators = 0; return DICT_OK; } static void _dictReset(dictht *ht) { ht-&gt;table = NULL; ht-&gt;size = 0; ht-&gt;sizemask = 0; ht-&gt;used = 0; }dictCreate为dict的数据结构分配空间并为各个变量赋初值。其中两个哈希表ht[0]和ht[1]起始都没有分配空间，table指针都赋为NULL。这意味着要等第一个数据插入时才会真正分配空间。 dict的查找（dictFind）#define dictIsRehashing(d) ((d)-&gt;rehashidx != -1) dictEntry *dictFind(dict *d, const void *key) { dictEntry *he; unsigned int h, idx, table; if (d-&gt;ht[0].used + d-&gt;ht[1].used == 0) return NULL; /* dict is empty */ if (dictIsRehashing(d)) _dictRehashStep(d); h = dictHashKey(d, key); for (table = 0; table &lt;= 1; table++) { idx = h &amp; d-&gt;ht[table].sizemask; he = d-&gt;ht[table].table[idx]; while(he) { if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) return he; he = he-&gt;next; } if (!dictIsRehashing(d)) return NULL; } return NULL; }上述dictFind的源码，根据dict当前是否正在重哈希，依次做了这么几件事： 如果当前正在进行重哈希，那么将重哈希过程向前推进一步（即调用_dictRehashStep）。实际上，除了查找，插入和删除也都会触发这一动作。这就将重哈希过程分散到各个查找、插入和删除操作中去了，而不是集中在某一个操作中一次性做完。 计算key的哈希值（调用dictHashKey，里面的实现会调用前面提到的hashFunction）。 先在第一个哈希表ht[0]上进行查找。在table数组上定位到哈希值对应的位置（如前所述，通过哈希值与sizemask进行按位与），然后在对应的dictEntry链表上进行查找。查找的时候需要对key进行比较，这时候调用dictCompareKeys，它里面的实现会调用到前面提到的keyCompare。如果找到就返回该项。否则，进行下一步。 判断当前是否在重哈希，如果没有，那么在ht[0]上的查找结果就是最终结果（没找到，返回NULL）。否则，在ht[1]上进行查找（过程与上一步相同）。 下面我们有必要看一下增量式重哈希的_dictRehashStep的实现。 static void _dictRehashStep(dict *d) { if (d-&gt;iterators == 0) dictRehash(d,1); } int dictRehash(dict *d, int n) { int empty_visits = n*10; /* Max number of empty buckets to visit. */ if (!dictIsRehashing(d)) return 0; while(n-- &amp;&amp; d-&gt;ht[0].used != 0) { dictEntry *de, *nextde; /* Note that rehashidx can&apos;t overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx); while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) { d-&gt;rehashidx++; if (--empty_visits == 0) return 1; } de = d-&gt;ht[0].table[d-&gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while(de) { unsigned int h; nextde = de-&gt;next; /* Get the index in the new hash table */ h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask; de-&gt;next = d-&gt;ht[1].table[h]; d-&gt;ht[1].table[h] = de; d-&gt;ht[0].used--; d-&gt;ht[1].used++; de = nextde; } d-&gt;ht[0].table[d-&gt;rehashidx] = NULL; d-&gt;rehashidx++; } /* Check if we already rehashed the whole table... */ if (d-&gt;ht[0].used == 0) { zfree(d-&gt;ht[0].table); d-&gt;ht[0] = d-&gt;ht[1]; _dictReset(&amp;d-&gt;ht[1]); d-&gt;rehashidx = -1; return 0; } /* More to rehash... */ return 1; }dictRehash每次将重哈希至少向前推进n步（除非不到n步整个重哈希就结束了），每一步都将ht[0]上某一个bucket（即一个dictEntry链表）上的每一个dictEntry移动到ht[1]上，它在ht[1]上的新位置根据ht[1]的sizemask进行重新计算。rehashidx记录了当前尚未迁移（有待迁移）的ht[0]的bucket位置。 如果dictRehash被调用的时候，rehashidx指向的bucket里一个dictEntry也没有，那么它就没有可迁移的数据。这时它尝试在ht[0].table数组中不断向后遍历，直到找到下一个存有数据的bucket位置。如果一直找不到，则最多走n*10步，本次重哈希暂告结束。 最后，如果ht[0]上的数据都迁移到ht[1]上了（即d-&gt;ht[0].used == 0），那么整个重哈希结束，ht[0]变成ht[1]的内容，而ht[1]重置为空。 根据以上对于重哈希过程的分析，我们容易看出，本文前面的dict结构图中所展示的正是rehashidx=2时的情况，前面两个bucket（ht[0].table[0]和ht[0].table[1]）都已经迁移到ht[1]上去了。 dict的插入（dictAdd和dictReplace）dictAdd插入新的一对key和value，如果key已经存在，则插入失败。 dictReplace也是插入一对key和value，不过在key存在的时候，它会更新value。 int dictAdd(dict *d, void *key, void *val) { dictEntry *entry = dictAddRaw(d,key); if (!entry) return DICT_ERR; dictSetVal(d, entry, val); return DICT_OK; } dictEntry *dictAddRaw(dict *d, void *key) { int index; dictEntry *entry; dictht *ht; if (dictIsRehashing(d)) _dictRehashStep(d); /* Get the index of the new element, or -1 if * the element already exists. */ if ((index = _dictKeyIndex(d, key)) == -1) return NULL; /* Allocate the memory and store the new entry. * Insert the element in top, with the assumption that in a database * system it is more likely that recently added entries are accessed * more frequently. */ ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0]; entry = zmalloc(sizeof(*entry)); entry-&gt;next = ht-&gt;table[index]; ht-&gt;table[index] = entry; ht-&gt;used++; /* Set the hash entry fields. */ dictSetKey(d, entry, key); return entry; } static int _dictKeyIndex(dict *d, const void *key) { unsigned int h, idx, table; dictEntry *he; /* Expand the hash table if needed */ if (_dictExpandIfNeeded(d) == DICT_ERR) return -1; /* Compute the key hash value */ h = dictHashKey(d, key); for (table = 0; table &lt;= 1; table++) { idx = h &amp; d-&gt;ht[table].sizemask; /* Search if this slot does not already contain the given key */ he = d-&gt;ht[table].table[idx]; while(he) { if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) return -1; he = he-&gt;next; } if (!dictIsRehashing(d)) break; } return idx; }以上是dictAdd的关键实现代码。我们主要需要注意以下几点： 它也会触发推进一步重哈希（_dictRehashStep）。 如果正在重哈希中，它会把数据插入到ht[1]；否则插入到ht[0]。 在对应的bucket中插入数据的时候，总是插入到dictEntry的头部。因为新数据接下来被访问的概率可能比较高，这样再次查找它时就比较次数较少。 _dictKeyIndex在dict中寻找插入位置。如果不在重哈希过程中，它只查找ht[0]；否则查找ht[0]和ht[1]。 _dictKeyIndex可能触发dict内存扩展（_dictExpandIfNeeded，它将哈希表长度扩展为原来两倍，具体请参考dict.c中源码）。 dictReplace在dictAdd基础上实现，如下： int dictReplace(dict *d, void *key, void *val) { dictEntry *entry, auxentry; /* Try to add the element. If the key * does not exists dictAdd will suceed. */ if (dictAdd(d, key, val) == DICT_OK) return 1; /* It already exists, get the entry */ entry = dictFind(d, key); /* Set the new value and free the old one. Note that it is important * to do that in this order, as the value may just be exactly the same * as the previous one. In this context, think to reference counting, * you want to increment (set), and then decrement (free), and not the * reverse. */ auxentry = *entry; dictSetVal(d, entry, val); dictFreeVal(d, &amp;auxentry); return 0; }在key已经存在的情况下，dictReplace会同时调用dictAdd和dictFind，这其实相当于两次查找过程。这里Redis的代码不够优化。 dict的删除（dictDelete）dictDelete的源码这里忽略，具体请参考dict.c。需要稍加注意的是： dictDelete也会触发推进一步重哈希（_dictRehashStep） 如果当前不在重哈希过程中，它只在ht[0]中查找要删除的key；否则ht[0]和ht[1]它都要查找。 删除成功后会调用key和value的析构函数（keyDestructor和valDestructor）。 dict的实现相对来说比较简单，本文就介绍到这。在下一篇中我们将会介绍Redis中动态字符串的实现——sds，敬请期待。 原创文章，转载请注明出处，并包含下面的二维码！否则拒绝转载！本文链接：http://zhangtielei.com/posts/blog-redis-dict.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索Redis设计与实现1：Redis 的基础数据结构概览]]></title>
    <url>%2F2019%2F09%2F14%2FRedis%2F%E6%8E%A2%E7%B4%A2Redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B01%EF%BC%9ARedis%20%E7%9A%84%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《探索Redis设计与实现》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，Redis基本的使用方法，Redis的基本数据结构，以及一些进阶的使用方法，同时也需要进一步了解Redis的底层数据结构，再接着，还会带来Redis主从复制、集群、分布式锁等方面的相关内容，以及作为缓存的一些使用方法和注意事项，以便让你更完整地了解整个Redis相关的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 这周开始学习 Redis，看看Redis是怎么实现的。所以会写一系列关于 Redis的文章。这篇文章关于 Redis 的基础数据。阅读这篇文章你可以了解： 动态字符串（SDS） 链表 字典 三个数据结构 Redis 是怎么实现的。 SDSSDS （Simple Dynamic String）是 Redis 最基础的数据结构。直译过来就是”简单的动态字符串“。Redis 自己实现了一个动态的字符串，而不是直接使用了 C 语言中的字符串。 sds 的数据结构： struct sdshdr { // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[]; }所以一个 SDS 的就如下图： 所以我们看到，sds 包含3个参数。buf 的长度 len，buf 的剩余长度，以及buf。 为什么这么设计呢？ 可以直接获取字符串长度。C 语言中，获取字符串的长度需要用指针遍历字符串，时间复杂度为 O(n)，而 SDS 的长度，直接从len 获取复杂度为 O(1)。 杜绝缓冲区溢出。由于C 语言不记录字符串长度，如果增加一个字符传的长度，如果没有注意就可能溢出，覆盖了紧挨着这个字符的数据。对于SDS 而言增加字符串长度需要验证 free的长度，如果free 不够就会扩容整个 buf，防止溢出。 减少修改字符串长度时造成的内存再次分配。redis 作为高性能的内存数据库，需要较高的相应速度。字符串也很大概率的频繁修改。 SDS 通过未使用空间这个参数，将字符串的长度和底层buf的长度之间的额关系解除了。buf的长度也不是字符串的长度。基于这个分设计 SDS 实现了空间的预分配和惰性释放。 预分配如果对 SDS 修改后，如果 len 小于 1MB 那 len = 2 * len + 1byte。 这个 1 是用于保存空字节。如果 SDS 修改后 len 大于 1MB 那么 len = 1MB + len + 1byte。 惰性释放如果缩短 SDS 的字符串长度，redis并不是马上减少 SDS 所占内存。只是增加 free 的长度。同时向外提供 API 。真正需要释放的时候，才去重新缩小 SDS 所占的内存 二进制安全。C 语言中的字符串是以 ”\0“ 作为字符串的结束标记。而 SDS 是使用 len 的长度来标记字符串的结束。所以SDS 可以存储字符串之外的任意二进制流。因为有可能有的二进制流在流中就包含了”\0“造成字符串提前结束。也就是说 SDS 不依赖 “\0” 作为结束的依据。 兼容C语言SDS 按照惯例使用 ”\0“ 作为结尾的管理。部分普通C 语言的字符串 API 也可以使用。 链表C语言中并没有链表这个数据结构所以 Redis 自己实现了一个。Redis 中的链表是： typedef struct listNode { // 前置节点 struct listNode *prev; // 后置节点 struct listNode *next; // 节点的值 void *value;} listNode;非常典型的双向链表的数据结构。 同时为双向链表提供了如下操作的函数： /* * 双端链表迭代器 */typedef struct listIter { // 当前迭代到的节点 listNode *next; // 迭代的方向 int direction;} listIter; /* * 双端链表结构 */typedef struct list { // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); // 链表所包含的节点数量 unsigned long len;} list;链表的结构比较简单，数据结构如下： 总结一下性质： 双向链表，某个节点寻找上一个或者下一个节点时间复杂度 O(1)。 list 记录了 head 和 tail，寻找 head 和 tail 的时间复杂度为 O(1)。 获取链表的长度 len 时间复杂度 O(1)。 字典字典数据结构极其类似 java 中的 Hashmap。 Redis的字典由三个基础的数据结构组成。最底层的单位是哈希表节点。结构如下： typedef struct dictEntry { // 键 void *key; // 值 union { void *val; uint64_t u64; int64_t s64; } v; // 指向下个哈希表节点，形成链表 struct dictEntry *next; } dictEntry;实际上哈希表节点就是一个单项列表的节点。保存了一下下一个节点的指针。 key 就是节点的键，v是这个节点的值。这个 v 既可以是一个指针，也可以是一个 uint64_t或者 int64_t 整数。*next 指向下一个节点。 通过一个哈希表的数组把各个节点链接起来： typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; } dictht;dictht 通过图示我们观察： 实际上，如果对java 的基本数据结构了解的同学就会发现，这个数据结构和 java 中的 HashMap 是很类似的，就是数组加链表的结构。 字典的数据结构： typedef struct dict { // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量 int iterators; /* number of iterators currently running */ } dict;其中的dictType 是一组方法，代码如下： /* * 字典类型特定函数 */ typedef struct dictType { // 计算哈希值的函数 unsigned int (*hashFunction)(const void *key); // 复制键的函数 void *(*keyDup)(void *privdata, const void *key); // 复制值的函数 void *(*valDup)(void *privdata, const void *obj); // 对比键的函数 int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 销毁键的函数 void (*keyDestructor)(void *privdata, void *key); // 销毁值的函数 void (*valDestructor)(void *privdata, void *obj); } dictType;字典的数据结构如下图： 这里我们可以看到一个dict 拥有两个 dictht。一般来说只使用 ht[0],当扩容的时候发生了rehash的时候，ht[1]才会被使用。 当我们观察或者研究一个hash结构的时候偶我们首先要考虑的这个 dict 如何插入一个数据？ 我们梳理一下插入数据的逻辑。 计算Key 的 hash 值。找到 hash 映射到 table 数组的位置。 如果数据已经有一个 key 存在了。那就意味着发生了 hash 碰撞。新加入的节点，就会作为链表的一个节点接到之前节点的 next 指针上。 如果 key 发生了多次碰撞，造成链表的长度越来越长。会使得字典的查询速度下降。为了维持正常的负载。Redis 会对 字典进行 rehash 操作。来增加 table 数组的长度。所以我们要着重了解一下 Redis 的 rehash。步骤如下： 根据ht[0] 的数据和操作的类型（扩大或缩小），分配 ht[1] 的大小。 将 ht[0] 的数据 rehash 到 ht[1] 上。 rehash 完成以后，将ht[1] 设置为 ht[0]，生成一个新的ht[1]备用。 渐进式的 rehash 。其实如果字典的 key 数量很大，达到千万级以上，rehash 就会是一个相对较长的时间。所以为了字典能够在 rehash 的时候能够继续提供服务。Redis 提供了一个渐进式的 rehash 实现，rehash的步骤如下： 分配 ht[1] 的空间，让字典同时持有 ht[1] 和 ht[0]。 在字典中维护一个 rehashidx，设置为 0 ，表示字典正在 rehash。 在rehash期间，每次对字典的操作除了进行指定的操作以外，都会根据 ht[0] 在 rehashidx 上对应的键值对 rehash 到 ht[1]上。 随着操作进行， ht[0] 的数据就会全部 rehash 到 ht[1] 。设置ht[0] 的 rehashidx 为 -1，渐进的 rehash 结束。 这样保证数据能够平滑的进行 rehash。防止 rehash 时间过久阻塞线程。 在进行 rehash 的过程中，如果进行了 delete 和 update 等操作，会在两个哈希表上进行。如果是 find 的话优先在ht[0] 上进行，如果没有找到，再去 ht[1] 中查找。如果是 insert 的话那就只会在 ht[1]中插入数据。这样就会保证了 ht[1] 的数据只增不减，ht[0]的数据只减不增。 var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列14：深入理解Java枚举类]]></title>
    <url>%2F2019%2F09%2F14%2F14%E6%9E%9A%E4%B8%BE%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 枚举（enum）类型是Java 5新增的特性，它是一种新的类型，允许用常量来表示特定的数据片断，而且全部都以类型安全的形式来表示。 初探枚举类 在程序设计中，有时会用到由若干个有限数据元素组成的集合，如一周内的星期一到星期日七个数据元素组成的集合，由三种颜色红、黄、绿组成的集合，一个工作班组内十个职工组成的集合等等，程序中某个变量取值仅限于集合中的元素。此时，可将这些数据集合定义为枚举类型。 因此，枚举类型是某类数据可能取值的集合，如一周内星期可能取值的集合为： { Sun,Mon,Tue,Wed,Thu,Fri,Sat} 该集合可定义为描述星期的枚举类型，该枚举类型共有七个元素，因而用枚举类型定义的枚举变量只能取集合中的某一元素值。由于枚举类型是导出数据类型，因此，必须先定义枚举类型，然后再用枚举类型定义枚举型变量。 enum &lt;枚举类型名&gt; { &lt;枚举元素表&gt; }; 其中：关键词enum表示定义的是枚举类型，枚举类型名由标识符组成，而枚举元素表由枚举元素或枚举常量组成。例如： enum weekdays { Sun,Mon,Tue,Wed,Thu,Fri,Sat }; 定义了一个名为 weekdays的枚举类型，它包含七个元素：Sun、Mon、Tue、Wed、Thu、Fri、Sat。 在编译器编译程序时，给枚举类型中的每一个元素指定一个整型常量值(也称为序号值)。若枚举类型定义中没有指定元素的整型常量值，则整型常量值从0开始依次递增，因此，weekdays枚举类型的七个元素Sun、Mon、Tue、Wed、Thu、Fri、Sat对应的整型常量值分别为0、1、2、3、4、5、6。 注意：在定义枚举类型时，也可指定元素对应的整型常量值。 例如，描述逻辑值集合{TRUE、FALSE}的枚举类型boolean可定义如下： enum boolean { TRUE=1 ,FALSE=0 }; 该定义规定：TRUE的值为1，而FALSE的值为0。 而描述颜色集合{red,blue,green,black,white,yellow}的枚举类型colors可定义如下： enum colors {red=5,blue=1,green,black,white,yellow}; 该定义规定red为5 ，blue为1，其后元素值从2 开始递增加1。green、black、white、yellow的值依次为2、3、4、5。 此时，整数5将用于表示二种颜色red与yellow。通常两个不同元素取相同的整数值是没有意义的。枚举类型的定义只是定义了一个新的数据类型，只有用枚举类型定义枚举变量才能使用这种数据类型。 枚举类-语法 enum 与 class、interface 具有相同地位；可以继承多个接口；可以拥有构造器、成员方法、成员变量；1.2 枚举类与普通类不同之处 默认继承 java.lang.Enum 类，所以不能继承其他父类；其中 java.lang.Enum 类实现了 java.lang.Serializable 和 java.lang.Comparable 接口； 使用 enum 定义，默认使用 final 修饰，因此不能派生子类； 构造器默认使用 private 修饰，且只能使用 private 修饰； 枚举类所有实例必须在第一行给出，默认添加 public static final 修饰，否则无法产生实例； 枚举类的具体使用这部分内容参考https://blog.csdn.net/qq_27093465/article/details/52180865 常量public class 常量 { } enum Color { Red, Green, Blue, Yellow }switchJDK1.6之前的switch语句只支持int,char,enum类型，使用枚举，能让我们的代码可读性更强。 public static void showColor(Color color) { switch (color) { case Red: System.out.println(color); break; case Blue: System.out.println(color); break; case Yellow: System.out.println(color); break; case Green: System.out.println(color); break; } }向枚举中添加新方法如果打算自定义自己的方法，那么必须在enum实例序列的最后添加一个分号。而且 Java 要求必须先定义 enum 实例。 enum Color { //每个颜色都是枚举类的一个实例，并且构造方法要和枚举类的格式相符合。 //如果实例后面有其他内容，实例序列结束时要加分号。 Red(&quot;红色&quot;, 1), Green(&quot;绿色&quot;, 2), Blue(&quot;蓝色&quot;, 3), Yellow(&quot;黄色&quot;, 4); String name; int index; Color(String name, int index) { this.name = name; this.index = index; } public void showAllColors() { //values是Color实例的数组，在通过index和name可以获取对应的值。 for (Color color : Color.values()) { System.out.println(color.index + &quot;:&quot; + color.name); } } }覆盖枚举的方法所有枚举类都继承自Enum类，所以可以重写该类的方法下面给出一个toString()方法覆盖的例子。 @Override public String toString() { return this.index + &quot;:&quot; + this.name; }实现接口所有的枚举都继承自java.lang.Enum类。由于Java 不支持多继承，所以枚举对象不能再继承其他类。 enum Color implements Print{ @Override public void print() { System.out.println(this.name); } }使用接口组织枚举 搞个实现接口，来组织枚举，简单讲，就是分类吧。如果大量使用枚举的话，这么干，在写代码的时候，就很方便调用啦。 public class 用接口组织枚举 { public static void main(String[] args) { Food cf = chineseFood.dumpling; Food jf = Food.JapaneseFood.fishpiece; for (Food food : chineseFood.values()) { System.out.println(food); } for (Food food : Food.JapaneseFood.values()) { System.out.println(food); } } } interface Food { enum JapaneseFood implements Food { suse, fishpiece } } enum chineseFood implements Food { dumpling, tofu }枚举类集合java.util.EnumSet和java.util.EnumMap是两个枚举集合。EnumSet保证集合中的元素不重复；EnumMap中的 key是enum类型，而value则可以是任意类型。 EnumSet在JDK中没有找到实现类，这里写一个EnumMap的例子 public class 枚举类集合 { public static void main(String[] args) { EnumMap&lt;Color, String&gt; map = new EnumMap&lt;Color, String&gt;(Color.class); map.put(Color.Blue, &quot;Blue&quot;); map.put(Color.Yellow, &quot;Yellow&quot;); map.put(Color.Red, &quot;Red&quot;); System.out.println(map.get(Color.Red)); } }使用枚举类的注意事项 枚举类型对象之间的值比较，是可以使用==，直接来比较值，是否相等的，不是必须使用equals方法的哟。 因为枚举类Enum已经重写了equals方法 /** * Returns true if the specified object is equal to this * enum constant. * * @param other the object to be compared for equality with this object. * @return true if the specified object is equal to this * enum constant. */ public final boolean equals(Object other) { return this==other; }枚举类的实现原理这部分参考https://blog.csdn.net/mhmyqn/article/details/48087247 Java从JDK1.5开始支持枚举，也就是说，Java一开始是不支持枚举的，就像泛型一样，都是JDK1.5才加入的新特性。通常一个特性如果在一开始没有提供，在语言发展后期才添加，会遇到一个问题，就是向后兼容性的问题。 像Java在1.5中引入的很多特性，为了向后兼容，编译器会帮我们写的源代码做很多事情，比如泛型为什么会擦除类型，为什么会生成桥接方法，foreach迭代，自动装箱/拆箱等，这有个术语叫“语法糖”，而编译器的特殊处理叫“解语法糖”。那么像枚举也是在JDK1.5中才引入的，又是怎么实现的呢？ Java在1.5中添加了java.lang.Enum抽象类，它是所有枚举类型基类。提供了一些基础属性和基础方法。同时，对把枚举用作Set和Map也提供了支持，即java.util.EnumSet和java.util.EnumMap。 接下来定义一个简单的枚举类 public enum Day { MONDAY { @Override void say() { System.out.println(&quot;MONDAY&quot;); } } , TUESDAY { @Override void say() { System.out.println(&quot;TUESDAY&quot;); } }, FRIDAY(&quot;work&quot;){ @Override void say() { System.out.println(&quot;FRIDAY&quot;); } }, SUNDAY(&quot;free&quot;){ @Override void say() { System.out.println(&quot;SUNDAY&quot;); } }; String work; //没有构造参数时，每个实例可以看做常量。 //使用构造参数时，每个实例都会变得不一样，可以看做不同的类型，所以编译后会生成实例个数对应的class。 private Day(String work) { this.work = work; } private Day() { } //枚举实例必须实现枚举类中的抽象方法 abstract void say (); }反编译结果 D:\MyTech\out\production\MyTech\com\javase\枚举类&gt;javap Day.class Compiled from &quot;Day.java&quot; public abstract class com.javase.枚举类.Day extends java.lang.Enum&lt;com.javase.枚举类.Day&gt; { public static final com.javase.枚举类.Day MONDAY; public static final com.javase.枚举类.Day TUESDAY; public static final com.javase.枚举类.Day FRIDAY; public static final com.javase.枚举类.Day SUNDAY; java.lang.String work; public static com.javase.枚举类.Day[] values(); public static com.javase.枚举类.Day valueOf(java.lang.String); abstract void say(); com.javase.枚举类.Day(java.lang.String, int, com.javase.枚举类.Day$1); com.javase.枚举类.Day(java.lang.String, int, java.lang.String, com.javase.枚举类.Day$1); static {}; } 可以看到，一个枚举在经过编译器编译过后，变成了一个抽象类，它继承了java.lang.Enum；而枚举中定义的枚举常量，变成了相应的public static final属性，而且其类型就抽象类的类型，名字就是枚举常量的名字. 同时我们可以在Operator.class的相同路径下看到四个内部类的.class文件com/mikan/Day$1.class、com/mikan/Day$2.class、com/mikan/Day$3.class、com/mikan/Day$4.class，也就是说这四个命名字段分别使用了内部类来实现的；同时添加了两个方法values()和valueOf(String)；我们定义的构造方法本来只有一个参数，但却变成了三个参数；同时还生成了一个静态代码块。这些具体的内容接下来仔细看看。 下面分析一下字节码中的各部分，其中： InnerClasses: static #23; //class com/javase/枚举类/Day$4 static #18; //class com/javase/枚举类/Day$3 static #14; //class com/javase/枚举类/Day$2 static #10; //class com/javase/枚举类/Day$1从中可以看到它有4个内部类，这四个内部类的详细信息后面会分析。 static {}; descriptor: ()V flags: ACC_STATIC Code: stack=5, locals=0, args_size=0 0: new #10 // class com/javase/枚举类/Day$1 3: dup 4: ldc #11 // String MONDAY 6: iconst_0 7: invokespecial #12 // Method com/javase/枚举类/Day$1.&quot;&lt;init&gt;&quot;:(Ljava/lang/String;I)V 10: putstatic #13 // Field MONDAY:Lcom/javase/枚举类/Day; 13: new #14 // class com/javase/枚举类/Day$2 16: dup 17: ldc #15 // String TUESDAY 19: iconst_1 20: invokespecial #16 // Method com/javase/枚举类/Day$2.&quot;&lt;init&gt;&quot;:(Ljava/lang/String;I)V //后面类似，这里省略 }其实编译器生成的这个静态代码块做了如下工作：分别设置生成的四个公共静态常量字段的值，同时编译器还生成了一个静态字段$VALUES，保存的是枚举类型定义的所有枚举常量编译器添加的values方法： public static com.javase.Day[] values(); flags: ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=0, args_size=0 0: getstatic #2 // Field $VALUES:[Lcom/javase/Day; 3: invokevirtual #3 // Method &quot;[Lcom/mikan/Day;&quot;.clone:()Ljava/lang/Object; 6: checkcast #4 // class &quot;[Lcom/javase/Day;&quot; 9: areturn 这个方法是一个公共的静态方法，所以我们可以直接调用该方法（Day.values()）,返回这个枚举值的数组，另外，这个方法的实现是，克隆在静态代码块中初始化的$VALUES字段的值，并把类型强转成Day[]类型返回。造方法为什么增加了两个参数？ 有一个问题，构造方法我们明明只定义了一个参数，为什么生成的构造方法是三个参数呢？ 从Enum类中我们可以看到，为每个枚举都定义了两个属性，name和ordinal，name表示我们定义的枚举常量的名称，如FRIDAY、TUESDAY，而ordinal是一个顺序号，根据定义的顺序分别赋予一个整形值，从0开始。在枚举常量初始化时，会自动为初始化这两个字段，设置相应的值，所以才在构造方法中添加了两个参数。即： 另外三个枚举常量生成的内部类基本上差不多，这里就不重复说明了。 我们可以从Enum类的代码中看到，定义的name和ordinal属性都是final的，而且大部分方法也都是final的，特别是clone、readObject、writeObject这三个方法，这三个方法和枚举通过静态代码块来进行初始化一起。 它保证了枚举类型的不可变性，不能通过克隆，不能通过序列化和反序列化来复制枚举，这能保证一个枚举常量只是一个实例，即是单例的，所以在effective java中推荐使用枚举来实现单例。 枚举类实战实战一无参（1）定义一个无参枚举类 123enum SeasonType &#123; SPRING, SUMMER, AUTUMN, WINTER&#125; （2）实战中的使用 123// 根据实际情况选择下面的用法即可SeasonType springType = SeasonType.SPRING; // 输出 SPRING String springString = SeasonType.SPRING.toString(); // 输出 SPRING 实战二有一参（1）定义只有一个参数的枚举类 1234567891011121314151617181920enum SeasonType &#123; // 通过构造函数传递参数并创建实例 SPRING(&quot;spring&quot;), SUMMER(&quot;summer&quot;), AUTUMN(&quot;autumn&quot;), WINTER(&quot;winter&quot;); // 定义实例对应的参数 private String msg; // 必写：通过此构造器给枚举值创建实例 SeasonType(String msg) &#123; this.msg = msg; &#125; // 通过此方法可以获取到对应实例的参数值 public String getMsg() &#123; return msg; &#125;&#125; （2）实战中的使用 12// 当我们为某个实例类赋值的时候可使用如下方式String msg = SeasonType.SPRING.getMsg(); // 输出 spring 实战三有两参（1）定义有两个参数的枚举类 12345678910111213141516171819202122232425262728293031323334353637public enum Season &#123; // 通过构造函数传递参数并创建实例 SPRING(1, &quot;spring&quot;), SUMMER(2, &quot;summer&quot;), AUTUMN(3, &quot;autumn&quot;), WINTER(4, &quot;winter&quot;); // 定义实例对应的参数 private Integer key; private String msg; // 必写：通过此构造器给枚举值创建实例 Season(Integer key, String msg) &#123; this.key = key; this.msg = msg; &#125; // 很多情况，我们可能从前端拿到的值是枚举类的 key ，然后就可以通过以下静态方法获取到对应枚举值 public static Season valueofKey(Integer key) &#123; for (Season season : Season.values()) &#123; if (season.key.equals(key)) &#123; return season; &#125; &#125; throw new IllegalArgumentException(&quot;No element matches &quot; + key); &#125; // 通过此方法可以获取到对应实例的 key 值 public Integer getKey() &#123; return key; &#125; // 通过此方法可以获取到对应实例的 msg 值 public String getMsg() &#123; return msg; &#125;&#125; （2）实战中的使用 123456// 输出 key 为 1 的枚举值实例Season season = Season.valueofKey(1);// 输出 SPRING 实例对应的 keyInteger key = Season.SPRING.getKey();// 输出 SPRING 实例对应的 msgString msg = Season.SPRING.getMsg(); 枚举类总结其实枚举类懂了其概念后，枚举就变得相当简单了，随手就可以写一个枚举类出来。所以如上几个实战小例子一定要先搞清楚概念，然后在练习几遍就 ok 了。 重要的概念，我在这里在赘述一遍，帮助老铁们快速掌握这块知识，首先记住，枚举类中的枚举值可以没有参数，也可以有多个参数，每一个枚举值都是一个实例； 并且还有一点很重要，就是如果枚举值有 n 个参数，那么构造函数中的参数值肯定有 n 个，因为声明的每一个枚举值都会调用构造函数去创建实例，所以参数一定是一一对应的；既然明白了这一点，那么我们只需要在枚举类中把这 n 个参数定义为 n 个成员变量，然后提供对应的 get() 方法，之后通过实例就可以随意的获取实例中的任意参数值了。 如果想让枚举类更加的好用，就可以模仿我在实战三中的写法那样，通过某一个参数值，比如 key 参数值，就能获取到其对应的枚举值，然后想要什么值，就 get 什么值就好了。 枚举 API我们使用 enum 定义的枚举类都是继承 java.lang.Enum 类的，那么就会继承其 API ，常用的 API 如下： String name() 获取枚举名称 int ordinal() 获取枚举的位置（下标，初始值为 0 ） valueof(String msg) 通过 msg 获取其对应的枚举类型。（比如实战二中的枚举类或其它枚举类都行，只要使用得当都可以使用此方法） values() 获取枚举类中的所有枚举值（比如在实战三中就使用到了） 总结枚举本质上是通过普通的类来实现的，只是编译器为我们进行了处理。每个枚举类型都继承自java.lang.Enum，并自动添加了values和valueOf方法。 而每个枚举常量是一个静态常量字段，使用内部类实现，该内部类继承了枚举类。所有枚举常量都通过静态代码块来进行初始化，即在类加载期间就初始化。 另外通过把clone、readObject、writeObject这三个方法定义为final的，同时实现是抛出相应的异常。这样保证了每个枚举类型及枚举常量都是不可变的。可以利用枚举的这两个特性来实现线程安全的单例。 参考文章https://blog.csdn.net/qq_34988624/article/details/86592229https://www.meiwen.com.cn/subject/slhvhqtx.htmlhttps://blog.csdn.net/qq_34988624/article/details/86592229https://segmentfault.com/a/1190000012220863https://my.oschina.net/wuxinshui/blog/1511484https://blog.csdn.net/hukailee/article/details/81107412 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java枚举类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列13：深入理解Java中的泛型]]></title>
    <url>%2F2019%2F09%2F13%2F13%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 泛型概述泛型在java中有很重要的地位，在面向对象编程及各种设计模式中有非常广泛的应用。 什么是泛型？为什么要使用泛型？ 泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 一个栗子一个被举了无数次的例子： List arrayList = new ArrayList(); arrayList.add(&quot;aaaa&quot;); arrayList.add(100); for(int i = 0; i&lt; arrayList.size();i++){ String item = (String)arrayList.get(i); Log.d(&quot;泛型测试&quot;,&quot;item = &quot; + item); }毫无疑问，程序的运行结果会以崩溃结束： java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String ArrayList可以存放任意类型，例子中添加了一个String类型，添加了一个Integer类型，再使用时都以String的方式使用，因此程序崩溃了。为了解决类似这样的问题（在编译阶段就可以解决），泛型应运而生。 我们将第一行声明初始化list的代码更改一下，编译器会在编译阶段就能够帮我们发现类似这样的问题。 List arrayList = new ArrayList();…//arrayList.add(100); 在编译阶段，编译器就会报错 特性泛型只在编译阶段有效。看下面的代码： List&lt;String&gt; stringArrayList = new ArrayList&lt;String&gt;(); List&lt;Integer&gt; integerArrayList = new ArrayList&lt;Integer&gt;(); Class classStringArrayList = stringArrayList.getClass(); Class classIntegerArrayList = integerArrayList.getClass(); if(classStringArrayList.equals(classIntegerArrayList)){ Log.d(&quot;泛型测试&quot;,&quot;类型相同&quot;); } 通过上面的例子可以证明，在编译之后程序会采取去泛型化的措施。也就是说Java中的泛型，只在编译阶段有效。在编译过程中，正确检验泛型结果后，会将泛型的相关信息擦出，并且在对象进入和离开方法的边界处添加类型检查和类型转换的方法。也就是说，泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 泛型的使用方式泛型有三种使用方式，分别为：泛型类、泛型接口、泛型方法 泛型类 泛型类型用于类的定义中，被称为泛型类。通过泛型可以完成对一组类的操作对外开放相同的接口。最典型的就是各种容器类，如：List、Set、Map。 泛型类的最基本写法（这么看可能会有点晕，会在下面的例子中详解）： class 类名称 &lt;泛型标识：可以随便写任意标识号，标识指定的泛型的类型&gt;{ private 泛型标识 /*（成员变量类型）*/ var; ..... }一个最普通的泛型类： //此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型 //在实例化泛型类时，必须指定T的具体类型 public class Generic&lt;T&gt;{ //在类中声明的泛型整个类里面都可以用，除了静态部分，因为泛型是实例化时声明的。 //静态区域的代码在编译时就已经确定，只与类相关 class A &lt;E&gt;{ T t; } //类里面的方法或类中再次声明同名泛型是允许的，并且该泛型会覆盖掉父类的同名泛型T class B &lt;T&gt;{ T t; } //静态内部类也可以使用泛型，实例化时赋予泛型实际类型 static class C &lt;T&gt; { T t; } public static void main(String[] args) { //报错，不能使用T泛型，因为泛型T属于实例不属于类 // T t = null; } //key这个成员变量的类型为T,T的类型由外部指定 private T key; public Generic(T key) { //泛型构造方法形参key的类型也为T，T的类型由外部指定 this.key = key; } public T getKey(){ //泛型方法getKey的返回值类型为T，T的类型由外部指定 return key; } } 12-27 09:20:04.432 13063-13063/? D/泛型测试: key is 123456 12-27 09:20:04.432 13063-13063/? D/泛型测试: key is key_vlaue 定义的泛型类，就一定要传入泛型类型实参么？并不是这样，在使用泛型的时候如果传入泛型实参，则会根据传入的泛型实参做相应的限制，此时泛型才会起到本应起到的限制作用。如果不传入泛型类型实参的话，在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型。 看一个例子： Generic generic = new Generic(&quot;111111&quot;); Generic generic1 = new Generic(4444); Generic generic2 = new Generic(55.55); Generic generic3 = new Generic(false); Log.d(&quot;泛型测试&quot;,&quot;key is &quot; + generic.getKey()); Log.d(&quot;泛型测试&quot;,&quot;key is &quot; + generic1.getKey()); Log.d(&quot;泛型测试&quot;,&quot;key is &quot; + generic2.getKey()); Log.d(&quot;泛型测试&quot;,&quot;key is &quot; + generic3.getKey()); D/泛型测试: key is 111111 D/泛型测试: key is 4444 D/泛型测试: key is 55.55 D/泛型测试: key is false注意：泛型的类型参数只能是类类型，不能是简单类型。不能对确切的泛型类型使用instanceof操作。如下面的操作是非法的，编译时会出错。 if(ex_num instanceof Generic){ } 泛型接口泛型接口与泛型类的定义及使用基本相同。泛型接口常被用在各种类的生产器中，可以看一个例子： //定义一个泛型接口 public interface Generator&lt;T&gt; { public T next(); }当实现泛型接口的类，未传入泛型实参时： /** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;{ * 如果不声明泛型，如：class FruitGenerator implements Generator&lt;T&gt;，编译器会报错：&quot;Unknown class&quot; */ class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;{ @Override public T next() { return null; } }当实现泛型接口的类，传入泛型实参时： /** * 传入泛型实参时： * 定义一个生产器实现这个接口,虽然我们只创建了一个泛型接口Generator&lt;T&gt; * 但是我们可以为T传入无数个实参，形成无数种类型的Generator接口。 * 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型 * 即：Generator&lt;T&gt;，public T next();中的的T都要替换成传入的String类型。 */ public class FruitGenerator implements Generator&lt;String&gt; { private String[] fruits = new String[]{&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;}; @Override public String next() { Random rand = new Random(); return fruits[rand.nextInt(3)]; } }泛型通配符我们知道Ingeter是Number的一个子类，同时在特性章节中我们也验证过Generic与Generic实际上是相同的一种基本类型。那么问题来了，在使用Generic作为形参的方法中，能否使用Generic的实例传入呢？在逻辑上类似于Generic和Generic是否可以看成具有父子关系的泛型类型呢？ 为了弄清楚这个问题，我们使用Generic这个泛型类继续看下面的例子： public void showKeyValue1(Generic&lt;Number&gt; obj){ Log.d(&quot;泛型测试&quot;,&quot;key value is &quot; + obj.getKey()); } Generic&lt;Integer&gt; gInteger = new Generic&lt;Integer&gt;(123); Generic&lt;Number&gt; gNumber = new Generic&lt;Number&gt;(456); showKeyValue(gNumber); // showKeyValue这个方法编译器会为我们报错：Generic&lt;java.lang.Integer&gt; // cannot be applied to Generic&lt;java.lang.Number&gt; // showKeyValue(gInteger);通过提示信息我们可以看到Generic不能被看作为`Generic的子类。由此可以看出:同一种泛型可以对应多个版本（因为参数类型是不确定的），不同版本的泛型类实例是不兼容的。 回到上面的例子，如何解决上面的问题？总不能为了定义一个新的方法来处理Generic类型的类，这显然与java中的多台理念相违背。因此我们需要一个在逻辑上可以表示同时是Generic和Generic父类的引用类型。由此类型通配符应运而生。 我们可以将上面的方法改一下： public void showKeyValue1(Generic&lt;?&gt; obj){ Log.d(&quot;泛型测试&quot;,&quot;key value is &quot; + obj.getKey());类型通配符一般是使用？代替具体的类型实参，注意， 此处的？和Number、String、Integer一样都是一种实际的类型，可以把？看成所有类型的父类。是一种真实的类型。 可以解决当具体类型不确定的时候，这个通配符就是 ? ；当操作类型时，不需要使用类型的具体功能时，只使用Object类中的功能。那么可以用 ? 通配符来表未知类型 public void showKeyValue(Generic obj){ System.out.println(obj); } Generic&lt;Integer&gt; gInteger = new Generic&lt;Integer&gt;(123); Generic&lt;Number&gt; gNumber = new Generic&lt;Number&gt;(456); public void test () { // showKeyValue(gInteger);该方法会报错 showKeyValue1(gInteger); } public void showKeyValue1(Generic&lt;?&gt; obj) { System.out.println(obj); } // showKeyValue这个方法编译器会为我们报错：Generic&lt;java.lang.Integer&gt; // cannot be applied to Generic&lt;java.lang.Number&gt; // showKeyValue(gInteger);。 泛型方法在java中,泛型类的定义非常简单，但是泛型方法就比较复杂了。 尤其是我们见到的大多数泛型类中的成员方法也都使用了泛型，有的甚至泛型类中也包含着泛型方法，这样在初学者中非常容易将泛型方法理解错了。泛型类，是在实例化类的时候指明泛型的具体类型；泛型方法，是在调用方法的时候指明泛型的具体类型 。 /** * 泛型方法的基本介绍 * @param tClass 传入的泛型实参 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型。 */ public &lt;T&gt; T genericMethod(Class&lt;T&gt; tClass)throws InstantiationException , IllegalAccessException{ T instance = tClass.newInstance(); return instance; } Object obj = genericMethod(Class.forName(&quot;com.test.test&quot;));泛型方法的基本用法光看上面的例子有的同学可能依然会非常迷糊，我们再通过一个例子，把我泛型方法再总结一下。 /** * 这才是一个真正的泛型方法。 * 首先在public与返回值之间的&lt;T&gt;必不可少，这表明这是一个泛型方法，并且声明了一个泛型T * 这个T可以出现在这个泛型方法的任意位置. * 泛型的数量也可以为任意多个 * 如：public &lt;T,K&gt; K showKeyName(Generic&lt;T&gt; container){ * ... * } */ public class 泛型方法 { @Test public void test() { test1(); test2(new Integer(2)); test3(new int[3],new Object()); //打印结果 // null // 2 // [I@3d8c7aca // java.lang.Object@5ebec15 } //该方法使用泛型T public &lt;T&gt; void test1() { T t = null; System.out.println(t); } //该方法使用泛型T //并且参数和返回值都是T类型 public &lt;T&gt; T test2(T t) { System.out.println(t); return t; } //该方法使用泛型T,E //参数包括T,E public &lt;T, E&gt; void test3(T t, E e) { System.out.println(t); System.out.println(e); } }​ 类中的泛型方法当然这并不是泛型方法的全部，泛型方法可以出现杂任何地方和任何场景中使用。但是有一种情况是非常特殊的，当泛型方法出现在泛型类中时，我们再通过一个例子看一下 //注意泛型类先写类名再写泛型，泛型方法先写泛型再写方法名 //类中声明的泛型在成员和方法中可用 class A &lt;T, E&gt;{ { T t1 ; } A (T t){ this.t = t; } T t; public void test1() { System.out.println(this.t); } public void test2(T t,E e) { System.out.println(t); System.out.println(e); } } @Test public void run () { A &lt;Integer,String &gt; a = new A&lt;&gt;(1); a.test1(); a.test2(2,&quot;ds&quot;); // 1 // 2 // ds } static class B &lt;T&gt;{ T t; public void go () { System.out.println(t); } }泛型方法与可变参数再看一个泛型方法和可变参数的例子： public class 泛型和可变参数 { @Test public void test () { printMsg(&quot;dasd&quot;,1,&quot;dasd&quot;,2.0,false); print(&quot;dasdas&quot;,&quot;dasdas&quot;, &quot;aa&quot;); } //普通可变参数只能适配一种类型 public void print(String ... args) { for(String t : args){ System.out.println(t); } } //泛型的可变参数可以匹配所有类型的参数。。有点无敌 public &lt;T&gt; void printMsg( T... args){ for(T t : args){ System.out.println(t); } } //打印结果： //dasd //1 //dasd //2.0 //false }静态方法与泛型静态方法有一种情况需要注意一下，那就是在类中的静态方法使用泛型：静态方法无法访问类上定义的泛型；如果静态方法操作的引用数据类型不确定的时候，必须要将泛型定义在方法上。 即：如果静态方法要使用泛型的话，必须将静态方法也定义成泛型方法 。 public class StaticGenerator&lt;T&gt; { .... .... /** * 如果在类中定义使用泛型的静态方法，需要添加额外的泛型声明（将这个方法定义成泛型方法） * 即使静态方法要使用泛型类中已经声明过的泛型也不可以。 * 如：public static void show(T t){..},此时编译器会提示错误信息： &quot;StaticGenerator cannot be refrenced from static context&quot; */ public static &lt;T&gt; void show(T t){ } }泛型方法总结泛型方法能使方法独立于类而产生变化，以下是一个基本的指导原则： 无论何时，如果你能做到，你就该尽量使用泛型方法。也就是说，如果使用泛型方法将整个类泛型化，那么就应该使用泛型方法。另外对于一个static的方法而已，无法访问泛型类型的参数。所以如果static方法要使用泛型能力，就必须使其成为泛型方法。 泛型上下边界在使用泛型的时候，我们还可以为传入的泛型类型实参进行上下边界的限制，如：类型实参只准传入某种类型的父类或某种类型的子类。 为泛型添加上边界，即传入的类型实参必须是指定类型的子类型 public class 泛型通配符与边界 { public void showKeyValue(Generic&lt;Number&gt; obj){ System.out.println(&quot;key value is &quot; + obj.getKey()); } @Test public void main() { Generic&lt;Integer&gt; gInteger = new Generic&lt;Integer&gt;(123); Generic&lt;Number&gt; gNumber = new Generic&lt;Number&gt;(456); showKeyValue(gNumber); //泛型中的子类也无法作为父类引用传入 // showKeyValue(gInteger); } //直接使用？通配符可以接受任何类型作为泛型传入 public void showKeyValueYeah(Generic&lt;?&gt; obj) { System.out.println(obj); } //只能传入number的子类或者number public void showKeyValue1(Generic&lt;? extends Number&gt; obj){ System.out.println(obj); } //只能传入Integer的父类或者Integer public void showKeyValue2(Generic&lt;? super Integer&gt; obj){ System.out.println(obj); } @Test public void testup () { //这一行代码编译器会提示错误，因为String类型并不是Number类型的子类 //showKeyValue1(generic1); Generic&lt;String&gt; generic1 = new Generic&lt;String&gt;(&quot;11111&quot;); Generic&lt;Integer&gt; generic2 = new Generic&lt;Integer&gt;(2222); Generic&lt;Float&gt; generic3 = new Generic&lt;Float&gt;(2.4f); Generic&lt;Double&gt; generic4 = new Generic&lt;Double&gt;(2.56); showKeyValue1(generic2); showKeyValue1(generic3); showKeyValue1(generic4); } @Test public void testdown () { Generic&lt;String&gt; generic1 = new Generic&lt;String&gt;(&quot;11111&quot;); Generic&lt;Integer&gt; generic2 = new Generic&lt;Integer&gt;(2222); Generic&lt;Number&gt; generic3 = new Generic&lt;Number&gt;(2); // showKeyValue2(generic1);本行报错，因为String并不是Integer的父类 showKeyValue2(generic2); showKeyValue2(generic3); } }== 关于泛型数组要提一下 == 看到了很多文章中都会提起泛型数组，经过查看sun的说明文档，在java中是”不能创建一个确切的泛型类型的数组”的。 也就是说下面的这个例子是不可以的： List&lt;String&gt;[] ls = new ArrayList&lt;String&gt;[10]; 而使用通配符创建泛型数组是可以的，如下面这个例子： List&lt;?&gt;[] ls = new ArrayList&lt;?&gt;[10]; 这样也是可以的： List&lt;String&gt;[] ls = new ArrayList[10];下面使用Sun的一篇文档的一个例子来说明这个问题： List&lt;String&gt;[] lsa = new List&lt;String&gt;[10]; // Not really allowed. Object o = lsa; Object[] oa = (Object[]) o; List&lt;Integer&gt; li = new ArrayList&lt;Integer&gt;(); li.add(new Integer(3)); oa[1] = li; // Unsound, but passes run time store check String s = lsa[1].get(0); // Run-time error: ClassCastException. 这种情况下，由于JVM泛型的擦除机制，在运行时JVM是不知道泛型信息的，所以可以给oa[1]赋上一个ArrayList而不会出现异常，但是在取出数据的时候却要做一次类型转换，所以就会出现ClassCastException，如果可以进行泛型数组的声明，上面说的这种情况在编译期将不会出现任何的警告和错误，只有在运行时才会出错。 而对泛型数组的声明进行限制，对于这样的情况，可以在编译期提示代码有类型安全问题，比没有任何提示要强很多。下面采用通配符的方式是被允许的:数组的类型不可以是类型变量，除非是采用通配符的方式，因为对于通配符的方式，最后取出数据是要做显式的类型转换的。 List&lt;?&gt;[] lsa = new List&lt;?&gt;[10]; // OK, array of unbounded wildcard type. Object o = lsa; Object[] oa = (Object[]) o; List&lt;Integer&gt; li = new ArrayList&lt;Integer&gt;(); li.add(new Integer(3)); oa[1] = li; // Correct. Integer i = (Integer) lsa[1].get(0); // OK 最后 本文中的例子主要是为了阐述泛型中的一些思想而简单举出的，并不一定有着实际的可用性。另外，一提到泛型，相信大家用到最多的就是在集合中，其实，在实际的编程过程中，自己可以使用泛型去简化开发，且能很好的保证代码质量。 泛型常见面试题 Java中的泛型是什么 ? 使用泛型的好处是什么? 这是在各种Java泛型面试中，一开场你就会被问到的问题中的一个，主要集中在初级和中级面试中。那些拥有Java1.4或更早版本的开发背景的人 都知道，在集合中存储对象并在使用前进行类型转换是多么的不方便。泛型防止了那种情况的发生。它提供了编译期的类型安全，确保你只能把正确类型的对象放入 集合中，避免了在运行时出现ClassCastException。 Java的泛型是如何工作的 ? 什么是类型擦除 ? 这是一道更好的泛型面试题。泛型是通过类型擦除来实现的，编译器在编译时擦除了所有类型相关的信息，所以在运行时不存在任何类型相关的信息。例如 List在运行时仅用一个List来表示。这样做的目的，是确保能和Java 5之前的版本开发二进制类库进行兼容。你无法在运行时访问到类型参数，因为编译器已经把泛型类型转换成了原始类型。根据你对这个泛型问题的回答情况，你会 得到一些后续提问，比如为什么泛型是由类型擦除来实现的或者给你展示一些会导致编译器出错的错误泛型代码。请阅读我的Java中泛型是如何工作的来了解更 多信息。 什么是泛型中的限定通配符和非限定通配符 ? 这是另一个非常流行的Java泛型面试题。限定通配符对类型进行了限制。有两种限定通配符，一种是它通过确保类型必须是T的子类来设定类型的上界，另一种是它通过确保类型必须是T的父类来设定类型的下界。泛型类型必须用限定内的类型来进行初始化，否则会导致编译错误。另一方面表 示了非限定通配符，因为&lt;?&gt;可以用任意类型来替代。更多信息请参阅我的文章泛型中限定通配符和非限定通配符之间的区别。 List&lt;? extends T&gt;和List &lt;? super T&gt;之间有什么区别 ? 这和上一个面试题有联系，有时面试官会用这个问题来评估你对泛型的理解，而不是直接问你什么是限定通配符和非限定通配符。这两个List的声明都是 限定通配符的例子，List&lt;? extends T&gt;可以接受任何继承自T的类型的List，而List&lt;? super T&gt;可以接受任何T的父类构成的List。例如List&lt;? extends Number&gt;可以接受List或List。在本段出现的连接中可以找到更多信息。 如何编写一个泛型方法，让它能接受泛型参数并返回泛型类型? 编写泛型方法并不困难，你需要用泛型类型来替代原始类型，比如使用T, E or K,V等被广泛认可的类型占位符。泛型方法的例子请参阅Java集合类框架。最简单的情况下，一个泛型方法可能会像这样: public V put(K key, V value) { return cache.put(key, value); } Java中如何使用泛型编写带有参数的类? 这是上一道面试题的延伸。面试官可能会要求你用泛型编写一个类型安全的类，而不是编写一个泛型方法。关键仍然是使用泛型类型来代替原始类型，而且要使用JDK中采用的标准占位符。 编写一段泛型程序来实现LRU缓存? 对于喜欢Java编程的人来说这相当于是一次练习。给你个提示，LinkedHashMap可以用来实现固定大小的LRU缓存，当LRU缓存已经满 了的时候，它会把最老的键值对移出缓存。LinkedHashMap提供了一个称为removeEldestEntry()的方法，该方法会被put() 和putAll()调用来删除最老的键值对。当然，如果你已经编写了一个可运行的JUnit测试，你也可以随意编写你自己的实现代码。 你可以把List传递给一个接受List参数的方法吗？ 对任何一个不太熟悉泛型的人来说，这个Java泛型题目看起来令人疑惑，因为乍看起来String是一种Object，所以 List应当可以用在需要List的地方，但是事实并非如此。真这样做的话会导致编译错误。如 果你再深一步考虑，你会发现Java这样做是有意义的，因为List可以存储任何类型的对象包括String, Integer等等，而List却只能用来存储Strings。 List objectList; List stringList; objectList = stringList; //compilation error incompatible types Array中可以用泛型吗? 这可能是Java泛型面试题中最简单的一个了，当然前提是你要知道Array事实上并不支持泛型，这也是为什么Joshua Bloch在Effective Java一书中建议使用List来代替Array，因为List可以提供编译期的类型安全保证，而Array却不能。 如何阻止Java中的类型未检查的警告? 如果你把泛型和原始类型混合起来使用，例如下列代码，Java 5的javac编译器会产生类型未检查的警告，例如 List rawList = new ArrayList() 注意: Hello.java使用了未检查或称为不安全的操作; 这种警告可以使用@SuppressWarnings(“unchecked”)注解来屏蔽。 参考文章https://www.cnblogs.com/huajiezh/p/6411123.htmlhttps://www.cnblogs.com/jpfss/p/9929045.htmlhttps://www.cnblogs.com/dengchengchao/p/9717097.htmlhttps://www.cnblogs.com/cat520/p/9353291.htmlhttps://www.cnblogs.com/coprince/p/8603492.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列12：深入理解Java中的反射机制]]></title>
    <url>%2F2019%2F09%2F12%2F12%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 回顾：什么是反射？反射(Reflection)是Java 程序开发语言的特征之一，它允许运行中的 Java 程序获取自身的信息，并且可以操作类或对象的内部属性。Oracle官方对反射的解释是 Reflection enables Java code to discover information about the fields, methods and constructors of loaded classes, and to use reflected fields, methods, and constructors to operate on their underlying counterparts, within security restrictions. The API accommodates applications that need access to either the public members of a target object (based on its runtime class) or the members declared by a given class. It also allows programs to suppress default reflective access control. 简而言之，通过反射，我们可以在运行时获得程序或程序集中每一个类型的成员和成员的信息。 程序中一般的对象的类型都是在编译期就确定下来的，而Java反射机制可以动态地创建对象并调用其属性，这样的对象的类型在编译期是未知的。所以我们可以通过反射机制直接创建对象，即使这个对象的类型在编译期是未知的。 反射的核心是JVM在运行时才动态加载类或调用方法/访问属性，它不需要事先（写代码的时候或编译期）知道运行对象是谁。 Java反射框架主要提供以下功能： 1.在运行时判断任意一个对象所属的类； 2.在运行时构造任意一个类的对象； 3.在运行时判断任意一个类所具有的成员变量和方法（通过反射甚至可以调用private方法）； 4.在运行时调用任意一个对象的方法 重点：是运行时而不是编译时 反射的主要用途 很多人都认为反射在实际的Java开发应用中并不广泛，其实不然。 当我们在使用IDE(如Eclipse，IDEA)时，当我们输入一个对象或类并想调用它的属性或方法时，一按点号，编译器就会自动列出它的属性或方法，这里就会用到反射。 反射最重要的用途就是开发各种通用框架。 很多框架（比如Spring）都是配置化的（比如通过XML文件配置JavaBean,Action之类的），为了保证框架的通用性，它们可能需要根据配置文件加载不同的对象或类，调用不同的方法，这个时候就必须用到反射——运行时动态加载需要加载的对象。 举一个例子，在运用Struts 2框架的开发中我们一般会在struts.xml里去配置Action，比如： &lt;action name=&quot;login&quot; class=&quot;org.ScZyhSoft.test.action.SimpleLoginAction&quot; method=&quot;execute&quot;&gt; &lt;result&gt;/shop/shop-index.jsp&lt;/result&gt; &lt;result name=&quot;error&quot;&gt;login.jsp&lt;/result&gt; &lt;/action&gt;配置文件与Action建立了一种映射关系，当View层发出请求时，请求会被StrutsPrepareAndExecuteFilter拦截，然后StrutsPrepareAndExecuteFilter会去动态地创建Action实例。 ——比如我们请求login.action，那么StrutsPrepareAndExecuteFilter就会去解析struts.xml文件，检索action中name为login的Action，并根据class属性创建SimpleLoginAction实例，并用invoke方法来调用execute方法，这个过程离不开反射。 对与框架开发人员来说，反射虽小但作用非常大，它是各种容器实现的核心。而对于一般的开发者来说，不深入框架开发则用反射用的就会少一点，不过了解一下框架的底层机制有助于丰富自己的编程思想，也是很有益的。 反射的基础：关于Class类更多关于Class类和Object类的原理和介绍请见上一节 1、Class是一个类，一个描述类的类（也就是描述类本身），封装了描述方法的Method，描述字段的Filed，描述构造器的Constructor等属性 2、对象照镜子后（反射）可以得到的信息：某个类的数据成员名、方法和构造器、某个类到底实现了哪些接口。 3、对于每个类而言，JRE 都为其保留一个不变的 Class 类型的对象。一个Class对象包含了特定某个类的有关信息。 4、Class 对象只能由系统建立对象 5、一个类在 JVM 中只会有一个Class实例 //总结一下就是，JDK有一个类叫做Class，这个类用来封装所有Java类型，包括这些类的所有信息，JVM中类信息是放在方法区的。 //所有类在加载后，JVM会为其在堆中创建一个Class&lt;类名称&gt;的对象，并且每个类只会有一个Class对象，这个类的所有对象都要通过Class&lt;类名称&gt;来进行实例化。 //上面说的是JVM进行实例化的原理，当然实际上在Java写代码时只需要用 类名称就可以进行实例化了。 public final class Class&lt;T&gt; implements java.io.Serializable, GenericDeclaration, Type, AnnotatedElement { 虚拟机会保持唯一一 //通过类名.class获得唯一的Class对象。 Class&lt;UserBean&gt; cls = UserBean.class; //通过integer.TYPEl来获取Class对象 Class&lt;Integer&gt; inti = Integer.TYPE; //接口本质也是一个类，一样可以通过.class获取 Class&lt;User&gt; userClass = User.class;JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。 要想解剖一个类,必须先要获取到该类的字节码文件对象。而解剖使用的就是Class类中的方法.所以先要获取到每一个字节码文件对应的Class类型的对象. 以上的总结就是什么是反射反射就是把java类中的各种成分映射成一个个的Java对象例如：一个类有：成员变量、方法、构造方法、包等等信息，利用反射技术可以对一个类进行解剖，把个个组成部分映射成一个个对象。（其实：一个类中这些成员方法、构造方法、在加入类中都有一个类来描述）如图是类的正常加载过程：反射的原理在与class对象。熟悉一下加载的时候：Class对象的由来是将class文件读入内存，并为之创建一个Class对象。 Java为什么需要反射？反射要解决什么问题？Java中编译类型有两种： 静态编译：在编译时确定类型，绑定对象即通过。动态编译：运行时确定类型，绑定对象。动态编译最大限度地发挥了Java的灵活性，体现了多态的应用，可以减低类之间的耦合性。Java反射是Java被视为动态（或准动态）语言的一个关键性质。这个机制允许程序在运行时透过Reflection APIs取得任何一个已知名称的class的内部信息，包括其modifiers（诸如public、static等）、superclass（例如Object）、实现之interfaces（例如Cloneable），也包括fields和methods的所有信息，并可于运行时改变fields内容或唤起methods。 Reflection可以在运行时加载、探知、使用编译期间完全未知的classes。即Java程序可以加载一个运行时才得知名称的class，获取其完整构造，并生成其对象实体、或对其fields设值、或唤起其methods。 反射（reflection）允许静态语言在运行时（runtime）检查、修改程序的结构与行为。在静态语言中，使用一个变量时，必须知道它的类型。在Java中，变量的类型信息在编译时都保存到了class文件中，这样在运行时才能保证准确无误；换句话说，程序在运行时的行为都是固定的。如果想在运行时改变，就需要反射这东西了。 实现Java反射机制的类都位于java.lang.reflect包中： Class类：代表一个类Field类：代表类的成员变量（类的属性）Method类：代表类的方法Constructor类：代表类的构造方法Array类：提供了动态创建数组，以及访问数组的元素的静态方法一句话概括就是使用反射可以赋予jvm动态编译的能力，否则类的元数据信息只能用静态编译的方式实现，例如热加载，Tomcat的classloader等等都没法支持。 反射的基本运用上面我们提到了反射可以用于判断任意对象所属的类，获得Class对象，构造任意一个对象以及调用一个对象。这里我们介绍一下基本反射功能的实现(反射相关的类一般都在java.lang.relfect包里)。 1、获得Class对象方法有三种 (1)使用Class类的forName静态方法: public static Class&lt;?&gt; forName(String className) ``` 在JDBC开发中常用此方法加载数据库驱动: 要使用全类名来加载这个类，一般数据库驱动的配置信息会写在配置文件中。加载这个驱动前要先导入jar包 ```java Class.forName(driver);(2)直接获取某一个对象的class，比如: //Class&lt;?&gt;是一个泛型表示，用于获取一个类的类型。 Class&lt;?&gt; klass = int.class; Class&lt;?&gt; classInt = Integer.TYPE;(3)调用某个对象的getClass()方法,比如: StringBuilder str = new StringBuilder(&quot;123&quot;); Class&lt;?&gt; klass = str.getClass();判断是否为某个类的实例一般地，我们用instanceof关键字来判断是否为某个类的实例。同时我们也可以借助反射中Class对象的isInstance()方法来判断是否为某个类的实例，它是一个Native方法： ==public native boolean isInstance(Object obj);== 创建实例通过反射来生成对象主要有两种方式。 （1）使用Class对象的newInstance()方法来创建Class对象对应类的实例。 注意：利用newInstance创建对象：调用的类必须有无参的构造器 //Class&lt;?&gt;代表任何类的一个类对象。 //使用这个类对象可以为其他类进行实例化 //因为jvm加载类以后自动在堆区生成一个对应的*.Class对象 //该对象用于让JVM对进行所有*对象实例化。 Class&lt;?&gt; c = String.class; //Class&lt;?&gt; 中的 ? 是通配符，其实就是表示任意符合泛类定义条件的类，和直接使用 Class //效果基本一致，但是这样写更加规范，在某些类型转换时可以避免不必要的 unchecked 错误。 Object str = c.newInstance();（2）先通过Class对象获取指定的Constructor对象，再调用Constructor对象的newInstance()方法来创建实例。这种方法可以用指定的构造器构造类的实例。 //获取String所对应的Class对象 Class&lt;?&gt; c = String.class; //获取String类带一个String参数的构造器 Constructor constructor = c.getConstructor(String.class); //根据构造器创建实例 Object obj = constructor.newInstance(&quot;23333&quot;); System.out.println(obj);获取方法获取某个Class对象的方法集合，主要有以下几个方法： getDeclaredMethods()方法返回类或接口声明的所有方法，==包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法==。 public Method[] getDeclaredMethods() throws SecurityExceptiongetMethods()方法返回某个类的所有公用（public）方法，==包括其继承类的公用方法。== public Method[] getMethods() throws SecurityExceptiongetMethod方法返回一个特定的方法，其中第一个参数为方法名称，后面的参数为方法的参数对应Class的对象 public Method getMethod(String name, Class&lt;?&gt;... parameterTypes)只是这样描述的话可能难以理解，我们用例子来理解这三个方法：本文中的例子用到了以下这些类，用于反射的测试。 //注解类，可可用于表示方法，可以通过反射获取注解的内容。 //Java注解的实现是很多注框架实现注解配置的基础 @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface Invoke { }userbean的父类personbean public class PersonBean { private String name; int id; public String getName() { return name; } public void setName(String name) { this.name = name; }​} 接口user public interface User { public void login (); }userBean实现user接口，继承personbean public class UserBean extends PersonBean implements User{ @Override public void login() { } class B { } public String userName; protected int i; static int j; private int l; private long userId; public UserBean(String userName, long userId) { this.userName = userName; this.userId = userId; } public String getName() { return userName; } public long getId() { return userId; } @Invoke public static void staticMethod(String devName,int a) { System.out.printf(&quot;Hi %s, I&apos;m a static method&quot;, devName); } @Invoke public void publicMethod() { System.out.println(&quot;I&apos;m a public method&quot;); } @Invoke private void privateMethod() { System.out.println(&quot;I&apos;m a private method&quot;); } }1 getMethods和getDeclaredMethods的区别 public class 动态加载类的反射 { public static void main(String[] args) { try { Class clazz = Class.forName(&quot;com.javase.反射.UserBean&quot;); for (Field field : clazz.getDeclaredFields()) { // field.setAccessible(true); System.out.println(field); } //getDeclaredMethod*()获取的是类自身声明的所有方法，包含public、protected和private方法。 System.out.println(&quot;------共有方法------&quot;); // getDeclaredMethod*()获取的是类自身声明的所有方法，包含public、protected和private方法。 // getMethod*()获取的是类的所有共有方法，这就包括自身的所有public方法，和从基类继承的、从接口实现的所有public方法。 for (Method method : clazz.getMethods()) { String name = method.getName(); System.out.println(name); //打印出了UserBean.java的所有方法以及父类的方法 } System.out.println(&quot;------独占方法------&quot;); for (Method method : clazz.getDeclaredMethods()) { String name = method.getName(); System.out.println(name); } } catch (ClassNotFoundException e) { e.printStackTrace(); } } }2 打印一个类的所有方法及详细信息： public class 打印所有方法 { public static void main(String[] args) { Class userBeanClass = UserBean.class; Field[] fields = userBeanClass.getDeclaredFields(); //注意，打印方法时无法得到局部变量的名称，因为jvm只知道它的类型 Method[] methods = userBeanClass.getDeclaredMethods(); for (Method method : methods) { //依次获得方法的修饰符，返回类型和名称，外加方法中的参数 String methodString = Modifier.toString(method.getModifiers()) + &quot; &quot; ; // private static methodString += method.getReturnType().getSimpleName() + &quot; &quot;; // void methodString += method.getName() + &quot;(&quot;; // staticMethod Class[] parameters = method.getParameterTypes(); Parameter[] p = method.getParameters(); for (Class parameter : parameters) { methodString += parameter.getSimpleName() + &quot; &quot; ; // String } methodString += &quot;)&quot;; System.out.println(methodString); } //注意方法只能获取到其类型，拿不到变量名 /* public String getName() public long getId() public static void staticMethod(String int ) public void publicMethod() private void privateMethod()*/ } }获取构造器信息获取类构造器的用法与上述获取方法的用法类似。主要是通过Class类的getConstructor方法得到Constructor类的一个实例，而Constructor类有一个newInstance方法可以创建一个对象实例: public class 打印构造方法 { public static void main(String[] args) { // constructors Class&lt;?&gt; clazz = UserBean.class; Class userBeanClass = UserBean.class; //获得所有的构造方法 Constructor[] constructors = userBeanClass.getDeclaredConstructors(); for (Constructor constructor : constructors) { String s = Modifier.toString(constructor.getModifiers()) + &quot; &quot;; s += constructor.getName() + &quot;(&quot;; //构造方法的参数类型 Class[] parameters = constructor.getParameterTypes(); for (Class parameter : parameters) { s += parameter.getSimpleName() + &quot;, &quot;; } s += &quot;)&quot;; System.out.println(s); //打印结果//public com.javase.反射.UserBean(String, long, ) } } }获取类的成员变量（字段）信息主要是这几个方法，在此不再赘述： getFiled: 访问公有的成员变量getDeclaredField：所有已声明的成员变量。但不能得到其父类的成员变量getFileds和getDeclaredFields用法同上（参照Method） public class 打印成员变量 { public static void main(String[] args) { Class userBeanClass = UserBean.class; //获得该类的所有成员变量，包括static private Field[] fields = userBeanClass.getDeclaredFields(); for(Field field : fields) { //private属性即使不用下面这个语句也可以访问 // field.setAccessible(true); //因为类的私有域在反射中默认可访问，所以flag默认为true。 String fieldString = &quot;&quot;; fieldString += Modifier.toString(field.getModifiers()) + &quot; &quot;; // `private` fieldString += field.getType().getSimpleName() + &quot; &quot;; // `String` fieldString += field.getName(); // `userName` fieldString += &quot;;&quot;; System.out.println(fieldString); //打印结果 // public String userName; // protected int i; // static int j; // private int l; // private long userId; } } }调用方法当我们从类中获取了一个方法后，我们就可以用invoke()方法来调用这个方法。invoke方法的原型为: public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException public class 使用反射调用方法 { public static void main(String[] args) throws InvocationTargetException, IllegalAccessException, InstantiationException, NoSuchMethodException { Class userBeanClass = UserBean.class; //获取该类所有的方法，包括静态方法，实例方法。 //此处也包括了私有方法，只不过私有方法在用invoke访问之前要设置访问权限 //也就是使用setAccessible使方法可访问，否则会抛出异常 // // IllegalAccessException的解释是 // * An IllegalAccessException is thrown when an application tries // * to reflectively create an instance (other than an array), // * set or get a field, or invoke a method, but the currently // * executing method does not have access to the definition of // * the specified class, field, method or constructor. // getDeclaredMethod*()获取的是类自身声明的所有方法，包含public、protected和private方法。 // getMethod*()获取的是类的所有共有方法，这就包括自身的所有public方法，和从基类继承的、从接口实现的所有public方法。 //就是说，当这个类，域或者方法被设为私有访问，使用反射调用但是却没有权限时会抛出异常。 Method[] methods = userBeanClass.getDeclaredMethods(); // 获取所有成员方法 for (Method method : methods) { //反射可以获取方法上的注解，通过注解来进行判断 if (method.isAnnotationPresent(Invoke.class)) { // 判断是否被 @Invoke 修饰 //判断方法的修饰符是是static if (Modifier.isStatic(method.getModifiers())) { // 如果是 static 方法 //反射调用该方法 //类方法可以直接调用，不必先实例化 method.invoke(null, &quot;wingjay&quot;,2); // 直接调用，并传入需要的参数 devName } else { //如果不是类方法，需要先获得一个实例再调用方法 //传入构造方法需要的变量类型 Class[] params = {String.class, long.class}; //获取该类指定类型的构造方法 //如果没有这种类型的方法会报错 Constructor constructor = userBeanClass.getDeclaredConstructor(params); // 获取参数格式为 String,long 的构造函数 //通过构造方法的实例来进行实例化 Object userBean = constructor.newInstance(&quot;wingjay&quot;, 11); // 利用构造函数进行实例化，得到 Object if (Modifier.isPrivate(method.getModifiers())) { method.setAccessible(true); // 如果是 private 的方法，需要获取其调用权限 // Set the {@code accessible} flag for this object to // * the indicated boolean value. A value of {@code true} indicates that // * the reflected object should suppress Java language access // * checking when it is used. A value of {@code false} indicates // * that the reflected object should enforce Java language access checks. //通过该方法可以设置其可见或者不可见，不仅可以用于方法 //后面例子会介绍将其用于成员变量 //打印结果 // I&apos;m a public method // Hi wingjay, I&apos;m a static methodI&apos;m a private method } method.invoke(userBean); // 调用 method，无须参数​ } } } } }利用反射创建数组数组在Java里是比较特殊的一种类型，它可以赋值给一个Object Reference。下面我们看一看利用反射创建数组的例子： public class 用反射创建数组 { public static void main(String[] args) { Class&lt;?&gt; cls = null; try { cls = Class.forName(&quot;java.lang.String&quot;); } catch (ClassNotFoundException e) { e.printStackTrace(); } Object array = Array.newInstance(cls,25); //往数组里添加内容 Array.set(array,0,&quot;hello&quot;); Array.set(array,1,&quot;Java&quot;); Array.set(array,2,&quot;fuck&quot;); Array.set(array,3,&quot;Scala&quot;); Array.set(array,4,&quot;Clojure&quot;); //获取某一项的内容 System.out.println(Array.get(array,3)); //Scala } }其中的Array类为java.lang.reflect.Array类。我们通过Array.newInstance()创建数组对象，它的原型是: public static Object newInstance(Class&lt;?&gt; componentType, int length) throws NegativeArraySizeException { return newArray(componentType, length); }而newArray()方法是一个Native方法，它在Hotspot JVM里的具体实现我们后边再研究，这里先把源码贴出来 private static native Object newArray(Class&lt;?&gt; componentType, int length) throws NegativeArraySizeException;​ Java反射常见面试题什么是反射？反射是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。 哪里用到反射机制？JDBC中，利用反射动态加载了数据库驱动程序。Web服务器中利用反射调用了Sevlet的服务方法。Eclispe等开发工具利用反射动态刨析对象的类型与结构，动态提示对象的属性和方法。很多框架都用到反射机制，注入属性，调用方法，如Spring。 什么叫对象序列化，什么是反序列化，实现对象序列化需要做哪些工作？对象序列化，将对象中的数据编码为字节序列的过程。反序列化；将对象的编码字节重新反向解码为对象的过程。JAVA提供了API实现了对象的序列化和反序列化的功能，使用这些API时需要遵守如下约定：被序列化的对象类型需要实现序列化接口，此接口是标志接口，没有声明任何的抽象方法，JAVA编译器识别这个接口，自动的为这个类添加序列化和反序列化方法。为了保持序列化过程的稳定，建议在类中添加序列化版本号。不想让字段放在硬盘上就加transient以下情况需要使用 Java 序列化：想把的内存中的对象状态保存到一个文件中或者数据库中时候；想用套接字在网络上传送对象的时候；想通过RMI（远程方法调用）传输对象的时候。 反射机制的优缺点？优点：可以动态执行，在运行期间根据业务功能动态执行方法、访问属性，最大限度发挥了java的灵活性。缺点：对性能有影响，这类操作总是慢于直接执行java代码。 动态代理是什么？有哪些应用？动态代理是运行时动态生成代理类。动态代理的应用有 Spring AOP数据查询、测试框架的后端 mock、rpc，Java注解对象获取等。 怎么实现动态代理？JDK 原生动态代理和 cglib 动态代理。JDK 原生动态代理是基于接口实现的，而 cglib 是基于继承当前类的子类实现的。 Java反射机制的作用在运行时判断任意一个对象所属的类在运行时构造任意一个类的对象在运行时判断任意一个类所具有的成员变量和方法在运行时调用任意一个对象的方法 如何使用Java的反射?通过一个全限类名创建一个对象 Class.forName(“全限类名”); 例如：com.mysql.jdbc.Driver Driver类已经被加载到 jvm中，并且完成了类的初始化工作就行了类名.class; 获取Class&lt;？&gt; clz 对象对象.getClass(); 获取构造器对象，通过构造器new出一个对象 Clazz.getConstructor([String.class]);Con.newInstance([参数]);通过class对象创建一个实例对象（就相当与new类名（）无参构造器)Cls.newInstance(); 通过class对象获得一个属性对象 Field c=cls.getFields()：获得某个类的所有的公共（public）的字段，包括父类中的字段。Field c=cls.getDeclaredFields()：获得某个类的所有声明的字段，即包括public、private和proteced，但是不包括父类的声明字段 通过class对象获得一个方法对象 Cls.getMethod(“方法名”,class……parameaType);（只能获取公共的）Cls.getDeclareMethod(“方法名”);（获取任意修饰的方法，不能执行私有）M.setAccessible(true);（让私有的方法可以执行）让方法执行1）. Method.invoke(obj实例对象,obj可变参数);—–（是有返回值的） 参考文章http://www.cnblogs.com/peida/archive/2013/04/26/3038503.htmlhttp://www.cnblogs.com/whoislcj/p/5671622.htmlhttps://blog.csdn.net/grandgrandpa/article/details/84832343http://blog.csdn.net/lylwo317/article/details/52163304https://blog.csdn.net/qq_37875585/article/details/89340495 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习MySQL数据库12：从实践sql语句优化开始]]></title>
    <url>%2F2019%2F09%2F12%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0MySQL%E6%95%B0%E6%8D%AE%E5%BA%9312%EF%BC%9A%E4%BB%8E%E5%AE%9E%E8%B7%B5sql%E8%AF%AD%E5%8F%A5%E4%BC%98%E5%8C%96%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量： 字段 尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED VARCHAR的长度只分配真正需要的空间 使用枚举或整数代替字符串类型 尽量使用TIMESTAMP而非DATETIME， 单表不要有太多字段，建议在20以内 避免使用NULL字段，很难查询优化且占用额外索引空间 用整型来存IP 索引 索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描 应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描 值分布很稀少的字段不适合建索引，例如”性别”这种只有两三个值的字段 字符字段只建前缀索引 字符字段最好不要做主键 不用外键，由程序保证约束 尽量不用UNIQUE，由程序保证约束 使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引 查询SQL 可通过开启慢查询日志来找出较慢的SQL 不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边 sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库 不用SELECT * OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内 不用函数和触发器，在应用程序实现 避免%xxx式查询 少用JOIN 使用同类型进行比较，比如用&#39;123&#39;和&#39;123&#39;比，123和123比 尽量避免在WHERE子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描 对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5 列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大 引擎目前广泛使用的是MyISAM和InnoDB两种引擎： MyISAMMyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是： 不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁 不支持事务 不支持外键 不支持崩溃后的安全恢复 在表有读取查询的同时，支持往表中插入新纪录 支持BLOB和TEXT的前500个字符索引，支持全文索引 支持延迟更新索引，极大提升写入性能 对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用 InnoDBInnoDB在MySQL 5.5后成为默认索引，它的特点是： 支持行锁，采用MVCC来支持高并发 支持事务 支持外键 支持崩溃后的安全恢复 不支持全文索引 总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表 0、自己写的海量数据sql优化实践首先是建表和导数据的过程。 参考https://nsimple.top/archives/mysql-create-million-data.html 有时候我们需要对大数据进行测试，本地一般没有那么多数据，就需要我们自己生成一些。下面会借助内存表的特点进行生成百万条测试数据。 创建一个临时内存表, 做数据插入的时候会比较快些 SQL 1-- 创建一个临时内存表DROP TABLE IF EXISTS `vote_record_memory`;CREATE TABLE `vote_record_memory` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL DEFAULT &apos;&apos;, `vote_num` int(10) unsigned NOT NULL DEFAULT &apos;0&apos;, `group_id` int(10) unsigned NOT NULL DEFAULT &apos;0&apos;, `status` tinyint(2) unsigned NOT NULL DEFAULT &apos;1&apos;, `create_time` datetime NOT NULL DEFAULT &apos;0000-00-00 00:00:00&apos;, PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING HASH) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; – 创建一个普通表，用作模拟大数据的测试用例 SQL 1DROP TABLE IF EXISTS `vote_record`;CREATE TABLE `vote_record` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;用户Id&apos;, `vote_num` int(10) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;投票数&apos;, `group_id` int(10) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;用户组id 0-未激活用户 1-普通用户 2-vip用户 3-管理员用户&apos;, `status` tinyint(2) unsigned NOT NULL DEFAULT &apos;1&apos; COMMENT &apos;状态 1-正常 2-已删除&apos;, `create_time` int(10) unsigned NOT NULL DEFAULT &apos;0000-00-00 00:00:00&apos; COMMENT &apos;创建时间&apos;, PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`) USING HASH COMMENT &apos;用户ID哈希索引&apos;) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;投票记录表&apos;; 为了数据的随机性和真实性，我们需要创建一个可生成长度为n的随机字符串的函数。 SQL 1-- 创建生成长度为n的随机字符串的函数DELIMITER // -- 修改MySQL delimiter：&apos;//&apos;DROP FUNCTION IF EXISTS `rand_string` //SET NAMES utf8 //CREATE FUNCTION `rand_string` (n INT) RETURNS VARCHAR(255) CHARSET &apos;utf8&apos;BEGIN DECLARE char_str varchar(100) DEFAULT &apos;abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&apos;; DECLARE return_str varchar(255) DEFAULT &apos;&apos;; DECLARE i INT DEFAULT 0; WHILE i &lt; n DO SET return_str = concat(return_str, substring(char_str, FLOOR(1 + RAND()*62), 1)); SET i = i+1; END WHILE; RETURN return_str;END // 为了操作方便，我们再创建一个插入数据的存储过程 SQL 1-- 创建插入数据的存储过程DROP PROCEDURE IF EXISTS `add_vote_record_memory` //CREATE PROCEDURE `add_vote_record_memory`(IN n INT)BEGIN DECLARE i INT DEFAULT 1; DECLARE vote_num INT DEFAULT 0; DECLARE group_id INT DEFAULT 0; DECLARE status TINYINT DEFAULT 1; WHILE i &lt; n DO SET vote_num = FLOOR(1 + RAND() * 10000); SET group_id = FLOOR(0 + RAND()*3); SET status = FLOOR(1 + RAND()*2); INSERT INTO `vote_record_memory` VALUES (NULL, rand_string(20), vote_num, group_id, status, NOW()); SET i = i + 1; END WHILE;END //DELIMITER ; -- 改回默认的 MySQL delimiter：&apos;;&apos; 开始执行存储过程，等待生成数据(10W条生成大约需要40分钟) SQL 1-- 调用存储过程 生成100W条数据CALL add_vote_record_memory(1000000); 查询内存表已生成记录(为了下步测试，目前仅生成了105645条) SQL 1SELECT count(*) FROM `vote_record_memory`;-- count(*)-- 105646 把数据从内存表插入到普通表中(10w条数据13s就插入完了) SQL 1INSERT INTO vote_record SELECT * FROM `vote_record_memory`; 查询普通表已的生成记录 SQL 1SELECT count(*) FROM `vote_record`;-- count(*)-- 105646 如果一次性插入普通表太慢，可以分批插入，这就需要写个存储过程了： SQL 1-- 参数n是每次要插入的条数-- lastid是已导入的最大idCREATE PROCEDURE `copy_data_from_tmp`(IN n INT)BEGIN DECLARE lastid INT DEFAULT 0; SELECT MAX(id) INTO lastid FROM `vote_record`; INSERT INTO `vote_record` SELECT * FROM `vote_record_memory` where id &gt; lastid LIMIT n;END 调用存储过程: SQL 1-- 调用存储过程 插入60w条CALL copy_data_from_tmp(600000); SELECT * FROM vote_record； 全表查询 建完表以后开启慢查询日志，具体参考下面的例子，然后学会用explain。windows慢日志的位置在c盘，另外，使用client工具也可以记录慢日志，所以不一定要用命令行来执行测试，否则大表数据在命令行中要显示的非常久。 1 全表扫描select * from vote_record 慢日志 SET timestamp=1529034398; select * from vote_record; Time: 2018-06-15T03:52:58.804850Z User@Host: root[root] @ localhost [::1] Id: 74 Query_time: 3.166424 Lock_time: 0.000000 Rows_sent: 900500 Rows_examined: 999999 耗时3秒，我设置的门槛是一秒。所以记录了下来。 explain执行计划 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N ALL \N \N \N \N 996507 100.00 \N 全表扫描耗时3秒多，用不到索引。 2 select * from vote_record where vote_num &gt; 1000 没有索引，所以相当于全表扫描，一样是3.5秒左右 3 select * from vote_record where vote_num &gt; 1000 *加索引create * CREATE INDEX vote ON vote_record(vote_num); explain查看执行计划 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N ALL votenum,vote \N \N \N 996507 50.00 Using where 还是没用到索引，因为不符合最左前缀匹配。查询需要3.5秒左右 最后修改一下sql语句 EXPLAIN SELECT * FROM vote_record WHERE id &gt; 0 AND vote_num &gt; 1000; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N range PRIMARY,votenum,vote PRIMARY 4 \N 498253 50.00 Using where 用到了索引，但是只用到了主键索引。再修改一次 EXPLAIN SELECT * FROM vote_record WHERE id &gt; 0 AND vote_num = 1000; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N index_merge PRIMARY,votenum,vote votenum,PRIMARY 8,4 \N 51 100.00 Using intersect(votenum,PRIMARY); Using where 用到了两个索引，votenum,PRIMARY。 这是为什么呢。 再看一个语句 EXPLAIN SELECT * FROM vote_record WHERE id = 1000 AND vote_num &gt; 1000 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N const PRIMARY,votenum PRIMARY 4 const 1 100.00 \N 也只有主键用到了索引。这是因为只有最左前缀索引可以用&gt;或&lt;，其他索引用&lt;或者&gt;会导致用不到索引。 下面是几个网上参考的例子： 一：索引是sql语句优化的关键，学会使用慢日志和执行计划分析sql 背景：使用A电脑安装mysql,B电脑通过xshell方式连接，数据内容我都已经创建好，现在我已正常的进入到mysql中 步骤1：设置慢查询日志的超时时间，先查看日志存放路径查询慢日志的地址，因为有慢查询的内容，就会到这个日志中： show global variables like "%slow%"; 2.开启慢查询日志 set global slow_query_log=on; 3.查看慢查询日志的设置时间，是否是自己需要的 show global variables like "%long%"; 4.如果不是自己想的时间，修改慢查询时间，只要超过了以下的设置时间，查询的日志就会到刚刚的日志中，我设置查询时间超过1S就进入到慢查询日志中 set global long_query_time=1; 5.大数据已准备，进行数据的查询，xshell最好开两个窗口，一个查看日志，一个执行内容 Sql查询语句：select sql_no_cache * from employees_tmp where first_name='Duangkaew' and gender='M' 发现查数据的总时间去掉了17.74S 查看日志：打开日志 标记1：执行的sql语句 标记2：执行sql的时间，我的是10点52执行的 标记3：使用那台机器 标记4：执行时间，query_tims,查询数据的时间 标记5：不知道是干嘛的 标记6：执行耗时的sql语句，我在想我1的应该是截取错了！但是记住最后一定是显示耗时是因为执行什么sql造成的 6.执行打印计划，主要是查看是否使用了索引等其他内容,主要就是在sql前面加上explain 关键字 explain select sql_no_cache * from employees_tmp where first_name='Duangkaew' and gender='M'; 描述extra中，表示只使用了where条件，没有其他什么索引之类的 7.进行sql优化，建一个fist_name的索引，索引就是将你需要的数据先给筛选出来，这样就可以节省很多扫描时间 create index firstname on employees_tmp(first_name); 注：创建索引时会很慢，是对整个表做了一个复制功能，并进行数据的一些分类（我猜是这样，所以会很慢） 8.查看建立的索引 show index from employees_tmp; 9.在执行查询语句，查看语句的执行时间 select sql_no_cache * from employees_tmp where first_name='Duangkaew' and gender='M' 发现时间已经有所提升了，其实选择索引也不一开始就知道，我们在试试使用性别，gender进行索引 10.删除已经有的索引，删除索引： drop index first_name on employees_tmp; 11.创建性别的索引(性别是不怎么好的索引方式，因为有很多重复数据) create index index_gendar on employees_tmp(gender); 在执行sql语句查询数据，查看查询执行时间，没有创建比较优秀的索引，导致查询时间还变长了， 为嘛还变长了，这个我没有弄懂 12.我们在试试使用创建组合索引，使用性别和姓名 alter table employees_tmp add index idx_union (first_name,gender); 在执行sql查看sql数据的执行时间 select sql_no_cache * from employees_tmp where first_name='Duangkaew' and gender='M' 速度提升了N多倍啊 查看创建的索引 show index from employees_tmp; 索引建的好真的一个好帮手，建不好就是费时的一个操作 目前还不知道为什么建立性别的索引会这么慢 二：sql优化注意要点，比如索引是否用到，查询优化是否改变了执行计划，以及一些细节 场景 我用的数据库是mysql5.6，下面简单的介绍下场景 课程表 1create table Course( c_id int PRIMARY KEY, name varchar(10) ) 数据100条 学生表: 1create table Student( id int PRIMARY KEY, name varchar(10) ) 数据70000条 学生成绩表SC 1CREATE table SC( sc_id int PRIMARY KEY, s_id int, c_id int, score int ) 数据70w条 查询目的： 查找语文考100分的考生 查询语句： 1select s.* from Student s where s.s_id in (select s_id from SC sc where sc.c_id = 0 and sc.score = 100 ) 执行时间：30248.271s 晕,为什么这么慢，先来查看下查询计划： 1EXPLAIN select s.* from Student s where s.s_id in (select s_id from SC sc where sc.c_id = 0 and sc.score = 100 ) 发现没有用到索引，type全是ALL，那么首先想到的就是建立一个索引，建立索引的字段当然是在where条件的字段。 先给sc表的c_id和score建个索引 1CREATE index sc_c_id_index on SC(c_id); 1CREATE index sc_score_index on SC(score); 再次执行上述查询语句，时间为: 1.054s 快了3w多倍，大大缩短了查询时间，看来索引能极大程度的提高查询效率，看来建索引很有必要，很多时候都忘记建 索引了，数据量小的的时候压根没感觉，这优化感觉挺爽。 但是1s的时间还是太长了，还能进行优化吗，仔细看执行计划： 查看优化后的sql: 1SELECT `YSB`.`s`.`s_id` AS `s_id`, `YSB`.`s`.`name` AS `name`FROM `YSB`.`Student` `s`WHERE &lt; in_optimizer &gt; ( `YSB`.`s`.`s_id` ,&lt; EXISTS &gt; ( SELECT 1 FROM `YSB`.`SC` `sc` WHERE ( (`YSB`.`sc`.`c_id` = 0) AND (`YSB`.`sc`.`score` = 100) AND ( &lt; CACHE &gt; (`YSB`.`s`.`s_id`) = `YSB`.`sc`.`s_id` ) ) ) ) 补充：这里有网友问怎么查看优化后的语句 方法如下： 在命令窗口执行 有type=all 按照我之前的想法，该sql的执行的顺序应该是先执行子查询 1select s_id from SC sc where sc.c_id = 0 and sc.score = 100 耗时：0.001s 得到如下结果： 然后再执行 1select s.* from Student s where s.s_id in(7,29,5000) 耗时：0.001s 这样就是相当快了啊，Mysql竟然不是先执行里层的查询，而是将sql优化成了exists子句，并出现了EPENDENT SUBQUERY， mysql是先执行外层查询，再执行里层的查询，这样就要循环70007*11=770077次。 那么改用连接查询呢？ 1SELECT s.* from Student s INNER JOIN SC sc on sc.s_id = s.s_id where sc.c_id=0 and sc.score=100 这里为了重新分析连接查询的情况，先暂时删除索引sc_c_id_index，sc_score_index 执行时间是：0.057s 效率有所提高，看看执行计划： 这里有连表的情况出现，我猜想是不是要给sc表的s_id建立个索引 CREATE index sc_s_id_index on SC(s_id); show index from SC 在执行连接查询 时间: 1.076s，竟然时间还变长了，什么原因？查看执行计划： 优化后的查询语句为： 1SELECT `YSB`.`s`.`s_id` AS `s_id`, `YSB`.`s`.`name` AS `name`FROM `YSB`.`Student` `s`JOIN `YSB`.`SC` `sc`WHERE ( ( `YSB`.`sc`.`s_id` = `YSB`.`s`.`s_id` ) AND (`YSB`.`sc`.`score` = 100) AND (`YSB`.`sc`.`c_id` = 0) ) 貌似是先做的连接查询，再执行的where过滤 回到前面的执行计划： 这里是先做的where过滤，再做连表，执行计划还不是固定的，那么我们先看下标准的sql执行顺序： 正常情况下是先join再where过滤，但是我们这里的情况，如果先join，将会有70w条数据发送join做操，因此先执行where 过滤是明智方案，现在为了排除mysql的查询优化，我自己写一条优化后的sql 1SELECT s.*FROM ( SELECT * FROM SC sc WHERE sc.c_id = 0 AND sc.score = 100 ) tINNER JOIN Student s ON t.s_id = s.s_id 即先执行sc表的过滤，再进行表连接，执行时间为：0.054s 和之前没有建s_id索引的时间差不多 查看执行计划： 先提取sc再连表，这样效率就高多了，现在的问题是提取sc的时候出现了扫描表，那么现在可以明确需要建立相关索引 1CREATE index sc_c_id_index on SC(c_id); 1CREATE index sc_score_index on SC(score); 再执行查询： 1SELECT s.*FROM ( SELECT * FROM SC sc WHERE sc.c_id = 0 AND sc.score = 100 ) tINNER JOIN Student s ON t.s_id = s.s_id 执行时间为：0.001s，这个时间相当靠谱，快了50倍 执行计划： 我们会看到，先提取sc，再连表，都用到了索引。 那么再来执行下sql 1SELECT s.* from Student s INNER JOIN SC sc on sc.s_id = s.s_id where sc.c_id=0 and sc.score=100 执行时间0.001s 执行计划： 这里是mysql进行了查询语句优化，先执行了where过滤，再执行连接操作，且都用到了索引。 总结： 1.mysql嵌套子查询效率确实比较低 2.可以将其优化成连接查询 3.建立合适的索引 4.学会分析sql执行计划，mysql会对sql进行优化，所以分析执行计划很重要 由于时间问题，这篇文章先写到这里，后续再分享其他的sql优化经历。 三、海量数据分页查找时如何使用主键索引进行优化 mysql百万级分页优化 普通分页 数据分页在网页中十分多见，分页一般都是limit start,offset,然后根据页码page计算start select * from user limit **1**,**20** 这种分页在几十万的时候分页效率就会比较低了，MySQL需要从头开始一直往后计算，这样大大影响效率 SELECT * from user limit **100001**,**20**; //time **0**.151s explain SELECT * from user limit **100001**,**20**; 我们可以用explain分析下语句，没有用到任何索引，MySQL执行的行数是16W+，于是我们可以想用到索引去实现分页 优化分页 使用主键索引来优化数据分页 select * from user where id>(select id from user where id>=**100000** limit **1**) limit **20**; //time **0**.003s 使用explain分析语句，MySQL这次扫描的行数是8W+，时间也大大缩短。 explain select * from user where id>(select id from user where id>=**100000** limit **1**) limit **20**; ![](https://oscimg.oschina.net/oscnet/05fffbffc5e3ef9add4719846ad53f25099.jpg) 总结 在数据量比较大的时候，我们尽量去利用索引来优化语句。上面的优化方法如果id不是主键索引，查询效率比第一种还要低点。我们可以先使用explain来分析语句，查看语句的执行顺序和执行性能。 转载于:https://my.oschina.net/alicoder/blog/3097141 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列11：深入理解Java中的回调机制]]></title>
    <url>%2F2019%2F09%2F11%2F11%E8%A7%A3%E8%AF%BBJava%E4%B8%AD%E7%9A%84%E5%9B%9E%E8%B0%83%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 模块间的调用本部分摘自https://www.cnblogs.com/xrq730/p/6424471.html 在一个应用系统中，无论使用何种语言开发，必然存在模块之间的调用，调用的方式分为几种： （1）同步调用 同步调用是最基本并且最简单的一种调用方式，类A的方法a()调用类B的方法b()，一直等待b()方法执行完毕，a()方法继续往下走。这种调用方式适用于方法b()执行时间不长的情况，因为b()方法执行时间一长或者直接阻塞的话，a()方法的余下代码是无法执行下去的，这样会造成整个流程的阻塞。 （2）异步调用 异步调用是为了解决同步调用可能出现阻塞，导致整个流程卡住而产生的一种调用方式。类A的方法方法a()通过新起线程的方式调用类B的方法b()，代码接着直接往下执行，这样无论方法b()执行时间多久，都不会阻塞住方法a()的执行。 但是这种方式，由于方法a()不等待方法b()的执行完成，在方法a()需要方法b()执行结果的情况下（视具体业务而定，有些业务比如启异步线程发个微信通知、刷新一个缓存这种就没必要），必须通过一定的方式对方法b()的执行结果进行监听。 在Java中，可以使用Future+Callable的方式做到这一点，具体做法可以参见我的这篇文章Java多线程21：多线程下其他组件之CyclicBarrier、Callable、Future和FutureTask。 （3）回调 1、什么是回调？一般来说，模块之间都存在一定的调用关系，从调用方式上看，可以分为三类同步调用、异步调用和回调。同步调用是一种阻塞式调用，即在函数A的函数体里通过书写函数B的函数名来调用之，使内存中对应函数B的代码得以执行。异步调用是一种类似消息或事件的机制解决了同步阻塞的问题，例如 A通知 B后，他们各走各的路，互不影响，不用像同步调用那样， A通知 B后，非得等到 B走完后， A才继续走 。回调是一种双向的调用模式，也就是说，被调用的接口被调用时也会调用对方的接口，例如A要调用B，B在执行完又要调用A。 2、回调的用途回调一般用于层间协作，上层将本层函数安装在下层，这个函数就是回调，而下层在一定条件下触发回调。例如作为一个驱动，是一个底层，他在收到一个数据时，除了完成本层的处理工作外，还将进行回调，将这个数据交给上层应用层来做进一步处理，这在分层的数据通信中很普遍。 多线程中的“回调”Java多线程中可以通过callable和future或futuretask结合来获取线程执行后的返回值。实现方法是通过get方法来调用callable的call方法获取返回值。 其实这种方法本质上不是回调，回调要求的是任务完成以后被调用者主动回调调用者的接口。而这里是调用者主动使用get方法阻塞获取返回值。 public class 多线程中的回调 { //这里简单地使用future和callable实现了线程执行完后 public static void main(String[] args) throws ExecutionException, InterruptedException { ExecutorService executor = Executors.newCachedThreadPool(); Future&lt;String&gt; future = executor.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { System.out.println(&quot;call&quot;); TimeUnit.SECONDS.sleep(1); return &quot;str&quot;; } }); //手动阻塞调用get通过call方法获得返回值。 System.out.println(future.get()); //需要手动关闭，不然线程池的线程会继续执行。 executor.shutdown(); //使用futuretask同时作为线程执行单元和数据请求单元。 FutureTask&lt;Integer&gt; futureTask = new FutureTask(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { System.out.println(&quot;dasds&quot;); return new Random().nextInt(); } }); new Thread(futureTask).start(); //阻塞获取返回值 System.out.println(futureTask.get()); } @Test public void test () { Callable callable = new Callable() { @Override public Object call() throws Exception { return null; } }; FutureTask futureTask = new FutureTask(callable); } }Java回调机制实战曾经自己偶尔听说过回调机制，隐隐约约能够懂一些意思，但是当让自己写一个简单的示例程序时，自己就傻眼了。随着工作经验的增加，自己经常听到这儿使用了回调，那儿使用了回调，自己是时候好好研究一下Java回调机制了。网上关于Java回调的文章一抓一大把，但是看完总是云里雾里，不知所云，特别是看到抓取别人的代码走两步时，总是现眼。于是自己决定写一篇关于Java机制的文章，以方便大家和自己更深入的学习Java回调机制。 首先，什么是回调函数，引用百度百科的解释：回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。回调函数不是由该函数的实现方直接调用，而是在特定的事件或条件发生时由另外的一方调用的，用于对该事件或条件进行响应[2]. 不好意思，上述解释我看了好几遍，也没理解其中深刻奥秘，相信一些读者你也一样。光说不练假把式，咱们还是以实战理解脉络。 实例一 ： 同步调用本文以底层服务BottomService和上层服务UpperService为示例，利用上层服务调用底层服务，整体执行过程如下： 第一步: 执行UpperService.callBottomService(); 第二步: 执行BottomService.bottom(); 第三步:执行UpperService.upperTaskAfterCallBottomService() 1.1 同步调用代码同步调用时序图： 同步调用时序图 1.1.1 底层服务类:BottomService.java 12345678910111213141516171819202122232425262728package synchronization.demo;/*** Created by lance on 2017/1/19.*/public class BottomService &#123;public String bottom(String param) &#123;try &#123; // 模拟底层处理耗时，上层服务需要等待Thread.sleep(3000);&#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125;return param +&quot; BottomService.bottom() execute --&gt;&quot;;&#125;&#125; 1.1.2 上层服务接口: UpperService.java 123456789101112131415package synchronization.demo;/*** Created by lance on 2017/1/19.*/public interface UpperService &#123;public void upperTaskAfterCallBottomService(String upperParam);public String callBottomService(final String param);&#125; 1.1.3 上层服务接口实现类:UpperServiceImpl.java 1234567891011121314151617181920212223242526272829303132333435package synchronization.demo;/*** Created by lance on 2017/1/19.*/public class UpperServiceImpl implements UpperService &#123;private BottomService bottomService;@Overridepublic void upperTaskAfterCallBottomService(String upperParam) &#123;System.out.println(upperParam + &quot; upperTaskAfterCallBottomService() execute.&quot;);&#125;public UpperServiceImpl(BottomService bottomService) &#123;this.bottomService = bottomService;&#125;@Overridepublic String callBottomService(final String param) &#123;return bottomService.bottom(param + &quot; callBottomService.bottom() execute --&gt; &quot;);&#125;&#125; 1.1.4 Test测试类:Test.java 12345678910111213141516171819202122232425262728293031package synchronization.demo;import java.util.Date;/*** Created by lance on 2017/1/19.*/public class Test &#123;public static void main(String[] args) &#123;BottomService bottomService = new BottomService();UpperService upperService = new UpperServiceImpl(bottomService);System.out.println(&quot;=============== callBottomService start ==================:&quot; + new Date());String result = upperService.callBottomService(&quot;callBottomService start --&gt; &quot;);//upperTaskAfterCallBottomService执行必须等待callBottomService()调用BottomService.bottom()方法返回后才能够执行upperService.upperTaskAfterCallBottomService(result);System.out.println(&quot;=============== callBottomService end ====================:&quot; + new Date());&#125;&#125; 1.1.5 输出结果: 12345=============== callBottomService start ==================:Thu Jan 19 14:59:58 CST 2017callBottomService start --&gt; callBottomService.bottom() execute --&gt; BottomService.bottom() execute --&gt; upperTaskAfterCallBottomService() execute.=============== callBottomService end ====================:Thu Jan 19 15:00:01 CST 2017 注意输出结果： 是同步方式，Test调用callBottomService()等待执行结束，然后再执行下一步，即执行结束。callBottomService开始执行时间为Thu Jan 19 14:59:58 CST 2017，执行结束时间为Thu Jan 19 15:00:01 CST 2017，耗时3秒钟，与模拟的耗时时间一致，即3000毫秒。 实例二：由浅入深前几天公司面试有问道java回调的问题，因为这方面也没有太多研究，所以回答的含糊不清，这回特意来补习一下。看了看网上的回调解释和例子，都那么的绕口，得看半天才能绕回来，其实吧，回调是个很简单的机制。在这里我用简单的语言先来解释一下：假设有两个类，分别是A和B，在A中有一个方法a()，B中有一个方法b()；在A里面调用B中的方法b()，而方法b()中调用了方法a()，这样子就同时实现了b()和a()两个方法的功能。 疑惑：为啥这么麻烦，我直接在类A中的B.b()方法下调用a()方法就行了呗。解答：回调更像是一个约定，就是如果我调用了b()方法，那么就必须要回调，而不需要显示调用一、Java的回调-浅我们用例子来解释：小明和小李相约一起去吃早饭，但是小李起的有点晚要先洗漱，等小李洗漱完成后，通知小明再一起去吃饭。小明就是类A，小李就是类B。一起去吃饭这个事件就是方法a(),小李去洗漱就是方法b()。 public class XiaoMing { //小明和小李一起吃饭 public void eatFood() { XiaoLi xl = new XiaoLi(); //A调用B的方法 xl.washFace(); } public void eat() { System.out.print(&quot;小明和小李一起去吃大龙虾&quot;); } } 那么怎么让小李洗漱完后在通知小明一起去吃饭呢 public class XiaoMing { //小明和小李一起吃饭 public void eatFood() { XiaoLi xl = new XiaoLi(); //A调用B的方法 xl.washFace(); eat(); } public void eat() { System.out.print(&quot;小明和小李一起去吃大龙虾&quot;); } }不过上面已经说过了这个不是回调函数，所以不能这样子，正确的方式如下 public class XiaoLi{//小李 public void washFace() { System.out.print(&quot;小李要洗漱&quot;); XiaoMing xm = new XiaoMing(); //B调用A的方法 xm.eat();//洗漱完后，一起去吃饭 } }这样子就可以实现washFace()同时也能实现eat()。小李洗漱完后，再通知小明一起去吃饭，这就是回调。 二、Java的回调-中可是细心的伙伴可能会发现，小李的代码完全写死了，这样子的场合可能适用和小明一起去吃饭，可是假如小李洗漱完不吃饭了，想和小王上网去，这样子就不适用了。其实上面是伪代码，仅仅是帮助大家理解的，真正情况下是需要利用接口来设置回调的。现在我们继续用小明和小李去吃饭的例子来讲讲接口是如何使用的。 小明和小李相约一起去吃早饭，但是小李起的有点晚要先洗漱，等小李洗漱完成后，通知小明再一起去吃饭。小明就是类A，小李就是类B。不同的是我们新建一个吃饭的接口EatRice，接口中有个抽象方法eat()。在小明中调用这个接口，并实现eat()；小李声明这个接口对象，并且调用这个接口的抽象方法。这里可能有点绕口，不过没关系，看看例子就很清楚了。 EatRice接口： public interface EatRice { public void eat(String food); } 小明： public class XiaoMing implements EatRice{//小明 //小明和小李一起吃饭 public void eatFood() { XiaoLi xl = new XiaoLi(); //A调用B的方法 xl.washFace(&quot;大龙虾&quot;, this);//this指的是小明这个类实现的EatRice接口 } @Override public void eat(String food) { // TODO Auto-generated method stub System.out.println(&quot;小明和小李一起去吃&quot; + food); } } 小李: public class XiaoLi{//小李 public void washFace(String food,EatRice er) { System.out.println(&quot;小李要洗漱&quot;); //B调用了A的方法 er.eat(food); } } 测试Demo: public class demo { public static void main(String args[]) { XiaoMing xm = new XiaoMing(); xm.eatFood(); } }测试结果： 这样子就通过接口的形式实现了软编码。通过接口的形式我可以实现小李洗漱完后，和小王一起去上网。代码如下 public class XiaoWang implements EatRice{//小王 //小王和小李一起去上网 public void eatFood() { XiaoLi xl = new XiaoLi(); //A调用B的方法 xl.washFace(&quot;轻舞飞扬上网&quot;, this); } @Override public void eat(String bar) { // TODO Auto-generated method stub System.out.println(&quot;小王和小李一起去&quot; + bar); } }实例三：Tom做题数学老师让Tom做一道题，并且Tom做题期间数学老师不用盯着Tom，而是在玩手机，等Tom把题目做完后再把答案告诉老师。 1 数学老师需要Tom的一个引用，然后才能将题目发给Tom。 2 数学老师需要提供一个方法以便Tom做完题目以后能够将答案告诉他。 3 Tom需要数学老师的一个引用，以便Tom把答案给这位老师，而不是隔壁的体育老师。 回调接口，可以理解为老师接口 //回调指的是A调用B来做一件事，B做完以后将结果告诉给A，这期间A可以做别的事情。 //这个接口中有一个方法，意为B做完题目后告诉A时使用的方法。 //所以我们必须提供这个接口以便让B来回调。 //回调接口， public interface CallBack { void tellAnswer(int res); }数学老师类 //老师类实例化回调接口，即学生写完题目之后通过老师的提供的方法进行回调。 //那么学生如何调用到老师的方法呢，只要在学生类的方法中传入老师的引用即可。 //而老师需要指定学生答题，所以也要传入学生的实例。 public class Teacher implements CallBack{ private Student student; Teacher(Student student) { this.student = student; } void askProblem (Student student, Teacher teacher) { //main方法是主线程运行，为了实现异步回调，这里开启一个线程来操作 new Thread(new Runnable() { @Override public void run() { student.resolveProblem(teacher); } }).start(); //老师让学生做题以后，等待学生回答的这段时间，可以做别的事，比如玩手机.\ //而不需要同步等待，这就是回调的好处。 //当然你可以说开启一个线程让学生做题就行了，但是这样无法让学生通知老师。 //需要另外的机制去实现通知过程。 // 当然，多线程中的future和callable也可以实现数据获取的功能。 for (int i = 1;i &lt; 4;i ++) { System.out.println(&quot;等学生回答问题的时候老师玩了 &quot; + i + &quot;秒的手机&quot;); } } @Override public void tellAnswer(int res) { System.out.println(&quot;the answer is &quot; + res); } }学生接口 //学生的接口，解决问题的方法中要传入老师的引用，否则无法完成对具体实例的回调。 //写为接口的好处就是，很多个学生都可以实现这个接口，并且老师在提问题时可以通过 //传入List&lt;Student&gt;来聚合学生，十分方便。 public interface Student { void resolveProblem (Teacher teacher); }学生Tom public class Tom implements Student{ @Override public void resolveProblem(Teacher teacher) { try { //学生思考了3秒后得到了答案，通过老师提供的回调方法告诉老师。 Thread.sleep(3000); System.out.println(&quot;work out&quot;); teacher.tellAnswer(111); } catch (InterruptedException e) { e.printStackTrace(); } }测试类 public class Test { public static void main(String[] args) { //测试 Student tom = new Tom(); Teacher lee = new Teacher(tom); lee.askProblem(tom, lee); //结果 // 等学生回答问题的时候老师玩了 1秒的手机 // 等学生回答问题的时候老师玩了 2秒的手机 // 等学生回答问题的时候老师玩了 3秒的手机 // work out // the answer is 111 } }参考文章https://blog.csdn.net/fengye454545/article/details/80198446https://blog.csdn.net/xiaanming/article/details/8703708/https://www.cnblogs.com/prayjourney/p/9667835.htmlhttps://blog.csdn.net/qq_25652949/article/details/86572948https://my.oschina.net/u/3703858/blog/1798627 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>回调机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习MySQL数据库11：以Java的视角来聊聊SQL注入]]></title>
    <url>%2F2019%2F09%2F11%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0MySQL%E6%95%B0%E6%8D%AE%E5%BA%9311%EF%BC%9A%E4%BB%A5Java%E7%9A%84%E8%A7%86%E8%A7%92%E6%9D%A5%E8%81%8A%E8%81%8ASQL%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言靶场准备首先我们来准备一个web接口服务，该服务可以提供管理员的信息查询，这里我们采用springboot + jersey 来构建web服务框架，数据库则采用最常用的mysql。下面，我们来准备测试环境，首先建立一张用户表jwtk_admin，SQL如下： 然后插入默认的管理员： 这样我们就有了两位系统内置管理员了，管理员密码采用MD5进行Hash，当然这是一个很简单的为了作为研究靶场的表，所以没有很全的字段。 接下来，我们创建 spring boot + jersey 构建的RESTFul web服务，这里我们提供了一个通过管理员用户名查询管理员具体信息的接口，如下： SQL注入测试首先我们以开发者正向思维向web服务发送管理员查询请求，这里我们用PostMan工具发送一个GET请求 不出我们和开发者所料，Web接口返回了我们想要的结果，用户名为admin的管理员信息。OK，现在开发任务完成，Git Push，Jira任务点为待测试，那么这样的接口就真的没有问题了吗？现在我们发送这样一条GET请求： 发送该请求后，我们发现PostMan没有接收到返回结果，而Web服务后台却开始抛 MySQLSyntaxErrorException异常了，错误如下： You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘‘xxxx’’’ at line 1 原因是在我们查询的 xxxx’ 处sql语句语法不正确导致。这里我们先不讨论SQL语法问题，我们继续实验，再次构造一条GET查询请求: 此时，我们可以惊讶的发现，查询接口非但没有报错，反而将我们数据库jwti_admin表中的所有管理员信息都查询出来了： 这是什么鬼，难道管理员表中还有 name=xxxx’or’a’=’a 的用户？这就是 SQL Injection。 注入原理分析在接口中接受了一个String类型的name参数，并且通过字符串拼接的方式构建了查询语句。在正常情况下，用户会传入合法的name进行查询，但是黑客却会传入精心构造的参数，只要参数通过字符串拼接后依然是一句合法的SQL查询，此时SQL注入就发生了。正如我们上文输入的name=xxxx’or’a’=’a与我们接口中的查询语句进行拼接后构成如下SQL语句： 当接口执行此句SQL后，系统后台也就相当于拱手送给黑客了，黑客一看到管理员密码这个hash，都不用去cmd5查了，直接就用123456密码去登录你的后台系统了。Why？因为123456的md5哈希太常见了，别笑，这就是很多中小网站的现实，弱口令横行，不见棺材不落泪！ 好了，现在我们应该明白了，SQL Injection原因就是由于传入的参数与系统的SQL拼接成了合法的SQL而导致的，而其本质还是将用户输入的数据当做了代码执行。在系统中只要有一个SQL注入点被黑客发现，那么黑客基本上可以执行任意想执行的SQL语句了，例如添加一个管理员，查询所有表，甚至“脱裤” 等等，当然本文不是讲解SQL注入技巧的文章，这里我们只探讨SQL注入发生的原因与防范方法。 JDBC的预处理在上文的接口中，DAO使用了比较基础的JDBC的方式进行数据库操作，直接使JDBC构建DAO在比较老的系统中还是很常见的，但这并不意味着使用JDBC就一定不安全，如果我将传入的参数 xxxx’or’a’=’a 整体作为参数进行name查询，那就不会产生SQL注入。在JDBC中，提供了 PreparedStatement （预处理执行语句）的方式，可以对SQL语句进行查询参数化，使用预处理后的代码如下： 同样，我们使用上文的注入方式注入 ，此时我们发现，SQL注入没能成功。现在，我们来打印一下被被预处理后的SQL，看看有什么变化： 看到了吗?所有的 ’ 都被 ’ 转义掉了,从而可以确保SQL的查询参数就是参数，不会被恶意执行，从而防止了SQL注入。 Mybatis下注入防范MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架， 其几乎避免了所有的 JDBC 代码和手动设置参数以及获取结果集。同时，MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录，因此mybatis现在在市场中采用率也非常高。这里我们定义如下一个mapper，来实现通过用户名查询管理员的接口： 同样提供Web访问接口： 接下来，我们尝试SQL注入name字段，可以发现注入并没有成功，通过打印mybatis的Log可以看到mybatis框架对参数进行了预处理处理，从而防止了注入： 那是否只要使用了mybatis就一定可以避免SQL注入的危险？我们把mapper做如下修改，将参数#{name}修改为${name}，并使用name=‘xxxx’ or ‘a’=‘a’ 作为GET请求的参数，可以发现SQL注入还是发生了： 那这是为什么，mybatis ${}与#{}的差别在哪里？ 原来在mybatis中如果以形式声明为SQL传递参数，mybatis将不会进行参数预处理，会直接动态拼接SQL语句，此时就会存在被注入的风险，所以在使用mybatis作为持久框架时应尽量避免采用 形式声明为SQL传递参数，mybatis将不会进行参数预处理，会直接动态拼接SQL语句，此时就会存在被注入的风险，所以在使用mybatis作为持久框架时应尽量避免采用形式声明为SQL传递参数。 mybatis将不会进行参数预处理，会直接动态拼接SQL语句，此时就会存在被注入的风险，所以在使用mybatis作为持久框架时应尽量避免采用{}的形式进行参数传递，如果无法避免（有些SQL如like、in、order by等，程序员可能依旧会选择${}的方式传参），那就需要对传入参数自行进行转义过滤。 JPA注入防范JPA是Sun公司用来整合ORM技术，实现天下归一的ORM标准而定义的Java Persistence API（java持久层API），JPA只是一套接口，目前引入JPA的项目都会采用Hibernate作为其具体实现，随着无配置Spring Boot框架的流行，JPA越来越具有作为持久化首选的技术，因为其能让程序员写更少的代码，就能完成现有的功能。 例如强大的JpaRepository，常规的SQL查询只需按照命名规则定义接口，便可以不写SQL（JPQL/SQL）就可以实现数据的查询操作，从SQL注入防范的角度来说，这种将安全责任抛给框架远比依靠程序员自身控制来的保险。因此如果项目使用JPA作为数据访问层，基本上可以很大程度的消除SQL注入的风险。 但是话不能说的太死，在我见过的一个Spring Boot项目中，虽然采用了JPA作为持久框架，但是有一位老程序员不熟悉于使用JPQL来构建查询接口，依旧使用字符串拼接的方式来实现业务，而为项目安全埋下了隐患。 安全需要一丝不苟，安全是100 - 1 = 0的业务，即使你防御了99%的攻击，那还不算胜利，只要有一次被入侵了，那就有可能给公司带来很严重的后果。 关于JPA的SQL注入，我们就不详细讨论了，因为框架下的注入漏洞属于框架漏洞范畴（如CVE-2016-6652），程序员只要遵循JPA的开发规范，就无需担心注入问题，框架都为你做好幕后工作了。 SQL注入的其他防范办法很多公司都会存在老系统中有大量SQL注入风险代码的问题，但是由于其已稳定支持公司业务很久，不宜采用大面积代码更新的方式来消除注入隐患，所以需要考虑其采用他方式来防范SQL注入。除了在在SQL执行方式上防范SQL注入，很多时候还可以通过架构上，或者通过其他过滤方式来达到防止SQL注入的效果。 一切输入都是不安全的：对于接口的调用参数，要进行格式匹配，例如admin的通过name查询的接口，与之匹配的Path应该使用正则匹配（因为用户名中不应该存在特殊字符），从而确保传入参数是程序控制范围之内的参数，即只接受已知的良好输入值，拒绝不良输入。注意：验证参数应将它与输出编码技术结合使用。 利用分层设计来避免危险：前端尽量静态化，尽量少的暴露可以访问到DAO层的接口到公网环境中，如果现有项目，很难修改存在注入的代码，可以考虑在web服务之前增加WAF进行流量过滤，当然代码上就不给hacker留有攻击的漏洞才最好的方案。也可以在拥有nginx的架构下，采用OpenRestry做流量过滤，将一些特殊字符进行转义处理。 尽量使用预编译SQL语句：由于动态SQL语句是引发SQL注入的根源。应使用预编译语句来组装SQL查询。 规范化：将输入安装规定编码解码后再进行输入参数过滤和输出编码处理；拒绝一切非规范格式的编码。 小结其实随着ORM技术的发展，Java web开发在大趋势上已经越来越远离SQL注入的问题了，而有着Entity Framework框架支持的ASP.NET MVC从来都是高冷范。在现在互联网中，使用PHP和Python构建的web应用是目前SQL注入的重灾区。本文虽然是从JAVA的角度来研究SQL注入的问题，但原理上同样适用于其他开发语言，希望读者可以通过此文，触类旁通。 珍爱数据，远离拼接，有输入的地方就会有江湖… 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列10：深入理解Java中的异常体系]]></title>
    <url>%2F2019%2F09%2F10%2F10Java%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 为什么要使用异常 首先我们可以明确一点就是异常的处理机制可以确保我们程序的健壮性，提高系统可用率。虽然我们不是特别喜欢看到它，但是我们不能不承认它的地位，作用。 在没有异常机制的时候我们是这样处理的：通过函数的返回值来判断是否发生了异常（这个返回值通常是已经约定好了的），调用该函数的程序负责检查并且分析返回值。虽然可以解决异常问题，但是这样做存在几个缺陷： 1、 容易混淆。如果约定返回值为-11111时表示出现异常，那么当程序最后的计算结果真的为-1111呢？ 2、 代码可读性差。将异常处理代码和程序代码混淆在一起将会降低代码的可读性。 3、 由调用函数来分析异常，这要求程序员对库函数有很深的了解。 在OO中提供的异常处理机制是提供代码健壮的强有力的方式。使用异常机制它能够降低错误处理代码的复杂度，如果不使用异常，那么就必须检查特定的错误，并在程序中的许多地方去处理它。 而如果使用异常，那就不必在方法调用处进行检查，因为异常机制将保证能够捕获这个错误，并且，只需在一个地方处理错误，即所谓的异常处理程序中。 这种方式不仅节约代码，而且把“概述在正常执行过程中做什么事”的代码和“出了问题怎么办”的代码相分离。总之，与以前的错误处理方法相比，异常机制使代码的阅读、编写和调试工作更加井井有条。（摘自《Think in java 》）。 该部分内容选自http://www.cnblogs.com/chenssy/p/3438130.html 异常基本定义 在《Think in java》中是这样定义异常的：异常情形是指阻止当前方法或者作用域继续执行的问题。在这里一定要明确一点：异常代码某种程度的错误，尽管Java有异常处理机制，但是我们不能以“正常”的眼光来看待异常，异常处理机制的原因就是告诉你：这里可能会或者已经产生了错误，您的程序出现了不正常的情况，可能会导致程序失败！ 那么什么时候才会出现异常呢？只有在你当前的环境下程序无法正常运行下去，也就是说程序已经无法来正确解决问题了，这时它所就会从当前环境中跳出，并抛出异常。抛出异常后，它首先会做几件事。 首先，它会使用new创建一个异常对象，然后在产生异常的位置终止程序，并且从当前环境中弹出对异常对象的引用，这时。异常处理机制就会接管程序，并开始寻找一个恰当的地方来继续执行程序，这个恰当的地方就是异常处理程序。 总的来说异常处理机制就是当程序发生异常时，它强制终止程序运行，记录异常信息并将这些信息反馈给我们，由我们来确定是否处理异常。 异常体系[外链图片转存失败(img-KNxcBTK0-1569073569353)(https://images0.cnblogs.com/blog/381060/201311/22185952-834d92bc2bfe498f9a33414cc7a2c8a4.png)] 从上面这幅图可以看出，Throwable是java语言中所有错误和异常的超类（万物即可抛）。它有两个子类：Error、Exception。 Java标准库内建了一些通用的异常，这些类以Throwable为顶层父类。 Throwable又派生出Error类和Exception类。 错误：Error类以及他的子类的实例，代表了JVM本身的错误。错误不能被程序员通过代码处理，Error很少出现。因此，程序员应该关注Exception为父类的分支下的各种异常类。 异常：Exception以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被Java异常处理机制使用，是异常处理的核心。 总体上我们根据Javac对异常的处理要求，将异常类分为2类。 非检查异常（unckecked exception）：Error 和 RuntimeException 以及他们的子类。javac在编译时，不会提示和发现这样的异常，不要求在程序处理这些异常。所以如果愿意，我们可以编写代码处理（使用try…catch…finally）这样的异常，也可以不处理。 对于这些异常，我们应该修正代码，而不是去通过异常处理器处理 。这样的异常发生的原因多半是代码写的有问题。如除0错误ArithmeticException，错误的强制类型转换错误ClassCastException，数组索引越界ArrayIndexOutOfBoundsException，使用了空对象NullPointerException等等。 检查异常（checked exception）：除了Error 和 RuntimeException的其它异常。javac强制要求程序员为这样的异常做预备处理工作（使用try…catch…finally或者throws）。在方法中要么用try-catch语句捕获它并处理，要么用throws子句声明抛出它，否则编译不会通过。 这样的异常一般是由程序的运行环境导致的。因为程序可能被运行在各种未知的环境下，而程序员无法干预用户如何使用他编写的程序，于是程序员就应该为这样的异常时刻准备着。如SQLException , IOException,ClassNotFoundException 等。 需要明确的是：检查和非检查是对于javac来说的，这样就很好理解和区分了。 这部分内容摘自http://www.importnew.com/26613.html 初识异常异常是在执行某个函数时引发的，而函数又是层级调用，形成调用栈的，因为，只要一个函数发生了异常，那么他的所有的caller都会被异常影响。当这些被影响的函数以异常信息输出时，就形成的了异常追踪栈。 异常最先发生的地方，叫做异常抛出点。 public class 异常 { public static void main (String [] args ) { System . out. println( &quot;----欢迎使用命令行除法计算器----&quot; ) ; CMDCalculate (); } public static void CMDCalculate () { Scanner scan = new Scanner ( System. in ); int num1 = scan .nextInt () ; int num2 = scan .nextInt () ; int result = devide (num1 , num2 ) ; System . out. println( &quot;result:&quot; + result) ; scan .close () ; } public static int devide (int num1, int num2 ){ return num1 / num2 ; } // ----欢迎使用命令行除法计算器---- // 1 // 0 // Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zero // at com.javase.异常.异常.devide(异常.java:24) // at com.javase.异常.异常.CMDCalculate(异常.java:19) // at com.javase.异常.异常.main(异常.java:12) // ----欢迎使用命令行除法计算器---- // r // Exception in thread &quot;main&quot; java.util.InputMismatchException // at java.util.Scanner.throwFor(Scanner.java:864) // at java.util.Scanner.next(Scanner.java:1485) // at java.util.Scanner.nextInt(Scanner.java:2117) // at java.util.Scanner.nextInt(Scanner.java:2076) // at com.javase.异常.异常.CMDCalculate(异常.java:17) // at com.javase.异常.异常.main(异常.java:12)[外链图片转存失败(img-9rqUQJQj-1569073569354)(http://incdn1.b0.upaiyun.com/2017/09/0b3e4ca2f4cf8d7116c7ad354940601f.png)] 从上面的例子可以看出，当devide函数发生除0异常时，devide函数将抛出ArithmeticException异常，因此调用他的CMDCalculate函数也无法正常完成，因此也发送异常，而CMDCalculate的caller——main 因为CMDCalculate抛出异常，也发生了异常，这样一直向调用栈的栈底回溯。 这种行为叫做异常的冒泡，异常的冒泡是为了在当前发生异常的函数或者这个函数的caller中找到最近的异常处理程序。由于这个例子中没有使用任何异常处理机制，因此异常最终由main函数抛给JRE，导致程序终止。 上面的代码不使用异常处理机制，也可以顺利编译，因为2个异常都是非检查异常。但是下面的例子就必须使用异常处理机制，因为异常是检查异常。 代码中我选择使用throws声明异常，让函数的调用者去处理可能发生的异常。但是为什么只throws了IOException呢？因为FileNotFoundException是IOException的子类，在处理范围内。 异常和错误下面看一个例子 //错误即error一般指jvm无法处理的错误 //异常是Java定义的用于简化错误处理流程和定位错误的一种工具。 public class 错误和错误 { Error error = new Error(); public static void main(String[] args) { throw new Error(); } //下面这四个异常或者错误有着不同的处理方法 public void error1 (){ //编译期要求必须处理，因为这个异常是最顶层异常，包括了检查异常，必须要处理 try { throw new Throwable(); } catch (Throwable throwable) { throwable.printStackTrace(); } } //Exception也必须处理。否则报错，因为检查异常都继承自exception，所以默认需要捕捉。 public void error2 (){ try { throw new Exception(); } catch (Exception e) { e.printStackTrace(); } } //error可以不处理，编译不报错,原因是虚拟机根本无法处理，所以啥都不用做 public void error3 (){ throw new Error(); } //runtimeexception众所周知编译不会报错 public void error4 (){ throw new RuntimeException(); } // Exception in thread &quot;main&quot; java.lang.Error // at com.javase.异常.错误.main(错误.java:11) }异常的处理方式在编写代码处理异常时，对于检查异常，有2种不同的处理方式： 使用try…catch…finally语句块处理它。 或者，在函数签名中使用throws 声明交给函数调用者caller去解决。 下面看几个具体的例子，包括error，exception和throwable 上面的例子是运行时异常，不需要显示捕获。下面这个例子是可检查异常需，要显示捕获或者抛出。 @Test public void testException() throws IOException { //FileInputStream的构造函数会抛出FileNotFoundException FileInputStream fileIn = new FileInputStream(&quot;E:\\a.txt&quot;); int word; //read方法会抛出IOException while((word = fileIn.read())!=-1) { System.out.print((char)word); } //close方法会抛出IOException fileIn.close(); }一般情况下的处理方式 try catch finally public class 异常处理方式 { @Test public void main() { try{ //try块中放可能发生异常的代码。 InputStream inputStream = new FileInputStream(&quot;a.txt&quot;); //如果执行完try且不发生异常，则接着去执行finally块和finally后面的代码（如果有的话）。 int i = 1/0; //如果发生异常，则尝试去匹配catch块。 throw new SQLException(); //使用1.8jdk同时捕获多个异常，runtimeexception也可以捕获。只是捕获后虚拟机也无法处理，所以不建议捕获。 }catch(SQLException | IOException | ArrayIndexOutOfBoundsException exception){ System.out.println(exception.getMessage()); //每一个catch块用于捕获并处理一个特定的异常，或者这异常类型的子类。Java7中可以将多个异常声明在一个catch中。 //catch后面的括号定义了异常类型和异常参数。如果异常与之匹配且是最先匹配到的，则虚拟机将使用这个catch块来处理异常。 //在catch块中可以使用这个块的异常参数来获取异常的相关信息。异常参数是这个catch块中的局部变量，其它块不能访问。 //如果当前try块中发生的异常在后续的所有catch中都没捕获到，则先去执行finally，然后到这个函数的外部caller中去匹配异常处理器。 //如果try中没有发生异常，则所有的catch块将被忽略。 }catch(Exception exception){ System.out.println(exception.getMessage()); //... }finally{ //finally块通常是可选的。 //无论异常是否发生，异常是否匹配被处理，finally都会执行。 //finally主要做一些清理工作，如流的关闭，数据库连接的关闭等。 }一个try至少要跟一个catch或者finally try { int i = 1; }finally { //一个try至少要有一个catch块，否则， 至少要有1个finally块。但是finally不是用来处理异常的，finally不会捕获异常。 } }异常出现时该方法后面的代码不会运行，即使异常已经被捕获。这里举出一个奇特的例子，在catch里再次使用try catch finally @Test public void test() { try { throwE(); System.out.println(&quot;我前面抛出异常了&quot;); System.out.println(&quot;我不会执行了&quot;); } catch (StringIndexOutOfBoundsException e) { System.out.println(e.getCause()); }catch (Exception ex) { //在catch块中仍然可以使用try catch finally try { throw new Exception(); }catch (Exception ee) { }finally { System.out.println(&quot;我所在的catch块没有执行，我也不会执行的&quot;); } } } //在方法声明中抛出的异常必须由调用方法处理或者继续往上抛， // 当抛到jre时由于无法处理终止程序 public void throwE (){ // Socket socket = new Socket(&quot;127.0.0.1&quot;, 80); //手动抛出异常时，不会报错，但是调用该方法的方法需要处理这个异常，否则会出错。 // java.lang.StringIndexOutOfBoundsException // at com.javase.异常.异常处理方式.throwE(异常处理方式.java:75) // at com.javase.异常.异常处理方式.test(异常处理方式.java:62) throw new StringIndexOutOfBoundsException(); }其实有的语言在遇到异常后仍然可以继续运行 有的编程语言当异常被处理后，控制流会恢复到异常抛出点接着执行，这种策略叫做：resumption model of exception handling（恢复式异常处理模式 ） 而Java则是让执行流恢复到处理了异常的catch块后接着执行，这种策略叫做：termination model of exception handling（终结式异常处理模式） “不负责任”的throwsthrows是另一种处理异常的方式，它不同于try…catch…finally，throws仅仅是将函数中可能出现的异常向调用者声明，而自己则不具体处理。 采取这种异常处理的原因可能是：方法本身不知道如何处理这样的异常，或者说让调用者处理更好，调用者需要为可能发生的异常负责。 public void foo() throws ExceptionType1 , ExceptionType2 ,ExceptionTypeN { //foo内部可以抛出 ExceptionType1 , ExceptionType2 ,ExceptionTypeN 类的异常，或者他们的子类的异常对象。 }纠结的finallyfinally块不管异常是否发生，只要对应的try执行了，则它一定也执行。只有一种方法让finally块不执行：System.exit()。因此finally块通常用来做资源释放操作：关闭文件，关闭数据库连接等等。 良好的编程习惯是：在try块中打开资源，在finally块中清理释放这些资源。 需要注意的地方: 1、finally块没有处理异常的能力。处理异常的只能是catch块。 2、在同一try…catch…finally块中 ，如果try中抛出异常，且有匹配的catch块，则先执行catch块，再执行finally块。如果没有catch块匹配，则先执行finally，然后去外面的调用者中寻找合适的catch块。 3、在同一try…catch…finally块中 ，try发生异常，且匹配的catch块中处理异常时也抛出异常，那么后面的finally也会执行：首先执行finally块，然后去外围调用者中寻找合适的catch块。 public class finally使用 { public static void main(String[] args) { try { throw new IllegalAccessException(); }catch (IllegalAccessException e) { // throw new Throwable(); //此时如果再抛异常，finally无法执行，只能报错。 //finally无论何时都会执行 //除非我显示调用。此时finally才不会执行 System.exit(0); }finally { System.out.println(&quot;算你狠&quot;); } } }throw : JRE也使用的关键字throw exceptionObject 程序员也可以通过throw语句手动显式的抛出一个异常。throw语句的后面必须是一个异常对象。 throw 语句必须写在函数中，执行throw 语句的地方就是一个异常抛出点，==它和由JRE自动形成的异常抛出点没有任何差别。== public void save(User user) { if(user == null) throw new IllegalArgumentException(&quot;User对象为空&quot;); //...... }后面开始的大部分内容都摘自http://www.cnblogs.com/lulipro/p/7504267.html 该文章写的十分细致到位，令人钦佩，是我目前为之看到关于异常最详尽的文章，可以说是站在巨人的肩膀上了。 异常调用链异常的链化 在一些大型的，模块化的软件开发中，一旦一个地方发生异常，则如骨牌效应一样，将导致一连串的异常。假设B模块完成自己的逻辑需要调用A模块的方法，如果A模块发生异常，则B也将不能完成而发生异常。 ==但是B在抛出异常时，会将A的异常信息掩盖掉，这将使得异常的根源信息丢失。异常的链化可以将多个模块的异常串联起来，使得异常信息不会丢失。== 异常链化:以一个异常对象为参数构造新的异常对象。新的异对象将包含先前异常的信息。这项技术主要是异常类的一个带Throwable参数的函数来实现的。这个当做参数的异常，我们叫他根源异常（cause）。 查看Throwable类源码，可以发现里面有一个Throwable字段cause，就是它保存了构造时传递的根源异常参数。这种设计和链表的结点类设计如出一辙，因此形成链也是自然的了。 public class Throwable implements Serializable { private Throwable cause = this; public Throwable(String message, Throwable cause) { fillInStackTrace(); detailMessage = message; this.cause = cause; } public Throwable(Throwable cause) { fillInStackTrace(); detailMessage = (cause==null ? null : cause.toString()); this.cause = cause; } //........ }下面看一个比较实在的异常链例子哈 public class 异常链 { @Test public void test() { C(); } public void A () throws Exception { try { int i = 1; i = i / 0; //当我注释掉这行代码并使用B方法抛出一个error时，运行结果如下 // 四月 27, 2018 10:12:30 下午 org.junit.platform.launcher.core.ServiceLoaderTestEngineRegistry loadTestEngines // 信息: Discovered TestEngines with IDs: [junit-jupiter] // java.lang.Error: B也犯了个错误 // at com.javase.异常.异常链.B(异常链.java:33) // at com.javase.异常.异常链.C(异常链.java:38) // at com.javase.异常.异常链.test(异常链.java:13) // at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) // Caused by: java.lang.Error // at com.javase.异常.异常链.B(异常链.java:29) }catch (ArithmeticException e) { //这里通过throwable类的构造方法将最底层的异常重新包装并抛出，此时注入了A方法的信息。最后打印栈信息时可以看到caused by A方法的异常。 //如果直接抛出，栈信息打印结果只能看到上层方法的错误信息，不能看到其实是A发生了错误。 //所以需要包装并抛出 throw new Exception(&quot;A方法计算错误&quot;, e); } } public void B () throws Exception,Error { try { //接收到A的异常， A(); throw new Error(); }catch (Exception e) { throw e; }catch (Error error) { throw new Error(&quot;B也犯了个错误&quot;, error); } } public void C () { try { B(); }catch (Exception | Error e) { e.printStackTrace(); } } //最后结果 // java.lang.Exception: A方法计算错误 // at com.javase.异常.异常链.A(异常链.java:18) // at com.javase.异常.异常链.B(异常链.java:24) // at com.javase.异常.异常链.C(异常链.java:31) // at com.javase.异常.异常链.test(异常链.java:11) // 省略 // Caused by: java.lang.ArithmeticException: / by zero // at com.javase.异常.异常链.A(异常链.java:16) // ... 31 more }自定义异常如果要自定义异常类，则扩展Exception类即可，因此这样的自定义异常都属于检查异常（checked exception）。如果要自定义非检查异常，则扩展自RuntimeException。 按照国际惯例，自定义的异常应该总是包含如下的构造函数： 一个无参构造函数一个带有String参数的构造函数，并传递给父类的构造函数。一个带有String参数和Throwable参数，并都传递给父类构造函数一个带有Throwable 参数的构造函数，并传递给父类的构造函数。下面是IOException类的完整源代码，可以借鉴。 public class IOException extends Exception { static final long serialVersionUID = 7818375828146090155L; public IOException() { super(); } public IOException(String message) { super(message); } public IOException(String message, Throwable cause) { super(message, cause); } public IOException(Throwable cause) { super(cause); } }异常的注意事项异常的注意事项 当子类重写父类的带有 throws声明的函数时，其throws声明的异常必须在父类异常的可控范围内——用于处理父类的throws方法的异常处理器，必须也适用于子类的这个带throws方法 。这是为了支持多态。 例如，父类方法throws 的是2个异常，子类就不能throws 3个及以上的异常。父类throws IOException，子类就必须throws IOException或者IOException的子类。 至于为什么？我想，也许下面的例子可以说明。 class Father { public void start() throws IOException { throw new IOException(); } } class Son extends Father { public void start() throws Exception { throw new SQLException(); } }/**假设上面的代码是允许的（实质是错误的）***/ class Test { public static void main(String[] args) { Father[] objs = new Father[2]; objs[0] = new Father(); objs[1] = new Son(); for(Father obj:objs) { //因为Son类抛出的实质是SQLException，而IOException无法处理它。 //那么这里的try。。catch就不能处理Son中的异常。 //多态就不能实现了。 try { obj.start(); }catch(IOException) { //处理IOException } } } }==Java的异常执行流程是线程独立的，线程之间没有影响== Java程序可以是多线程的。每一个线程都是一个独立的执行流，独立的函数调用栈。如果程序只有一个线程，那么没有被任何代码处理的异常 会导致程序终止。如果是多线程的，那么没有被任何代码处理的异常仅仅会导致异常所在的线程结束。 也就是说，Java中的异常是线程独立的，线程的问题应该由线程自己来解决，而不要委托到外部，也不会直接影响到其它线程的执行。 下面看一个例子 public class 多线程的异常 { @Test public void test() { go(); } public void go () { ExecutorService executorService = Executors.newFixedThreadPool(3); for (int i = 0;i &lt;= 2;i ++) { int finalI = i; try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } executorService.execute(new Runnable() { @Override //每个线程抛出异常时并不会影响其他线程的继续执行 public void run() { try { System.out.println(&quot;start thread&quot; + finalI); throw new Exception(); }catch (Exception e) { System.out.println(&quot;thread&quot; + finalI + &quot; go wrong&quot;); } } }); } // 结果： // start thread0 // thread0 go wrong // start thread1 // thread1 go wrong // start thread2 // thread2 go wrong } }当finally遇上return首先一个不容易理解的事实： 在 try块中即便有return，break，continue等改变执行流的语句，finally也会执行。 public static void main(String[] args) { int re = bar(); System.out.println(re); } private static int bar() { try{ return 5; } finally{ System.out.println(&quot;finally&quot;); } } /*输出： finally */很多人面对这个问题时，总是在归纳执行的顺序和规律，不过我觉得还是很难理解。我自己总结了一个方法。用如下GIF图说明。 [外链图片转存失败(img-SceF4t85-1569073569354)(http://incdn1.b0.upaiyun.com/2017/09/0471c2805ebd5a463211ced478eaf7f8.gif)] 也就是说：try…catch…finally中的return 只要能执行，就都执行了，他们共同向同一个内存地址（假设地址是0×80）写入返回值，后执行的将覆盖先执行的数据，而真正被调用者取的返回值就是最后一次写入的。那么，按照这个思想，下面的这个例子也就不难理解了。 finally中的return 会覆盖 try 或者catch中的返回值。 public static void main(String[] args) { int result; result = foo(); System.out.println(result); /////////2 result = bar(); System.out.println(result); /////////2 } @SuppressWarnings(&quot;finally&quot;) public static int foo() { trz{ int a = 5 / 0; } catch (Exception e){ return 1; } finally{ return 2; } } @SuppressWarnings(&quot;finally&quot;) public static int bar() { try { return 1; }finally { return 2; } }finally中的return会抑制（消灭）前面try或者catch块中的异常 class TestException { public static void main(String[] args) { int result; try{ result = foo(); System.out.println(result); //输出100 } catch (Exception e){ System.out.println(e.getMessage()); //没有捕获到异常 } try{ result = bar(); System.out.println(result); //输出100 } catch (Exception e){ System.out.println(e.getMessage()); //没有捕获到异常 } } //catch中的异常被抑制 @SuppressWarnings(&quot;finally&quot;) public static int foo() throws Exception { try { int a = 5/0; return 1; }catch(ArithmeticException amExp) { throw new Exception(&quot;我将被忽略，因为下面的finally中使用了return&quot;); }finally { return 100; } } //try中的异常被抑制 @SuppressWarnings(&quot;finally&quot;) public static int bar() throws Exception { try { int a = 5/0; return 1; }finally { return 100; } } }finally中的异常会覆盖（消灭）前面try或者catch中的异常 class TestException { public static void main(String[] args) { int result; try{ result = foo(); } catch (Exception e){ System.out.println(e.getMessage()); //输出：我是finaly中的Exception } try{ result = bar(); } catch (Exception e){ System.out.println(e.getMessage()); //输出：我是finaly中的Exception } } //catch中的异常被抑制 @SuppressWarnings(&quot;finally&quot;) public static int foo() throws Exception { try { int a = 5/0; return 1; }catch(ArithmeticException amExp) { throw new Exception(&quot;我将被忽略，因为下面的finally中抛出了新的异常&quot;); }finally { throw new Exception(&quot;我是finaly中的Exception&quot;); } } //try中的异常被抑制 @SuppressWarnings(&quot;finally&quot;) public static int bar() throws Exception { try { int a = 5/0; return 1; }finally { throw new Exception(&quot;我是finaly中的Exception&quot;); } } }上面的3个例子都异于常人的编码思维，因此我建议： 不要在fianlly中使用return。 不要在finally中抛出异常。 减轻finally的任务，不要在finally中做一些其它的事情，finally块仅仅用来释放资源是最合适的。 将尽量将所有的return写在函数的最后面，而不是try … catch … finally中。 JAVA异常常见面试题 下面是我个人总结的在Java和J2EE开发者在面试中经常被问到的有关Exception和Error的知识。在分享我的回答的时候，我也给这些问题作了快速修订，并且提供源码以便深入理解。我总结了各种难度的问题，适合新手码农和高级Java码农。如果你遇到了我列表中没有的问题，并且这个问题非常好，请在下面评论中分享出来。你也可以在评论中分享你面试时答错的情况。 1) Java中什么是Exception? 这个问题经常在第一次问有关异常的时候或者是面试菜鸟的时候问。我从来没见过面高级或者资深工程师的时候有人问这玩意，但是对于菜鸟，是很愿意问这个的。简单来说，异常是Java传达给你的系统和程序错误的方式。在java中，异常功能是通过实现比如Throwable，Exception，RuntimeException之类的类，然后还有一些处理异常时候的关键字，比如throw，throws，try，catch，finally之类的。 所有的异常都是通过Throwable衍生出来的。Throwable把错误进一步划分为 java.lang.Exception和 java.lang.Error. java.lang.Error 用来处理系统错误，例如java.lang.StackOverFlowError 之类的。然后 Exception用来处理程序错误，请求的资源不可用等等。 2) Java中的检查型异常和非检查型异常有什么区别？ 这又是一个非常流行的Java异常面试题，会出现在各种层次的Java面试中。检查型异常和非检查型异常的主要区别在于其处理方式。检查型异常需要使用try, catch和finally关键字在编译期进行处理，否则会出现编译器会报错。对于非检查型异常则不需要这样做。Java中所有继承自java.lang.Exception类的异常都是检查型异常，所有继承自RuntimeException的异常都被称为非检查型异常。 3) Java中的NullPointerException和ArrayIndexOutOfBoundException之间有什么相同之处？ 在Java异常面试中这并不是一个很流行的问题，但会出现在不同层次的初学者面试中，用来测试应聘者对检查型异常和非检查型异常的概念是否熟悉。顺便说一下，该题的答案是，这两个异常都是非检查型异常，都继承自RuntimeException。该问题可能会引出另一个问题，即Java和C的数组有什么不同之处，因为C里面的数组是没有大小限制的，绝对不会抛出ArrayIndexOutOfBoundException。 4)在Java异常处理的过程中，你遵循的那些最好的实践是什么？ 这个问题在面试技术经理是非常常见的一个问题。因为异常处理在项目设计中是非常关键的，所以精通异常处理是十分必要的。异常处理有很多最佳实践，下面列举集中，它们提高你代码的健壮性和灵活性： 1) 调用方法的时候返回布尔值来代替返回null，这样可以 NullPointerException。由于空指针是java异常里最恶心的异常 2) catch块里别不写代码。空catch块是异常处理里的错误事件，因为它只是捕获了异常，却没有任何处理或者提示。通常你起码要打印出异常信息，当然你最好根据需求对异常信息进行处理。 3)能抛受控异常（checked Exception）就尽量不抛受非控异常(checked Exception)。通过去掉重复的异常处理代码，可以提高代码的可读性。 4) 绝对不要让你的数据库相关异常显示到客户端。由于绝大多数数据库和SQLException异常都是受控异常，在Java中，你应该在DAO层把异常信息处理，然后返回处理过的能让用户看懂并根据异常提示信息改正操作的异常信息。 5) 在Java中，一定要在数据库连接，数据库查询，流处理后，在finally块中调用close()方法。 5) 既然我们可以用RuntimeException来处理错误，那么你认为为什么Java中还存在检查型异常? 这是一个有争议的问题，在回答该问题时你应当小心。虽然他们肯定愿意听到你的观点，但其实他们最感兴趣的还是有说服力的理由。我认为其中一个理由是，存在检查型异常是一个设计上的决定，受到了诸如C++等比Java更早编程语言设计经验的影响。绝大多数检查型异常位于java.io包内，这是合乎情理的，因为在你请求了不存在的系统资源的时候，一段强壮的程序必须能够优雅的处理这种情况。通过把IOException声明为检查型异常，Java 确保了你能够优雅的对异常进行处理。另一个可能的理由是，可以使用catch或finally来确保数量受限的系统资源（比如文件描述符）在你使用后尽早得到释放。 JoshuaBloch编写的 Effective Java 一书 中多处涉及到了该话题，值得一读。 6) throw 和 throws这两个关键字在java中有什么不同? 一个java初学者应该掌握的面试问题。 throw 和 throws乍看起来是很相似的尤其是在你还是一个java初学者的时候。尽管他们看起来相似，都是在处理异常时候使用到的。但在代码里的使用方法和用到的地方是不同的。throws总是出现在一个函数头中，用来标明该成员函数可能抛出的各种异常, 你也可以申明未检查的异常，但这不是编译器强制的。如果方法抛出了异常那么调用这个方法的时候就需要将这个异常处理。另一个关键字 throw 是用来抛出任意异常的，按照语法你可以抛出任意 Throwable (i.e. Throwable或任何Throwable的衍生类) , throw可以中断程序运行，因此可以用来代替return . 最常见的例子是用 throw 在一个空方法中需要return的地方抛出 UnSupportedOperationException 代码如下 : 123 private``static void show() {``throw``new UnsupportedOperationException(``&quot;Notyet implemented&quot;``);``} 可以看下这篇 文章查看这两个关键字在java中更多的差异 。 7) 什么是“异常链”? “异常链”是Java中非常流行的异常处理概念，是指在进行一个异常处理时抛出了另外一个异常，由此产生了一个异常链条。该技术大多用于将“ 受检查异常” （ checked exception）封装成为“非受检查异常”（unchecked exception)或者RuntimeException。顺便说一下，如果因为因为异常你决定抛出一个新的异常，你一定要包含原有的异常，这样，处理程序才可以通过getCause()和initCause()方法来访问异常最终的根源。 ) 你曾经自定义实现过异常吗？怎么写的? 很显然，我们绝大多数都写过自定义或者业务异常，像AccountNotFoundException。在面试过程中询问这个Java异常问题的主要原因是去发现你如何使用这个特性的。这可以更准确和精致的去处理异常，当然这也跟你选择checked 还是unchecked exception息息相关。通过为每一个特定的情况创建一个特定的异常，你就为调用者更好的处理异常提供了更好的选择。相比通用异常（general exception)，我更倾向更为精确的异常。大量的创建自定义异常会增加项目class的个数，因此，在自定义异常和通用异常之间维持一个平衡是成功的关键。 9) JDK7中对异常处理做了什么改变？ 这是最近新出的Java异常处理的面试题。JDK7中对错误(Error)和异常(Exception)处理主要新增加了2个特性，一是在一个catch块中可以出来多个异常，就像原来用多个catch块一样。另一个是自动化资源管理(ARM), 也称为try-with-resource块。这2个特性都可以在处理异常时减少代码量，同时提高代码的可读性。对于这些特性了解，不仅帮助开发者写出更好的异常处理的代码，也让你在面试中显的更突出。我推荐大家读一下Java 7攻略，这样可以更深入的了解这2个非常有用的特性。 10) 你遇到过 OutOfMemoryError 错误嘛？你是怎么搞定的？ 这个面试题会在面试高级程序员的时候用，面试官想知道你是怎么处理这个危险的OutOfMemoryError错误的。必须承认的是，不管你做什么项目，你都会碰到这个问题。所以你要是说没遇到过，面试官肯定不会买账。要是你对这个问题不熟悉，甚至就是没碰到过，而你又有3、4年的Java经验了，那么准备好处理这个问题吧。在回答这个问题的同时，你也可以借机向面试秀一下你处理内存泄露、调优和调试方面的牛逼技能。我发现掌握这些技术的人都能给面试官留下深刻的印象。 11) 如果执行finally代码块之前方法返回了结果，或者JVM退出了，finally块中的代码还会执行吗？ 这个问题也可以换个方式问：“如果在try或者finally的代码块中调用了System.exit()，结果会是怎样”。了解finally块是怎么执行的，即使是try里面已经使用了return返回结果的情况，对了解Java的异常处理都非常有价值。只有在try里面是有System.exit(0)来退出JVM的情况下finally块中的代码才不会执行。 12)Java中final,finalize,finally关键字的区别 这是一个经典的Java面试题了。我的一个朋友为Morgan Stanley招电信方面的核心Java开发人员的时候就问过这个问题。final和finally是Java的关键字，而finalize则是方法。final关键字在创建不可变的类的时候非常有用，只是声明这个类是final的。而finalize()方法则是垃圾回收器在回收一个对象前调用，但也Java规范里面没有保证这个方法一定会被调用。finally关键字是唯一一个和这篇文章讨论到的异常处理相关的关键字。在你的产品代码中，在关闭连接和资源文件的是时候都必须要用到finally块。 参考文章https://www.xuebuyuan.com/3248044.htmlhttps://www.jianshu.com/p/49d2c3975c56http://c.biancheng.net/view/1038.htmlhttps://blog.csdn.net/Lisiluan/article/details/88745820https://blog.csdn.net/michaelgo/article/details/82790253 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习MySQL数据库10：MySQL里的那些日志们]]></title>
    <url>%2F2019%2F09%2F10%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0MySQL%E6%95%B0%E6%8D%AE%E5%BA%9310%EF%BC%9AMySQL%E9%87%8C%E7%9A%84%E9%82%A3%E4%BA%9B%E6%97%A5%E5%BF%97%E4%BB%AC%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 重新学习MySQL数据库10：MySQL里的那些日志们同大多数关系型数据库一样，日志文件是MySQL数据库的重要组成部分。MySQL有几种不同的日志文件，通常包括错误日志文件，二进制日志，通用日志，慢查询日志，等等。这些日志可以帮助我们定位mysqld内部发生的事件，数据库性能故障，记录数据的变更历史，用户恢复数据库等等。本文主要描述错误日志文件。 1.MySQL日志文件系统的组成a、错误日志：记录启动、运行或停止mysqld时出现的问题。 b、通用日志：记录建立的客户端连接和执行的语句。 c、更新日志：记录更改数据的语句。该日志在MySQL 5.1中已不再使用。 d、二进制日志：记录所有更改数据的语句。还用于复制。 e、慢查询日志：记录所有执行时间超过long_query_time秒的所有查询或不使用索引的查询。 f、Innodb日志：innodb redo log 和undo log 缺省情况下，所有日志创建于mysqld数据目录中。 可以通过刷新日志，来强制mysqld来关闭和重新打开日志文件（或者在某些情况下切换到一个新的日志）。 当你执行一个FLUSH LOGS语句或执行mysqladmin flush-logs或mysqladmin refresh时，则日志被老化。 对于存在MySQL复制的情形下，从复制服务器将维护更多日志文件，被称为接替日志。 2.错误日志错误日志是一个文本文件。 错误日志记录了MySQL Server每次启动和关闭的详细信息以及运行过程中所有较为严重的警告和错误信息。 可以用–log-error[=file_name]选项来开启mysql错误日志，该选项指定mysqld保存错误日志文件的位置。 对于指定–log-error[=file_name]选项而未给定file_name值，mysqld使用错误日志名host_name.err 并在数据目录中写入日志文件。 在mysqld正在写入错误日志到文件时，执行FLUSH LOGS 或者mysqladmin flush-logs时，服务器将关闭并重新打开日志文件。 建议在flush之前手动重命名错误日志文件，之后mysql服务将使用原始文件名打开一个新文件。 以下为错误日志备份方法： shell&gt; mv host_name.err host_name.err-old shell&gt; mysqladmin flush-logs shell&gt; mv host_name.err-old backup-directory 3.InnoDB中的日志MySQL数据库InnoDB存储引擎Log漫游 1 – Undo Log Undo Log 是为了实现事务的原子性，在MySQL数据库InnoDB存储引擎中，还用Undo Log来实现多版本并发控制(简称：MVCC)。 事务的原子性(Atomicity) 事务中的所有操作，要么全部完成，要么不做任何操作，不能只做部分操作。如果在执行的过程中发生 了错误，要回滚(Rollback)到事务开始前的状态，就像这个事务从来没有执行过。 原理 Undo Log的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方 （这个存储数据备份的地方称为Undo Log）。然后进行数据的修改。如果出现了错误或者用户执行了 ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 除了可以保证事务的原子性，Undo Log也可以用来辅助完成事务的持久化。 事务的持久性(Durability) 事务一旦完成，该事务对数据库所做的所有修改都会持久的保存到数据库中。为了保证持久性，数据库 系统会将修改后的数据完全的记录到持久的存储上。 用Undo Log实现原子性和持久化的事务的简化过程 假设有A、B两个数据，值分别为1,2。 A.事务开始. B.记录A=1到undo log. C.修改A=3. D.记录B=2到undo log. E.修改B=4. F.将undo log写到磁盘。 G.将数据写到磁盘。 H.事务提交 这里有一个隐含的前提条件：‘数据都是先读到内存中，然后修改内存中的数据，最后将数据写回磁盘’。 之所以能同时保证原子性和持久化，是因为以下特点： A. 更新数据前记录Undo log。 B. 为了保证持久性，必须将数据在事务提交前写到磁盘。只要事务成功提交，数据必然已经持久化。 C. Undo log必须先于数据持久化到磁盘。如果在G,H之间系统崩溃，undo log是完整的， 可以用来回滚事务。 D. 如果在A-F之间系统崩溃,因为数据没有持久化到磁盘。所以磁盘上的数据还是保持在事务开始前的状态。 缺陷：每个事务提交前将数据和Undo Log写入磁盘，这样会导致大量的磁盘IO，因此性能很低。 如果能够将数据缓存一段时间，就能减少IO提高性能。但是这样就会丧失事务的持久性。因此引入了另外一 种机制来实现持久化，即Redo Log. 2 – Redo Log 原理 和Undo Log相反，Redo Log记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可， 不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是Redo Log已经持久化。系统可以根据 Redo Log的内容，将所有数据恢复到最新的状态。 Undo + Redo事务的简化过程 假设有A、B两个数据，值分别为1,2. A.事务开始. B.记录A=1到undo log. C.修改A=3. D.记录A=3到redo log. E.记录B=2到undo log. F.修改B=4. G.记录B=4到redo log. H.将redo log写入磁盘。 I.事务提交 undo log 保存的是修改前的数据,并且保存到内存中,回滚的时候在读取里面的内容(从而实现了原子性),redolog保存的是修改后的数据(对新数据的备份,同时也会将redo log备份),在事务提交写入到磁盘,从而保证了持久性 4- 慢查询日志概述 数据库查询快慢是影响项目性能的一大因素，对于数据库，我们除了要优化 SQL，更重要的是得先找到需要优化的 SQL。如何找到低效的 SQL 是写这篇文章的主要目的。 MySQL 数据库有一个“慢查询日志”功能，用来记录查询时间超过某个设定值的SQL，这将极大程度帮助我们快速定位到问题所在，以便对症下药。至于查询时间的多少才算慢，每个项目、业务都有不同的要求，传统企业的软件允许查询时间高于某个值，但是把这个标准放在互联网项目或者访问量大的网站上，估计就是一个bug，甚至可能升级为一个功能性缺陷。 为避免误导读者，特申明本文的讨论限制在 Win 64位 + MySQL 5.6 范围内。其他平台或数据库种类及版本，我没有尝试过，不做赘述。 设置日志功能 关于慢查询日志，主要涉及到下面几个参数： slow_query_log ：是否开启慢查询日志功能（必填） long_query_time ：超过设定值，将被视作慢查询，并记录至慢查询日志文件中（必填） log-slow-queries ：慢查询日志文件（不可填），自动在 \data\ 创建一个 [hostname]-slow.log 文件 也就是说，只有满足以上三个条件，“慢查询功能”才可能正确开启或关闭。 5.二进制日志 主从复制的基础：binlog日志和relaylog日志 什么是MySQL主从复制 简单来说就是保证主SQL（Master）和从SQL（Slave）的数据是一致性的，向Master插入数据后，Slave会自动从Master把修改的数据同步过来（有一定的延迟），通过这种方式来保证数据的一致性，就是主从复制 复制方式 MySQL5.6开始主从复制有两种方式：基于日志（binlog）、基于GTID（全局事务标示符）。 本文只涉及基于日志binlog的主从配置 复制原理 1、Master将数据改变记录到二进制日志(binary log)中，也就是配置文件log-bin指定的文件，这些记录叫做二进制日志事件(binary log events) 2、Slave通过I/O线程读取Master中的binary log events并写入到它的中继日志(relay log) 3、Slave重做中继日志中的事件，把中继日志中的事件信息一条一条的在本地执行一次，完成数据在本地的存储，从而实现将改变反映到它自己的数据(数据重放) 1、什么是binlog binlog是一个二进制格式的文件，用于记录用户对数据库更新的SQL语句信息，例如更改数据库表和更改内容的SQL语句都会记录到binlog里，但是对库表等内容的查询不会记录。 默认情况下，binlog日志是二进制格式的，不能使用查看文本工具的命令（比如，cat，vi等）查看，而使用mysqlbinlog解析查看。 2.binlog的作用 当有数据写入到数据库时，还会同时把更新的SQL语句写入到对应的binlog文件里，这个文件就是上文说的binlog文件。使用mysqldump备份时，只是对一段时间的数据进行全备，但是如果备份后突然发现数据库服务器故障，这个时候就要用到binlog的日志了。 主要作用是用于数据库的主从复制及数据的增量恢复。 1.啥是binlog? 记录数据库增删改,不记录查询的二进制日志. 2.作用:用于数据同步. 3、如何开启binlog日志功能 在mysql的配置文件my.cnf中，增加log_bin参数即可开启binlog日志，也可以通过赋值来指定binlog日志的文件名，实例如下： [root@DB02 ~]# grep log_bin /etc/my.cnf log_bin = /application/mysql/logs/dadong-bin log_bin [root@DB02 ~]# 提示：也可以按“log_bin = /application/mysql/logs/dadong-bin”命名，目录要存在 为什么要刷新binlog?找到全备数据和binlog文件的恢复临界点. 总结mysql数据库的binlog和relay log日志有着举足轻重的作用，并且relay log仅仅存在于mysql 的slave库，它的作用就是记录slave库中的io进程接收的从主库传过来的binlog,然后等待slave库的sql进程去读取和应用，保证主从同步，但是binlog主库和从库（slave）都可以存在，记录对数据发生或潜在发生更改的SQL语句，并以二进制的形式保存在磁盘，所以可以通过binlog来实时备份和恢复数据库。 1、什么是binlog binlog是一个二进制格式的文件，用于记录用户对数据库更新的SQL语句信息，例如更改数据库表和更改内容的SQL语句都会记录到binlog里，但是对库表等内容的查询不会记录。 默认情况下，binlog日志是二进制格式的，不能使用查看文本工具的命令（比如，cat，vi等）查看，而使用mysqlbinlog解析查看。 2.binlog的作用 当有数据写入到数据库时，还会同时把更新的SQL语句写入到对应的binlog文件里，这个文件就是上文说的binlog文件。使用mysqldump备份时，只是对一段时间的数据进行全备，但是如果备份后突然发现数据库服务器故障，这个时候就要用到binlog的日志了。 主要作用是用于数据库的主从复制及数据的增量恢复。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列9：深入理解Class类和Object类]]></title>
    <url>%2F2019%2F09%2F09%2F9Java%E4%B8%AD%E7%9A%84Class%E7%B1%BB%E5%92%8CObject%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java中Class类及用法Java程序在运行时，Java运行时系统一直对所有的对象进行所谓的运行时类型标识，即所谓的RTTI。 这项信息纪录了每个对象所属的类。虚拟机通常使用运行时类型信息选准正确方法去执行，用来保存这些类型信息的类是Class类。Class类封装一个对象和接口运行时的状态，当装载类时，Class类型的对象自动创建。 说白了就是： Class类也是类的一种，只是名字和class关键字高度相似。Java是大小写敏感的语言。 Class类的对象内容是你创建的类的类型信息，比如你创建一个shapes类，那么，Java会生成一个内容是shapes的Class类的对象 Class类的对象不能像普通类一样，以 new shapes() 的方式创建，它的对象只能由JVM创建，因为这个类没有public构造函数 /* * Private constructor. Only the Java Virtual Machine creates Class objects. * This constructor is not used and prevents the default constructor being * generated. */ //私有构造方法，只能由jvm进行实例化 private Class(ClassLoader loader) { // Initialize final field for classLoader. The initialization value of non-null // prevents future JIT optimizations from assuming this final field is null. classLoader = loader; } Class类的作用是运行时提供或获得某个对象的类型信息，和C++中的typeid()函数类似。这些信息也可用于反射。 Class类原理看一下Class类的部分源码 //Class类中封装了类型的各种信息。在jvm中就是通过Class类的实例来获取每个Java类的所有信息的。 public class Class类 { Class aClass = null; // private EnclosingMethodInfo getEnclosingMethodInfo() { // Object[] enclosingInfo = getEnclosingMethod0(); // if (enclosingInfo == null) // return null; // else { // return new EnclosingMethodInfo(enclosingInfo); // } // } /**提供原子类操作 * Atomic operations support. */ // private static class Atomic { // // initialize Unsafe machinery here, since we need to call Class.class instance method // // and have to avoid calling it in the static initializer of the Class class... // private static final Unsafe unsafe = Unsafe.getUnsafe(); // // offset of Class.reflectionData instance field // private static final long reflectionDataOffset; // // offset of Class.annotationType instance field // private static final long annotationTypeOffset; // // offset of Class.annotationData instance field // private static final long annotationDataOffset; // // static { // Field[] fields = Class.class.getDeclaredFields0(false); // bypass caches // reflectionDataOffset = objectFieldOffset(fields, &quot;reflectionData&quot;); // annotationTypeOffset = objectFieldOffset(fields, &quot;annotationType&quot;); // annotationDataOffset = objectFieldOffset(fields, &quot;annotationData&quot;); // } //提供反射信息 // reflection data that might get invalidated when JVM TI RedefineClasses() is called // private static class ReflectionData&lt;T&gt; { // volatile Field[] declaredFields; // volatile Field[] publicFields; // volatile Method[] declaredMethods; // volatile Method[] publicMethods; // volatile Constructor&lt;T&gt;[] declaredConstructors; // volatile Constructor&lt;T&gt;[] publicConstructors; // // Intermediate results for getFields and getMethods // volatile Field[] declaredPublicFields; // volatile Method[] declaredPublicMethods; // volatile Class&lt;?&gt;[] interfaces; // // // Value of classRedefinedCount when we created this ReflectionData instance // final int redefinedCount; // // ReflectionData(int redefinedCount) { // this.redefinedCount = redefinedCount; // } // } //方法数组 // static class MethodArray { // // Don&apos;t add or remove methods except by add() or remove() calls. // private Method[] methods; // private int length; // private int defaults; // // MethodArray() { // this(20); // } // // MethodArray(int initialSize) { // if (initialSize &lt; 2) // throw new IllegalArgumentException(&quot;Size should be 2 or more&quot;); // // methods = new Method[initialSize]; // length = 0; // defaults = 0; // } //注解信息 // annotation data that might get invalidated when JVM TI RedefineClasses() is called // private static class AnnotationData { // final Map&lt;Class&lt;? extends Annotation&gt;, Annotation&gt; annotations; // final Map&lt;Class&lt;? extends Annotation&gt;, Annotation&gt; declaredAnnotations; // // // Value of classRedefinedCount when we created this AnnotationData instance // final int redefinedCount; // // AnnotationData(Map&lt;Class&lt;? extends Annotation&gt;, Annotation&gt; annotations, // Map&lt;Class&lt;? extends Annotation&gt;, Annotation&gt; declaredAnnotations, // int redefinedCount) { // this.annotations = annotations; // this.declaredAnnotations = declaredAnnotations; // this.redefinedCount = redefinedCount; // } // } } 我们都知道所有的java类都是继承了object这个类，在object这个类中有一个方法：getclass().这个方法是用来取得该类已经被实例化了的对象的该类的引用，这个引用指向的是Class类的对象。 我们自己无法生成一个Class对象（构造函数为private)，而 这个Class类的对象是在当各类被调入时，由 Java 虚拟机自动创建 Class 对象，或通过类装载器中的 defineClass 方法生成。 //通过该方法可以动态地将字节码转为一个Class类对象 protected final Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len) throws ClassFormatError { return defineClass(name, b, off, len, null); } 我们生成的对象都会有个字段记录该对象所属类在CLass类的对象的所在位置。如下图所示： [外链图片转存失败(img-ZfMJTzO4-1569074134147)(http://dl.iteye.com/upload/picture/pic/101542/0047a6e9-6608-3c3c-a67c-d8ee95e7fcb8.jpg)] 如何获得一个Class类对象请注意，以下这些方法都是值、指某个类对应的Class对象已经在堆中生成以后，我们通过不同方式获取对这个Class对象的引用。而上面说的DefineClass才是真正将字节码加载到虚拟机的方法，会在堆中生成新的一个Class对象。 第一种办法，Class类的forName函数 public class shapes{}Class obj= Class.forName(“shapes”);第二种办法，使用对象的getClass()函数 public class shapes{}shapes s1=new shapes();Class obj=s1.getClass();Class obj1=s1.getSuperclass();//这个函数作用是获取shapes类的父类的类型 第三种办法，使用类字面常量 Class obj=String.class;Class obj1=int.class;注意，使用这种办法生成Class类对象时，不会使JVM自动加载该类（如String类）。==而其他办法会使得JVM初始化该类。== 使用Class类的对象来生成目标类的实例 生成不精确的object实例 ==获取一个Class类的对象后，可以用 newInstance() 函数来生成目标类的一个实例。然而，该函数并不能直接生成目标类的实例，只能生成object类的实例== Class obj=Class.forName(“shapes”);Object ShapesInstance=obj.newInstance();使用泛化Class引用生成带类型的目标实例 Class obj=shapes.class;shapes newShape=obj.newInstance();因为有了类型限制，所以使用泛化Class语法的对象引用不能指向别的类。 Class obj1=int.class; Class&lt;Integer&gt; obj2=int.class; obj1=double.class; //obj2=double.class; 这一行代码是非法的，obj2不能改指向别的类 然而，有个灵活的用法，使得你可以用Class的对象指向基类的任何子类。 Class&lt;? extends Number&gt; obj=int.class; obj=Number.class; obj=double.class; 因此，以下语法生成的Class对象可以指向任何类。 Class&lt;?&gt; obj=int.class; obj=double.class; obj=shapes.class; 最后一个奇怪的用法是，当你使用这种泛型语法来构建你手头有的一个Class类的对象的基类对象时，必须采用以下的特殊语法 public class shapes{} class round extends shapes{} Class&lt;round&gt; rclass=round.class; Class&lt;? super round&gt; sclass= rclass.getSuperClass(); //Class&lt;shapes&gt; sclass=rclass.getSuperClass(); 我们明知道，round的基类就是shapes，但是却不能直接声明 Class &lt; shapes &gt;，必须使用特殊语法 Class &lt; ? super round &gt;这个记住就可以啦。 Object类这部分主要参考http://ihenu.iteye.com/blog/2233249 Object类是Java中其他所有类的祖先，没有Object类Java面向对象无从谈起。作为其他所有类的基类，Object具有哪些属性和行为，是Java语言设计背后的思维体现。 Object类位于java.lang包中，java.lang包包含着Java最基础和核心的类，在编译时会自动导入。Object类没有定义属性，一共有13个方法，13个方法之中并不是所有方法都是子类可访问的，一共有9个方法是所有子类都继承了的。 先大概介绍一下这些方法 1．clone方法 保护方法，实现对象的浅复制，只有实现了Cloneable接口才可以调用该方法，否则抛出CloneNotSupportedException异常。 2．getClass方法 final方法，获得运行时类型。 3．toString方法 该方法用得比较多，一般子类都有覆盖。 4．finalize方法 该方法用于释放资源。因为无法确定该方法什么时候被调用，很少使用。 5．equals方法 该方法是非常重要的一个方法。一般equals和==是不一样的，但是在Object中两者是一样的。子类一般都要重写这个方法。 6．hashCode方法 该方法用于哈希查找，重写了equals方法一般都要重写hashCode方法。这个方法在一些具有哈希功能的Collection中用到。 一般必须满足obj1.equals(obj2)==true。可以推出obj1.hash- Code()==obj2.hashCode()，但是hashCode相等不一定就满足equals。不过为了提高效率，应该尽量使上面两个条件接近等价。 7．wait方法 wait方法就是使当前线程等待该对象的锁，当前线程必须是该对象的拥有者，也就是具有该对象的锁。wait()方法一直等待，直到获得锁或者被中断。wait(long timeout)设定一个超时间隔，如果在规定时间内没有获得锁就返回。 调用该方法后当前线程进入睡眠状态，直到以下事件发生。 （1）其他线程调用了该对象的notify方法。 （2）其他线程调用了该对象的notifyAll方法。 （3）其他线程调用了interrupt中断该线程。 （4）时间间隔到了。 此时该线程就可以被调度了，如果是被中断的话就抛出一个InterruptedException异常。 8．notify方法 该方法唤醒在该对象上等待的某个线程。 9．notifyAll方法 该方法唤醒在该对象上等待的所有线程。类构造器public Object(); 大部分情况下，Java中通过形如 new A(args..)形式创建一个属于该类型的对象。其中A即是类名，A(args..)即此类定义中相对应的构造函数。通过此种形式创建的对象都是通过类中的构造函数完成。 为体现此特性，Java中规定：在类定义过程中，对于未定义构造函数的类，默认会有一个无参数的构造函数，作为所有类的基类，Object类自然要反映出此特性，在源码中，未给出Object类构造函数定义，但实际上，此构造函数是存在的。 当然，并不是所有的类都是通过此种方式去构建，也自然的，并不是所有的类构造函数都是public。 registerNatives()方法;private static native void registerNatives(); registerNatives函数前面有native关键字修饰，Java中，用native关键字修饰的函数表明该方法的实现并不是在Java中去完成，而是由C/C++去完成，并被编译成了.dll，由Java去调用。 方法的具体实现体在dll文件中，对于不同平台，其具体实现应该有所不同。用native修饰，即表示操作系统，需要提供此方法，Java本身需要使用。 具体到registerNatives()方法本身，其主要作用是将C/C++中的方法映射到Java中的native方法，实现方法命名的解耦。 既然如此，可能有人会问，registerNatives()修饰符为private，且并没有执行，作用何以达到？其实，在Java源码中，此方法的声明后有紧接着一段静态代码块： private static native void registerNatives(); static { registerNatives(); } Clone()方法实现浅拷贝protected native Object clone() throwsCloneNotSupportedException; 看，clode()方法又是一个被声明为native的方法，因此，我们知道了clone()方法并不是Java的原生方法，具体的实现是有C/C++完成的。clone英文翻译为”克隆”，其目的是创建并返回此对象的一个副本。 形象点理解，这有一辆科鲁兹，你看着不错，想要个一模一样的。你调用此方法即可像变魔术一样变出一辆一模一样的科鲁兹出来。配置一样，长相一样。但从此刻起，原来的那辆科鲁兹如果进行了新的装饰，与你克隆出来的这辆科鲁兹没有任何关系了。 你克隆出来的对象变不变完全在于你对克隆出来的科鲁兹有没有进行过什么操作了。Java术语表述为：clone函数返回的是一个引用，指向的是新的clone出来的对象，此对象与原对象分别占用不同的堆空间。 明白了clone的含义后，接下来看看如果调用clone()函数对象进行此克隆操作。 首先看一下下面的这个例子： package com.corn.objectsummary; import com.corn.Person; public class ObjectTest { public static void main(String[] args) { Object o1 = new Object(); // The method clone() from the type Object is not visible Object clone = o1.clone(); } } 例子很简单，在main()方法中，new一个Oject对象后，想直接调用此对象的clone方法克隆一个对象，但是出现错误提示：”The method clone() from the type Object is not visible” why? 根据提示，第一反应是ObjectTest类中定义的Oject对象无法访问其clone()方法。回到Object类中clone()方法的定义，可以看到其被声明为protected，估计问题就在这上面了，protected修饰的属性或方法表示：在同一个包内或者不同包的子类可以访问。 显然，Object类与ObjectTest类在不同的包中，但是ObjectTest继承自Object，是Object类的子类，于是，现在却出现子类中通过Object引用不能访问protected方法，原因在于对”不同包中的子类可以访问”没有正确理解。 “不同包中的子类可以访问”，是指当两个类不在同一个包中的时候，继承自父类的子类内部且主调（调用者）为子类的引用时才能访问父类用protected修饰的成员（属性/方法）。 在子类内部，主调为父类的引用时并不能访问此protected修饰的成员。！（super关键字除外） 于是，上例改成如下形式，我们发现，可以正常编译： public class clone方法 { public static void main(String[] args) { } public void test1() { User user = new User(); // User copy = user.clone(); } public void test2() { User user = new User(); // User copy = (User)user.clone(); } }是的，因为此时的主调已经是子类的引用了。 上述代码在运行过程中会抛出”java.lang.CloneNotSupportedException”,表明clone()方法并未正确执行完毕，问题的原因在与Java中的语法规定： clone()的正确调用是需要实现Cloneable接口，如果没有实现Cloneable接口，并且子类直接调用Object类的clone()方法，则会抛出CloneNotSupportedException异常。 Cloneable接口仅是一个表示接口，接口本身不包含任何方法，用来指示Object.clone()可以合法的被子类引用所调用。 于是，上述代码改成如下形式，即可正确指定clone()方法以实现克隆。 public class User implements Cloneable{ public int id; public String name; public UserInfo userInfo; public static void main(String[] args) { User user = new User(); UserInfo userInfo = new UserInfo(); user.userInfo = userInfo; System.out.println(user); System.out.println(user.userInfo); try { User copy = (User) user.clone(); System.out.println(copy); System.out.println(copy.userInfo); } catch (CloneNotSupportedException e) { e.printStackTrace(); } } //拷贝的User实例与原来不一样，是两个对象。 // com.javase.Class和Object.Object方法.用到的类.User@4dc63996 // com.javase.Class和Object.Object方法.用到的类.UserInfo@d716361 //而拷贝后对象的userinfo引用对象是同一个。 //所以这是浅拷贝 // com.javase.Class和Object.Object方法.用到的类.User@6ff3c5b5 // com.javase.Class和Object.Object方法.用到的类.UserInfo@d716361 }总结：clone方法实现的是浅拷贝，只拷贝当前对象，并且在堆中分配新的空间，放这个复制的对象。但是对象如果里面有其他类的子对象，那么就不会拷贝到新的对象中。 ==深拷贝和浅拷贝的区别== 浅拷贝浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。如果属性是基本类型，拷贝的就是基本类型的值；如果属性是内存地址（引用类型），拷贝的就是内存地址 ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。 深拷贝深拷贝会拷贝所有的属性,并拷贝属性指向的动态分配的内存。当对象和它所引用的对象一起拷贝时即发生深拷贝。深拷贝相比于浅拷贝速度较慢并且花销较大。现在为了要在clone对象时进行深拷贝， 那么就要Clonable接口，覆盖并实现clone方法，除了调用父类中的clone方法得到新的对象， 还要将该类中的引用变量也clone出来。如果只是用Object中默认的clone方法，是浅拷贝的。 那么这两种方式有什么相同和不同呢？ new操作符的本意是分配内存。程序执行到new操作符时， 首先去看new操作符后面的类型，因为知道了类型，才能知道要分配多大的内存空间。 分配完内存之后，再调用构造函数，填充对象的各个域，这一步叫做对象的初始化，构造方法返回后，一个对象创建完毕，可以把他的引用（地址）发布到外部，在外部就可以使用这个引用操纵这个对象。 而clone在第一步是和new相似的， 都是分配内存，调用clone方法时，分配的内存和源对象（即调用clone方法的对象）相同，然后再使用原对象中对应的各个域，填充新对象的域， 填充完成之后，clone方法返回，一个新的相同的对象被创建，同样可以把这个新对象的引用发布到外部。 ==也就是说，一个对象在浅拷贝以后，只是把对象复制了一份放在堆空间的另一个地方，但是成员变量如果有引用指向其他对象，这个引用指向的对象和被拷贝的对象中引用指向的对象是一样的。当然，基本数据类型还是会重新拷贝一份的。== getClass()方法4.public final native Class&lt;?&gt; getClass(); getClass()也是一个native方法，返回的是此Object对象的类对象/运行时类对象Class&lt;?&gt;。效果与Object.class相同。 首先解释下”类对象”的概念：在Java中，类是是对具有一组相同特征或行为的实例的抽象并进行描述，对象则是此类所描述的特征或行为的具体实例。 作为概念层次的类，其本身也具有某些共同的特性，如都具有类名称、由类加载器去加载，都具有包，具有父类，属性和方法等。 于是，Java中有专门定义了一个类，Class，去描述其他类所具有的这些特性，因此，从此角度去看，类本身也都是属于Class类的对象。为与经常意义上的对象相区分，在此称之为”类对象”。 public class getClass方法 { public static void main(String[] args) { User user = new User(); //getclass方法是native方法，可以取到堆区唯一的Class&lt;User&gt;对象 Class&lt;?&gt; aClass = user.getClass(); Class bClass = User.class; try { Class cClass = Class.forName(&quot;com.javase.Class和Object.Object方法.用到的类.User&quot;); } catch (ClassNotFoundException e) { e.printStackTrace(); } System.out.println(aClass); System.out.println(bClass); // class com.javase.Class和Object.Object方法.用到的类.User // class com.javase.Class和Object.Object方法.用到的类.User try { User a = (User) aClass.newInstance(); } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } } } 此处主要大量涉及到Java中的反射知识 equals()方法5.public boolean equals(Object obj); 与equals在Java中经常被使用，大家也都知道与equals的区别： ==表示的是变量值完成相同（对于基础类型，地址中存储的是值，引用类型则存储指向实际对象的地址）； equals表示的是对象的内容完全相同，此处的内容多指对象的特征/属性。 实际上，上面说法是不严谨的，更多的只是常见于String类中。首先看一下Object类中关于equals()方法的定义： public boolean equals(Object obj) { return (this == obj); } 由此可见，Object原生的equals()方法内部调用的正是==，与==具有相同的含义。既然如此，为什么还要定义此equals()方法？ equals()方法的正确理解应该是：判断两个对象是否相等。那么判断对象相等的标尺又是什么？ 如上，在object类中，此标尺即为==。当然，这个标尺不是固定的，其他类中可以按照实际的需要对此标尺含义进行重定义。如String类中则是依据字符串内容是否相等来重定义了此标尺含义。如此可以增加类的功能型和实际编码的灵活性。当然了，如果自定义的类没有重写equals()方法来重新定义此标尺，那么默认的将是其父类的equals()，直到object基类。 如下场景的实际业务需求，对于User bean，由实际的业务需求可知当属性uid相同时，表示的是同一个User，即两个User对象相等。则可以重写equals以重定义User对象相等的标尺。 ObjectTest中打印出true，因为User类定义中重写了equals()方法，这很好理解，很可能张三是一个人小名，张三丰才是其大名，判断这两个人是不是同一个人，这时只用判断uid是否相同即可。 如上重写equals方法表面上看上去是可以了，实则不然。因为它破坏了Java中的约定：重写equals()方法必须重写hasCode()方法。 hashCode()方法; public native int hashCode() hashCode()方法返回一个整形数值，表示该对象的哈希码值。 hashCode()具有如下约定： 1).在Java应用程序程序执行期间，对于同一对象多次调用hashCode()方法时，其返回的哈希码是相同的，前提是将对象进行equals比较时所用的标尺信息未做修改。在Java应用程序的一次执行到另外一次执行，同一对象的hashCode()返回的哈希码无须保持一致； 2).如果两个对象相等（依据：调用equals()方法），那么这两个对象调用hashCode()返回的哈希码也必须相等； 3).反之，两个对象调用hasCode()返回的哈希码相等，这两个对象不一定相等。 即严格的数学逻辑表示为： 两个对象相等 &lt;=&gt; equals()相等 =&gt; hashCode()相等。因此，重写equlas()方法必须重写hashCode()方法，以保证此逻辑严格成立，同时可以推理出：hasCode()不相等 =&gt; equals（）不相等 &lt;=&gt; 两个对象不相等。 可能有人在此产生疑问：既然比较两个对象是否相等的唯一条件（也是冲要条件）是equals，那么为什么还要弄出一个hashCode()，并且进行如此约定，弄得这么麻烦？ 其实，这主要体现在hashCode()方法的作用上，其主要用于增强哈希表的性能。 以集合类中，以Set为例，当新加一个对象时，需要判断现有集合中是否已经存在与此对象相等的对象，如果没有hashCode()方法，需要将Set进行一次遍历，并逐一用equals()方法判断两个对象是否相等，此种算法时间复杂度为o(n)。通过借助于hasCode方法，先计算出即将新加入对象的哈希码，然后根据哈希算法计算出此对象的位置，直接判断此位置上是否已有对象即可。（注：Set的底层用的是Map的原理实现） 在此需要纠正一个理解上的误区：对象的hashCode()返回的不是对象所在的物理内存地址。甚至也不一定是对象的逻辑地址，hashCode()相同的两个对象，不一定相等，换言之，不相等的两个对象，hashCode()返回的哈希码可能相同。 因此，在上述代码中，重写了equals()方法后，需要重写hashCode()方法。 public class equals和hashcode方法 { @Override //修改equals时必须同时修改hashcode方法，否则在作为key时会出问题 public boolean equals(Object obj) { return (this == obj); } @Override //相同的对象必须有相同hashcode，不同对象可能有相同hashcode public int hashCode() { return hashCode() &gt;&gt; 2; } }toString()方法7.public String toString(); toString()方法返回该对象的字符串表示。先看一下Object中的具体方法体： public String toString() { return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); } toString()方法相信大家都经常用到，即使没有显式调用，但当我们使用System.out.println(obj)时，其内部也是通过toString()来实现的。 getClass()返回对象的类对象，getClassName()以String形式返回类对象的名称（含包名）。Integer.toHexString(hashCode())则是以对象的哈希码为实参，以16进制无符号整数形式返回此哈希码的字符串表示形式。 如上例中的u1的哈希码是638，则对应的16进制为27e，调用toString()方法返回的结果为：com.corn.objectsummary.User@27e。 因此：toString()是由对象的类型和其哈希码唯一确定，同一类型但不相等的两个对象分别调用toString()方法返回的结果可能相同。 wait() notify() notifAll()8/9/10/11/12. wait(…) / notify() / notifyAll() 一说到wait(…) / notify() | notifyAll()几个方法，首先想到的是线程。确实，这几个方法主要用于java多线程之间的协作。先具体看下这几个方法的主要含义： wait()：调用此方法所在的当前线程等待，直到在其他线程上调用此方法的主调（某一对象）的notify()/notifyAll()方法。 wait(long timeout)/wait(long timeout, int nanos)：调用此方法所在的当前线程等待，直到在其他线程上调用此方法的主调（某一对象）的notisfy()/notisfyAll()方法，或超过指定的超时时间量。 notify()/notifyAll()：唤醒在此对象监视器上等待的单个线程/所有线程。 wait(…) / notify() | notifyAll()一般情况下都是配套使用。下面来看一个简单的例子： 这是一个生产者消费者的模型，只不过这里只用flag来标识哪个线程需要工作 public class wait和notify { //volatile保证线程可见性 volatile static int flag = 1; //object作为锁对象，用于线程使用wait和notify方法 volatile static Object o = new Object(); public static void main(String[] args) { new Thread(new Runnable() { @Override public void run() { //wait和notify只能在同步代码块内使用 synchronized (o) { while (true) { if (flag == 0) { try { Thread.sleep(2000); System.out.println(&quot;thread1 wait&quot;); //释放锁，线程挂起进入object的等待队列，后续代码运行 o.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(&quot;thread1 run&quot;); System.out.println(&quot;notify t2&quot;); flag = 0; //通知等待队列的一个线程获取锁 o.notify(); } } } }).start(); //解释同上 new Thread(new Runnable() { @Override public void run() { while (true) { synchronized (o) { if (flag == 1) { try { Thread.sleep(2000); System.out.println(&quot;thread2 wait&quot;); o.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(&quot;thread2 run&quot;); System.out.println(&quot;notify t1&quot;); flag = 1; o.notify(); } } } }).start(); } //输出结果是 // thread1 run // notify t2 // thread1 wait // thread2 run // notify t1 // thread2 wait // thread1 run // notify t2 //不断循环 } 从上述例子的输出结果中可以得出如下结论： 1、wait(…)方法调用后当前线程将立即阻塞，且适当其所持有的同步代码块中的锁，直到被唤醒或超时或打断后且重新获取到锁后才能继续执行； 2、notify()/notifyAll()方法调用后，其所在线程不会立即释放所持有的锁，直到其所在同步代码块中的代码执行完毕，此时释放锁，因此，如果其同步代码块后还有代码，其执行则依赖于JVM的线程调度。 在Java源码中，可以看到wait()具体定义如下： public final void wait() throws InterruptedException { wait(0); } 且wait(long timeout, int nanos)方法定义内部实质上也是通过调用wait(long timeout)完成。而wait(long timeout)是一个native方法。因此，wait(…)方法本质上都是native方式实现。 notify()/notifyAll()方法也都是native方法。 Java中线程具有较多的知识点，是一块比较大且重要的知识点。后期会有博文专门针对Java多线程作出详细总结。此处不再细述。 finalize()方法 protected void finalize(); finalize方法主要与Java垃圾回收机制有关。首先我们看一下finalized方法在Object中的具体定义： protected void finalize() throws Throwable { } 我们发现Object类中finalize方法被定义成一个空方法，为什么要如此定义呢？finalize方法的调用时机是怎么样的呢？ 首先，Object中定义finalize方法表明Java中每一个对象都将具有finalize这种行为，其具体调用时机在：JVM准备对此对形象所占用的内存空间进行垃圾回收前，将被调用。由此可以看出，此方法并不是由我们主动去调用的（虽然可以主动去调用，此时与其他自定义方法无异）。 CLass类和Object类的关系 Object类和Class类没有直接的关系。 Object类是一切java类的父类，对于普通的java类，即便不声明，也是默认继承了Object类。典型的，可以使用Object类中的toString()方法。 Class类是用于java反射机制的，一切java类，都有一个对应的Class对象，他是一个final类。Class 类的实例表示，正在运行的 Java 应用程序中的类和接口。 转一个知乎很有趣的问题https://www.zhihu.com/question/30301819 Java的对象模型中： 1 所有的类都是Class类的实例，Object是类，那么Object也是Class类的一个实例。 2 所有的类都最终继承自Object类，Class是类，那么Class也继承自Object。 3 这就像是先有鸡还是先有蛋的问题，请问实际中JVM是怎么处理的？ 这个问题中，第1个假设是错的：java.lang.Object是一个Java类，但并不是java.lang.Class的一个实例。后者只是一个用于描述Java类与接口的、用于支持反射操作的类型。这点上Java跟其它一些更纯粹的面向对象语言（例如Python和Ruby）不同。 而第2个假设是对的：java.lang.Class是java.lang.Object的派生类，前者继承自后者。虽然第1个假设不对，但“鸡蛋问题”仍然存在：在一个已经启动完毕、可以使用的Java对象系统里，必须要有一个java.lang.Class实例对应java.lang.Object这个类；而java.lang.Class是java.lang.Object的派生类，按“一般思维”前者应该要在后者完成初始化之后才可以初始化… 事实是：这些相互依赖的核心类型完全可以在“混沌”中一口气都初始化好，然后对象系统的状态才叫做完成了“bootstrap”，后面就可以按照Java对象系统的一般规则去运行。JVM、JavaScript、Python、Ruby等的运行时都有这样的bootstrap过程。 在“混沌”（boostrap过程）里，JVM可以为对象系统中最重要的一些核心类型先分配好内存空间，让它们进入[已分配空间]但[尚未完全初始化]状态。此时这些对象虽然已经分配了空间，但因为状态还不完整所以尚不可使用。 然后，通过这些分配好的空间把这些核心类型之间的引用关系串好。到此为止所有动作都由JVM完成，尚未执行任何Java字节码。然后这些核心类型就进入了[完全初始化]状态，对象系统就可以开始自我运行下去，也就是可以开始执行Java字节码来进一步完成Java系统的初始化了。 参考文章https://www.cnblogs.com/congsg2016/p/5317362.htmlhttps://www.jb51.net/article/125936.htmhttps://blog.csdn.net/dufufd/article/details/80537638https://blog.csdn.net/farsight1/article/details/80664104https://blog.csdn.net/xiaomingdetianxia/article/details/77429180 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Object类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习MySQL数据库9：Innodb中的事务隔离级别和锁的关系]]></title>
    <url>%2F2019%2F09%2F09%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0MySQL%E6%95%B0%E6%8D%AE%E5%BA%939%EF%BC%9AInnodb%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E9%94%81%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Innodb中的事务隔离级别和锁的关系 前言: 我们都知道事务的几种性质，数据库为了维护这些性质，尤其是一致性和隔离性，一般使用加锁这种方式。同时数据库又是个高并发的应用，同一时间会有大量的并发访问，如果加锁过度，会极大的降低并发处理能力。所以对于加锁的处理，可以说就是数据库对于事务处理的精髓所在。这里通过分析MySQL中InnoDB引擎的加锁机制，来抛砖引玉，让读者更好的理解，在事务处理中数据库到底做了什么。 一次封锁or两段锁？ 因为有大量的并发访问，为了预防死锁，一般应用中推荐使用一次封锁法，就是在方法的开始阶段，已经预先知道会用到哪些数据，然后全部锁住，在方法运行之后，再全部解锁。这种方式可以有效的避免循环死锁，但在数据库中却不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据。 数据库遵循的是两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁） 加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得S锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得X锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。 事务 加锁/解锁处理 begin； insert into test ….. 加insert对应的锁 update test set… 加update对应的锁 delete from test …. 加delete对应的锁 commit; 事务提交时，同时释放insert、update、delete对应的锁 这种方式虽然无法避免死锁，但是两段锁协议可以保证事务的并发调度是串行化（串行化很重要，尤其是在数据恢复和备份的时候）的。 事务中的加锁方式##事务的四种隔离级别 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻读 串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 Read Uncommitted这种级别，数据库一般都不会用，而且任何操作都不会加锁，这里就不讨论了。 MySQL中锁的种类MySQL中锁的种类很多，有常见的表锁和行锁，也有新加入的Metadata Lock等等,表锁是对一整张表加锁，虽然可分为读锁和写锁，但毕竟是锁住整张表，会导致并发能力下降，一般是做ddl处理时使用。 行锁则是锁住数据行，这种加锁方法比较复杂，但是由于只锁住有限的数据，对于其它数据不加限制，所以并发能力强，MySQL一般都是用行锁来处理并发事务。这里主要讨论的也就是行锁。 Read Committed（读取提交内容）在RC级别中，数据的读取都是不加锁的，但是数据的写入、修改和删除是需要加锁的。效果如下 MySQL> show create table class_teacher \G\ ​ ​ ​ Table: class_teacher ​ ​ ​ Create Table: CREATE TABLE `class_teacher` ( ​ ​ ​ `id` int(11) NOT NULL AUTO_INCREMENT, ​ ​ ​ `class_name` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL, ​ ​ ​ `teacher_id` int(11) NOT NULL, ​ ​ ​ PRIMARY KEY (`id`), ​ ​ ​ KEY `idx_teacher_id` (`teacher_id`) ​ ​ ​ ) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ​ ​ ​ 1 row in set (0.02 sec) ​ ​ ​ MySQL> select * from class_teacher; ​ ​ ​ +----+--------------+------------+ ​ ​ ​ | id | class_name | teacher_id | ​ ​ ​ +----+--------------+------------+ ​ ​ ​ | 1 | 初三一班 | 1 | ​ ​ ​ | 3 | 初二一班 | 2 | ​ ​ ​ | 4 | 初二二班 | 2 | ​ ​ ​ +----+--------------+------------+ 由于MySQL的InnoDB默认是使用的RR级别，所以我们先要将该session开启成RC级别，并且设置binlog的模式 SET session transaction isolation level read committed; ​ ​ ​ SET SESSION binlog_format = 'ROW';（或者是MIXED） 事务A 事务B begin; begin; update class_teacher set class_name=’初三二班’ where teacher_id=1; update class_teacher set class_name=’初三三班’ where teacher_id=1; ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction commit; 为了防止并发过程中的修改冲突，事务A中MySQL给teacher_id=1的数据行加锁，并一直不commit（释放锁），那么事务B也就一直拿不到该行锁，wait直到超时。 这时我们要注意到，teacher_id是有索引的，如果是没有索引的class_name呢？update class_teacher set teacher_id=3 where class_name = ‘初三一班’; 那么MySQL会给整张表的所有数据行的加行锁。这里听起来有点不可思议，但是当sql运行的过程中，MySQL并不知道哪些数据行是 class_name = ‘初三一班’的（没有索引嘛），如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由MySQL Server层进行过滤。 但在实际使用过程当中，MySQL做了一些改进，在MySQL Server过滤条件，发现不满足后，会调用unlock_row方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。可见即使是MySQL，为了效率也是会违反规范的。（参见《高性能MySQL》中文第三版p181） 这种情况同样适用于MySQL的默认隔离级别RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。 Repeatable Read（可重读）这是MySQL中InnoDB默认的隔离级别。我们姑且分“读”和“写”两个模块来讲解。 ####读 读就是可重读，可重读这个概念是一事务的多个实例在并发读取数据时，会看到同样的数据行，有点抽象，我们来看一下效果。 RC（不可重读）模式下的展现 事务A 事务B begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=1; idclass_name,teacher_id1初三二班12初三一班1 update class_teacher set class_name=’初三三班’ where id=1; commit; select id,class_name,teacher_id from class_teacher where teacher_id=1; idclass_name,teacher_id1初三三班12初三一班1 读到了事务B修改的数据，和第一次查询的结果不一样，是不可重读的。 commit; 事务B修改id=1的数据提交之后，事务A同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样）。这就很可能带来一些问题，那么我们来看看在RR级别中MySQL的表现： 事务A 事务B 事务C begin; begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1 update class_teacher set class_name=’初三三班’ where id=1;commit; insert into class_teacher values (null,’初三三班’,1); commit; select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1 没有读到事务B修改的数据，和第一次sql读取的一样，是可重复读的。没有读到事务C新添加的数据。 commit; 我们注意到，当teacher_id=1时，事务A先做了一次读取，事务B中间修改了id=1的数据，并commit之后，事务A第二次读到的数据和第一次完全相同。所以说它是可重读的。那么MySQL是怎么做到的呢？这里姑且卖个关子，我们往下看。 不可重复读和幻读的区别很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于update和delete，而幻读的重点在于insert。 如果使用锁机制来实现这两种隔离级别，在可重复读中，该sql第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住insert的数据，所以当事务A先前读取了数据，或者修改了全部数据，事务B还是可以insert数据提交，这时事务A就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要Serializable隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 所以说不可重复读和幻读最大的区别，就在于如何通过锁机制来解决他们产生的问题。 上文说的，是使用悲观锁机制来处理这两种问题，但是MySQL、ORACLE、PostgreSQL等成熟的数据库，出于性能考虑，都是使用了以乐观锁为理论基础的MVCC（多版本并发控制）来避免这两种问题。 悲观锁和乐观锁 悲观锁 正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 在悲观锁的情况下，为了保证事务的隔离性，就需要一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。 乐观锁 相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。 而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 要说明的是，MVCC的实现没有固定的规范，每个数据库都会有不同的实现方式，这里讨论的是InnoDB的MVCC。 MVCC在MySQL的InnoDB中的实现在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下： SELECT时，读取创建版本号&lt;=当前事务版本号，删除版本号为空或&gt;当前事务版本号。 INSERT时，保存当前事务版本号为行的创建版本号 DELETE时，保存当前事务版本号为行的删除版本号 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 我们不管从数据库方面的教课书中学到，还是从网络上看到，大都是上文中事务的四种隔离级别这一模块列出的意思，RR级别是可重复读的，但无法解决幻读，而只有在Serializable级别才能解决幻读。于是我就加了一个事务C来展示效果。在事务C中添加了一条teacher_id=1的数据commit，RR级别中应该会有幻读现象，事务A在查询teacher_id=1的数据时会读到事务C新加的数据。但是测试后发现，在MySQL中是不存在这种情况的，在事务C提交后，事务A还是不会读到这条数据。可见在MySQL的RR级别中，是解决了幻读的读问题的。参见下图 读问题解决了，根据MVCC的定义，并发提交数据时会出现冲突，那么冲突时如何解决呢？我们再来看看InnoDB中RR级别对于写数据的处理。 “读”与“读”的区别可能有读者会疑惑，事务的隔离级别其实都是对于读数据的定义，但到了这里，就被拆成了读和写两个模块来讲解。这主要是因为MySQL中的读，和事务隔离级别中的读，是不一样的。 我们且看，在RR级别中，通过MVCC机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。 对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在MVCC中： 快照读：就是select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 事务的隔离级别实际上都是定义了当前读的级别，MySQL为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得select不用加锁。而update、insert这些“当前读”，就需要另外的模块来解决了。 ###写（”当前读”） 事务的隔离级别中虽然只定义了读数据的要求，实际上这也可以说是写数据的要求。上文的“读”，实际是讲的快照读；而这里说的“写”就是当前读了。 为了解决当前读中的幻读问题，MySQL事务使用了Next-Key锁。 ####Next-Key锁 Next-Key锁是行锁和GAP（间隙锁）的合并，行锁上文已经介绍了，接下来说下GAP间隙锁。 行锁可以防止不同事务版本的数据修改提交时造成数据冲突的情况。但如何避免别的事务插入数据就成了问题。我们可以看看RR级别和RC级别的对比 RC级别： 事务A 事务B begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30 update class_teacher set class_name=’初三四班’ where teacher_id=30; insert into class_teacher values (null,’初三二班’,30);commit; select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班3010初三二班30 RR级别： 事务A 事务B begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30 update class_teacher set class_name=’初三四班’ where teacher_id=30; insert into class_teacher values (null,’初三二班’,30);waiting…. select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班30 commit; 事务Acommit后，事务B的insert执行。 通过对比我们可以发现，在RC级别中，事务A修改了所有teacher_id=30的数据，但是当事务B insert进新数据后，事务A发现莫名其妙多了一行teacher_id=30的数据，而且没有被之前的update语句所修改，这就是“当前读”的幻读。 RR级别中，事务A在update后加锁，事务B无法插入新数据，这样事务A在update前后读的数据保持一致，避免了幻读。这个锁，就是Gap锁。 MySQL是这么实现的： 在class_teacher这张表中，teacher_id是个索引，那么它就会维护一套B+树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同） 如图所示，InnoDB使用的是聚集索引，teacher_id身为二级索引，就要维护一个索引字段和主键id的树状结构（这里用链表形式表现），并保持顺序排列。 Innodb将这段数据分成几个个区间 (negative infinity, 5], (5,30], (30,positive infinity)； update class_teacher set class_name=’初三四班’ where teacher_id=30;不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30]和（30，positive infinity），都加入了gap锁。这样事务B就无法在这个两个区间insert进新数据。 受限于这种实现方式，Innodb很多时候会锁住不需要锁的区间。如下所示： 事务A 事务B 事务C begin; begin; begin; select id,class_name,teacher_id from class_teacher;idclass_nameteacher_id1初三一班52初三二班30 update class_teacher set class_name=’初一一班’ where teacher_id=20; insert into class_teacher values (null,’初三五班’,10);waiting ….. insert into class_teacher values (null,’初三五班’,40); commit; 事务A commit之后，这条语句才插入成功 commit; commit; update的teacher_id=20是在(5，30]区间，即使没有修改任何数据，Innodb也会在这个区间加gap锁，而其它区间不会影响，事务C正常插入。 如果使用的是没有索引的字段，比如update class_teacher set teacher_id=7 where class_name=’初三八班（即使没有匹配到任何数据）’,那么会给全表加入gap锁。同时，它不能像上文中行锁一样经过MySQL Server过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。 行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在写数据时的幻读问题。 ###Serializable 这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。 这里要吐槽一句，不要看到select就说不会加锁了，在Serializable这个级别，还是会加锁的！ 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列8：Java基础数据类型，以及自动拆装箱里隐藏的秘密]]></title>
    <url>%2F2019%2F09%2F08%2F8Java%E8%87%AA%E5%8A%A8%E6%8B%86%E7%AE%B1%E8%A3%85%E7%AE%B1%E9%87%8C%E9%9A%90%E8%97%8F%E7%9A%84%E7%A7%98%E5%AF%86%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java 基本数据类型变量就是申请内存来存储值。也就是说，当创建变量的时候，需要在内存中申请空间。 内存管理系统根据变量的类型为变量分配存储空间，分配的空间只能用来储存该类型数据。 因此，通过定义不同类型的变量，可以在内存中储存整数、小数或者字符。 Java 的两大数据类型: 内置数据类型 引用数据类型 内置数据类型Java语言提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。 byte： byte 数据类型是8位、有符号的，以二进制补码表示的整数； 最小值是 -128（-2^7）； 最大值是 127（2^7-1）； 默认值是 0； byte 类型用在大型数组中节约空间，主要代替整数，因为 byte 变量占用的空间只有 int 类型的四分之一； 例子：byte a = 100，byte b = -50。 short： short 数据类型是 16 位、有符号的以二进制补码表示的整数 最小值是 -32768（-2^15）； 最大值是 32767（2^15 - 1）； Short 数据类型也可以像 byte 那样节省空间。一个short变量是int型变量所占空间的二分之一； 默认值是 0； 例子：short s = 1000，short r = -20000。 int： int 数据类型是32位、有符号的以二进制补码表示的整数； 最小值是 -2,147,483,648（-2^31）； 最大值是 2,147,483,647（2^31 - 1）； 一般地整型变量默认为 int 类型； 默认值是 0 ； 例子：int a = 100000, int b = -200000。 long： long 数据类型是 64 位、有符号的以二进制补码表示的整数； 最小值是 -9,223,372,036,854,775,808（-2^63）； 最大值是 9,223,372,036,854,775,807（2^63 -1）； 这种类型主要使用在需要比较大整数的系统上； 默认值是 0L； 例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float： float 数据类型是单精度、32位、符合IEEE 754标准的浮点数； float 在储存大型浮点数组的时候可节省内存空间； 默认值是 0.0f； 浮点数不能用来表示精确的值，如货币； 例子：float f1 = 234.5f。 double： double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数； 浮点数的默认类型为double类型； double类型同样不能表示精确的值，如货币； 默认值是 0.0d； 例子：double d1 = 123.4。 boolean： boolean数据类型表示一位的信息； 只有两个取值：true 和 false； 这种类型只作为一种标志来记录 true/false 情况； 默认值是 false； 例子：boolean one = true。 char： char类型是一个单一的 16 位 Unicode 字符； 最小值是 \u0000（即为0）； 最大值是 \uffff（即为65,535）； char 数据类型可以储存任何字符； 例子：char letter = ‘A’;。 123456789101112131415161718192021222324//8位byte bx = Byte.MAX_VALUE;byte bn = Byte.MIN_VALUE;//16位short sx = Short.MAX_VALUE;short sn = Short.MIN_VALUE;//32位int ix = Integer.MAX_VALUE;int in = Integer.MIN_VALUE;//64位long lx = Long.MAX_VALUE;long ln = Long.MIN_VALUE;//32位float fx = Float.MAX_VALUE;float fn = Float.MIN_VALUE;//64位double dx = Double.MAX_VALUE;double dn = Double.MIN_VALUE;//16位char cx = Character.MAX_VALUE;char cn = Character.MIN_VALUE;//1位boolean bt = Boolean.TRUE;boolean bf = Boolean.FALSE; 127-12832767-327682147483647-21474836489223372036854775807-92233720368547758083.4028235E381.4E-451.7976931348623157E3084.9E-324￿ truefalse 引用类型 在Java中，引用类型的变量非常类似于C/C++的指针。引用类型指向一个对象，指向对象的变量是引用变量。这些变量在声明时被指定为一个特定的类型，比如 Employee、Puppy 等。变量一旦声明后，类型就不能被改变了。 对象、数组都是引用数据类型。 所有引用类型的默认值都是null。 一个引用变量可以用来引用任何与之兼容的类型。 例子：Site site = new Site(“Runoob”)。 Java 常量常量在程序运行时是不能被修改的。 在 Java 中使用 final 关键字来修饰常量，声明方式和变量类似： 1final double PI = 3.1415927; 虽然常量名也可以用小写，但为了便于识别，通常使用大写字母表示常量。 字面量可以赋给任何内置类型的变量。例如： 12byte a = 68;char a = &apos;A&apos; 自动拆箱和装箱（详解）Java 5增加了自动装箱与自动拆箱机制，方便基本类型与包装类型的相互转换操作。在Java 5之前，如果要将一个int型的值转换成对应的包装器类型Integer，必须显式的使用new创建一个新的Integer对象，或者调用静态方法Integer.valueOf()。 //在Java 5之前，只能这样做Integer value = new Integer(10);//或者这样做Integer value = Integer.valueOf(10);//直接赋值是错误的//Integer value = 10;` 在Java 5中，可以直接将整型赋给Integer对象，由编译器来完成从int型到Integer类型的转换，这就叫自动装箱。 //在Java 5中，直接赋值是合法的，由编译器来完成转换Integer value = 10;与此对应的，自动拆箱就是可以将包装类型转换为基本类型，具体的转换工作由编译器来完成。//在Java 5 中可以直接这么做Integer value = new Integer(10);int i = value; 自动装箱与自动拆箱为程序员提供了很大的方便，而在实际的应用中，自动装箱与拆箱也是使用最广泛的特性之一。自动装箱和自动拆箱其实是Java编译器提供的一颗语法糖（语法糖是指在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是更方便程序员使用。通过可提高开发效率，增加代码可读性，增加代码的安全性）。 简易实现在八种包装类型中，每一种包装类型都提供了两个方法： 静态方法valueOf(基本类型)：将给定的基本类型转换成对应的包装类型； 实例方法xxxValue()：将具体的包装类型对象转换成基本类型；下面我们以int和Integer为例，说明Java中自动装箱与自动拆箱的实现机制。看如下代码： class Auto //code1{ public static void main(String[] args) { //自动装箱 Integer inte = 10; //自动拆箱 int i = inte; //再double和Double来验证一下 Double doub = 12.40; double d = doub; }}上面的代码先将int型转为Integer对象，再讲Integer对象转换为int型，毫无疑问，这是可以正确运行的。可是，这种转换是怎么进行的呢？使用反编译工具，将生成的Class文件在反编译为Java文件，让我们看看发生了什么：class Auto//code2{ public static void main(String[] paramArrayOfString) { Integer localInteger = Integer.valueOf(10); int i = localInteger.intValue(); Double localDouble = Double.valueOf(12.4D); double d = localDouble.doubleValue(); }}我们可以看到经过javac编译之后，code1的代码被转换成了code2，实际运行时，虚拟机运行的就是code2的代码。也就是说，虚拟机根本不知道有自动拆箱和自动装箱这回事；在将Java源文件编译为class文件的过程中，javac编译器在自动装箱的时候，调用了Integer.valueOf()方法，在自动拆箱时，又调用了intValue()方法。我们可以看到，double和Double也是如此。实现总结：其实自动装箱和自动封箱是编译器为我们提供的一颗语法糖。在自动装箱时，编译器调用包装类型的valueOf()方法；在自动拆箱时，编译器调用了相应的xxxValue()方法。 自动装箱与拆箱中的“坑”在使用自动装箱与自动拆箱时，要注意一些陷阱，为了避免这些陷阱，我们有必要去看一下各种包装类型的源码。 Integer源码 public final class Integer extends Number implements Comparable { private final int value; /*Integer的构造方法，接受一个整型参数,Integer对象表示的int值，保存在value中*/ public Integer(int value) { this.value = value; } /*equals()方法判断的是:所代表的int型的值是否相等*/ public boolean equals(Object obj) { if (obj instanceof Integer) { return value == ((Integer)obj).intValue(); } return false; } /*返回这个Integer对象代表的int值，也就是保存在value中的值*/ public int intValue() { return value; } /** * 首先会判断i是否在[IntegerCache.low,Integer.high]之间 * 如果是，直接返回Integer.cache中相应的元素 * 否则，调用构造方法，创建一个新的Integer对象 */ public static Integer valueOf(int i) { assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } /** * 静态内部类，缓存了从[low,high]对应的Integer对象 * low -128这个值不会被改变 * high 默认是127，可以改变，最大不超过：Integer.MAX_VALUE - (-low) -1 * cache 保存从[low,high]对象的Integer对象 */ private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); } private IntegerCache() {} }}以上是Oracle(Sun)公司JDK 1.7中Integer源码的一部分，通过分析上面的代码，得到：1）Integer有一个实例域value，它保存了这个Integer所代表的int型的值，且它是final的，也就是说这个Integer对象一经构造完成，它所代表的值就不能再被改变。2）Integer重写了equals()方法，它通过比较两个Integer对象的value，来判断是否相等。3）重点是静态内部类IntegerCache，通过类名就可以发现：它是用来缓存数据的。它有一个数组，里面保存的是连续的Integer对象。 (a) low：代表缓存数据中最小的值，固定是-128。 (b) high：代表缓存数据中最大的值，它可以被该改变，默认是127。high最小是127，最大是Integer.MAX_VALUE-(-low)-1，如果high超过了这个值，那么cache[ ]的长度就超过Integer.MAX_VALUE了，也就溢出了。 (c) cache[]：里面保存着从[low,high]所对应的Integer对象，长度是high-low+1(因为有元素0，所以要加1)。4）调用valueOf(int i)方法时，首先判断i是否在[low,high]之间，如果是，则复用Integer.cache[i-low]。比如，如果Integer.valueOf(3)，直接返回Integer.cache[131]；如果i不在这个范围，则调用构造方法，构造出一个新的Integer对象。5）调用intValue()，直接返回value的值。通过3）和4）可以发现，默认情况下，在使用自动装箱时，VM会复用[-128,127]之间的Integer对象。 Integer a1 = 1;Integer a2 = 1;Integer a3 = new Integer(1);//会打印true，因为a1和a2是同一个对象,都是Integer.cache[129]System.out.println(a1 == a2);//false，a3构造了一个新的对象，不同于a1,a2System.out.println(a1 == a3); 了解基本类型缓存（常量池）的最佳实践//基本数据类型的常量池是-128到127之间。 // 在这个范围中的基本数据类的包装类可以自动拆箱，比较时直接比较数值大小。 public static void main(String[] args) { //int的自动拆箱和装箱只在-128到127范围中进行，超过该范围的两个integer的 == 判断是会返回false的。 Integer a1 = 128; Integer a2 = -128; Integer a3 = -128; Integer a4 = 128; System.out.println(a1 == a4); System.out.println(a2 == a3); Byte b1 = 127; Byte b2 = 127; Byte b3 = -128; Byte b4 = -128; //byte都是相等的，因为范围就在-128到127之间 System.out.println(b1 == b2); System.out.println(b3 == b4); // Long c1 = 128L; Long c2 = 128L; Long c3 = -128L; Long c4 = -128L; System.out.println(c1 == c2); System.out.println(c3 == c4); //char没有负值 //发现char也是在0到127之间自动拆箱 Character d1 = 128; Character d2 = 128; Character d3 = 127; Character d4 = 127; System.out.println(d1 == d2); System.out.println(d3 == d4);结果 falsetruetruetruefalsetruefalsetrue Integer i = 10; Byte b = 10; //比较Byte和Integer.两个对象无法直接比较，报错 //System.out.println(i == b); System.out.println(&quot;i == b &quot; + i.equals(b)); //答案是false,因为包装类的比较时先比较是否是同一个类，不是的话直接返回false. int ii = 128; short ss = 128; long ll = 128; char cc = 128; System.out.println(&quot;ii == bb &quot; + (ii == ss)); System.out.println(&quot;ii == ll &quot; + (ii == ll)); System.out.println(&quot;ii == cc &quot; + (ii == cc)); 结果 i == b false ii == bb true ii == ll true ii == cc true //这时候都是true，因为基本数据类型直接比较值，值一样就可以。总结：通过上面的代码，我们分析一下自动装箱与拆箱发生的时机： （1）当需要一个对象的时候会自动装箱，比如Integer a = 10;equals(Object o)方法的参数是Object对象，所以需要装箱。 （2）当需要一个基本类型时会自动拆箱，比如int a = new Integer(10);算术运算是在基本类型间进行的，所以当遇到算术运算时会自动拆箱，比如代码中的 c == (a + b); （3） 包装类型 == 基本类型时，包装类型自动拆箱； 需要注意的是：“==”在没遇到算术运算时，不会自动拆箱；基本类型只会自动装箱为对应的包装类型，代码中最后一条说明的内容。 在JDK 1.5中提供了自动装箱与自动拆箱，这其实是Java 编译器的语法糖，编译器通过调用包装类型的valueOf()方法实现自动装箱，调用xxxValue()方法自动拆箱。自动装箱和拆箱会有一些陷阱，那就是包装类型复用了某些对象。 （1）Integer默认复用了[-128,127]这些对象，其中高位置可以修改； （2）Byte复用了全部256个对象[-128,127]； （3）Short服用了[-128,127]这些对象； （4）Long服用了[-128,127]; （5）Character复用了[0,127],Charater不能表示负数; Double和Float是连续不可数的，所以没法复用对象，也就不存在自动装箱复用陷阱。 Boolean没有自动装箱与拆箱，它也复用了Boolean.TRUE和Boolean.FALSE，通过Boolean.valueOf(boolean b)返回的Blooean对象要么是TRUE，要么是FALSE，这点也要注意。 本文介绍了“真实的”自动装箱与拆箱，为了避免写出错误的代码，又从包装类型的源码入手，指出了各种包装类型在自动装箱和拆箱时存在的陷阱，同时指出了自动装箱与拆箱发生的时机。 基本数据类型的存储方式上面自动拆箱和装箱的原理其实与常量池有关。 存在栈中：public void(int a){int i = 1;int j = 1;}方法中的i 存在虚拟机栈的局部变量表里，i是一个引用，j也是一个引用，它们都指向局部变量表里的整型值 1.int a是传值引用，所以a也会存在局部变量表。 存在堆里class A{int i = 1;A a = new A();}i是类的成员变量。类实例化的对象存在堆中，所以成员变量也存在堆中，引用a存的是对象的地址，引用i存的是值，这个值1也会存在堆中。可以理解为引用i指向了这个值1。也可以理解为i就是1. 3 包装类对象怎么存其实我们说的常量池也可以叫对象池。比如String a= new String(“a”).intern()时会先在常量池找是否有“a”对象如果有的话直接返回“a”对象在常量池的地址，即让引用a指向常量”a”对象的内存地址。public native String intern();Integer也是同理。 下图是Integer类型在常量池中查找同值对象的方法。 public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; } private IntegerCache() {} }所以基本数据类型的包装类型可以在常量池查找对应值的对象，找不到就会自动在常量池创建该值的对象。 而String类型可以通过intern来完成这个操作。 JDK1.7后，常量池被放入到堆空间中，这导致intern()函数的功能不同，具体怎么个不同法，且看看下面代码，这个例子是网上流传较广的一个例子，分析图也是直接粘贴过来的，这里我会用自己的理解去解释这个例子： 123456789101112131415[java] view plain copyString s = new String(&quot;1&quot;); s.intern(); String s2 = &quot;1&quot;; System.out.println(s == s2); String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;); s3.intern(); String s4 = &quot;11&quot;; System.out.println(s3 == s4); 输出结果为：[java] view plain copyJDK1.6以及以下：false false JDK1.7以及以上：false true JDK1.6查找到常量池存在相同值的对象时会直接返回该对象的地址。 JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。 那么其他字符串在常量池找值时就会返回另一个堆中对象的地址。 下一节详细介绍String以及相关包装类。 具体请见：https://blog.csdn.net/a724888/article/details/80042298 关于Java面向对象三大特性，请参考： https://blog.csdn.net/a724888/article/details/80033043 参考文章https://www.runoob.com/java/java-basic-datatypes.html https://www.cnblogs.com/zch1126/p/5335139.html https://blog.csdn.net/jreffchen/article/details/81015884 https://blog.csdn.net/yuhongye111/article/details/31850779 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java基本数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库8：MySQL的事务隔离级别实战]]></title>
    <url>%2F2019%2F09%2F08%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%938%EF%BC%9AMySQL%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[本文转自:https://blog.csdn.net/sinat_27143551/article/details/80876127 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 事务的基本要素（ACID） 1、原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 2、一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 3、隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 4、持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 事务的并发问题 1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。 3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表 MySQL事务隔离级别事务隔离级别 脏读 不可重复读 幻读读未提交（read-uncommitted） 是 是 是读已提交（read-committed） 否 是 是可重复读（repeatable-read） 否 否 是串行化（serializable） 否 否 否默认事务隔离级别： 1、 读未提交例子 （1）打开客户端A，设置事务模式为read uncommitted，查询表 （1） 在A提交事务之前，打开客户端B，更新表 （3）此时，B的事务没提交，但是A已经可以查到B更新的数据 （4）一旦B的事务因某原因回滚，则A查到的数据就是脏数据。 （5）在A执行更新语句，在不知道有其他事务回滚时，会发现结果好奇怪。要解决这个问题可以采用读已提交的事务隔离级别。 2、 读已提交 （1） 在客户端A设置事务模式为read committed; （2） 在客户端A事务提交之前，打开客户端B，起事务更新表 （3） B的事务还未提交，A不能查到已经更新的数据，解决了脏读问题： （4） 此时提交客户端B的事务 （5） A执行与上一步相同的查询，结果发现与上一步不同，这就是不可重复读的问题： 3、 可重复读 （1） 打开客户端A，设置事务模式为repeatable read。 （2） 在A事务提交之前，打开客户端B，更新表account并提交： （3） 在客户端A执行步骤1的查询，zhangsan的balance依然是450与步骤（1）查询结果一致，没有出现不可重复读的问题；接着执行update balance = balance – 50 where id=4;balance没有变成450-50=400；zhangsan的balance值用的是步骤 （2）中的400来算的，所以是350。数据的一致性没有被破坏。 （4）在客户端A提交事务 （5） 在客户端A开启事务，随后在客户端B开启事务，新增一条数据。提交 （6） 在A计算balance之和，值为350+16000+2400=18750，没有把客户端B新增的数据算进去，客户端A提交后再计算balance之和，居然变成了19350，这时因为把客户端B的600算进去了。站在客户的角度，客户是看不到客户端B的，他会觉得天上掉馅饼了，多了600块，这就是幻读，站在开发者的角度，数据的一致性没有破坏。但是在应用程序中，我们的代码可能会把18750提交给用户了，如果一定要避免这种小概率状况的发生，那么就要采取“串行化”的事务隔离级别了。 4、 串行化 （1） 打开客户端A，设置事务隔离级别为serializable并开启事务。 （2） 打开客户端B，同样设置事务隔离级别为serializable，开启事务插入数据，报错。表被锁了，插入失败，mysql中事务隔离级别为serializable时会锁表，因此不会出现幻读的情况，这种隔离级别并发性很低，开发中很少用到。 补充： 1、SQL规范所规定的标准，不同的数据库具体的实现可能会有些差异 2、mysql中默认事务隔离级别是可重复读时并不会锁住读取到的行 3、事务隔离级别为读提交时，写数据只会锁住相应的行 4、事务隔离级别为可重复读时，如果有索引（包括主键索引）的时候，以索引列为条件更新数据，会存在间隙锁间隙锁、行锁、下一键锁的问题，从而锁住一些行；如果没有索引，更新数据时会锁住整张表。 5、事务隔离级别为串行化时，读写数据都会锁住整张表 6、隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列7：Java 代码块和执行顺序]]></title>
    <url>%2F2019%2F09%2F07%2F7%E4%BB%A3%E7%A0%81%E5%9D%97%E5%92%8C%E4%BB%A3%E7%A0%81%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java中的构造方法构造方法简介构造方法是类的一种特殊方法，用来初始化类的一个新的对象。Java 中的每个类都有一个默认的构造方法，它必须具有和类名相同的名称，而且没有返回类型。构造方法的默认返回类型就是对象类型本身，并且构造方法不能被 static、final、synchronized、abstract 和 native 修饰。 提示：构造方法用于初始化一个新对象，所以用 static 修饰没有意义；构造方法不能被子类继承，所以用 final 和 abstract 修饰没有意义；多个线程不会同时创建内存地址相同的同一个对象，所以用 synchronized 修饰没有必要。 构造方法的语法格式如下： class class_name { public class_name(){} //默认无参构造方法 public ciass_name([paramList]){} //定义构造方法 … //类主体 } 在一个类中，与类名相同的方法就是构造方法。每个类可以具有多个构造方法，但要求它们各自包含不同的方法参数。 构造方法实例例 1构造方法主要有无参构造方法和有参构造方法两种，示例如下： public class MyClass { private int m; //定义私有变量 MyClass() { //定义无参的构造方法 m=0; } MyCiass(int m) { //定义有参的构造方法 this.m=m; } } 该示例定义了两个构造方法，分别是无参构造方法和有参构造方法。在一个类中定义多个具有不同参数的同名方法，这就是方法的重载。这两个构造方法的名称都与类名相同，均为 MyClass。在实例化该类时可以调用不同的构造方法进行初始化。 注意：类的构造方法不是要求必须定义的。如果在类中没有定义任何一个构造方法，则 Java 会自动为该类生成一个默认的构造方法。默认的构造方法不包含任何参数，并且方法体为空。如果类中显式地定义了一个或多个构造方法，则 Java 不再提供默认构造方法。 例 2要在不同的条件下使用不同的初始化行为创建类的对象，这时候就需要在一个类中创建多个构造方法。下面通过一个示例来演示构造方法的使用。 (1) 首先在员工类 Worker 中定义两个构造方法，代码如下： public class Worker { public String name; //姓名 private int age; //年龄 //定义带有一个参数的构造方法 public Worker(String name) { this.name=name; } //定义带有两个参数的构造方法 public Worker(String name,int age) { this.name=name; this.age=age; } public String toString() { return"大家好！我是新来的员工，我叫"+name+"，今年"+age+"岁。"; } } 在 Worker 类中定义了两个属性，其中 name 属性不可改变。分别定义了带有一个参数和带有两个参数的构造方法，并对其属性进行初始化。最后定义了该类的 toString() 方法，返回一条新进员工的介绍语句。 提示：Object 类具有一个 toString() 方法，该方法是个特殊的方法，创建的每个类都会继承该方法，它返回一个 String 类型的字符串。如果一个类中定义了该方法，则在调用该类对象时，将会自动调用该类对象的 toString() 方法返回一个字符串，然后使用“System.out.println(对象名)”就可以将返回的字符串内容打印出来。 (2) 在 TestWorker 类中创建 main() 方法作为程序的入口处，在 main() 方法中调用不同的构造方法实例化 Worker 对象，并对该对象中的属性进行初始化，代码如下： public class TestWorker { public static void main(String[] args) { System.out.println("-----------带有一个参数的构造方法-----------"); //调用带有一个参数的构造方法，Staff类中的sex和age属性值不变 Worker worker1=new Worker("张强"); System.out.println(worker1); System.out.println("-----------带有两个参数的构造方法------------"); //调用带有两个参数的构造方法，Staff类中的sex属性值不变 Worker worker2=new Worker("李丽",25); System.out.println(worker2); } } 在上述代码中，创建了两个不同的 Worker 对象：一个是姓名为张强的员工对象，一个是姓名为李丽、年龄为 25 的员工对象。对于第一个 Worker 对象 Worker1，并未指定 age 属性值，因此程序会将其值采用默认值 0。对于第二个 Worker 对象 Worker2，分别对其指定了 name 属性值和 age 属性值，因此程序会将传递的参数值重新赋值给 Worker 类中的属性值。 运行 TestWorker 类，输出的结果如下： -----------带有一个参数的构造方法----------- 大家好！我是新来的员工，我叫张强，今年0岁。 -----------带有两个参数的构造方法------------ 大家好！我是新来的员工，我叫李丽，今年25岁。 通过调用带参数的构造方法，在创建对象时，一并完成了对象成员的初始化工作，简化了对象初始化的代码。 Java中的几种构造方法详解普通构造方法方法名与类名相同 无返回类型 子类不能继承父类的构造方法 不能被static、final、abstract修饰（有final和static修饰的是不能被子类继承的，abstract修饰的是抽象类，抽象类是不能实例化的，也就是不能new） 可以被private修饰，可以在本类里面实例化，但是外部不能实例化对象（注意！！！） public class A{ int i=0; public A(){ i=2; } public A(int i){ this.i=i; } }默认构造方法如果没有任何的构造方法，编译时系统会自动添加一个默认无参构造方法 隐含的默认构造方法 public A(){}显示的默认构造方法 public A(){ System.out.print(&quot;显示的默认构造方法&quot;) }重载构造方法比如原本的类里的构造方法是一个参数的，现在新建的对象是有三个参数，此时就要重载构造方法 当一个类中有多个构造方法，有可能会出现重复性操作，这时可以用this语句调用其他的构造方法。 public class A{ private int age; private String name; public A(int age,String name){ this.age=age; this.name=name; } public A(int age){ this(age,&quot;无名氏&quot;);//调用 A(int age,String name)构造方法 } public A(){ this(1);//调用 A(int age)构造方法 } public void setName(String name) {this.name=name;} public String getName() {return name;} public void setAge(int age) {this.age=age;} public int getAge() {return age;} } A a=new A(20,&quot;周一&quot;); A b=new A(20); A c=new A(); String name = a.getName(); String name1 = b.getName(); int age = c.getAge(); System.out.println(name); System.out.println(name1); System.out.println(age);java子类构造方法调用父类构造方法首先父类构造方法是绝对不能被子类继承的。 子类构造方法调用父类的构造方法重点是：子类构造方法无论如何都要调用父类的构造方法。 子类构造方法要么调用父类无参构造方法（包括当父类没有构造方法时。系统默认给的无参构造方法），要么调用父类有参构造方法。当子类构造方法调用父类无参构造方法，一般都是默认不写的，要写的话就是super（），且要放在构造方法的第一句。当子类构造方法要调用父类有参数的构造方法，那么子类的构造方法中必须要用super（参数）调用父类构造方法，且要放在构造方法的第一句。 当子类的构造方法是无参构造方法时，必须调用父类无参构造方法。因为系统会自动找父类有没有无参构造方法，如果没有的话系统会报错：说父类没有定义无参构造方法。 当子类构造方法是有参构造方法时，这时就会有两种情况。第一种：子类构造方法没有写super，也就是说你默认调用父类无参构造方法，这样的话就和子类是无参构造方法一样。 第二种：子类构造方法有super（参数）时，就是调用父类有参构造方法，系统会找父类有没有参数一致（参数数量，且类型顺序要相同）的有参构造方法，如果没有的话，同样也会报错。 但是这里会遇到和重载构造方法this一样问题，一个参数的构造方法可以调用多个参数构造方法，没有的参数给一个自己定义值也是可以的。 Java中的代码块简介在java中用{}括起来的称为代码块，代码块可分为以下四种： 一.简介 1.普通代码块： 类中方法的方法体 2.构造代码块： 构造块会在创建对象时被调用，每次创建时都会被调用，优先于类构造函数执行。 3.静态代码块： 用static{}包裹起来的代码片段，只会执行一次。静态代码块优先于构造块执行。 4.同步代码块： 使用synchronized（）{}包裹起来的代码块，在多线程环境下，对共享数据的读写操作是需要互斥进行的，否则会导致数据的不一致性。同步代码块需要写在方法中。 二.静态代码块和构造代码块的异同点 相同点：都是JVM加载类后且在构造函数执行之前执行，在类中可定义多个，一般在代码块中对一些static变量进行赋值。 不同点：静态代码块在非静态代码块之前执行。静态代码块只在第一次new时执行一次，之后不在执行。而非静态代码块每new一次就执行一次。 Java代码块使用局部代码块 位置：局部位置（方法内部） 作用：限定变量的生命周期，尽早释放，节约内存 调用：调用其所在的方法时执行 public class 局部代码块 { @Test public void test (){ B b = new B(); b.go(); } } class B { B(){} public void go() { //方法中的局部代码块，一般进行一次性地调用，调用完立刻释放空间，避免在接下来的调用过程中占用栈空间 //因为栈空间内存是有限的，方法调用可能会会生成很多局部变量导致栈内存不足。 //使用局部代码块可以避免这样的情况发生。 { int i = 1; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (i &lt; 1000) { list.add(i ++); } for (Integer j : list) { System.out.println(j); } System.out.println(&quot;gogogo&quot;); } System.out.println(&quot;hello&quot;); } }构造代码块 位置：类成员的位置，就是类中方法之外的位置 作用：把多个构造方法共同的部分提取出来，共用构造代码块 调用：每次调用构造方法时，都会优先于构造方法执行，也就是每次new一个对象时自动调用，对 对象的初始化 class A{ int i = 1; int initValue;//成员变量的初始化交给代码块来完成 { //代码块的作用体现于此：在调用构造方法之前，用某段代码对成员变量进行初始化。 //而不是在构造方法调用时再进行。一般用于将构造方法的相同部分提取出来。 // for (int i = 0;i &lt; 100;i ++) { initValue += i; } } { System.out.println(initValue); System.out.println(i);//此时会打印1 int i = 2;//代码块里的变量和成员变量不冲突，但会优先使用代码块的变量 System.out.println(i);//此时打印2 //System.out.println(j);//提示非法向后引用，因为此时j的的初始化还没开始。 // } { System.out.println(&quot;代码块运行&quot;); } int j = 2; { System.out.println(j); System.out.println(i);//代码块中的变量运行后自动释放，不会影响代码块之外的代码 } A(){ System.out.println(&quot;构造方法运行&quot;); } } public class 构造代码块 { @Test public void test() { A a = new A(); } }静态代码块 位置：类成员位置，用static修饰的代码块 作用：对类进行一些初始化 只加载一次，当new多个对象时，只有第一次会调用静态代码块，因为，静态代码块 是属于类的，所有对象共享一份 调用: new 一个对象时自动调用 public class 静态代码块 { @Test public void test() { C c1 = new C(); C c2 = new C(); //结果,静态代码块只会调用一次，类的所有对象共享该代码块 //一般用于类的全局信息初始化 //静态代码块调用 //代码块调用 //构造方法调用 //代码块调用 //构造方法调用 } } class C{ C(){ System.out.println(&quot;构造方法调用&quot;); } { System.out.println(&quot;代码块调用&quot;); } static { System.out.println(&quot;静态代码块调用&quot;); } }Java代码块、构造方法（包含继承关系）的执行顺序这是一道常见的面试题，要回答这个问题，先看看这个实例吧。 一共3个类：A、B、C其中A是B的父类，C无继承仅作为输出 A类： public class A { static { Log.i(&quot;HIDETAG&quot;, &quot;A静态代码块&quot;); } private static C c = new C(&quot;A静态成员&quot;); private C c1 = new C(&quot;A成员&quot;); { Log.i(&quot;HIDETAG&quot;, &quot;A代码块&quot;); } static { Log.i(&quot;HIDETAG&quot;, &quot;A静态代码块2&quot;); } public A() { Log.i(&quot;HIDETAG&quot;, &quot;A构造方法&quot;); } }B类： public class B extends A { private static C c1 = new C(&quot;B静态成员&quot;); { Log.i(&quot;HIDETAG&quot;, &quot;B代码块&quot;); } private C c = new C(&quot;B成员&quot;); static { Log.i(&quot;HIDETAG&quot;, &quot;B静态代码块2&quot;); } static { Log.i(&quot;HIDETAG&quot;, &quot;B静态代码块&quot;); } public B() { Log.i(&quot;HIDETAG&quot;, &quot;B构造方法&quot;); } }C类： public class C { public C(String str) { Log.i(&quot;HIDETAG&quot;, str + &quot;构造方法&quot;); } }执行语句：new B(); 输出结果如下： I/HIDETAG: A静态代码块 I/HIDETAG: A静态成员构造方法 I/HIDETAG: A静态代码块2 I/HIDETAG: B静态成员构造方法 I/HIDETAG: B静态代码块2 I/HIDETAG: B静态代码块 I/HIDETAG: A成员构造方法 I/HIDETAG: A代码块 I/HIDETAG: A构造方法 I/HIDETAG: B代码块 I/HIDETAG: B成员构造方法 I/HIDETAG: B构造方法得出结论： 执行顺序依次为： 父类的静态成员和代码块 子类静态成员和代码块 父类成员初始化和代码快 父类构造方法 子类成员初始化和代码块 子类构造方法注意：可以发现，同一级别的代码块和成员初始化是按照代码顺序从上到下依次执行 看完上面这个demo，再来看看下面这道题，看看你搞得定吗？ 看下面一段代码，求执行顺序： class A { public A() { System.out.println(&quot;1A类的构造方法&quot;); } { System.out.println(&quot;2A类的构造快&quot;); } static { System.out.println(&quot;3A类的静态块&quot;); } } public class B extends A { public B() { System.out.println(&quot;4B类的构造方法&quot;); } { System.out.println(&quot;5B类的构造快&quot;); } static { System.out.println(&quot;6B类的静态块&quot;); } public static void main(String[] args) { System.out.println(&quot;7&quot;); new B(); new B(); System.out.println(&quot;8&quot;); } }执行顺序结果为：367215421548 为什么呢？ 首先我们要知道下面这5点： 每次new都会执行构造方法以及构造块。构造块的内容会在构造方法之前执行。非主类的静态块会在类加载时，构造方法和构造块之前执行，切只执行一次。主类（public class）里的静态块会先于main执行。继承中，子类实例化，会先执行父类的构造方法，产生父类对象，再调用子类构造方法。所以题目里，由于主类B继承A，所以会先加载A，所以第一个执行的是第3句。 从第4点我们知道6会在7之前执行，所以前三句是367。 之后实例化了B两次，每次都会先实例化他的父类A，然后再实例化B，而根据第1、2、5点，知道顺序为2154。 最后执行8 所以顺序是367215421548 参考文章https://blog.csdn.net/likunkun__/article/details/83066062https://www.jianshu.com/p/6877aae403f7https://www.jianshu.com/p/49e45af288eahttps://blog.csdn.net/du_du1/article/details/91383128http://c.biancheng.net/view/976.htmlhttps://blog.csdn.net/evilcry2012/article/details/79499786https://www.jb51.net/article/129990.htm 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>代码块</tag>
        <tag>构造方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库7：详解MyIsam与InnoDB引擎的锁实现]]></title>
    <url>%2F2019%2F09%2F07%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%937%EF%BC%9A%E8%AF%A6%E8%A7%A3MyIsam%E4%B8%8EInnoDB%E5%BC%95%E6%93%8E%E7%9A%84%E9%94%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 说到锁机制之前，先来看看Mysql的存储引擎，毕竟不同的引擎的锁机制也随着不同。 三类常见引擎：MyIsam ：不支持事务，不支持外键，所以访问速度快。锁机制是表锁，支持全文索引 InnoDB ：支持事务、支持外键，所以对比MyISAM，InnoDB的处理效率差一些，并要占更多的磁盘空间保留数据和索引。锁机制是行锁，不支持全文索引 Memory：数据是存放在内存中的，默认哈希索引，非常适合存储临时数据，服务器关闭后，数据会丢失掉。 如何选择存储引擎：MyISAM：应用是以读操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高。 InnoDB：用于事务处理应用程序，支持外键，如果应用对事务的完整性有比较高的要求，在并发条件下要求数据的一致性。更新删除等频繁（InnoDB可以有效的降低由于删除和更新导致的锁定），对于数据准确性要求比较高的，此引擎适合。 Memory：通常用于更新不太频繁的小表，用以快速得到访问结果。 Mysql中的锁如果熟悉多线程，那么对锁肯定是有概念的，锁是计算机协调多个进程或线程对某一资源并发访问的机制。 Mysql中的锁分为表锁和行锁： 顾名思义，表锁就是锁住一张表，而行锁就是锁住一行。 表锁的特点：开销小，不会产生死锁，发生锁冲突的概率高，并且并发度低。 行锁的特点：开销大，会产生死锁，发生锁冲突的概率低，并发度高。 因此MyISAM和Memory引擎采用的是表锁，而InnoDB存储引擎采用的是行锁。 MyISAM的锁机制：分为共享读锁和独占写锁。 读锁是：当某一进程对某张表进行读操作时（select），其他线程也可以读，但是不能写。简单的理解就是，我读的时候你不能写。 写锁是：当某一进程对某种表某张表的写时（insert，update，，delete），其他线程不能写也不能读。可以理解为，我写的时候，你不能读，也不能写。 因此MyISAM的读操作和写操作，以及写操作之间是串行的！MyISAM在执行读写操作的时候会自动给表加相应的锁（也就是说不用显示的使用lock table命令），MyISAM总是一次获得SQL语句所需要的全部锁，这也是MyISAM不会出现死锁的原因。 下面分别举关于写锁和读锁的例子： 写锁： 事务1 事务2 取得first_test表的写锁：mysql&gt; lock table first_test write;Query OK, 0 rows affected (0.00 sec) 当前事务对查询、更新和插入操作都可以执行mysql&gt; select * from first_test ;+—-+——+ id mysql&gt; unlock table;Query OK, 0 rows affected (0.00 sec) 等待 mysql&gt; select * from first_test;+—-+——+ 读锁例子如下： 事务1 事务2 获得表first_read的锁定mysql&gt; lock table first_test read;Query OK, 0 rows affected (0.00 sec) 当前事务可以查询该表记录：mysql&gt; select * from first_test;+—-+——+ id 但是当前事务不能查询没有锁定的表：mysql&gt; select * from goods;ERROR 1100 (HY000): Table ‘goods’ was not locked with LOCK TABLES 其他事务可以查询或更新未锁定的表：mysql&gt; select * from goods;+—-+————+——+ 而且插入更新锁定的表都会报错：mysql&gt; insert into first_test(age) values(15);ERROR 1099 (HY000): Table ‘first_test’ was locked with a READ lock and can’t be updatedmysql&gt; update first_test set age=100 where id =1;ERROR 1099 (HY000): Table ‘first_test’ was locked with a READ lock and can’t be updated 当更新被锁定的表时会等待：mysql&gt; update first_test set age=100 where id =1;等待…… mysql&gt; unlock table;Query OK, 0 rows affected (0.00 sec) mysql&gt; update first_test set age=100 where id =1;Query OK, 1 row affected (38.82 sec)Rows matched: 1 Changed: 1 Warnings: 0 并发插入刚说到Mysql在插入和修改的时候都是串行的，但是MyISAM也支持查询和插入的并发操作。 MyISAM中有一个系统变量concurrent_insert（默认为1），用以控制并发插入（用户在表尾插入数据）行为。 当concurrent_insert为0时，不允许并发插入。 当concurrent_insert为1时，如果表中没有空洞（中间没有被删除的行），MyISAM允许一个进程在读表的同时，另一个进程从表尾插入记录。 当concurrent_insert为2时，无论MyISAM表中有没有空洞，都可以在末尾插入记录 事务1 事务2 mysql&gt; lock table first_test read local;Query OK, 0 rows affected (0.00 sec)–加入local选项是说明，在表满足并发插入的前提下，允许在末尾插入数据 当前进程不能进行插入和更新操作mysql&gt; insert into first_test(age) values(15);ERROR 1099 (HY000): Table ‘first_test’ was locked with a READ lock and can’t be updatedmysql&gt; update first_test set age=200 where id =1;ERROR 1099 (HY000): Table ‘first_test’ was locked with a READ lock and can’t be updated 其他进程可以进行插入，但是更新会等待：mysql&gt; insert into first_test(age) values(15);Query OK, 1 row affected (0.00 sec)mysql&gt; update first_test set age=200 where id =2;等待….. 当前进程不能不能访问其他进程插入的数据mysql&gt; select * from first_test;+—-+——+ id 释放锁以后皆大欢喜mysql&gt; unlock table;Query OK, 0 rows affected (0.00 sec) 等待 插入的和更新的都出来的：mysql&gt; select * from first_test;+—-+——+ id 需要注意的： 并发插入是解决对同一表中的查询和插入的锁争用。 如果对有空洞的表进行并发插入会产生碎片，所以在空闲时可以利用optimize table命令回收因删除记录产生的空洞。 锁调度在MyISAM中当一个进程请求某张表的读锁，而另一个进程同时也请求写锁，Mysql会先让后者获得写锁。即使读请求比写请求先到达锁等待队列，写锁也会插入到读锁之前。 因为Mysql总是认为写请求一般比读请求重要，这也就是MyISAM不太适合有大量的读写操作的应用的原因，因为大量的写请求会让查询操作很难获取到读锁，有可能永远阻塞。 处理办法： 1、指定Insert、update、delete语句的low_priority属性，降低其优先级。 2、指定启动参数low-priority-updates，使得MyISAM默认给读请求优先的权利。 3、执行命令set low_priority_updates=1，使该连接发出的请求降低。 4、指定max_write_lock_count设置一个合适的值，当写锁达到这个值后，暂时降低写请求的优先级，让读请求获取锁。 但是上面的处理办法造成的原因就是当遇到复杂的查询语句时，写请求可能很难获取到锁，这是一个很纠结的问题，所以我们一般避免使用复杂的查询语句，如果如法避免，则可以再数据库空闲阶段（深夜）执行。 我们知道mysql在以前，存储引擎默认是MyISAM，但是随着对事务和并发的要求越来越高，便引入了InnoDB引擎，它具有支持事务安全等一系列特性。 InnoDB锁模式InnoDB实现了两种类型的行锁。 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同的数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，但是组织其他事务获得相同数据集的共享锁和排他锁。 可以这么理解： 共享锁就是我读的时候，你可以读，但是不能写。排他锁就是我写的时候，你不能读也不能写。其实就是MyISAM的读锁和写锁，但是针对的对象不同了而已。 除此之外InnoDB还有两个表锁： 意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。 InnoDB行锁模式兼容列表： 注意： 当一个事务请求的锁模式与当前的锁兼容，InnoDB就将请求的锁授予该事务；反之如果请求不兼容，则该事务就等待锁释放。 意向锁是InnoDB自动加的，不需要用户干预。 对于insert、update、delete，InnoDB会自动给涉及的数据加排他锁（X）；对于一般的Select语句，InnoDB不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。 共享锁：select * from table_name where …..lock in share mode 排他锁：select * from table_name where …..for update 加入共享锁的例子： 利用select ….for update加入排他锁 锁的实现方式：InnoDB行锁是通过给索引项加锁实现的，如果没有索引，InnoDB会通过隐藏的聚簇索引来对记录加锁。 也就是说：如果不通过索引条件检索数据，那么InnoDB将对表中所有数据加锁，实际效果跟表锁一样。 行锁分为三种情形： Record lock ：对索引项加锁，即锁定一条记录。 Gap lock：对索引项之间的‘间隙’、对第一条记录前的间隙或最后一条记录后的间隙加锁，即锁定一个范围的记录，不包含记录本身 Next-key Lock：锁定一个范围的记录并包含记录本身（上面两者的结合）。 注意：InnoDB默认级别是repeatable-read级别，所以下面说的都是在RR级别中的。 之前一直搞不懂Gap Lock和Next-key Lock的区别，直到在网上看到一句话豁然开朗，希望对各位有帮助。 Next-Key Lock是行锁与间隙锁的组合，这样，当InnoDB扫描索引记录的时候，会首先对选中的索引记录加上行锁（Record Lock），再对索引记录两边的间隙加上间隙锁（Gap Lock）。如果一个间隙被事务T1加了锁，其它事务是不能在这个间隙插入记录的。 干巴巴的说没意思，我们来看看具体实例： 假设我们有一张表： +—-+——+ | id | age | +—-+——+ | 1 | 3 | | 2 | 6 | | 3 | 9 | +—-+——+ 表结构如下： CREATE TABLE test ( id int(11) NOT NULL AUTO_INCREMENT, age int(11) DEFAULT NULL, PRIMARY KEY (id), KEY keyname (age)) ENGINE=InnoDB AUTO_INCREMENT=302 DEFAULT CHARSET=gbk ; 这样我们age段的索引就分为 (negative infinity, 3], (3,6], (6,9], (9,positive infinity)； 我们来看一下几种情况： 1、当事务A执行以下语句： mysql&gt; select * from fenye where age=6for update ; 不仅使用行锁锁住了相应的数据行，同时也在两边的区间，（5,6]和（6，9] 都加入了gap锁。 这样事务B就无法在这个两个区间insert进新数据,但是事务B可以在两个区间外的区间插入数据。 2、当事务A执行 select * from fenye where age=7 for update ; 那么就会给(6,9]这个区间加锁，别的事务无法在此区间插入或更新数据。 3、如果查询的数据不再范围内， 比如事务A执行 select * from fenye where age=100 for update ; 那么加锁区间就是(9,positive infinity)。 小结： 行锁防止别的事务修改或删除，GAP锁防止别的事务新增，行锁和GAP锁结合形成的的Next-Key锁共同解决了RR级别在写数据时的幻读问题。 何时在InnoDB中使用表锁：InnoDB在绝大部分情况会使用行级锁，因为事务和行锁往往是我们选择InnoDB的原因，但是有些情况我们也考虑使用表级锁。 1、当事务需要更新大部分数据时，表又比较大，如果使用默认的行锁，不仅效率低，而且还容易造成其他事务长时间等待和锁冲突。 2、事务比较复杂，很可能引起死锁导致回滚。 死锁：我们说过MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 在上面的例子中我们可以看到，当两个事务都需要获得对方持有的锁才能够继续完成事务，导致双方都在等待，产生死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 避免死锁：有多种方法可以避免死锁，这里只介绍常见的三种： 1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列6:一文搞懂抽象类和接口，从基础到面试题，揭秘其本质区别！]]></title>
    <url>%2F2019%2F09%2F06%2F6%E6%8A%BD%E8%B1%A1%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 抽象类介绍什么是抽象？ 百度给出的解释是：从具体事物抽出、概括出它们共同的方面、本质属性与关系等，而将个别的、非本质的方面、属性与关系舍弃，这种思维过程，称为抽象。 这句话概括了抽象的概念，而在Java中，你可以只给出方法的定义不去实现方法的具体事物，由子类去根据具体需求来具体实现。 这种只给出方法定义而不具体实现的方法被称为抽象方法，抽象方法是没有方法体的，在代码的表达上就是没有“{}”。 包含一个或多个抽象方法的类也必须被声明为抽象类。 使用abstract修饰符来表示抽象方法以及抽象类。 //有抽象方法的类也必须被声明为abstract public abstract class Test1 { //抽象方法，不能有“{}” public abstract void f(); }抽象类除了包含抽象方法外，还可以包含具体的变量和具体的方法。类即使不包含抽象方法，也可以被声明为抽象类，防止被实例化。 抽象类不能被实例化，也就是不能使用new关键字来得到一个抽象类的实例，抽象方法必须在子类中被实现。 //有抽象方法的类也必须被声明为abstract public class Test1 { public static void main(String[] args) { Teacher teacher=new Teacher(&quot;教师&quot;); teacher.work(); Driver driver=new Driver(&quot;驾驶员&quot;); driver.work(); } } //一个抽象类 abstract class People{ //抽象方法 public abstract void work(); } class Teacher extends People{ private String work; public Teacher(String work) { this.work=work; } @Override public void work() { System.out.println(&quot;我的职业是&quot;+this.work); } } class Driver extends People{ private String work; public Driver(String work) { this.work=work; } @Override public void work() { System.out.println(&quot;我的职业是&quot;+this.work); } } 运行结果：我的职业是教师我的职业是驾驶员几点说明: 抽象类不能直接使用，需要子类去实现抽象类，然后使用其子类的实例。然而可以创建一个变量，其类型也是一个抽象类，并让他指向具体子类的一个实例，也就是可以使用抽象类来充当形参，实际实现类为实参，也就是多态的应用。 People people=new Teacher(&quot;教师&quot;); people.work();不能有抽象构造方法或抽象静态方法。 如果非要使用new关键在来创建一个抽象类的实例的话，可以这样： People people=new People() { @Override public void work() { //实现这个方法的具体功能 } };个人不推荐这种方法，代码读起来有点累。 在下列情况下，一个类将成为抽象类： 当一个类的一个或多个方法是抽象方法时。 当类是一个抽象类的子类，并且不能实现父类的所有抽象方法时。 当一个类实现一个接口，并且不能实现接口的所有抽象方法时。 注意： 上面说的是这些情况下一个类将称为抽象类，没有说抽象类就一定会是这些情况。 抽象类可以不包含抽象方法，包含抽象方法的类就一定是抽象类。 事实上，抽象类可以是一个完全正常实现的类。为什么要用抽象类老是在想为什么要引用抽象类，一般类不就够用了吗。一般类里定义的方法，子类也可以覆盖，没必要定义成抽象的啊。 看了下面的文章，明白了一点。 其实不是说抽象类有什么用，一般类确实也能满足应用，但是现实中确实有些父类中的方法确实没有必要写，因为各个子类中的这个方法肯定会有不同，所以没有必要再父类里写。当然你也可以把抽象类都写成非抽象类，但是这样没有必要。 而写成抽象类，这样别人看到你的代码，或你看到别人的代码，你就会注意抽象方法，而知道这个方法是在子类中实现的，所以，有个提示作用。 一个抽象类小故事下面看一个关于抽象类的小故事 问你个问题，你知道什么是“东西”吗？什么是“物体”吗？ “麻烦你，小王。帮我把那个东西拿过来好吗” 在生活中，你肯定用过这个词－－东西。 小王：“你要让我帮你拿那个水杯吗？” 你要的是水杯类的对象。而东西是水杯的父类。通常东西类没有实例对象，但我们有时需要东西的引用指向它的子类实例。 你看你的房间乱成什么样子了，以后不要把东西乱放了，知道么？ 上面讲的只是子类和父类。而没有说明抽象类的作用。抽象类是据有一个或多个抽象方法的类，必须声明为抽象类。抽象类的特点是，不能创建实例。 这些该死的抽象类，也不知道它有什么屁用。我非要把它改一改不可。把抽象类中的抽象方法都改为空实现。也就是给抽象方法加上一个方法体，不过这个方法体是空的。这回抽象类就没有抽象方法了。它就可以不在抽象了。 当你这么尝试之后，你发现，原来的代码没有任何变化。大家都还是和原来一样，工作的很好。你这回可能更加相信，抽象类根本就没有什么用。但总是不死心，它应该有点用吧，不然创造Java的这伙传说中的天才不成了傻子了吗？ ### 一个抽象类小游戏 接下来，我们来写一个小游戏。俄罗斯方块！我们来分析一下它需要什么类？ 我知道它要在一个矩形的房子里完成。这个房子的上面出现一个方块，慢慢的下落，当它接触到地面或是其它方块的尸体时，它就停止下落了。然后房子的上面又会出现一个新的方块，与前一个方块一样，也会慢慢的下落。在它还没有死亡之前，我可以尽量的移动和翻转它。这样可以使它起到落地时起到一定的作用，如果好的话，还可以减下少几行呢。这看起来好象人生一样，它在为后来人努力着。 当然，我们不是真的要写一个游戏。所以我们简化它。我抽象出两个必须的类，一个是那个房间，或者就它地图也行。另一个是方块。我发现方块有很多种，数一下，共6种。它们都是四个小矩形构成的。但是它们还有很多不同，例如：它们的翻转方法不同。先把这个问题放到一边去，我们回到房子这个类中。 房子上面总是有方块落下来，房子应该有个属性是方块。当一个方块死掉后，再创建一个方块，让它出现在房子的上面。当玩家要翻转方法时，它翻转的到底是哪个方块呢？当然，房子中只有一个方块可以被翻转，就是当前方块。它是房子的一个属性。那这个属性到底是什么类型的呢？方块有很多不同啊，一共有6种之多，我需要写六个类。一个属性不可能有六种类型吧。当然一个属性只能有一种类型。 我们写一个方块类，用它来派生出6个子类。而房子类的当前方块属性的类型是方块类型。它可以指向任何子类。但是，当我调用当前方块的翻转方法时，它的子类都有吗？如果你把翻转方法写到方块类中，它的子类自然也就有了。可以这六种子类的翻转方法是不同的。我们知道'田'方块，它只有一种状态，无论你怎么翻转它。而长条的方块有两种状态。一种是‘－’，另一种是‘｜’。这可怎么办呢？我们知道Java的多态性，你可以让子类来重写父类的方法。也就是说，在父类中定义这个方法，子类在重写这个方法。 那么在父类的这个翻转方法中，我写一些什么代码呢？让它有几种状态呢？因为我们不可能实例化一个方块类的实例，所以它的翻转方法中的代码并不重要。而子类必须去重写它。那么你可以在父类的翻转方法中不写任何代码，也就是空方法。 我们发现，方法类不可能有实例，它的翻转方法的内容可以是任何的代码。而子类必须重写父类的翻转方法。这时，你可以把方块类写成抽象类，而它的抽象方法就是翻转方法。当然，你也可以把方块类写为非抽象的，也可以在方块类的翻转方法中写上几千行的代码。但这样好吗？难道你是微软派来的，非要说Java中的很多东西都是没有用的吗？ 当我看到方块类是抽象的，我会很关心它的抽象方法。我知道它的子类一定会重写它，而且，我会去找到抽象类的引用。它一定会有多态性的体现。 但是，如果你没有这样做，我会认为可能会在某个地方，你会实例化一个方块类的实例，但我找了所有的地方都没有找到。最后我会大骂你一句，你是来欺骗我的吗，你这个白痴。 把那些和“东西”差不多的类写成抽象的。而水杯一样的类就可以不是抽象的了。当然水杯也有几千块钱一个的和几块钱一个的。水杯也有子类，例如，我用的水杯都很高档，大多都是一次性的纸水杯。 记住一点，面向对象不是来自于Java，面向对象就在你的生活中。而Java的面向对象是方便你解决复杂的问题。这不是说面向对象很简单，虽然面向对象很复杂，但Java知道，你很了解面向对象，因为它就在你身边。 接口介绍接口（英文：Interface），在JAVA编程语言中是一个抽象类型，是抽象方法的集合，接口通常以interface来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。 接口并不是类，编写接口的方式和类很相似，但是它们属于不同的概念。类描述对象的属性和方法。接口则包含类要实现的方法。 除非实现接口的类是抽象类，否则该类要定义接口中的所有方法。 接口无法被实例化，但是可以被实现。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。另外，在 Java 中，接口类型可用来声明一个变量，他们可以成为一个空指针，或是被绑定在一个以此接口实现的对象。 接口与类相似点： 一个接口可以有多个方法。 接口文件保存在 .java 结尾的文件中，文件名使用接口名。 接口的字节码文件保存在 .class 结尾的文件中。 接口相应的字节码文件必须在与包名称相匹配的目录结构中。 接口与类的区别： 接口不能用于实例化对象。 接口没有构造方法。 接口中所有的方法必须是抽象方法。 接口不能包含成员变量，除了 static 和 final 变量。 接口不是被类继承了，而是要被类实现。 接口支持多继承。 接口特性 接口中每一个方法也是隐式抽象的,接口中的方法会被隐式的指定为 public abstract（只能是 public abstract，其他修饰符都会报错）。 接口中可以含有变量，但是接口中的变量会被隐式的指定为 public static final 变量（并且只能是 public，用 private 修饰会报编译错误）。 接口中的方法是不能在接口中实现的，只能由实现接口的类来实现接口中的方法。 抽象类和接口的区别 1. 抽象类中的方法可以有方法体，就是能实现方法的具体功能，但是接口中的方法不行。 2. 抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是 public static final 类型的。 3. 接口中不能含有静态代码块以及静态方法(用 static 修饰的方法)，而抽象类是可以有静态代码块和静态方法。 4. 一个类只能继承一个抽象类，而一个类却可以实现多个接口。 注：JDK 1.8 以后，接口里可以有静态方法和方法体了。 接口的使用： 我们来举个例子，定义一个抽象类People，一个普通子类Student，两个接口。子类Student继承父类People,并实现接口Study，Write 代码演示： package demo; //构建一个抽象类People abstract class People{ //父类属性私有化 private String name; private int age; //提供父类的构造器 public People(String name,int age){ this.name = name; this.age = age; } //提供获取和设置属性的getter()/setter()方法 public String getName() { return name; } public int getAge() { return age; } public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } //提供一个抽象方法 public abstract void talk(); } //定义一个接口 interface Study{ //设置课程数量为3 int COURSENUM = 3; //构建一个默认方法 default void stu(){ System.out.println(&quot;学生需要学习&quot;+COURSENUM+&quot;门课程&quot;); } } //再定义一个接口 interface Write{ //定义一个抽象方法 void print(); } //子类继承People,实现接口Study,Write class Student extends People implements Study,Write{ //通过super关键字调用父类的构造器 public Student(String name, int age) { super(name, age); } //实现父类的抽象方法 public void talk() { System.out.println(&quot;我的名字叫&quot;+this.getName()+&quot;,今年&quot;+this.getAge()+&quot;岁&quot;); } //实现Write接口的抽象方法 public void print() { System.out.println(&quot;学生会写作业&quot;); } } public class InterfaceDemo{ public static void main(String[] args) { //构建student对象 Student student = new Student(&quot;dodo&quot;, 22); //调用父类的抽象方法 student.talk(); //调用接口Write中的抽象方法 student.print(); //调用接口Study中的默认方法 student.stu(); } }代码讲解：上述例子结合了抽象类和接口的知识，内容较多，同学们可以多看多敲一下，学习学习。 接口的实现：类名 implements 接口名，有多个接口名，用“，”隔开即可。 接口的作用——制定标准 接口师表尊，所谓的标准，指的是各方共同遵守一个守则，只有操作标准统一了，所有的参与者才可以按照统一的规则操作。 如电脑可以和各个设备连接，提供统一的USB接口，其他设备只能通过USB接口和电脑相连 代码实现： package demo; interface USB { public void work() ; // 拿到USB设备就表示要进行工作 } class Print implements USB //实现类（接口类） { // 打印机实现了USB接口标准（对接口的方法实现） public void work() { System.out.println(&quot;打印机用USB接口，连接,开始工作。&quot;) ; } } class Flash implements USB //实现类（接口类） { // U盘实现了USB接口标准（对接口的方法实现） public void work() { System.out.println(&quot;U盘使用USB接口，连接,开始工作。&quot;) ; } } class Computer { public void plugin(USB usb) //plugin的意思是插件，参数为接收接口类 { usb.work() ; // 按照固定的方式进行工作 } } public class InterfaceStandards { public static void main(String args[]) { Computer computer = new Computer() ; computer.plugin(new Print()) ; //实例化接口类， 在电脑上使用打印机 computer.plugin(new Flash()) ; //实例化接口类， 在电脑上使用U盘 }}代码讲解：上述例子，就给我们展示了接口制定标准的作用，怎么指定的呢？看下面代码 class Computer { public void plugin(USB usb) //plugin的意思是插件，参数为接收接口类 { usb.work() ; // 按照固定的方式进行工作 } }我们可以看到，Computer类里面定义了一个方法plugin()，它的参数内写的是USB usb,即表示plugin()方法里，接收的是一个usb对象，而打印机和U盘对象可以通过向上转型当参数，传入方法里。我们来重新写一个main方法帮助大家理解 代码演示： public class InterfaceStandards { public static void main(String args[]) { Computer computer = new Computer() ; USB usb = new Print(); computer.plugin(usb) ; //实例化接口类， 在电脑上使用打印机 usb = new Flash(); computer.plugin(usb) ; //实例化接口类， 在电脑上使用U盘 } }代码讲解：我们修改了主函数后，发现，使用了两次的向上转型给了USB，虽然使用的都是usb对象，但赋值的子类对象不一样，实现的方法体也不同，这就很像现实生活，无论我使用的是打印机，还是U盘，我都是通过USB接口和电脑连接的，这就是接口的作用之一——制定标准 我们来个图继续帮助大家理解一下： 上面的图：我们学习前面的章节多态可以知道对象的多态可以通过动态绑定来实现，即使用向上转型，我们知道类，数组，接口都是引用类型变量，什么是引用类型变量？ 引用类型变量都会有一个地址的概念，即指向性的概念，当USB usb = new Print(),此时usb对象是指向new Print()的，当usb = new Flash()后，这时候usb变量就会指向new Flash()，我们会说这是子类对象赋值给了父类对象usb，而在内存中，我们应该说，usb指向了new Flash(); 接口最佳实践：设计模式中的工厂模式首先我们来认识一下什么是工厂模式？工厂模式是为了解耦：把对象的创建和使用的过程分开。就是Class A 想调用 Class B ，那么A只是调用B的方法，而至于B的实例化，就交给工厂类。 其次，工厂模式可以降低代码重复。如果创建对象B的过程都很复杂，需要一定的代码量，而且很多地方都要用到，那么就会有很多的重复代码。我们可以这些创建对象B的代码放到工厂里统一管理。既减少了重复代码，也方便以后对B的创建过程的修改维护。 由于创建过程都由工厂统一管理，所以发生业务逻辑变化，不需要找到所有需要创建B的地方去逐个修正，只需要在工厂里修改即可，降低维护成本。同理，想把所有调用B的地方改成B的子类C，只需要在对应生产B的工厂中或者工厂的方法中修改其生产的对象为C即可，而不需要找到所有的new B（）改为newC()。代码演示： package demo; import java.util.Scanner; interface Fruit //定义一个水果标准 { public abstract void eat(); } class Apple implements Fruit { public void eat() { System.out.println(&quot;吃苹果&quot;); } } class Orange implements Fruit { public void eat() { System.out.println(&quot;吃橘子&quot;); } } class factory { public static Fruit getInstance(String className) //返回值是Fruit的子类 { if(&quot;apple&quot;.equals(className)) { return new Apple(); } else if(&quot;orange&quot;.equals(className)) { return new Orange(); } else { return null; } } } public class ComplexFactory { public static void main(String[] args) { System.out.println(&quot;请输入水果的英文名:&quot;); Scanner sc = new Scanner(System.in); String ans = sc.nextLine(); Fruit f = factory.getInstance(ans); //初始化参数 f.eat(); sc.close(); } }代码讲解：上述代码部分我们讲一下factory这个类，类中有一个getInstance方法，我们用了static关键字修饰，在使用的时候我们就在main中使用类名.方法名调用。 Fruit f = factory.getInstance(ans); //初始化参数在Factory的getInstance()方法中，我们就可以通过逻辑的实现，将对象的创建和使用的过程分开了。 总结点评：在接口的学习中，大家可以理解接口是特殊的抽象类，java中类可以实现多个接口，接口中成员属性默认是public static final修饰，可以省略；成员方法默认是public abstract修饰，同样可以省略，接口中还可定义带方法体的默认方法，需要使用default修饰。利用接口我们还可以制定标准，还能够使用工厂模式，将对象的创建和使用过程分开。接口与抽象类的本质区别是什么？基本语法区别在 Java 中，接口和抽象类的定义语法是不一样的。这里以动物类为例来说明，其中定义接口的示意代码如下： public interface Animal { //所有动物都会吃 public void eat(); //所有动物都会飞 public void fly(); } 定义抽象类的示意代码如下： public abstract class Animal { //所有动物都会吃 public abstract void eat(); //所有动物都会飞 public void fly(){}; } 可以看到，在接口内只能是功能的定义，而抽象类中则可以包括功能的定义和功能的实现。在接口中，所有的属性肯定是 public、static 和 final，所有的方法都是 abstract，所以可以默认不写上述标识符；在抽象类中，既可以包含抽象的定义，也可以包含具体的实现方法。 在具体的实现类上，接口和抽象类的实 现类定义方式也是不一样的，其中接口实现类的示意代码如下： public class concreteAnimal implements Animal { //所有动物都会吃 public void eat(){} //所有动物都会飞 public void fly(){} } 抽象类的实现类示意代码如下： public class concreteAnimal extends Animal { //所有动物都会吃 public void eat(){} //所有动物都会飞 public void fly(){} } 可以看到，在接口的实现类中使用 implements 关键字；而在抽象类的实现类中，则使用 extends 关键字。一个接口的实现类可以实现多个接口，而一个抽象类的实现类则只能实现一个抽象类。 设计思想区别从前面抽象类的具体实现类的实现方式可以看出，其实在 Java 中，抽象类和具体实现类之间是一种继承关系，也就是说如果釆用抽象类的方式，则父类和子类在概念上应该是相同的。接口却不一样，如果采用接口的方式，则父类和子类在概念上不要求相同。 接口只是抽取相互之间没有关系的类的共同特征，而不用关注类之间的关系，它可以使没有层次关系的类具有相同的行为。因此，可以这样说：抽象类是对一组具有相同属性和方法的逻辑上有关系的事物的一种抽象，而接口则是对一组具有相同属性和方法的逻辑上不相关的事物的一种抽象。 仍然以前面动物类的设计为例来说明接口和抽象类关于设计思想的区别，该动物类默认所有的动物都具有吃的功能，其中定义接口的示意代码如下： public interface Animal { //所有动物都会吃 public void eat(); } 定义抽象类的示意代码如下： public abstract class Animal { //所有动物都会吃 public abstract void eat(); } 不管是实现接口，还是继承抽象类的具体动物，都具有吃的功能，具体的动物类的示意代码如下。 接口实现类的示意代码如下： public class concreteAnimal implements Animal { //所有动物都会吃 public void eat(){} } 抽象类的实现类示意代码如下： public class concreteAnimal extends Animal { //所有动物都会吃 public void eat(){} } 当然，具体的动物类不光具有吃的功能，比如有些动物还会飞，而有些动物却会游泳，那么该如何设计这个抽象的动物类呢？可以别在接口和抽象类中增加飞的功能，其中定义接口的示意代码如下： public interface Animal { //所有动物都会吃 public void eat(); //所有动物都会飞 public void fly(); } 定义抽象类的示意代码如下： public abstract class Animal { //所有动物都会吃 public abstract void eat(); //所有动物都会飞 public void fly(){}; }这样一来，不管是接口还是抽象类的实现类，都具有飞的功能，这显然不能满足要求，因为只有一部分动物会飞，而会飞的却不一定是动物，比如飞机也会飞。那该如何设计呢？有很多种方案，比如再设计一个动物的接口类，该接口具有飞的功能，示意代码如下： public interface AnimaiFly { //所有动物都会飞 public void fly(); } 那些具体的动物类，如果有飞的功能的话，除了实现吃的接口外，再实现飞的接口，示意代码如下： public class concreteAnimal implements Animal,AnimaiFly { //所有动物都会吃 public void eat(){} //动物会飞 public void fly(); } 那些不需要飞的功能的具体动物类只实现具体吃的功能的接口即可。另外一种解决方案是再设计一个动物的抽象类，该抽象类具有飞的功能，示意代码如下： public abstract class AnimaiFly { //动物会飞 public void fly(); } 但此时没有办法实现那些既有吃的功能，又有飞的功能的具体动物类。因为在 Java 中具体的实现类只能实现一个抽象类。一个折中的解决办法是，让这个具有飞的功能的抽象类，继承具有吃的功能的抽象类，示意代码如下： public abstract class AnimaiFly extends Animal { //动物会飞 public void fly(); } 此时，对那些只需要吃的功能的具体动物类来说，继承 Animal 抽象类即可。对那些既有吃的功能又有飞的功能的具体动物类来说，则需要继承 AnimalFly 抽象类。 但此时对客户端有一个问题，那就是不能针对所有的动物类都使用 Animal 抽象类来进行编程，因为 Animal 抽象类不具有飞的功能，这不符合面向对象的设计原则，因此这种解决方案其实是行不通的。 还有另外一种解决方案，即具有吃的功能的抽象动物类用抽象类来实现，而具有飞的功能的类用接口实现；或者具有吃的功能的抽象动物类用接口来实现，而具有飞的功能的类用抽象类实现。 具有吃的功能的抽象动物类用抽象类来实现，示意代码如下： public abstract class Animal { //所有动物都会吃 public abstract void eat(); } 具有飞的功能的类用接口实现，示意代码如下： public interface AnimaiFly { //动物会飞 public void fly(); } 既具有吃的功能又具有飞的功能的具体的动物类，则继承 Animal 动物抽象类，实现 AnimalFly 接口，示意代码如下： public class concreteAnimal extends Animal implements AnimaiFly { //所有动物都会吃 public void eat(){} //动物会飞 public void fly(); } 或者具有吃的功能的抽象动物类用接口来实现，示意代码如下： public interface Animal { //所有动物都会吃 public abstract void eat(); } 具有飞的功能的类用抽象类实现，示意代码如下： public abstract class AnimaiFly { //动物会飞 public void fly(){}; } 既具有吃的功能又具有飞的功能的具体的动物类，则实现 Animal 动物类接口，继承 AnimaiFly 抽象类，示意代码如下： public class concreteAnimal extends AnimaiFly implements Animal { //所有动物都会吃 public void eat(){} //动物会飞 public void fly(); } 这些解决方案有什么不同呢？再回过头来看接口和抽象类的区别：抽象类是对一组具有相同属性和方法的逻辑上有关系的事物的一种抽象，而接口则是对一组具有相同属性和方法的逻辑上不相关的事物的一种抽象，因此抽象类表示的是“is a”关系，接口表示的是“like a”关系。 假设现在要研究的系统只是动物系统，如果设计人员认为对既具有吃的功能又具有飞的功能的具体的动物类来说，它和只具有吃的功能的动物一样，都是动物，是一组逻辑上有关系的事物，因此这里应该使用抽象类来抽象具有吃的功能的动物类，即继承 Animal 动物抽象类，实现 AnimalFly 接口。 如果设计人员认为对既具有吃的功能，又具有飞的功能的具体的动物类来说，它和只具有飞的功能的动物一样，都是动物，是一组逻辑上有关系的事物，因此这里应该使用抽象类来抽象具有飞的功能的动物类，即实现 Animal 动物类接口，继承 AnimaiFly 抽象类。 假设现在要研究的系统不只是动物系统，如果设计人员认为不管是吃的功能，还是飞的功能和动物类没有什么关系，因为飞机也会飞，人也会吃，则这里应该实现两个接口来分别抽象吃的功能和飞的功能，即除实现吃的 Animal 接口外，再实现飞的 AnimalFly 接口。 从上面的分析可以看出，对于接口和抽象类的选择，反映出设计人员看待问题的不同角度，即抽象类用于一组相关的事物，表示的是“is a”的关系，而接口用于一组不相关的事物，表示的是“like a”的关系。 如何回答面试题：接口和抽象类的区别?接口(interface)和抽象类(abstract class)是支持抽象类定义的两种机制。 接口是公开的，不能有私有的方法或变量，接口中的所有方法都没有方法体，通过关键字interface实现。 抽象类是可以有私有方法或私有变量的，通过把类或者类中的方法声明为abstract来表示一个类是抽象类，被声明为抽象的方法不能包含方法体。子类实现方法必须含有相同的或者更低的访问级别(public-&gt;protected-&gt;private)。抽象类的子类为父类中所有抽象方法的具体实现，否则也是抽象类。 接口可以被看作是抽象类的变体，接口中所有的方法都是抽象的，可以通过接口来间接的实现多重继承。接口中的成员变量都是static final类型，由于抽象类可以包含部分方法的实现，所以，在一些场合下抽象类比接口更有优势。 相同点 （1）都不能被实例化（2）接口的实现类或抽象类的子类都只有实现了接口或抽象类中的方法后才能实例化。 不同点 （1）接口只有定义，不能有方法的实现，java 1.8中可以定义default方法体，而抽象类可以有定义与实现，方法可在抽象类中实现。 （2）实现接口的关键字为implements，继承抽象类的关键字为extends。一个类可以实现多个接口，但一个类只能继承一个抽象类。所以，使用接口可以间接地实现多重继承。 （3）接口强调特定功能的实现，而抽象类强调所属关系。 （4）接口成员变量默认为public static final，必须赋初值，不能被修改；其所有的成员方法都是public、abstract的。抽象类中成员变量默认default，可在子类中被重新定义，也可被重新赋值；抽象方法被abstract修饰，不能被private、static、synchronized和native等修饰，必须以分号结尾，不带花括号。 （5）接口被用于常用的功能，便于日后维护和添加删除，而抽象类更倾向于充当公共类的角色，不适用于日后重新对立面的代码修改。功能需要累积时用抽象类，不需要累积时用接口。 参考文章http://c.biancheng.net/view/1012.htmlhttps://blog.csdn.net/wxw20147854/article/details/88712029https://blog.csdn.net/zhangquan2015/article/details/82808399https://blog.csdn.net/qq_38741971/article/details/80099567https://www.runoob.com/java/java-interfaces.htmlhttps://blog.csdn.net/fengyunjh/article/details/6605085https://blog.csdn.net/xkfanhua/article/details/80567557 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>接口</tag>
        <tag>抽象类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习MySQL数据库6：浅谈MySQL的中事务与锁]]></title>
    <url>%2F2019%2F09%2F06%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0MySQL%E6%95%B0%E6%8D%AE%E5%BA%936%EF%BC%9A%E6%B5%85%E8%B0%88MySQL%E7%9A%84%E4%B8%AD%E4%BA%8B%E5%8A%A1%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 『浅入深出』MySQL 中事务的实现在关系型数据库中，事务的重要性不言而喻，只要对数据库稍有了解的人都知道事务具有 ACID 四个基本属性，而我们不知道的可能就是数据库是如何实现这四个属性的；在这篇文章中，我们将对事务的实现进行分析，尝试理解数据库是如何实现事务的，当然我们也会在文章中简单对 MySQL 中对 ACID 的实现进行简单的介绍。 事务其实就是并发控制的基本单位；相信我们都知道，事务是一个序列操作，其中的操作要么都执行，要么都不执行，它是一个不可分割的工作单位；数据库事务的 ACID 四大特性是事务的基础，了解了 ACID 是如何实现的，我们也就清除了事务的实现，接下来我们将依次介绍数据库是如何实现这四个特性的。 原子性在学习事务时，经常有人会告诉你，事务就是一系列的操作，要么全部都执行，要都不执行，这其实就是对事务原子性的刻画；虽然事务具有原子性，但是原子性并不是只与事务有关系，它的身影在很多地方都会出现。 由于操作并不具有原子性，并且可以再分为多个操作，当这些操作出现错误或抛出异常时，整个操作就可能不会继续执行下去，而已经进行的操作造成的副作用就可能造成数据更新的丢失或者错误。 事务其实和一个操作没有什么太大的区别，它是一系列的数据库操作（可以理解为 SQL）的集合，如果事务不具备原子性，那么就没办法保证同一个事务中的所有操作都被执行或者未被执行了，整个数据库系统就既不可用也不可信。 回滚日志想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，而在 MySQL 中，恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。 这个过程其实非常好理解，为了能够在发生错误时撤销之前的全部操作，肯定是需要将之前的操作都记录下来的，这样在发生错误时才可以回滚。 回滚日志除了能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息，它还能够在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。 回滚日志并不能将数据库物理地恢复到执行语句或者事务之前的样子；它是逻辑日志，当回滚日志被使用时，它只会按照日志逻辑地将数据库中的修改撤销掉看，可以理解为，我们在事务中使用的每一条 INSERT 都对应了一条 DELETE，每一条 UPDATE 也都对应一条相反的 UPDATE 语句。 在这里，我们并不会介绍回滚日志的格式以及它是如何被管理的，本文重点关注在它到底是一个什么样的东西，究竟解决了、如何解决了什么样的问题，如果想要了解具体实现细节的读者，相信网络上关于回滚日志的文章一定不少。 事务的状态因为事务具有原子性，所以从远处看的话，事务就是密不可分的一个整体，事务的状态也只有三种：Active、Commited 和 Failed，事务要不就在执行中，要不然就是成功或者失败的状态： 但是如果放大来看，我们会发现事务不再是原子的，其中包括了很多中间状态，比如部分提交，事务的状态图也变得越来越复杂。 事务的状态图以及状态的描述取自 Database System Concepts 一书中第 14 章的内容。 Active：事务的初始状态，表示事务正在执行； Partially Commited：在最后一条语句执行之后； Failed：发现事务无法正常执行之后； Aborted：事务被回滚并且数据库恢复到了事务进行之前的状态之后； Commited：成功执行整个事务； 虽然在发生错误时，整个数据库的状态可以恢复，但是如果我们在事务中执行了诸如：向标准输出打印日志、向外界发出邮件、没有通过数据库修改了磁盘上的内容甚至在事务执行期间发生了转账汇款，那么这些操作作为可见的外部输出都是没有办法回滚的；这些问题都是由应用开发者解决和负责的，在绝大多数情况下，我们都需要在整个事务提交后，再触发类似的无法回滚的操作 以订票为例，哪怕我们在整个事务结束之后，才向第三方发起请求，由于向第三方请求并获取结果是一个需要较长事件的操作，如果在事务刚刚提交时，数据库或者服务器发生了崩溃，那么我们就非常有可能丢失发起请求这一过程，这就造成了非常严重的问题；而这一点就不是数据库所能保证的，开发者需要在适当的时候查看请求是否被发起、结果是成功还是失败。 并行事务的原子性到目前为止，所有的事务都只是串行执行的，一直都没有考虑过并行执行的问题；然而在实际工作中，并行执行的事务才是常态，然而并行任务下，却可能出现非常复杂的问题： 当 Transaction1 在执行的过程中对 id = 1 的用户进行了读写，但是没有将修改的内容进行提交或者回滚，在这时 Transaction2 对同样的数据进行了读操作并提交了事务；也就是说 Transaction2 是依赖于 Transaction1 的，当 Transaction1 由于一些错误需要回滚时，因为要保证事务的原子性，需要对 Transaction2 进行回滚，但是由于我们已经提交了 Transaction2，所以我们已经没有办法进行回滚操作，在这种问题下我们就发生了问题，Database System Concepts 一书中将这种现象称为不可恢复安排（Nonrecoverable Schedule），那什么情况下是可以恢复的呢？ A recoverable schedule is one where, for each pair of transactions Ti and Tj such that Tj reads a data item previously written by Ti , the commit operation of Ti appears before the commit operation of Tj . 简单理解一下，如果 Transaction2 依赖于事务 Transaction1，那么事务 Transaction1 必须在 Transaction2 提交之前完成提交的操作： 然而这样还不算完，当事务的数量逐渐增多时，整个恢复流程也会变得越来越复杂，如果我们想要从事务发生的错误中恢复，也不是一件那么容易的事情。 在上图所示的一次事件中，Transaction2 依赖于 Transaction1，而 Transaction3 又依赖于 Transaction1，当 Transaction1 由于执行出现问题发生回滚时，为了保证事务的原子性，就会将 Transaction2 和 Transaction3 中的工作全部回滚，这种情况也叫做级联回滚（Cascading Rollback），级联回滚的发生会导致大量的工作需要撤回，是我们难以接受的，不过如果想要达到绝对的原子性，这件事情又是不得不去处理的，我们会在文章的后面具体介绍如何处理并行事务的原子性。 持久性既然是数据库，那么一定对数据的持久存储有着非常强烈的需求，如果数据被写入到数据库中，那么数据一定能够被安全存储在磁盘上；而事务的持久性就体现在，一旦事务被提交，那么数据一定会被写入到数据库中并持久存储起来。 当事务已经被提交之后，就无法再次回滚了，唯一能够撤回已经提交的事务的方式就是创建一个相反的事务对原操作进行『补偿』，这也是事务持久性的体现之一。 重做日志与原子性一样，事务的持久性也是通过日志来实现的，MySQL 使用重做日志（redo log）实现事务的持久性，重做日志由两部分组成，一是内存中的重做日志缓冲区，因为重做日志缓冲区在内存中，所以它是易失的，另一个就是在磁盘上的重做日志文件，它是持久的 当我们在一个事务中尝试对数据进行修改时，它会先将数据从磁盘读入内存，并更新内存中缓存的数据，然后生成一条重做日志并写入重做日志缓存，当事务真正提交时，MySQL 会将重做日志缓存中的内容刷新到重做日志文件，再将内存中的数据更新到磁盘上，图中的第 4、5 步就是在事务提交时执行的。 在 InnoDB 中，重做日志都是以 512 字节的块的形式进行存储的，同时因为块的大小与磁盘扇区大小相同，所以重做日志的写入可以保证原子性，不会由于机器断电导致重做日志仅写入一半并留下脏数据。 除了所有对数据库的修改会产生重做日志，因为回滚日志也是需要持久存储的，它们也会创建对应的重做日志，在发生错误后，数据库重启时会从重做日志中找出未被更新到数据库磁盘中的日志重新执行以满足事务的持久性。 回滚日志和重做日志到现在为止我们了解了 MySQL 中的两种日志，回滚日志（undo log）和重做日志（redo log）；在数据库系统中，事务的原子性和持久性是由事务日志（transaction log）保证的，在实现时也就是上面提到的两种日志，前者用于对事务的影响进行撤销，后者在错误处理时对已经提交的事务进行重做，它们能保证两点： 发生错误或者需要回滚的事务能够成功回滚（原子性）； 在事务提交后，数据没来得及写会磁盘就宕机时，在下次重新启动后能够成功恢复数据（持久性）； 在数据库中，这两种日志经常都是一起工作的，我们可以将它们整体看做一条事务日志，其中包含了事务的 ID、修改的行元素以及修改前后的值。 一条事务日志同时包含了修改前后的值，能够非常简单的进行回滚和重做两种操作，在这里我们也不会对重做和回滚日志展开进行介绍，可能会在之后的文章谈一谈数据库系统的恢复机制时提到两种日志的使用。 隔离性其实作者在之前的文章 『浅入浅出』MySQL 和 InnoDB 就已经介绍过数据库事务的隔离性，不过为了保证文章的独立性和完整性，我们还会对事务的隔离性进行介绍，介绍的内容可能稍微有所不同。 事务的隔离性是数据库处理数据的几大基础之一，如果没有数据库的事务之间没有隔离性，就会发生在 并行事务的原子性 一节中提到的级联回滚等问题，造成性能上的巨大损失。如果所有的事务的执行顺序都是线性的，那么对于事务的管理容易得多，但是允许事务的并行执行却能能够提升吞吐量和资源利用率，并且可以减少每个事务的等待时间。 当多个事务同时并发执行时，事务的隔离性可能就会被违反，虽然单个事务的执行可能没有任何错误，但是从总体来看就会造成数据库的一致性出现问题，而串行虽然能够允许开发者忽略并行造成的影响，能够很好地维护数据库的一致性，但是却会影响事务执行的性能。 事务的隔离级别所以说数据库的隔离性和一致性其实是一个需要开发者去权衡的问题，为数据库提供什么样的隔离性层级也就决定了数据库的性能以及可以达到什么样的一致性；在 SQL 标准中定义了四种数据库的事务的隔离级别：READ UNCOMMITED、READ COMMITED、REPEATABLE READ 和 SERIALIZABLE；每个事务的隔离级别其实都比上一级多解决了一个问题： RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）； READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）； REPEATABLE READ：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）； SERIALIZABLE：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题； 以上的所有的事务隔离级别都不允许脏写入（Dirty Write），也就是当前事务更新了另一个事务已经更新但是还未提交的数据，大部分的数据库中都使用了 READ COMMITED 作为默认的事务隔离级别，但是 MySQL 使用了 REPEATABLE READ 作为默认配置；从 RAED UNCOMMITED 到 SERIALIZABLE，随着事务隔离级别变得越来越严格，数据库对于并发执行事务的性能也逐渐下降。 对于数据库的使用者，从理论上说，并不需要知道事务的隔离级别是如何实现的，我们只需要知道这个隔离级别解决了什么样的问题，但是不同数据库对于不同隔离级别的是实现细节在很多时候都会让我们遇到意料之外的坑。 如果读者不了解脏读、不可重复读和幻读究竟是什么，可以阅读之前的文章 『浅入浅出』MySQL 和 InnoDB，在这里我们仅放一张图来展示各个隔离层级对这几个问题的解决情况。 隔离级别的实现数据库对于隔离级别的实现就是使用并发控制机制对在同一时间执行的事务进行控制，限制不同的事务对于同一资源的访问和更新，而最重要也最常见的并发控制机制，在这里我们将简单介绍三种最重要的并发控制器机制的工作原理。 锁锁是一种最为常见的并发控制机制，在一个事务中，我们并不会将整个数据库都加锁，而是只会锁住那些需要访问的数据项， MySQL 和常见数据库中的锁都分为两种，共享锁（Shared）和互斥锁（Exclusive），前者也叫读锁，后者叫写锁。 读锁保证了读操作可以并发执行，相互不会影响，而写锁保证了在更新数据库数据时不会有其他的事务访问或者更改同一条记录造成不可预知的问题。 时间戳除了锁，另一种实现事务的隔离性的方式就是通过时间戳，使用这种方式实现事务的数据库，例如 PostgreSQL 会为每一条记录保留两个字段；读时间戳中报错了所有访问该记录的事务中的最大时间戳，而记录行的写时间戳中保存了将记录改到当前值的事务的时间戳。 使用时间戳实现事务的隔离性时，往往都会使用乐观锁，先对数据进行修改，在写回时再去判断当前值，也就是时间戳是否改变过，如果没有改变过，就写入，否则，生成一个新的时间戳并再次更新数据，乐观锁其实并不是真正的锁机制，它只是一种思想，在这里并不会对它进行展开介绍。 多版本和快照隔离通过维护多个版本的数据，数据库可以允许事务在数据被其他事务更新时对旧版本的数据进行读取，很多数据库都对这一机制进行了实现；因为所有的读操作不再需要等待写锁的释放，所以能够显著地提升读的性能，MySQL 和 PostgreSQL 都对这一机制进行自己的实现，也就是 MVCC，虽然各自实现的方式有所不同，MySQL 就通过文章中提到的回滚日志实现了 MVCC，保证事务并行执行时能够不等待互斥锁的释放直接获取数据。 隔离性与原子性在这里就需要简单提一下在在原子性一节中遇到的级联回滚等问题了，如果一个事务对数据进行了写入，这时就会获取一个互斥锁，其他的事务就想要获得改行数据的读锁就必须等待写锁的释放，自然就不会发生级联回滚等问题了。 不过在大多数的数据库，比如 MySQL 中都使用了 MVCC 等特性，也就是正常的读方法是不需要获取锁的，在想要对读取的数据进行更新时需要使用 SELECT … FOR UPDATE 尝试获取对应行的互斥锁，以保证不同事务可以正常工作。 一致性作者认为数据库的一致性是一个非常让人迷惑的概念，原因是数据库领域其实包含两个一致性，一个是 ACID 中的一致性、另一个是 CAP 定义中的一致性。 这两个数据库的一致性说的完全不是一个事情，很多很多人都对这两者的概念有非常深的误解，当我们在讨论数据库的一致性时，一定要清楚上下文的语义是什么，尽量明确的问出我们要讨论的到底是 ACID 中的一致性还是 CAP 中的一致性。 ACID数据库对于 ACID 中的一致性的定义是这样的：如果一个事务原子地在一个一致地数据库中独立运行，那么在它执行之后，数据库的状态一定是一致的。对于这个概念，它的第一层意思就是对于数据完整性的约束，包括主键约束、引用约束以及一些约束检查等等，在事务的执行的前后以及过程中不会违背对数据完整性的约束，所有对数据库写入的操作都应该是合法的，并不能产生不合法的数据状态。 A transaction must preserve database consistency - if a transaction is run atomically in isolation starting from a consistent database, the database must again be consistent at the end of the transaction. 我们可以将事务理解成一个函数，它接受一个外界的 SQL 输入和一个一致的数据库，它一定会返回一个一致的数据库。 而第二层意思其实是指逻辑上的对于开发者的要求，我们要在代码中写出正确的事务逻辑，比如银行转账，事务中的逻辑不可能只扣钱或者只加钱，这是应用层面上对于数据库一致性的要求。 Ensuring consistency for an individual transaction is the responsibility of the application programmer who codes the transaction. - Database System Concepts 数据库 ACID 中的一致性对事务的要求不止包含对数据完整性以及合法性的检查，还包含应用层面逻辑的正确。 CAP 定理中的数据一致性，其实是说分布式系统中的各个节点中对于同一数据的拷贝有着相同的值；而 ACID 中的一致性是指数据库的规则，如果 schema 中规定了一个值必须是唯一的，那么一致的系统必须确保在所有的操作中，该值都是唯一的，由此来看 CAP 和 ACID 对于一致性的定义有着根本性的区别。 总结事务的 ACID 四大基本特性是保证数据库能够运行的基石，但是完全保证数据库的 ACID，尤其是隔离性会对性能有比较大影响，在实际的使用中我们也会根据业务的需求对隔离性进行调整，除了隔离性，数据库的原子性和持久性相信都是比较好理解的特性，前者保证数据库的事务要么全部执行、要么全部不执行，后者保证了对数据库的写入都是持久存储的、非易失的，而一致性不仅是数据库对本身数据的完整性的要求，同时也对开发者提出了要求 - 写出逻辑正确并且合理的事务。 最后，也是最重要的，当别人在将一致性的时候，一定要搞清楚他的上下文，如果对文章的内容有疑问，可以在评论中留言。 浅谈数据库并发控制 - 锁和 MVCC转自https://draveness.me/database-concurrency-control 在学习几年编程之后，你会发现所有的问题都没有简单、快捷的解决方案，很多问题都需要权衡和妥协，而本文介绍的就是数据库在并发性能和可串行化之间做的权衡和妥协 - 并发控制机制。 如果数据库中的所有事务都是串行执行的，那么它非常容易成为整个应用的性能瓶颈，虽然说没法水平扩展的节点在最后都会成为瓶颈，但是串行执行事务的数据库会加速这一过程；而并发（Concurrency）使一切事情的发生都有了可能，它能够解决一定的性能问题，但是它会带来更多诡异的错误。 引入了并发事务之后，如果不对事务的执行进行控制就会出现各种各样的问题，你可能没有享受到并发带来的性能提升就已经被各种奇怪的问题折磨的欲仙欲死了。 概述如何控制并发是数据库领域中非常重要的问题之一，不过到今天为止事务并发的控制已经有了很多成熟的解决方案，而这些方案的原理就是这篇文章想要介绍的内容，文章中会介绍最为常见的三种并发控制机制： 分别是悲观并发控制、乐观并发控制和多版本并发控制，其中悲观并发控制其实是最常见的并发控制机制，也就是锁；而乐观并发控制其实也有另一个名字：乐观锁，乐观锁其实并不是一种真实存在的锁，我们会在文章后面的部分中具体介绍；最后就是多版本并发控制（MVCC）了，与前两者对立的命名不同，MVCC 可以与前两者中的任意一种机制结合使用，以提高数据库的读性能。 既然这篇文章介绍了不同的并发控制机制，那么一定会涉及到不同事务的并发，我们会通过示意图的方式分析各种机制是如何工作的。 悲观并发控制控制不同的事务对同一份数据的获取是保证数据库的一致性的最根本方法，如果我们能够让事务在同一时间对同一资源有着独占的能力，那么就可以保证操作同一资源的不同事务不会相互影响。 最简单的、应用最广的方法就是使用锁来解决，当事务需要对资源进行操作时需要先获得资源对应的锁，保证其他事务不会访问该资源后，在对资源进行各种操作；在悲观并发控制中，数据库程序对于数据被修改持悲观的态度，在数据处理的过程中都会被锁定，以此来解决竞争的问题。 读写锁为了最大化数据库事务的并发能力，数据库中的锁被设计为两种模式，分别是共享锁和互斥锁。当一个事务获得共享锁之后，它只可以进行读操作，所以共享锁也叫读锁；而当一个事务获得一行数据的互斥锁时，就可以对该行数据进行读和写操作，所以互斥锁也叫写锁。 共享锁和互斥锁除了限制事务能够执行的读写操作之外，它们之间还有『共享』和『互斥』的关系，也就是多个事务可以同时获得某一行数据的共享锁，但是互斥锁与共享锁和其他的互斥锁并不兼容，我们可以很自然地理解这么设计的原因：多个事务同时写入同一数据难免会发生各种诡异的问题。 如果当前事务没有办法获取该行数据对应的锁时就会陷入等待的状态，直到其他事务将当前数据对应的锁释放才可以获得锁并执行相应的操作。 两阶段锁协议两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。 在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁，如果只看 2PL 的定义，那么到这里就已经介绍完了，但是它还有两个变种： Strict 2PL：事务持有的互斥锁必须在提交后再释放； Rigorous 2PL：事务持有的所有锁必须在提交后释放； 虽然锁的使用能够为我们解决不同事务之间由于并发执行造成的问题，但是两阶段锁的使用却引入了另一个严重的问题，死锁；不同的事务等待对方已经锁定的资源就会造成死锁，我们在这里举一个简单的例子： 两个事务在刚开始时分别获取了 draven 和 beacon 资源面的锁，然后再请求对方已经获得的锁时就会发生死锁，双方都没有办法等到锁的释放，如果没有死锁的处理机制就会无限等待下去，两个事务都没有办法完成。 死锁的处理死锁在多线程编程中是经常遇到的事情，一旦涉及多个线程对资源进行争夺就需要考虑当前的几个线程或者事务是否会造成死锁；解决死锁大体来看有两种办法，一种是从源头杜绝死锁的产生和出现，另一种是允许系统进入死锁的状态，但是在系统出现死锁时能够及时发现并且进行恢复。 预防死锁有两种方式可以帮助我们预防死锁的出现，一种是保证事务之间的等待不会出现环，也就是事务之间的等待图应该是一张有向无环图，没有循环等待的情况或者保证一个事务中想要获得的所有资源都在事务开始时以原子的方式被锁定，所有的资源要么被锁定要么都不被锁定。 但是这种方式有两个问题，在事务一开始时很难判断哪些资源是需要锁定的，同时因为一些很晚才会用到的数据被提前锁定，数据的利用率与事务的并发率也非常的低。一种解决的办法就是按照一定的顺序为所有的数据行加锁，同时与 2PL 协议结合，在加锁阶段保证所有的数据行都是从小到大依次进行加锁的，不过这种方式依然需要事务提前知道将要加锁的数据集。 另一种预防死锁的方法就是使用抢占加事务回滚的方式预防死锁，当事务开始执行时会先获得一个时间戳，数据库程序会根据事务的时间戳决定事务应该等待还是回滚，在这时也有两种机制供我们选择，一种是 wait-die 机制： 当执行事务的时间戳小于另一事务时，即事务 A 先于 B 开始，那么它就会等待另一个事务释放对应资源的锁，否则就会保持当前的时间戳并回滚。 另一种机制叫做 wound-wait，这是一种抢占的解决方案，它和 wait-die 机制的结果完全相反，当前事务如果先于另一事务执行并请求了另一事务的资源，那么另一事务会立刻回滚，将资源让给先执行的事务，否则就会等待其他事务释放资源： 两种方法都会造成不必要的事务回滚，由此会带来一定的性能损失，更简单的解决死锁的方式就是使用超时时间，但是超时时间的设定是需要仔细考虑的，否则会造成耗时较长的事务无法正常执行，或者无法及时发现需要解决的死锁，所以它的使用还是有一定的局限性。 死锁检测和恢复如果数据库程序无法通过协议从原理上保证死锁不会发生，那么就需要在死锁发生时及时检测到并从死锁状态恢复到正常状态保证数据库程序可以正常工作。在使用检测和恢复的方式解决死锁时，数据库程序需要维护数据和事务之间的引用信息，同时也需要提供一个用于判断当前数据库是否进入死锁状态的算法，最后需要在死锁发生时提供合适的策略及时恢复。 在上一节中我们其实提到死锁的检测可以通过一个有向的等待图来进行判断，如果一个事务依赖于另一个事务正在处理的数据，那么当前事务就会等待另一个事务的结束，这也就是整个等待图中的一条边： 如上图所示，如果在这个有向图中出现了环，就说明当前数据库进入了死锁的状态 TransB -&gt; TransE -&gt; TransF -&gt; TransD -&gt; TransB，在这时就需要死锁恢复机制接入了。 如何从死锁中恢复其实非常简单，最常见的解决办法就是选择整个环中一个事务进行回滚，以打破整个等待图中的环，在整个恢复的过程中有三个事情需要考虑： 每次出现死锁时其实都会有多个事务被波及，而选择其中哪一个任务进行回滚是必须要做的事情，在选择牺牲品（Victim）时的黄金原则就是最小化代价，所以我们需要综合考虑事务已经计算的时间、使用的数据行以及涉及的事务等因素；当我们选择了牺牲品之后就可以开始回滚了，回滚其实有两种选择一种是全部回滚，另一种是部分回滚，部分回滚会回滚到事务之前的一个检查点上，如果没有检查点那自然没有办法进行部分回滚。 在死锁恢复的过程中，其实还可能出现某些任务在多次死锁时都被选择成为牺牲品，一直都不会成功执行，造成饥饿（Starvation），我们需要保证事务会在有穷的时间内执行，所以要在选择牺牲品时将时间戳加入考虑的范围。 锁的粒度到目前为止我们都没有对不同粒度的锁进行讨论，一直以来我们都讨论的都是数据行锁，但是在有些时候我们希望将多个节点看做一个数据单元，使用锁直接将这个数据单元、表甚至数据库锁定起来。这个目标的实现需要我们在数据库中定义不同粒度的锁： 当我们拥有了不同粒度的锁之后，如果某个事务想要锁定整个数据库或者整张表时只需要简单的锁住对应的节点就会在当前节点加上显示（explicit）锁，在所有的子节点上加隐式（implicit）锁；虽然这种不同粒度的锁能够解决父节点被加锁时，子节点不能被加锁的问题，但是我们没有办法在子节点被加锁时，立刻确定父节点不能被加锁。 在这时我们就需要引入意向锁来解决这个问题了，当需要给子节点加锁时，先给所有的父节点加对应的意向锁，意向锁之间是完全不会互斥的，只是用来帮助父节点快速判断是否可以对该节点进行加锁： 这里是一张引入了两种意向锁，意向共享锁和意向互斥锁之后所有的锁之间的兼容关系；到这里，我们通过不同粒度的锁和意向锁加快了数据库的吞吐量。 乐观并发控制除了悲观并发控制机制 - 锁之外，我们其实还有其他的并发控制机制，乐观并发控制（Optimistic Concurrency Control）。乐观并发控制也叫乐观锁，但是它并不是真正的锁，很多人都会误以为乐观锁是一种真正的锁，然而它只是一种并发控制的思想。 在这一节中，我们将会先介绍基于时间戳的并发控制机制，然后在这个协议的基础上进行扩展，实现乐观的并发控制机制。 基于时间戳的协议锁协议按照不同事务对同一数据项请求的时间依次执行，因为后面执行的事务想要获取的数据已将被前面的事务加锁，只能等待锁的释放，所以基于锁的协议执行事务的顺序与获得锁的顺序有关。在这里想要介绍的基于时间戳的协议能够在事务执行之前先决定事务的执行顺序。 每一个事务都会具有一个全局唯一的时间戳，它即可以使用系统的时钟时间，也可以使用计数器，只要能够保证所有的时间戳都是唯一并且是随时间递增的就可以。 基于时间戳的协议能够保证事务并行执行的顺序与事务按照时间戳串行执行的效果完全相同；每一个数据项都有两个时间戳，读时间戳和写时间戳，分别代表了当前成功执行对应操作的事务的时间戳。 该协议能够保证所有冲突的读写操作都能按照时间戳的大小串行执行，在执行对应的操作时不需要关注其他的事务只需要关心数据项对应时间戳的值就可以了： 无论是读操作还是写操作都会从左到右依次比较读写时间戳的值，如果小于当前值就会直接被拒绝然后回滚，数据库系统会给回滚的事务添加一个新的时间戳并重新执行这个事务。 基于验证的协议乐观并发控制其实本质上就是基于验证的协议，因为在多数的应用中只读的事务占了绝大多数，事务之间因为写操作造成冲突的可能非常小，也就是说大多数的事务在不需要并发控制机制也能运行的非常好，也可以保证数据库的一致性；而并发控制机制其实向整个数据库系统添加了很多的开销，我们其实可以通过别的策略降低这部分开销。 而验证协议就是我们找到的解决办法，它根据事务的只读或者更新将所有事务的执行分为两到三个阶段： 在读阶段，数据库会执行事务中的全部读操作和写操作，并将所有写后的值存入临时变量中，并不会真正更新数据库中的内容；在这时候会进入下一个阶段，数据库程序会检查当前的改动是否合法，也就是是否有其他事务在 RAED PHASE 期间更新了数据，如果通过测试那么直接就进入 WRITE PHASE 将所有存在临时变量中的改动全部写入数据库，没有通过测试的事务会直接被终止。 为了保证乐观并发控制能够正常运行，我们需要知道一个事务不同阶段的发生时间，包括事务开始时间、验证阶段的开始时间以及写阶段的结束时间；通过这三个时间戳，我们可以保证任意冲突的事务不会同时写入数据库，一旦由一个事务完成了验证阶段就会立即写入，其他读取了相同数据的事务就会回滚重新执行。 作为乐观的并发控制机制，它会假定所有的事务在最终都会通过验证阶段并且执行成功，而锁机制和基于时间戳排序的协议是悲观的，因为它们会在发生冲突时强制事务进行等待或者回滚，哪怕有不需要锁也能够保证事务之间不会冲突的可能。 多版本并发控制到目前为止我们介绍的并发控制机制其实都是通过延迟或者终止相应的事务来解决事务之间的竞争条件（Race condition）来保证事务的可串行化；虽然前面的两种并发控制机制确实能够从根本上解决并发事务的可串行化的问题，但是在实际环境中数据库的事务大都是只读的，读请求是写请求的很多倍，如果写请求和读请求之前没有并发控制机制，那么最坏的情况也是读请求读到了已经写入的数据，这对很多应用完全是可以接受的。 在这种大前提下，数据库系统引入了另一种并发控制机制 - 多版本并发控制（Multiversion Concurrency Control），每一个写操作都会创建一个新版本的数据，读操作会从有限多个版本的数据中挑选一个最合适的结果直接返回；在这时，读写操作之间的冲突就不再需要被关注，而管理和快速挑选数据的版本就成了 MVCC 需要解决的主要问题。 MVCC 并不是一个与乐观和悲观并发控制对立的东西，它能够与两者很好的结合以增加事务的并发量，在目前最流行的 SQL 数据库 MySQL 和 PostgreSQL 中都对 MVCC 进行了实现；但是由于它们分别实现了悲观锁和乐观锁，所以 MVCC 实现的方式也不同。 MySQL 与 MVCCMySQL 中实现的多版本两阶段锁协议（Multiversion 2PL）将 MVCC 和 2PL 的优点结合了起来，每一个版本的数据行都具有一个唯一的时间戳，当有读事务请求时，数据库程序会直接从多个版本的数据项中具有最大时间戳的返回。 更新操作就稍微有些复杂了，事务会先读取最新版本的数据计算出数据更新后的结果，然后创建一个新版本的数据，新数据的时间戳是目前数据行的最大版本 ＋1： 数据版本的删除也是根据时间戳来选择的，MySQL 会将版本最低的数据定时从数据库中清除以保证不会出现大量的遗留内容。 PostgreSQL 与 MVCC与 MySQL 中使用悲观并发控制不同，PostgreSQL 中都是使用乐观并发控制的，这也就导致了 MVCC 在于乐观锁结合时的实现上有一些不同，最终实现的叫做多版本时间戳排序协议（Multiversion Timestamp Ordering），在这个协议中，所有的的事务在执行之前都会被分配一个唯一的时间戳，每一个数据项都有读写两个时间戳： 当 PostgreSQL 的事务发出了一个读请求，数据库直接将最新版本的数据返回，不会被任何操作阻塞，而写操作在执行时，事务的时间戳一定要大或者等于数据行的读时间戳，否则就会被回滚。 这种 MVCC 的实现保证了读事务永远都不会失败并且不需要等待锁的释放，对于读请求远远多于写请求的应用程序，乐观锁加 MVCC 对数据库的性能有着非常大的提升；虽然这种协议能够针对一些实际情况做出一些明显的性能提升，但是也会导致两个问题，一个是每一次读操作都会更新读时间戳造成两次的磁盘写入，第二是事务之间的冲突是通过回滚解决的，所以如果冲突的可能性非常高或者回滚代价巨大，数据库的读写性能还不如使用传统的锁等待方式。 1. MVCC简介与实践 MySQL 在InnoDB引擎下有当前读和快照读两种模式。 1 当前读即加锁读，读取记录的最新版本号，会加锁保证其他并发事物不能修改当前记录，直至释放锁。插入/更新/删除操作默认使用当前读，显示的为select语句加lock in share mode或for update的查询也采用当前读模式。 2 快照读：不加锁，读取记录的快照版本，而非最新版本，使用MVCC机制，最大的好处是读取不需要加锁，读写不冲突，用于读操作多于写操作的应用，因此在不显示加[lock in share mode]/[for update]的select语句，即普通的一条select语句默认都是使用快照读MVCC实现模式。所以楼主的为了让大家明白所做的演示操作，既有当前读也有快照读…… 1.1 什么是MVCC MVCC是一种多版本并发控制机制。 1.2 MVCC是为了解决什么问题? 大多数的MYSQL事务型存储引擎,如,InnoDB，Falcon以及PBXT都不使用一种简单的行锁机制.事实上,他们都和MVCC–多版本并发控制来一起使用. 大家都应该知道,锁机制可以控制并发操作,但是其系统开销较大,而MVCC可以在大多数情况下代替行级锁,使用MVCC,能降低其系统开销. 1.3 MVCC实现 MVCC是通过保存数据在某个时间点的快照来实现的. 不同存储引擎的MVCC. 不同存储引擎的MVCC实现是不同的,典型的有乐观并发控制和悲观并发控制. 2.MVCC 具体实现分析 下面,我们通过InnoDB的MVCC实现来分析MVCC使怎样进行并发控制的. InnoDB的MVCC,是通过在每行记录后面保存两个隐藏的列来实现的,这两个列，分别保存了这个行的创建时间，一个保存的是行的删除时间。这里存储的并不是实际的时间值,而是系统版本号(可以理解为事务的ID)，没开始一个新的事务，系统版本号就会自动递增，事务开始时刻的系统版本号会作为事务的ID.下面看一下在REPEATABLE READ隔离级别下,MVCC具体是如何操作的. 2.1简单的小例子 create table yang( id int primary key auto_increment, name varchar(20)); 假设系统的版本号从1开始. INSERT InnoDB为新插入的每一行保存当前系统版本号作为版本号. 第一个事务ID为1； start transaction; insert into yang values(NULL,'yang') ; insert into yang values(NULL,'long'); insert into yang values(NULL,'fei'); commit; 对应在数据中的表如下(后面两列是隐藏列,我们通过查询语句并看不到) id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 undefined 2 long 1 undefined 3 fei 1 undefined SELECT InnoDB会根据以下两个条件检查每行记录: a.InnoDB只会查找版本早于当前事务版本的数据行(也就是,行的系统版本号小于或等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的. b.行的删除版本要么未定义,要么大于当前事务版本号,这可以确保事务读取到的行，在事务开始之前未被删除. 只有a,b同时满足的记录，才能返回作为查询结果. DELETE InnoDB会为删除的每一行保存当前系统的版本号(事务的ID)作为删除标识. 看下面的具体例子分析: 第二个事务,ID为2; start transaction; select * from yang; //(1) select * from yang; //(2) commit; 假设1 假设在执行这个事务ID为2的过程中,刚执行到(1),这时,有另一个事务ID为3往这个表里插入了一条数据; 第三个事务ID为3; start transaction; insert into yang values(NULL,'tian'); commit; 这时表中的数据如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 undefined 2 long 1 undefined 3 fei 1 undefined 4 tian 3 undefined 然后接着执行事务2中的(2),由于id=4的数据的创建时间(事务ID为3),执行当前事务的ID为2,而InnoDB只会查找事务ID小于等于当前事务ID的数据行,所以id=4的数据行并不会在执行事务2中的(2)被检索出来,在事务2中的两条select 语句检索出来的数据都只会下表: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 undefined 2 long 1 undefined 3 fei 1 undefined 假设2 假设在执行这个事务ID为2的过程中,刚执行到(1),假设事务执行完事务3后，接着又执行了事务4; 第四个事务: start transaction; delete from yang where id=1; commit; 此时数据库中的表如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 4 2 long 1 undefined 3 fei 1 undefined 4 tian 3 undefined 接着执行事务ID为2的事务(2),根据SELECT 检索条件可以知道,它会检索创建时间(创建事务的ID)小于当前事务ID的行和删除时间(删除事务的ID)大于当前事务的行,而id=4的行上面已经说过,而id=1的行由于删除时间(删除事务的ID)大于当前事务的ID,所以事务2的(2)select * from yang也会把id=1的数据检索出来.所以,事务2中的两条select 语句检索出来的数据都如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 4 2 long 1 undefined 3 fei 1 undefined UPDATE InnoDB执行UPDATE，实际上是新插入了一行记录，并保存其创建时间为当前事务的ID，同时保存当前事务ID到要UPDATE的行的删除时间. 假设3 假设在执行完事务2的(1)后又执行,其它用户执行了事务3,4,这时，又有一个用户对这张表执行了UPDATE操作: 第5个事务: start transaction; update yang set name='Long' where id=2; commit; 根据update的更新原则:会生成新的一行,并在原来要修改的列的删除时间列上添加本事务ID,得到表如下: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 4 2 long 1 5 3 fei 1 undefined 4 tian 3 undefined 2 Long 5 undefined 继续执行事务2的(2),根据select 语句的检索条件,得到下表: id name 创建时间(事务ID) 删除时间(事务ID) 1 yang 1 4 2 long 1 5 3 fei 1 undefined 还是和事务2中(1)select 得到相同的结果. 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列5：Java文件和Java包结构]]></title>
    <url>%2F2019%2F09%2F05%2F5Java%E7%B1%BB%E5%92%8C%E5%8C%85%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java中的包概念Java中的包是封装一组类，子包和接口的机制。软件包用于： 防止命名冲突。例如，可以有两个名称分别为Employee的类，college.staff.cse.Employee和college.staff.ee.Employee更轻松地搜索/定位和使用类，接口，枚举和注释 提供受控访问：受保护和默认有包级别访问控制。受保护的成员可以通过同一个包及其子类中的类访问。默认成员（没有任何访问说明符）只能由同一个包中的类访问。 包可以被视为数据封装（或数据隐藏）。 我们所需要做的就是将相关类放入包中。之后，我们可以简单地从现有的软件包中编写一个导入类，并将其用于我们的程序中。一个包是一组相关类的容器，其中一些类可以访问，并且其他类被保存用于内部目的。我们可以在程序中尽可能多地重用包中的现有类。 为了更好地组织类，Java 提供了包机制，用于区别类名的命名空间。 包的作用 1、把功能相似或相关的类或接口组织在同一个包中，方便类的查找和使用。 2、如同文件夹一样，包也采用了树形目录的存储方式。同一个包中的类名字是不同的，不同的包中的类的名字是可以相同的，当同时调用两个不同包中相同类名的类时，应该加上包名加以区别。因此，包可以避免名字冲突。 3、包也限定了访问权限，拥有包访问权限的类才能访问某个包中的类。 Java 使用包（package）这种机制是为了防止命名冲突，访问控制，提供搜索和定位类（class）、接口、枚举（enumerations）和注释（annotation）等。 包语句的语法格式为： package pkg1[．pkg2[．pkg3…]]; 例如,一个Something.java 文件它的内容 package net.java.util; public class Something{ ... }那么它的路径应该是 net/java/util/Something.java 这样保存的。 package(包) 的作用是把不同的 java 程序分类保存，更方便的被其他 java 程序调用。 一个包（package）可以定义为一组相互联系的类型（类、接口、枚举和注释），为这些类型提供访问保护和命名空间管理的功能。 以下是一些 Java 中的包： java.lang-打包基础的类 java.io-包含输入输出功能的函数 开发者可以自己把一组类和接口等打包，并定义自己的包。而且在实际开发中这样做是值得提倡的，当你自己完成类的实现之后，将相关的类分组，可以让其他的编程者更容易地确定哪些类、接口、枚举和注释等是相关的。 由于包创建了新的命名空间（namespace），所以不会跟其他包中的任何名字产生命名冲突。使用包这种机制，更容易实现访问控制，并且让定位相关类更加简单。 package 的目录结构类放在包中会有两种主要的结果： 包名成为类名的一部分，正如我们前面讨论的一样。 包名必须与相应的字节码所在的目录结构相吻合。 下面是管理你自己 java 中文件的一种简单方式： 将类、接口等类型的源码放在一个文本中，这个文件的名字就是这个类型的名字，并以.java作为扩展名。例如： // 文件名 : Car.java package vehicle; public class Car { // 类实现 } 接下来，把源文件放在一个目录中，这个目录要对应类所在包的名字。 ….\vehicle\Car.java 现在，正确的类名和路径将会是如下样子： 类名 -&gt; vehicle.Car 路径名 -&gt; vehicle\Car.java (在 windows 系统中) 通常，一个公司使用它互联网域名的颠倒形式来作为它的包名.例如：互联网域名是 runoob.com，所有的包名都以 com.runoob 开头。包名中的每一个部分对应一个子目录。 例如：有一个 com.runoob.test 的包，这个包包含一个叫做 Runoob.java 的源文件，那么相应的，应该有如下面的一连串子目录： ….\com\runoob\test\Runoob.java 编译的时候，编译器为包中定义的每个类、接口等类型各创建一个不同的输出文件，输出文件的名字就是这个类型的名字，并加上 .class 作为扩展后缀。 例如： // 文件名: Runoob.java package com.runoob.test; public class Runoob { } class Google { } 现在，我们用-d选项来编译这个文件，如下： $javac -d . Runoob.java 这样会像下面这样放置编译了的文件： .\com\runoob\test\Runoob.class .\com\runoob\test\Google.class 你可以像下面这样来导入所有** \com\runoob\test\ **中定义的类、接口等： import com.runoob.test.*; 编译之后的 .class 文件应该和 .java 源文件一样，它们放置的目录应该跟包的名字对应起来。但是，并不要求 .class 文件的路径跟相应的 .java 的路径一样。你可以分开来安排源码和类的目录。 \sources\com\runoob\test\Runoob.java \classes\com\runoob\test\Google.class 这样，你可以将你的类目录分享给其他的编程人员，而不用透露自己的源码。用这种方法管理源码和类文件可以让编译器和java 虚拟机（JVM）可以找到你程序中使用的所有类型。 类目录的绝对路径叫做 class path。设置在系统变量 CLASSPATH 中。编译器和 java 虚拟机通过将 package 名字加到 class path 后来构造 .class 文件的路径。 \classes 是 class path，package 名字是 com.runoob.test,而编译器和 JVM 会在 \classes\com\runoob\test 中找 .class 文件。 一个 class path 可能会包含好几个路径，多路径应该用分隔符分开。默认情况下，编译器和 JVM 查找当前目录。JAR 文件按包含 Java 平台相关的类，所以他们的目录默认放在了 class path 中。 设置 CLASSPATH 系统变量用下面的命令显示当前的CLASSPATH变量： Windows 平台（DOS 命令行下）：C:&gt; set CLASSPATH UNIX 平台（Bourne shell 下）：# echo $CLASSPATH 删除当前CLASSPATH变量内容： Windows 平台（DOS 命令行下）：C:&gt; set CLASSPATH= UNIX 平台（Bourne shell 下）：# unset CLASSPATH; export CLASSPATH 设置CLASSPATH变量: Windows 平台（DOS 命令行下）： C:&gt; set CLASSPATH=C:\users\jack\java\classes UNIX 平台（Bourne shell 下）：# CLASSPATH=/home/jack/java/classes; export CLASSPATH Java包（package）详解java包的作用是为了区别类名的命名空间 1、把功能相似或相关的类或接口组织在同一个包中，方便类的查找和使用。、 2、如同文件夹一样，包也采用了树形目录的存储方式。同一个包中的类名字是不同的，不同的包中的类的名字是可以相同的， 当同时调用两个不同包中相同类名的类时，应该加上包名加以区别。因此，包可以避免名字冲突。 3、包也限定了访问权限，拥有包访问权限的类才能访问某个包中的类。 创建包创建包的时候，你需要为这个包取一个合适的名字。之后，如果其他的一个源文件包含了这个包提供的类、接口、枚举或者注释类型的时候，都必须将这个包的声明放在这个源文件的开头。 包声明应该在源文件的第一行，每个源文件只能有一个包声明，这个文件中的每个类型都应用于它。 如果一个源文件中没有使用包声明，那么其中的类，函数，枚举，注释等将被放在一个无名的包（unnamed package）中。 例子让我们来看一个例子，这个例子创建了一个叫做animals的包。通常使用小写的字母来命名避免与类、接口名字的冲突。 在 animals 包中加入一个接口（interface）： package animals; interface Animal { public void eat(); public void travel(); } 接下来，在同一个包中加入该接口的实现： package animals; /* 文件名 : MammalInt.java */ public class MammalInt implements Animal{ public void eat(){ System.out.println(&quot;Mammal eats&quot;); } public void travel(){ System.out.println(&quot;Mammal travels&quot;); } public int noOfLegs(){ return 0; } public static void main(String args[]){ MammalInt m = new MammalInt(); m.eat(); m.travel(); } }import 关键字为了能够使用某一个包的成员，我们需要在 Java 程序中明确导入该包。使用 “import” 语句可完成此功能。 在 java 源文件中 import 语句应位于 package 语句之后，所有类的定义之前，可以没有，也可以有多条，其语法格式为： 1import package1[.package2…].(classname|*); 如果在一个包中，一个类想要使用本包中的另一个类，那么该包名可以省略。 通常，一个公司使用它互联网域名的颠倒形式来作为它的包名.例如：互联网域名是 runoob.com，所有的包名都以 com.runoob 开头。包名中的每一个部分对应一个子目录。 例如：有一个 com.runoob.test 的包，这个包包含一个叫做 Runoob.java 的源文件，那么相应的，应该有如下面的一连串子目录： 1….\com\runoob\test\Runoob.java 常用jar包java软件包的类型软件包的类型有内置的软件包和用户定义的软件包内置软件包这些软件包由大量的类组成，这些类是Java API的一部分。一些常用的内置软件包有： 1）java.lang：包含语言支持类（例如分类，用于定义基本数据类型，数学运算）。该软件包会自动导入。 2） java.io：包含分类以支持输入/输出操作。 3） java.util：包含实现像链接列表，字典和支持等数据结构的实用类; 用于日期/时间操作。 4） java.applet：包含用于创建Applets的类。 5） java.awt：包含用于实现图形用户界面组件的类（如按钮，菜单等）。 6） java.net：包含支持网络操作的类。 dt.jar SUN对于dt.jar的定义：Also includes dt.jar, the DesignTime archive of BeanInfo files that tell interactive development environments (IDE’s) how to display the Java components and how to let the developer customize them for the application。 中文翻译过来就是：dt.jar是BeanInfo文件的DesignTime归档，BeanInfo文件用来告诉集成开发环境（IDE）如何显示Java组件还有如何让开发人员根据应用程序自定义它们。这段文字中提到了几个关键字：DesignTime,BeanInfo,IDE，Java components。其实dt.jar就是DesignTime Archive的缩写。那么何为DesignTime。 何为DesignTime?翻译过来就是设计时。其实了解JavaBean的人都知道design time和runtime（运行时）这两个术语的含义。设计时（DesignTIme）是指在开发环境中通过添加控件，设置控件或窗体属性等方法，建立应用程序的时间。 与此相对应的运行时（RunTIme）是指可以象用户那样与应用程序交互作用的时间。那么现在再理解一下上面的翻译，其实dt.jar包含了swing控件中的BeanInfo，而IDE的GUI Designer需要这些信息。那让我们看一下dt.jar中到底有什么？下面是一张dt.jar下面的内容截图： 从上面的截图可以看出，dt.jar中全部是Swing组件的BeanInfo。那么到底什么是BeanInfo呢？ 何为BeanInfo?JavaBean和BeanInfo有很大的关系。Sun所制定的JavaBean规范，很大程度上是为IDE准备的——它让IDE能够以可视化的方式设置JavaBean的属性。如果在IDE中开发一个可视化应用程序，我们需要通过属性设置的方式对组成应用的各种组件进行定制，IDE通过属性编辑器让开发人员使用可视化的方式设置组件的属性。 一般的IDE都支持JavaBean规范所定义的属性编辑器，当组件开发商发布一个组件时，它往往将组件对应的属性编辑器捆绑发行，这样开发者就可以在IDE环境下方便地利用属性编辑器对组件进行定制工作。JavaBean规范通过java.beans.PropertyEditor定义了设置JavaBean属性的方法，通过BeanInfo描述了JavaBean哪些属性是可定制的，此外还描述了可定制属性与PropertyEditor的对应关系。 BeanInfo与JavaBean之间的对应关系，通过两者之间规范的命名确立：对应JavaBean的BeanInfo采用如下的命名规范：&lt;Bean&gt;BeanInfo。当JavaBean连同其属性编辑器相同的组件注册到IDE中后，当在开发界面中对JavaBean进行定制时，IDE就会根据JavaBean规范找到对应的BeanInfo，再根据BeanInfo中的描述信息找到JavaBean属性描述（是否开放、使用哪个属性编辑器），进而为JavaBean生成特定开发编辑界面。 dt.jar里面主要是swing组件的BeanInfo。IDE根据这些BeanInfo显示这些组件以及开发人员如何定制他们。rt.jarrt.jar是runtime的归档。Java基础类库，也就是Java doc里面看到的所有的类的class文件。 rt.jar 默认就在Root Classloader的加载路径里面的，而在Claspath配置该变量是不需要的；同时jre/lib目录下的其他jar:jce.jar、jsse.jar、charsets.jar、resources.jar都在Root Classloader中。 *.java文件的奥秘*.Java文件简介.java文件你可以认为只是一个文本文件， 这个文件即是用java语言写成的程序，或者说任务的代码块。 .class文件本质上是一种二进制文件， 它一般是由.java文件通过 javac这个命令（jdk本身提供的工具）生成的一个文件， 而这个文件可以由jvm(java虚拟机)装载（类装载），然后进java解释执行， 这也就是运行你的程序。 你也可以这样比较一下：.java与 .c , .cpp, .asm等等文件，本质 上一样的， 只是用一种 语言来描述你要怎么去完成一件事（一个任务）， 而这种语言 计算机本身 是没有办法知道是什么含义的， 它面向的只是程序员本身， 程序员可以通过 语言本身（语法） 来描述或组织这个任务，这也就 是所谓的编程。 最后你当然是需要计算机按照你的意图来运行你的程序， 这时候就先得有一个翻译（编译， 汇编， 链接等等复杂的过程）把它变成机器可理解的指令（这就是大家说的机器语言，机器语言本身也是一种编程语言，只是程序很难写，很难读懂，基本上没有办法维护）。 这里的.class文件在计算的体系结构中本质上对应的是一种机器语言（而这里的机器叫作JVM），所以JVM本身是可以直接运行这里的.class文件。所以 你可以进一步地认为，.java与.class与其它的编程语法一样，它们都是程序员用来描述自己的任务的一种语言，只是它们面向的对象不一样，而计算机本身只能识别它自已定义的那些指令什么的（再次强调，这里的计算机本身没有那么严格的定义） In short： .java是Java的源文件后缀，里面存放程序员编写的功能代码。 .class文件是字节码文件，由.java源文件通过javac命令编译后生成的文件。是可以运行在任何支持Java虚拟机的硬件平台和操作系统上的二进制文件。 .class文件并不本地的可执行程序。Java虚拟机就是去运行.class文件从而实现程序的运行。 为什么一个java源文件中只能有一个public类？ 在java编程思想（第四版）一书中有这样3段话（6.4 类的访问权限）： 1.每个编译单元（文件）都只能有一个public类，这表示，每个编译单元都有单一的公共接口，用public类来表现。该接口可以按要求包含众多的支持包访问权限的类。如果在某个编译单元内有一个以上的public类，编译器就会给出错误信息。 2.public类的名称必须完全与含有该编译单元的文件名相同，包含大小写。如果不匹配，同样将得到编译错误。 3.虽然不是很常用，但编译单元内完全不带public类也是可能的。在这种情况下，可以随意对文件命名。 总结相关的几个问题： 1、一个”.java”源文件中是否可以包括多个类（不是内部类）？有什么限制？ 答：可以有多个类，但只能有一个public的类，并且public的类名必须与文件名相一致。 2、为什么一个文件中只能有一个public的类 答：编译器在编译时，针对一个java源代码文件（也称为“编译单元”）只会接受一个public类。否则报错。 3、在java文件中是否可以没有public类 答：public类不是必须的，java文件中可以没有public类。 4、为什么这个public的类的类名必须和文件名相同 答： 是为了方便虚拟机在相应的路径中找到相应的类所对应的字节码文件。 Main方法主函数：是一个特殊的函数，作为程序的入口，可以被JVM调用 主函数的定义： public：代表着该函数访问权限是最大的 static：代表主函数随着类的加载就已经存在了 void：主函数没有具体的返回值 main：不是关键字，但是一个特殊的单词，能够被JVM识别 （String[] args）：函数的参数，参数类型是一个数组，该数组中的元素师字符串，字符串数组。main(String[] args) 字符串数组的 此时空数组的长度是0，但也可以在 运行的时候向其中传入参数。 主函数时固定格式的，JVM识别 主函数可以被重载，但是JVM只识别main（String[] args），其他都是作为一般函数。这里面的args知识数组变量可以更改，其他都不能更改。 一个java文件中可以包含很多个类，每个类中有且仅有一个主函数，但是每个java文件中可以包含多个主函数，在运行时，需要指定JVM入口是哪个。例如一个类的主函数可以调用另一个类的主函数。不一定会使用public类的主函数。 外部类的访问权限外部类只能用public和default修饰。 为什么要对外部类或类做修饰呢？ 1.存在包概念：public 和 default 能区分这个外部类能对不同包作一个划分 （default修饰的类，其他包中引入不了这个类，public修饰的类才能被import） 2.protected是包内可见并且子类可见，但是当一个外部类想要继承一个protected修饰的非同包类时，压根找不到这个类，更别提几层了 3.private修饰的外部类，其他任何外部类都无法导入它。 //Java中的文件名要和public修饰的类名相同，否则会报错 //如果没有public修饰的类，则文件可以随意命名 public class Java中的类文件 { } //非公共开类的访问权限默认是包访问权限，不能用private和protected //一个外部类的访问权限只有两种，一种是包内可见，一种是包外可见。 //如果用private修饰，其他类根本无法看到这个类，也就没有意义了。 //如果用protected，虽然也是包内可见，但是如果有子类想要继承该类但是不同包时， //压根找不到这个类，也不可能继承它了，所以干脆用default代替。 class A{ }Java包的命名规则 以 java.* 开头的是Java的核心包，所有程序都会使用这些包中的类； 以 javax.* 开头的是扩展包，x 是 extension 的意思，也就是扩展。虽然 javax.* 是对 java.* 的优化和扩展，但是由于 javax.* 使用的越来越多，很多程序都依赖于 javax.，所以 javax. 也是核心的一部分了，也随JDK一起发布。 以 org.* 开头的是各个机构或组织发布的包，因为这些组织很有影响力，它们的代码质量很高，所以也将它们开发的部分常用的类随JDK一起发布。 在包的命名方面，为了防止重名，有一个惯例：大家都以自己域名的倒写形式作为开头来为自己开发的包命名，例如百度发布的包会以 com.baidu.* 开头，w3c组织发布的包会以 org.w3c.* 开头，微学苑发布的包会以 net.weixueyuan.* 开头…… 组织机构的域名后缀一般为 org，公司的域名后缀一般为 com，可以认为 org.* 开头的包为非盈利组织机构发布的包，它们一般是开源的，可以免费使用在自己的产品中，不用考虑侵权问题，而以 com.* 开头的包往往由盈利性的公司发布，可能会有版权问题，使用时要注意。 参考文章https://www.cnblogs.com/ryanzheng/p/8465701.htmlhttps://blog.csdn.net/fuhanghang/article/details/84102404https://www.runoob.com/java/java-package.htmlhttps://www.breakyizhan.com/java/4260.htmlhttps://blog.csdn.net/qq_36626914/article/details/80627454 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库5：根据MySQL索引原理进行分析与优化]]></title>
    <url>%2F2019%2F09%2F05%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%935%EF%BC%9A%E6%A0%B9%E6%8D%AEMySQL%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 一：Mysql原理与慢查询MySQL凭借着出色的性能、低廉的成本、丰富的资源，已经成为绝大多数互联网公司的首选关系型数据库。虽然性能出色，但所谓“好马配好鞍”，如何能够更好的使用它，已经成为开发工程师的必修课，我们经常会从职位描述上看到诸如“精通MySQL”、“SQL语句优化”、“了解数据库原理”等要求。我们知道一般的应用系统，读写比例在10:1左右，而且插入操作和一般的更新操作很少出现性能问题，遇到最多的，也是最容易出问题的，还是一些复杂的查询操作，所以查询语句的优化显然是重中之重。 本人从13年7月份起，一直在美团核心业务系统部做慢查询的优化工作，共计十余个系统，累计解决和积累了上百个慢查询案例。随着业务的复杂性提升，遇到的问题千奇百怪，五花八门，匪夷所思。本文旨在以开发工程师的角度来解释数据库索引的原理和如何优化慢查询。 一个慢查询引发的思考1select count(*) from task where status=2 and operator_id=20839 and operate_time&gt;1371169729 and operate_time&lt;1371174603 and type=2; 系统使用者反应有一个功能越来越慢，于是工程师找到了上面的SQL。并且兴致冲冲的找到了我，“这个SQL需要优化，给我把每个字段都加上索引”我很惊讶，问道“为什么需要每个字段都加上索引？”“把查询的字段都加上索引会更快”工程师信心满满“这种情况完全可以建一个联合索引，因为是最左前缀匹配，所以operate_time需要放到最后，而且还需要把其他相关的查询都拿来，需要做一个综合评估。”“联合索引？最左前缀匹配？综合评估？”工程师不禁陷入了沉思。多数情况下，我们知道索引能够提高查询效率，但应该如何建立索引？索引的顺序如何？许多人却只知道大概。其实理解这些概念并不难，而且索引的原理远没有想象的那么复杂。 二：索引建立1. 主键索引 primary key() 要求关键字不能重复，也不能为null,同时增加主键约束主键索引定义时，不能命名 2. 唯一索引 unique index() 要求关键字不能重复，同时增加唯一约束 3. 普通索引 index() 对关键字没有要求 4. 全文索引 fulltext key() 关键字的来源不是所有字段的数据，而是字段中提取的特别关键字 关键字：可以是某个字段或多个字段，多个字段称为复合索引 1建表：creat table student( stu_id int unsigned not null auto_increment, name varchar(32) not null default &apos;&apos;, phone char(11) not null default &apos;&apos;, stu_code varchar(32) not null default &apos;&apos;, stu_desc text, primary key (&apos;stu_id&apos;), //主键索引 unique index &apos;stu_code&apos; (&apos;stu_code&apos;), //唯一索引 index &apos;name_phone&apos; (&apos;name&apos;,&apos;phone&apos;), //普通索引，复合索引 fulltext index &apos;stu_desc&apos; (&apos;stu_desc&apos;), //全文索引) engine=myisam charset=utf8; 更新：alert table student add primary key (&apos;stu_id&apos;), //主键索引 add unique index &apos;stu_code&apos; (&apos;stu_code&apos;), //唯一索引 add index &apos;name_phone&apos; (&apos;name&apos;,&apos;phone&apos;), //普通索引，复合索引 add fulltext index &apos;stu_desc&apos; (&apos;stu_desc&apos;); //全文索引 删除：alert table sutdent drop primary key, drop index &apos;stu_code&apos;, drop index &apos;name_phone&apos;, drop index &apos;stu_desc&apos;; 三：浅析explain用法有什么用？在MySQL中，当数据量增长的特别大的时候就需要用到索引来优化SQL语句，而如何才能判断我们辛辛苦苦写出的SQL语句是否优良？这时候explain就派上了用场。 怎么使用？1explain + SQL语句即可 如：explain select * from table; 如下 相信第一次使用explain参数的朋友一定会疑惑这一大堆参数究竟有什么用呢？笔者搜集了一些资料，在这儿做一个总结希望能够帮助大家理解。 参数介绍id1如果是子查询，id的序号会递增，id的值越大优先级越高，越先被执行 select_type查询的类型，主要用于区别普通查询、联合查询、子查询等的复杂查询 SIMPLE:简单的select查询，查询中不包含子查询或者UNION PRIMARY:查询中若包含任何复杂的子部分，最外层查询则被标记为PRIMARY（最后加载的那一个 ） SUBQUERY:在SELECT或WHERE列表中包含了子查询 DERIVED:在FROM列表中包含的子查询被标记为DERIVED（衍生）Mysql会递归执行这些子查询，把结果放在临时表里。 UNION:若第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM字句的查询中，外层SELECT将被标记为:DERIVED UNION RESULT:从UNION表获取结果的SELECT type 显示查询使用了何种类型 从最好到最差依次是System&gt;const&gt;eq_ref&gt;range&gt;index&gt;All（全表扫描） 一般来说至少达到range级别，最好达到ref System:表只有一行记录，这是const类型的特例，平时不会出现(忽略不计)const:表示通过索引一次就找到了,const用于比较primary key或者unique索引，因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL就能将该查询转换为一个常量。 eq_ref:唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键或唯一索引扫描。 ref：非唯一索引扫描，返回匹配某个单独值的行，本质上也是一种索引访问，它返回所有匹配某个单独值的行，然而它可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体range：只检索给定范围的行，使用一个索引来选择行。 key列显示使用了哪个索引，一般就是在你的where语句中出现了between、&lt;、&gt;、in等的查询。这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。index:FULL INDEX SCAN,index与all区别为index类型只遍历索引树。这通常比all快，因为索引文件通常比数据文件小。 extra包含不适合在其他列中显示但十分重要的额外信息 包含的信息： （危险!）Using filesort:说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取，MYSQL中无法利用索引完成的排序操作称为“文件排序” （特别危险!）Using temporary:使用了临时表保存中间结果，MYSQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by Using index:表示相应的select操作中使用了覆盖索引，避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找操作。 possible_keys显示可能应用在这张表中的索引，一个或多个。查询涉及到的字段上若存在索引，则该索引将被列出， 但不一定被查询实际使用 key实际使用的索引，如果为NULL，则没有使用索引。查询中若使用了覆盖索引，则该索引仅出现在key列表中，key参数可以作为使用了索引的判断标准 key_len:表示索引中使用的字节数，可通过该列计算查询中索引的长度，在不损失精确性的情况下，长度越短越好，key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。 ref显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引上的值。 rows根据表统计信息及索引选用情况，大致估算出找到所需记录所需要读取的行数 四：慢查询优化关于MySQL索引原理是比较枯燥的东西，大家只需要有一个感性的认识，并不需要理解得非常透彻和深入。我们回头来看看一开始我们说的慢查询，了解完索引原理之后，大家是不是有什么想法呢？先总结一下索引的几大基本原则 建索引的几大原则1.最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 2.=和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 3.尽量选择区分度高的列作为索引,区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录 4.索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’);5.尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可 回到开始的慢查询根据最左匹配原则，最开始的sql语句的索引应该是status、operator_id、type、operate_time的联合索引；其中status、operator_id、type的顺序可以颠倒，所以我才会说，把这个表的所有相关查询都找到，会综合分析； 比如还有如下查询 1select * from task where status = 0 and type = 12 limit 10; 1select count(*) from task where status = 0 ; 那么索引建立成(status,type,operator_id,operate_time)就是非常正确的，因为可以覆盖到所有情况。这个就是利用了索引的最左匹配的原则 查询优化神器 - explain命令关于explain命令相信大家并不陌生，具体用法和字段含义可以参考官网explain-output，这里需要强调rows是核心指标，绝大部分rows小的语句执行一定很快（有例外，下面会讲到）。所以优化语句基本上都是在优化rows。 慢查询优化基本步骤0.先运行看看是否真的很慢，注意设置SQL_NO_CACHE1.where条件单表查，锁定最小返回记录表。这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起，单表每个字段分别查询，看哪个字段的区分度最高2.explain查看执行计划，是否与1预期一致（从锁定记录较少的表开始查询）3.order by limit 形式的sql语句让排序的表优先查4.了解业务方使用场景5.加索引时参照建索引的几大原则 6.观察结果，不符合预期继续从0分析 五：最左前缀原理与相关优化高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。 这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。 以employees.titles表为例，下面先查看其上都有哪些索引： SHOW INDEX FROM employees.titles; +——–+————+———-+————–+————-+———–+————-+——+————+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type | +——–+————+———-+————–+————-+———–+————-+——+————+ | titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE | | titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE | | titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE | | titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE | +——–+————+———-+————–+————-+———–+————-+——+————+ 从结果中可以到titles表的主索引为&lt;emp_no, title, from_date&gt;，还有一个辅助索引。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉： ALTER TABLE employees.titles DROP INDEX emp_no; 这样就可以专心分析索引PRIMARY的行为了。 情况一：全列匹配。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’ AND title=’Senior Engineer’ AND from_date=’1986-06-26’; +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ | 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | | +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ 很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒： EXPLAIN SELECT * FROM employees.titles WHERE from_date=’1986-06-26’ AND emp_no=’10001’ AND title=’Senior Engineer’; +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ | 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | | +—-+————-+——–+——-+—————+———+———+——————-+——+——-+ 效果是一样的。 情况二：最左前缀匹配。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’; +—-+————-+——–+——+—————+———+———+——-+——+——-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——+—————+———+———+——-+——+——-+ | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | | +—-+————-+——–+——+—————+———+———+——-+——+——-+ 当查询条件精确匹配索引的左边连续一个或几个列时，如或&lt;emp_no, title&gt;，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。 情况三：查询条件用到了索引中列的精确匹配，但是中间某个条件未提供。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’ AND from_date=’1986-06-26’; +—-+————-+——–+——+—————+———+———+——-+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——+—————+———+———+——-+——+————-+ | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where | +—-+————-+——–+——+—————+———+———+——-+——+————-+ 此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。 如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引&lt;emp_no, from_date&gt;，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。 首先我们看下title一共有几种不同的值： SELECT DISTINCT(title) FROM employees.titles; +——————–+ | title | +——————–+ | Senior Engineer | | Staff | | Engineer | | Senior Staff | | Assistant Engineer | | Technique Leader | | Manager | +——————–+ 只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀： EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’ AND title IN (‘Senior Engineer’, ‘Staff’, ‘Engineer’, ‘Senior Staff’, ‘Assistant Engineer’, ‘Technique Leader’, ‘Manager’) AND from_date=’1986-06-26’; +—-+————-+——–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where | +—-+————-+——–+——-+—————+———+———+——+——+————-+ 这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较： SHOW PROFILES; +———-+————+——————————————————————————-+ | Query_ID | Duration | Query | +———-+————+——————————————————————————-+ | 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no=’10001’ AND from_date=’1986-06-26’| | 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no=’10001’ AND title IN … | +———-+————+——————————————————————————-+ “填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。 情况四：查询条件没有指定索引第一列。 EXPLAIN SELECT * FROM employees.titles WHERE from_date=’1986-06-26’; +—-+————-+——–+——+—————+——+———+——+——–+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——+—————+——+———+——+——–+————-+ | 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where | +—-+————-+——–+——+—————+——+———+——+——–+————-+ 由于不是最左前缀，索引这样的查询显然用不到索引。 情况五：匹配某列的前缀字符串。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’ AND title LIKE ‘Senior%’; +—-+————-+——–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where | +—-+————-+——–+——-+—————+———+———+——+——+————-+ 此时可以用到索引，但是如果通配符不是只出现在末尾，则无法使用索引。（原文表述有误，如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀） 情况六：范围查询。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; ‘10010’ and title=’Senior Engineer’; +—-+————-+——–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where | +—-+————-+——–+——-+—————+———+———+——+——+————-+ 范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。 EXPLAIN SELECT * FROM employees.titles WHERE emp_no &lt; ‘10010’ AND title=’Senior Engineer’ AND from_date BETWEEN ‘1986-01-01’ AND ‘1986-12-31’; +—-+————-+——–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where | +—-+————-+——–+——-+—————+———+———+——+——+————-+ 可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询： EXPLAIN SELECT * FROM employees.titles WHERE emp_no BETWEEN ‘10001’ AND ‘10010’ AND title=’Senior Engineer’ AND from_date BETWEEN ‘1986-01-01’ AND ‘1986-12-31’; +—-+————-+——–+——-+—————+———+———+——+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——-+—————+———+———+——+——+————-+ | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where | +—-+————-+——–+——-+—————+———+———+——+——+————-+ 看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。 情况七：查询条件中含有函数或表达式。很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如： EXPLAIN SELECT * FROM employees.titles WHERE emp_no=’10001’ AND left(title, 6)=’Senior’; +—-+————-+——–+——+—————+———+———+——-+——+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——+—————+———+———+——-+——+————-+ | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where | +—-+————-+——–+——+—————+———+———+——-+——+————-+ 虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如： EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1=’10000’; +—-+————-+——–+——+—————+——+———+——+——–+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+——–+——+—————+——+———+——+——–+————-+ | 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where | +—-+————-+——–+——+—————+——+———+——+——–+————-+ 显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。 索引选择性与前缀索引既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。 第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。 另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值： Index Selectivity = Cardinality / #T 显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性： SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles; +————-+ | Selectivity | +————-+ | 0.0000 | +————-+ title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。 有一种与索引选择性有关的索引优化策略叫做前缀索引，就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。 从图12可以看到employees表只有一个索引，那么如果我们想按名字搜索一个人，就只能全表扫描了： EXPLAIN SELECT * FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’; +—-+————-+———–+——+—————+——+———+——+——–+————-+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +—-+————-+———–+——+—————+——+———+——+——–+————-+ | 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where | +—-+————-+———–+——+—————+——+———+——+——–+————-+ 如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建或&lt;first_name, last_name&gt;，看下两个索引的选择性： SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees; +————-+ | Selectivity | +————-+ | 0.0042 | +————-+ SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees; +————-+ | Selectivity | +————-+ | 0.9313 | +————-+ 显然选择性太低，&lt;first_name, last_name&gt;选择性很好，但是first_name和last_name加起来长度为30，有没有兼顾长度和选择性的办法？可以考虑用first_name和last_name的前几个字符建立索引，例如&lt;first_name, left(last_name, 3)&gt;，看看其选择性： SELECT count(DISTINCT(concat(first_name, left(last_name, 3))))/count(*) AS Selectivity FROM employees.employees; +————-+ | Selectivity | +————-+ | 0.7879 | +————-+ 选择性还不错，但离0.9313还是有点距离，那么把last_name前缀加到4： SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees; +————-+ | Selectivity | +————-+ | 0.9007 | +————-+ 这时选择性已经很理想了，而这个索引的长度只有18，比&lt;first_name, last_name&gt;短了接近一半，我们把这个前缀索引 建上： ALTER TABLE employees.employees ADD INDEX first_name_last_name4 (first_name, last_name(4)); 此时再执行一遍按名字查询，比较分析一下与建索引前的结果： SHOW PROFILES; +———-+————+———————————————————————————+ | Query_ID | Duration | Query | +———-+————+———————————————————————————+ | 87 | 0.11941700 | SELECT * FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’ | | 90 | 0.00092400 | SELECT * FROM employees.employees WHERE first_name=’Eric’ AND last_name=’Anido’ | +———-+————+———————————————————————————+ 性能的提升是显著的，查询速度提高了120多倍。 前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身 六：InnoDB的主键选择与插入优化在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 经常看到有帖子或博客讨论主键选择问题，有人建议使用业务无关的自增主键，有人觉得没有必要，完全可以使用如学号或身份证号这种唯一字段作为主键。不论支持哪种论点，大多数论据都是业务层面的。如果从数据库索引优化角度看，使用InnoDB引擎而不使用自增主键绝对是一个糟糕的主意。 上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示： 图13 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置： 图14 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 因此，只要可以，请尽量在InnoDB上采用自增字段做主键。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列4：一文了解final关键字的特性、使用方法，以及实现原理]]></title>
    <url>%2F2019%2F09%2F04%2F4final%E5%85%B3%E9%94%AE%E5%AD%97%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 final关键字在java中使用非常广泛，可以申明成员变量、方法、类、本地变量。一旦将引用声明为final，将无法再改变这个引用。final关键字还能保证内存同步，本博客将会从final关键字的特性到从java内存层面保证同步讲解。这个内容在面试中也有可能会出现。 final使用final变量final变量有成员变量或者是本地变量(方法内的局部变量)，在类成员中final经常和static一起使用，作为类常量使用。其中类常量必须在声明时初始化，final成员常量可以在构造函数初始化。 12345678public class Main &#123; public static final int i; //报错，必须初始化 因为常量在常量池中就存在了，调用时不需要类的初始化，所以必须在声明时初始化 public static final int j; Main() &#123; i = 2; j = 3; &#125;&#125; 就如上所说的，对于类常量，JVM会缓存在常量池中，在读取该变量时不会加载这个类。 12345678910public class Main &#123; public static final int i = 2; Main() &#123; System.out.println(&quot;调用构造函数&quot;); // 该方法不会调用 &#125; public static void main(String[] args) &#123; System.out.println(Main.i); &#125;&#125; final修饰基本数据类型变量和引用@Test public void final修饰基本类型变量和引用() { final int a = 1; final int[] b = {1}; final int[] c = {1}; // b = c;报错 b[0] = 1; final String aa = &quot;a&quot;; final Fi f = new Fi(); //aa = &quot;b&quot;;报错 // f = null;//报错 f.a = 1; }final方法表示该方法不能被子类的方法重写，将方法声明为final，在编译的时候就已经静态绑定了，不需要在运行时动态绑定。final方法调用时使用的是invokespecial指令。 12345678910111213141516class PersonalLoan&#123; public final String getName()&#123; return&quot;personal loan”; &#125;&#125;class CheapPersonalLoan extends PersonalLoan&#123; @Override public final String getName()&#123; return&quot;cheap personal loan&quot;;//编译错误，无法被重载 &#125; public String test() &#123; return getName(); //可以调用，因为是public方法 &#125;&#125; final类final类不能被继承，final类中的方法默认也会是final类型的，java中的String类和Integer类都是final类型的。 class Si{ //一般情况下final修饰的变量一定要被初始化。 //只有下面这种情况例外，要求该变量必须在构造方法中被初始化。 //并且不能有空参数的构造方法。 //这样就可以让每个实例都有一个不同的变量，并且这个变量在每个实例中只会被初始化一次 //于是这个变量在单个实例里就是常量了。 final int s ; Si(int s) { this.s = s; } } class Bi { final int a = 1; final void go() { //final修饰方法无法被继承 } } class Ci extends Bi { final int a = 1; // void go() { // //final修饰方法无法被继承 // } } final char[]a = {&apos;a&apos;}; final int[]b = {1};1234final class PersonalLoan&#123;&#125;class CheapPersonalLoan extends PersonalLoan &#123; //编译错误，无法被继承 &#125; @Test public void final修饰类() { //引用没有被final修饰，所以是可变的。 //final只修饰了Fi类型，即Fi实例化的对象在堆中内存地址是不可变的。 //虽然内存地址不可变，但是可以对内部的数据做改变。 Fi f = new Fi(); f.a = 1; System.out.println(f); f.a = 2; System.out.println(f); //改变实例中的值并不改变内存地址。 Fi ff = f; //让引用指向新的Fi对象，原来的f对象由新的引用ff持有。 //引用的指向改变也不会改变原来对象的地址 f = new Fi(); System.out.println(f); System.out.println(ff); }final关键字的知识点 final成员变量必须在声明的时候初始化或者在构造器中初始化，否则就会报编译错误。final变量一旦被初始化后不能再次赋值。 本地变量必须在声明时赋值。 因为没有初始化的过程 在匿名类中所有变量都必须是final变量。 final方法不能被重写, final类不能被继承 接口中声明的所有变量本身是final的。类似于匿名类 final和abstract这两个关键字是反相关的，final类就不可能是abstract的。 final方法在编译阶段绑定，称为静态绑定(static binding)。 将类、方法、变量声明为final能够提高性能，这样JVM就有机会进行估计，然后优化。 final方法的好处: 提高了性能，JVM在常量池中会缓存final变量 final变量在多线程中并发安全，无需额外的同步开销 final方法是静态编译的，提高了调用速度 final类创建的对象是只可读的，在多线程可以安全共享 final关键字的最佳实践 final的用法1、final 对于常量来说，意味着值不能改变，例如 final int i=100。这个i的值永远都是100。但是对于变量来说又不一样，只是标识这个引用不可被改变，例如 final File f=new File(&quot;c:\test.txt&quot;); 那么这个f一定是不能被改变的，如果f本身有方法修改其中的成员变量，例如是否可读，是允许修改的。有个形象的比喻：一个女子定义了一个final的老公，这个老公的职业和收入都是允许改变的，只是这个女人不会换老公而已。 关于空白finalfinal修饰的变量有三种：静态变量、实例变量和局部变量，分别表示三种类型的常量。 另外，final变量定义的时候，可以先声明，而不给初值，这中变量也称为final空白，无论什么情况，编译器都确保空白final在使用之前必须被初始化。 但是，final空白在final关键字final的使用上提供了更大的灵活性，为此，一个类中的final数据成员就可以实现依对象而有所不同，却有保持其恒定不变的特征。 public class FinalTest { final int p; final int q=3; FinalTest(){ p=1; } FinalTest(int i){ p=i;//可以赋值，相当于直接定义p q=i;//不能为一个final变量赋值 } } final内存分配刚提到了内嵌机制，现在详细展开。要知道调用一个函数除了函数本身的执行时间之外，还需要额外的时间去寻找这个函数（类内部有一个函数签名和函数地址的映射表）。所以减少函数调用次数就等于降低了性能消耗。 final修饰的函数会被编译器优化，优化的结果是减少了函数调用的次数。如何实现的，举个例子给你看： public class Test{ final void func(){System.out.println(&quot;g&quot;);}; public void main(String[] args){ for(int j=0;j&lt;1000;j++) func(); }} 经过编译器优化之后，这个类变成了相当于这样写： public class Test{ final void func(){System.out.println(&quot;g&quot;);}; public void main(String[] args){ for(int j=0;j&lt;1000;j++) {System.out.println(&quot;g&quot;);} }} 看出来区别了吧？编译器直接将func的函数体内嵌到了调用函数的地方，这样的结果是节省了1000次函数调用，当然编译器处理成字节码，只是我们可以想象成这样，看个明白。 不过，当函数体太长的话，用final可能适得其反，因为经过编译器内嵌之后代码长度大大增加，于是就增加了jvm解释字节码的时间。 在使用final修饰方法的时候，编译器会将被final修饰过的方法插入到调用者代码处，提高运行速度和效率，但被final修饰的方法体不能过大，编译器可能会放弃内联，但究竟多大的方法会放弃，我还没有做测试来计算过。 下面这些内容是通过两个疑问来继续阐述的 使用final修饰方法会提高速度和效率吗见下面的测试代码，我会执行五次： public class Test { public static void getJava() { String str1 = &quot;Java &quot;; String str2 = &quot;final &quot;; for (int i = 0; i &lt; 10000; i++) { str1 += str2; } } public static final void getJava_Final() { String str1 = &quot;Java &quot;; String str2 = &quot;final &quot;; for (int i = 0; i &lt; 10000; i++) { str1 += str2; } } public static void main(String[] args) { long start = System.currentTimeMillis(); getJava(); System.out.println(&quot;调用不带final修饰的方法执行时间为:&quot; + (System.currentTimeMillis() - start) + &quot;毫秒时间&quot;); start = System.currentTimeMillis(); String str1 = &quot;Java &quot;; String str2 = &quot;final &quot;; for (int i = 0; i &lt; 10000; i++) { str1 += str2; } System.out.println(&quot;正常的执行时间为:&quot; + (System.currentTimeMillis() - start) + &quot;毫秒时间&quot;); start = System.currentTimeMillis(); getJava_Final(); System.out.println(&quot;调用final修饰的方法执行时间为:&quot; + (System.currentTimeMillis() - start) + &quot;毫秒时间&quot;); } } 结果为： 第一次： 调用不带final修饰的方法执行时间为:1732毫秒时间 正常的执行时间为:1498毫秒时间 调用final修饰的方法执行时间为:1593毫秒时间 第二次： 调用不带final修饰的方法执行时间为:1217毫秒时间 正常的执行时间为:1031毫秒时间 调用final修饰的方法执行时间为:1124毫秒时间 第三次： 调用不带final修饰的方法执行时间为:1154毫秒时间 正常的执行时间为:1140毫秒时间 调用final修饰的方法执行时间为:1202毫秒时间 第四次： 调用不带final修饰的方法执行时间为:1139毫秒时间 正常的执行时间为:999毫秒时间 调用final修饰的方法执行时间为:1092毫秒时间 第五次： 调用不带final修饰的方法执行时间为:1186毫秒时间 正常的执行时间为:1030毫秒时间 调用final修饰的方法执行时间为:1109毫秒时间 由以上运行结果不难看出，执行最快的是“正常的执行”即代码直接编写，而使用final修饰的方法，不像有些书上或者文章上所说的那样，速度与效率与“正常的执行”无异，而是位于第二位，最差的是调用不加final修饰的方法。 观点：加了比不加好一点。 使用final修饰变量会让变量的值不能被改变吗；见代码： public class Final { public static void main(String[] args) { Color.color[3] = &quot;white&quot;; for (String color : Color.color) System.out.print(color+&quot; &quot;); } } class Color { public static final String[] color = { &quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;black&quot; }; } 执行结果： red blue yellow white 看！，黑色变成了白色。 ​​ 在使用findbugs插件时，就会提示public static String[] color = { “red”, “blue”, “yellow”, “black” };这行代码不安全，但加上final修饰，这行代码仍然是不安全的，因为final没有做到保证变量的值不会被修改！​ 原因是：final关键字只能保证变量本身不能被赋与新值，而不能保证变量的内部结构不被修改。例如在main方法有如下代码Color.color = new String[]{“”};就会报错了。 如何保证数组内部不被修改那可能有的同学就会问了，加上final关键字不能保证数组不会被外部修改，那有什么方法能够保证呢？答案就是降低访问级别，把数组设为private。这样的话，就解决了数组在外部被修改的不安全性，但也产生了另一个问题，那就是这个数组要被外部使用的。 解决这个问题见代码： import java.util.AbstractList; import java.util.List; public class Final { public static void main(String[] args) { for (String color : Color.color) System.out.print(color + &quot; &quot;); Color.color.set(3, &quot;white&quot;); } } class Color { private static String[] _color = { &quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;black&quot; }; public static List&lt;String&gt; color = new AbstractList&lt;String&gt;() { @Override public String get(int index) { return _color[index]; } @Override public String set(int index, String value) { throw new RuntimeException(&quot;为了代码安全,不能修改数组&quot;); } @Override public int size() { return _color.length; } }; }这样就OK了，既保证了代码安全，又能让数组中的元素被访问了。 final方法的三条规则规则1：final修饰的方法不可以被重写。 规则2：final修饰的方法仅仅是不能重写，但它完全可以被重载。 规则3：父类中private final方法，子类可以重新定义，这种情况不是重写。 代码示例 规则1代码 public class FinalMethodTest { public final void test(){} } class Sub extends FinalMethodTest { // 下面方法定义将出现编译错误，不能重写final方法 public void test(){} } 规则2代码 public class Finaloverload { //final 修饰的方法只是不能重写，完全可以重载 public final void test(){} public final void test(String arg){} } 规则3代码 public class PrivateFinalMethodTest { private final void test(){} } class Sub extends PrivateFinalMethodTest { // 下面方法定义将不会出现问题 public void test(){} }final 和 jvm的关系与前面介绍的锁和 volatile 相比较，对 final 域的读和写更像是普通的变量访问。对于 final 域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 下面，我们通过一些示例性的代码来分别说明这两个规则： public class FinalExample { int i; // 普通变量 final int j; //final 变量 static FinalExample obj; public void FinalExample () { // 构造函数 i = 1; // 写普通域 j = 2; // 写 final 域 } public static void writer () { // 写线程 A 执行 obj = new FinalExample (); } public static void reader () { // 读线程 B 执行 FinalExample object = obj; // 读对象引用 int a = object.i; // 读普通域 int b = object.j; // 读 final 域 } } 这里假设一个线程 A 执行 writer () 方法，随后另一个线程 B 执行 reader () 方法。下面我们通过这两个线程的交互来说明这两个规则。 写 final 域的重排序规则写 final 域的重排序规则禁止把 final 域的写重排序到构造函数之外。这个规则的实现包含下面 2 个方面： JMM 禁止编译器把 final 域的写重排序到构造函数之外。 编译器会在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障禁止处理器把 final 域的写重排序到构造函数之外。 现在让我们分析 writer () 方法。writer () 方法只包含一行代码：finalExample = new FinalExample ()。这行代码包含两个步骤： 构造一个 FinalExample 类型的对象； 把这个对象的引用赋值给引用变量 obj。 假设线程 B 读对象引用与读对象的成员域之间没有重排序（马上会说明为什么需要这个假设），下图是一种可能的执行时序： 在上图中，写普通域的操作被编译器重排序到了构造函数之外，读线程 B 错误的读取了普通变量 i 初始化之前的值。而写 final 域的操作，被写 final 域的重排序规则“限定”在了构造函数之内，读线程 B 正确的读取了 final 变量初始化之后的值。 写 final 域的重排序规则可以确保：在对象引用为任意线程可见之前，对象的 final 域已经被正确初始化过了，而普通域不具有这个保障。以上图为例，在读线程 B“看到”对象引用 obj 时，很可能 obj 对象还没有构造完成（对普通域 i 的写操作被重排序到构造函数外，此时初始值 2 还没有写入普通域 i）。 读 final 域的重排序规则读 final 域的重排序规则如下： 在一个线程中，初次读对象引用与初次读该对象包含的 final 域，JMM 禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读 final 域操作的前面插入一个 LoadLoad 屏障。 初次读对象引用与初次读该对象包含的 final 域，这两个操作之间存在间接依赖关系。由于编译器遵守间接依赖关系，因此编译器不会重排序这两个操作。大多数处理器也会遵守间接依赖，大多数处理器也不会重排序这两个操作。但有少数处理器允许对存在间接依赖关系的操作做重排序（比如 alpha 处理器），这个规则就是专门用来针对这种处理器。 reader() 方法包含三个操作： 初次读引用变量 obj; 初次读引用变量 obj 指向对象的普通域 j。 初次读引用变量 obj 指向对象的 final 域 i。 现在我们假设写线程 A 没有发生任何重排序，同时程序在不遵守间接依赖的处理器上执行，下面是一种可能的执行时序： 在上图中，读对象的普通域的操作被处理器重排序到读对象引用之前。读普通域时，该域还没有被写线程 A 写入，这是一个错误的读取操作。而读 final 域的重排序规则会把读对象 final 域的操作“限定”在读对象引用之后，此时该 final 域已经被 A 线程初始化过了，这是一个正确的读取操作。 读 final 域的重排序规则可以确保：在读一个对象的 final 域之前，一定会先读包含这个 final 域的对象的引用。在这个示例程序中，如果该引用不为 null，那么引用对象的 final 域一定已经被 A 线程初始化过了。 如果 final 域是引用类型上面我们看到的 final 域是基础数据类型，下面让我们看看如果 final 域是引用类型，将会有什么效果？ 请看下列示例代码： public class FinalReferenceExample { final int[] intArray; //final 是引用类型 static FinalReferenceExample obj; public FinalReferenceExample () { // 构造函数 intArray = new int[1]; //1 intArray[0] = 1; //2 } public static void writerOne () { // 写线程 A 执行 obj = new FinalReferenceExample (); //3 } public static void writerTwo () { // 写线程 B 执行 obj.intArray[0] = 2; //4 } public static void reader () { // 读线程 C 执行 if (obj != null) { //5 int temp1 = obj.intArray[0]; //6 } } } 这里 final 域为一个引用类型，它引用一个 int 型的数组对象。对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 对上面的示例程序，我们假设首先线程 A 执行 writerOne() 方法，执行完后线程 B 执行 writerTwo() 方法，执行完后线程 C 执行 reader () 方法。下面是一种可能的线程执行时序： 在上图中，1 是对 final 域的写入，2 是对这个 final 域引用的对象的成员域的写入，3 是把被构造的对象的引用赋值给某个引用变量。这里除了前面提到的 1 不能和 3 重排序外，2 和 3 也不能重排序。 JMM 可以确保读线程 C 至少能看到写线程 A 在构造函数中对 final 引用对象的成员域的写入。即 C 至少能看到数组下标 0 的值为 1。而写线程 B 对数组元素的写入，读线程 C 可能看的到，也可能看不到。JMM 不保证线程 B 的写入对读线程 C 可见，因为写线程 B 和读线程 C 之间存在数据竞争，此时的执行结果不可预知。 如果想要确保读线程 C 看到写线程 B 对数组元素的写入，写线程 B 和读线程 C 之间需要使用同步原语（lock 或 volatile）来确保内存可见性。 参考文章https://www.infoq.cn/article/java-memory-model-6https://www.jianshu.com/p/067b6c89875ahttps://www.jianshu.com/p/f68d6ef2dcf0https://www.cnblogs.com/xiaoxi/p/6392154.htmlhttps://www.iteye.com/blog/cakin24-2334965https://blog.csdn.net/chengqiuming/article/details/70139503https://blog.csdn.net/hupuxiang/article/details/7362267 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>final</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库4：Mysql索引实现原理和相关数据结构算法]]></title>
    <url>%2F2019%2F09%2F04%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%934%EF%BC%9AMysql%E7%B4%A2%E5%BC%95%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%92%8C%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 MySQL索引一、简介MySQL目前主要有以下几种索引类型：1.普通索引2.唯一索引3.主键索引4.组合索引5.全文索引 二、语句CREATE TABLE table_name[col_name data type] [unique|fulltext][index|key][index_name](col_name[length])[asc|desc] 1.unique|fulltext为可选参数，分别表示唯一索引、全文索引2.index和key为同义词，两者作用相同，用来指定创建索引3.col_name为需要创建索引的字段列，该列必须从数据表中该定义的多个列中选择4.index_name指定索引的名称，为可选参数，如果不指定，默认col_name为索引值5.length为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度6.asc或desc指定升序或降序的索引值存储 三、索引类型1.普通索引是最基本的索引，它没有任何限制。它有以下几种创建方式：（1）直接创建索引 CREATE INDEX index_name ON table(column(length)) （2）修改表结构的方式添加索引 ALTER TABLE table_name ADD INDEX index_name ON (column(length)) （3）创建表的时候同时创建索引 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length)) ) （4）删除索引 DROP INDEX index_name ON table 2.唯一索引与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：（1）创建唯一索引 CREATE UNIQUE INDEX indexName ON table(column(length)) （2）修改表结构 ALTER TABLE table_name ADD UNIQUE indexName ON (column(length)) （3）创建表的时候直接指定 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , UNIQUE indexName (title(length)) ); 3.主键索引是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引： CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) NOT NULL , PRIMARY KEY (`id`) ); 4.组合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合 ALTER TABLE `table` ADD INDEX name_city_age (name,city,age); 5.全文索引主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。 它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。（1）创建表的适合添加全文索引 CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), FULLTEXT (content) ); （2）修改表结构添加全文索引 ALTER TABLE article ADD FULLTEXT index_content(content) （3）直接创建索引 CREATE FULLTEXT INDEX index_content ON article(content) 四、缺点1.虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行insert、update和delete。因为更新表时，不仅要保存数据，还要保存一下索引文件。 2.建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会增长很快。索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。 五、注意事项使用索引时，有以下一些技巧和注意事项： 1.索引不会包含有null值的列只要列中包含有null值都将不会被包含在索引中，复合索引中只要有一列含有null值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为null。 2.使用短索引对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个char(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 3.索引列排序查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 4.like语句操作一般情况下不推荐使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。5.不要在列上进行运算这将导致索引失效而进行全表扫描，例如 SELECT * FROM table_name WHERE YEAR(column_name) key) return BTree_Search(point[i]->node); 7. } 8. return BTree_Search(point[i+1]->node); 9. } 10. data = BTree_Search(root, my_key); 关于B-Tree有一系列有趣的性质，例如一个度为d的B-Tree，设其索引N个key，从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 另外，由于插入删除新的数据记录会破坏B-Tree的性质，因此在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质，本文不打算完整讨论B-Tree这些内容，因为已经有许多资料详细说明了B-Tree的数学性质及插入删除算法，有兴趣的朋友可以在本文末的参考文献一栏找到相应的资料进行阅读。 B+TreeB-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 与B-Tree相比，B+Tree有以下不同点： 每个节点的指针上限为2d而不是2d+1。 内节点不存储data，只存储key；叶子节点不存储指针。 图3是一个简单的B+Tree示意。 图3 由于并不是所有节点都具有相同的域，因此B+Tree中叶节点和内节点一般大小不同。这点与B-Tree不同，虽然B-Tree中不同节点存放的key和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中B-Tree往往对每个节点申请同等大小的空间。 一般来说，B+Tree比B-Tree更适合实现外存储索引结构，具体原因与外存储器原理及计算机存取原理有关，将在下面讨论。 带有顺序访问指针的B+Tree一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 图4 如图4所示，在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。做这个优化的目的是为了提高区间访问的性能，例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 这一节对B-Tree和B+Tree进行了一个简单的介绍，下一节结合存储器存取原理介绍为什么目前B+Tree是数据库系统实现索引的首选数据结构。 为什么使用B-Tree（B+Tree）上文说过，红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构，这一节将结合计算机组成原理相关知识讨论B-/+Tree作为索引的理论基础。 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。下面先介绍内存和磁盘存取原理，然后再结合这些原理分析B-/+Tree作为索引的效率。 主存存取原理目前计算机使用的主存基本都是随机读写存储器（RAM），现代RAM的结构和存取原理比较复杂，这里本文抛却具体差别，抽象出一个十分简单的存取模型来说明RAM的工作原理。 图5 从抽象角度看，主存是一系列的存储单元组成的矩阵，每个存储单元存储固定大小的数据。每个存储单元有唯一的地址，现代主存的编址规则比较复杂，这里将其简化成一个二维地址：通过一个行地址和一个列地址可以唯一定位到一个存储单元。图5展示了一个4 x 4的主存模型。 主存的存取过程如下： 当系统需要读取主存时，则将地址信号放到地址总线上传给主存，主存读到地址信号后，解析信号并定位到指定存储单元，然后将此存储单元数据放到数据总线上，供其它部件读取。 写主存的过程类似，系统将要写入单元地址和数据分别放在地址总线和数据总线上，主存读取两个总线的内容，做相应的写操作。 这里可以看出，主存存取的时间仅与存取次数呈线性关系，因为不存在机械操作，两次存取的数据的“距离”不会对时间有任何影响，例如，先取A0再取A1和先取A0再取D3的时间消耗是一样的。 磁盘存取原理上文说过，索引一般以文件形式存储在磁盘上，索引检索需要磁盘I/O操作。与主存不同，磁盘I/O存在机械运动耗费，因此磁盘I/O的时间消耗是巨大的。 图6是磁盘的整体结构示意图。 图6 一个磁盘由大小相同且同轴的圆形盘片组成，磁盘可以转动（各个磁盘必须同步转动）。在磁盘的一侧有磁头支架，磁头支架固定了一组磁头，每个磁头负责存取一个磁盘的内容。磁头不能转动，但是可以沿磁盘半径方向运动（实际是斜切向运动），每个磁头同一时刻也必须是同轴的，即从正上方向下看，所有磁头任何时候都是重叠的（不过目前已经有多磁头独立技术，可不受此限制）。 图7是磁盘结构的示意图。 图7 盘片被划分成一系列同心环，圆心是盘片中心，每个同心环叫做一个磁道，所有半径相同的磁道组成一个柱面。磁道被沿半径线划分成一个个小的段，每个段叫做一个扇区，每个扇区是磁盘的最小存储单元。为了简单起见，我们下面假设磁盘只有一个盘片和一个磁头。 当需要从磁盘读取数据时，系统会将数据逻辑地址传给磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点，磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间，然后磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。 局部性原理与磁盘预读由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理： 当一个数据被用到时，其附近的数据也通常会马上被使用。 程序运行期间所需要的数据通常比较集中。 由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。 B-/+Tree索引的性能分析到这里终于可以分析B-/+Tree索引的性能了。 上文说过一般使用磁盘I/O次数评价索引结构的优劣。先从B-Tree分析，根据B-Tree的定义，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 floor表示向下取整。由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 这一章从理论角度讨论了与索引相关的数据结构与算法问题，下一章将讨论B+Tree是如何具体实现为MySQL中索引，同时将结合MyISAM和InnDB存储引擎介绍非聚集索引和聚集索引两种不同的索引实现形式。 MySQL索引实现在MySQL中，索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的，本文主要讨论MyISAM和InnoDB两个存储引擎的索引实现方式。 MyISAM索引实现MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。下图是MyISAM索引的原理图： 图8 这里设表一共有三列，假设我们以Col1为主键，则图8是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示： 图9 同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 图10 图10是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。例如，图11为定义在Col3上的一个辅助索引： 图11 这里以英文字符的ASCII码作为比较准则。聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 了解不同存储引擎的索引实现方式对于正确使用和优化索引都非常有帮助，例如知道了InnoDB的索引实现后，就很容易明白为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。再例如，用非单调的字段作为主键在InnoDB中不是个好主意，因为InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效，而使用自增字段作为主键则是一个很好的选择。 下一章将具体讨论这些与索引有关的优化策略。 索引使用策略及优化MySQL的优化主要分为结构优化（Scheme optimization）和查询优化（Query optimization）。本章讨论的高性能索引策略主要属于结构优化范畴。本章的内容完全基于上文的理论基础，实际上一旦理解了索引背后的机制，那么选择高性能的策略就变成了纯粹的推理，并且可以理解这些策略背后的逻辑。 示例数据库为了讨论索引策略，需要一个数据量不算小的数据库作为示例。本文选用MySQL官方文档中提供的示例数据库之一：employees。这个数据库关系复杂度适中，且数据量较大。下图是这个数据库的E-R关系图（引用自MySQL官方手册）： 图12 MySQL官方文档中关于此数据库的页面为http://dev.mysql.com/doc/employee/en/employee.html。里面详细介绍了此数据库，并提供了下载地址和导入方法，如果有兴趣导入此数据库到自己的MySQL可以参考文中内容。 最左前缀原理与相关优化高效使用索引的首要条件是知道什么样的查询会使用到索引，这个问题和B+Tree中的“最左前缀原理”有关，下面通过例子说明最左前缀原理。 这里先说一下联合索引的概念。在上文中，我们都是假设索引只引用了单个的列，实际上，MySQL中的索引可以以一定顺序引用多个列，这种索引叫做联合索引，一般的，一个联合索引是一个有序元组，其中各个元素均为数据表的一列，实际上要严格定义索引需要用到关系代数，但是这里我不想讨论太多关系代数的话题，因为那样会显得很枯燥，所以这里就不再做严格定义。另外，单列索引可以看成联合索引元素数为1的特例。 以employees.titles表为例，下面先查看其上都有哪些索引： 1. SHOW INDEX FROM employees.titles; 2. +--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+ 3. | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Null | Index_type | 4. +--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+ 5. | titles | 0 | PRIMARY | 1 | emp_no | A | NULL | | BTREE | 6. | titles | 0 | PRIMARY | 2 | title | A | NULL | | BTREE | 7. | titles | 0 | PRIMARY | 3 | from_date | A | 443308 | | BTREE | 8. | titles | 1 | emp_no | 1 | emp_no | A | 443308 | | BTREE | 9. +--------+------------+----------+--------------+-------------+-----------+-------------+------+------------+ 从结果中可以到titles表的主索引为&lt;emp_no, title, from_date&gt;，还有一个辅助索引。为了避免多个索引使事情变复杂（MySQL的SQL优化器在多索引时行为比较复杂），这里我们将辅助索引drop掉： 1. ALTER TABLE employees.titles DROP INDEX emp_no; 这样就可以专心分析索引PRIMARY的行为了。 情况一：全列匹配。 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND title='Senior Engineer' AND from_date='1986-06-26'; 2. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 5. | 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | | 6. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 很明显，当按照索引中所有列进行精确匹配（这里精确匹配指“=”或“IN”匹配）时，索引可以被用到。这里有一点需要注意，理论上索引对顺序是敏感的，但是由于MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引，例如我们将where中的条件顺序颠倒： 1. EXPLAIN SELECT * FROM employees.titles WHERE from_date='1986-06-26' AND emp_no='10001' AND title='Senior Engineer'; 2. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 5. | 1 | SIMPLE | titles | const | PRIMARY | PRIMARY | 59 | const,const,const | 1 | | 6. +----+-------------+--------+-------+---------------+---------+---------+-------------------+------+-------+ 效果是一样的。 情况二：最左前缀匹配。 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001'; 2. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------+ 5. | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | | 6. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------+ 当查询条件精确匹配索引的左边连续一个或几个列时，如或&lt;emp_no, title&gt;，所以可以被用到，但是只能用到一部分，即条件所组成的最左前缀。上面的查询从分析结果看用到了PRIMARY索引，但是key_len为4，说明只用到了索引的第一列前缀。 情况三：查询条件用到了索引中列的精确匹配，但是中间某个条件未提供。 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND from_date='1986-06-26'; 2. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 5. | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where | 6. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 此时索引使用情况和情况二相同，因为title未提供，所以查询只用到了索引的第一列，而后面的from_date虽然也在索引中，但是由于title不存在而无法和左前缀连接，因此需要对结果进行扫描过滤from_date（这里由于emp_no唯一，所以不存在扫描）。如果想让from_date也使用索引而不是where过滤，可以增加一个辅助索引&lt;emp_no, from_date&gt;，此时上面的查询会使用这个索引。除此之外，还可以使用一种称之为“隔离列”的优化方法，将emp_no与from_date之间的“坑”填上。 首先我们看下title一共有几种不同的值： 1. SELECT DISTINCT(title) FROM employees.titles; 2. +--------------------+ 3. | title | 4. +--------------------+ 5. | Senior Engineer | 6. | Staff | 7. | Engineer | 8. | Senior Staff | 9. | Assistant Engineer | 10. | Technique Leader | 11. | Manager | 12. +--------------------+ 只有7种。在这种成为“坑”的列值比较少的情况下，可以考虑用“IN”来填补这个“坑”从而形成最左前缀： 1. EXPLAIN SELECT * FROM employees.titles 2. WHERE emp_no='10001' 3. AND title IN ('Senior Engineer', 'Staff', 'Engineer', 'Senior Staff', 'Assistant Engineer', 'Technique Leader', 'Manager') 4. AND from_date='1986-06-26'; 5. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 6. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 7. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 8. | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 7 | Using where | 9. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 这次key_len为59，说明索引被用全了，但是从type和rows看出IN实际上执行了一个range查询，这里检查了7个key。看下两种查询的性能比较： 1. SHOW PROFILES; 2. +----------+------------+-------------------------------------------------------------------------------+ 3. | Query_ID | Duration | Query | 4. +----------+------------+-------------------------------------------------------------------------------+ 5. | 10 | 0.00058000 | SELECT * FROM employees.titles WHERE emp_no='10001' AND from_date='1986-06-26'| 6. | 11 | 0.00052500 | SELECT * FROM employees.titles WHERE emp_no='10001' AND title IN ... | 7. +----------+------------+-------------------------------------------------------------------------------+ “填坑”后性能提升了一点。如果经过emp_no筛选后余下很多数据，则后者性能优势会更加明显。当然，如果title的值很多，用填坑就不合适了，必须建立辅助索引。 情况四：查询条件没有指定索引第一列。 1. EXPLAIN SELECT * FROM employees.titles WHERE from_date='1986-06-26'; 2. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 5. | 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where | 6. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 由于不是最左前缀，索引这样的查询显然用不到索引。 情况五：匹配某列的前缀字符串。 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND title LIKE 'Senior%'; 2. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 5. | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 56 | NULL | 1 | Using where | 6. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 此时可以用到索引，但是如果通配符不是只出现在末尾，则无法使用索引。（原文表述有误，如果通配符%不出现在开头，则可以用到索引，但根据具体情况不同可能只会用其中一个前缀） 情况六：范围查询。 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no < '10010' and title='Senior Engineer'; 2. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 5. | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where | 6. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 范围列可以用到索引（必须是最左前缀），但是范围列后面的列无法用到索引。同时，索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。 1. EXPLAIN SELECT * FROM employees.titles 2. WHERE emp_no < '10010' 3. AND title='Senior Engineer' 4. AND from_date BETWEEN '1986-01-01' AND '1986-12-31'; 5. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 6. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 7. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 8. | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 4 | NULL | 16 | Using where | 9. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 可以看到索引对第二个范围索引无能为力。这里特别要说明MySQL一个有意思的地方，那就是仅用explain可能无法区分范围索引和多值匹配，因为在type中这两者都显示为range。同时，用了“between”并不意味着就是范围查询，例如下面的查询： 1. EXPLAIN SELECT * FROM employees.titles 2. WHERE emp_no BETWEEN '10001' AND '10010' 3. AND title='Senior Engineer' 4. AND from_date BETWEEN '1986-01-01' AND '1986-12-31'; 5. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 6. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 7. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 8. | 1 | SIMPLE | titles | range | PRIMARY | PRIMARY | 59 | NULL | 16 | Using where | 9. +----+-------------+--------+-------+---------------+---------+---------+------+------+-------------+ 看起来是用了两个范围查询，但作用于emp_no上的“BETWEEN”实际上相当于“IN”，也就是说emp_no实际是多值精确匹配。可以看到这个查询用到了索引全部三个列。因此在MySQL中要谨慎地区分多值匹配和范围匹配，否则会对MySQL的行为产生困惑。 情况七：查询条件中含有函数或表达式。很不幸，如果查询条件中含有函数或表达式，则MySQL不会为这列使用索引（虽然某些在数学意义上可以使用）。例如： 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no='10001' AND left(title, 6)='Senior'; 2. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 5. | 1 | SIMPLE | titles | ref | PRIMARY | PRIMARY | 4 | const | 1 | Using where | 6. +----+-------------+--------+------+---------------+---------+---------+-------+------+-------------+ 虽然这个查询和情况五中功能相同，但是由于使用了函数left，则无法为title列应用索引，而情况五中用LIKE则可以。再如： 1. EXPLAIN SELECT * FROM employees.titles WHERE emp_no - 1='10000'; 2. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 5. | 1 | SIMPLE | titles | ALL | NULL | NULL | NULL | NULL | 443308 | Using where | 6. +----+-------------+--------+------+---------------+------+---------+------+--------+-------------+ 显然这个查询等价于查询emp_no为10001的函数，但是由于查询条件是一个表达式，MySQL无法为其使用索引。看来MySQL还没有智能到自动优化常量表达式的程度，因此在写查询语句时尽量避免表达式出现在查询中，而是先手工私下代数运算，转换为无表达式的查询语句。 索引选择性与前缀索引既然索引可以加快查询速度，那么是不是只要是查询语句需要，就建上索引？答案是否定的。因为索引虽然加快了查询速度，但索引也是有代价的：索引文件本身要消耗存储空间，同时索引会加重插入、删除和修改记录时的负担，另外，MySQL在运行时也要消耗资源维护索引，因此索引并不是越多越好。一般两种情况下不建议建索引。 第一种情况是表记录比较少，例如一两千条甚至只有几百条记录的表，没必要建索引，让查询做全表扫描就好了。至于多少条记录才算多，这个个人有个人的看法，我个人的经验是以2000作为分界线，记录数不超过 2000可以考虑不建索引，超过2000条可以酌情考虑索引。 另一种不建议建索引的情况是索引的选择性较低。所谓索引的选择性（Selectivity），是指不重复的索引值（也叫基数，Cardinality）与表记录数（#T）的比值： Index Selectivity = Cardinality / #T 显然选择性的取值范围为(0, 1]，选择性越高的索引价值越大，这是由B+Tree的性质决定的。例如，上文用到的employees.titles表，如果title字段经常被单独查询，是否需要建索引，我们看一下它的选择性： 1. SELECT count(DISTINCT(title))/count(*) AS Selectivity FROM employees.titles; 2. +-------------+ 3. | Selectivity | 4. +-------------+ 5. | 0.0000 | 6. +-------------+ title的选择性不足0.0001（精确值为0.00001579），所以实在没有什么必要为其单独建索引。 有一种与索引选择性有关的索引优化策略叫做前缀索引，就是用列的前缀代替整个列作为索引key，当前缀长度合适时，可以做到既使得前缀索引的选择性接近全列索引，同时因为索引key变短而减少了索引文件的大小和维护开销。下面以employees.employees表为例介绍前缀索引的选择和使用。 从图12可以看到employees表只有一个索引，那么如果我们想按名字搜索一个人，就只能全表扫描了： 1. EXPLAIN SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido'; 2. +----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+ 3. | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | 4. +----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+ 5. | 1 | SIMPLE | employees | ALL | NULL | NULL | NULL | NULL | 300024 | Using where | 6. +----+-------------+-----------+------+---------------+------+---------+------+--------+-------------+ 如果频繁按名字搜索员工，这样显然效率很低，因此我们可以考虑建索引。有两种选择，建或&lt;first_name, last_name&gt;，看下两个索引的选择性： 1. SELECT count(DISTINCT(first_name))/count(*) AS Selectivity FROM employees.employees; 2. +-------------+ 3. | Selectivity | 4. +-------------+ 5. | 0.0042 | 6. +-------------+ 7. SELECT count(DISTINCT(concat(first_name, last_name)))/count(*) AS Selectivity FROM employees.employees; 8. +-------------+ 9. | Selectivity | 10. +-------------+ 11. | 0.9313 | 12. +-------------+ 显然选择性太低，&lt;first_name, last_name&gt;选择性很好，但是first_name和last_name加起来长度为30，有没有兼顾长度和选择性的办法？可以考虑用first_name和last_name的前几个字符建立索引，例如&lt;first_name, left(last_name, 3)&gt;，看看其选择性： 1. SELECT count(DISTINCT(concat(first_name, left(last_name, 3))))/count(*) AS Selectivity FROM employees.employees; 2. +-------------+ 3. | Selectivity | 4. +-------------+ 5. | 0.7879 | 6. +-------------+ 选择性还不错，但离0.9313还是有点距离，那么把last_name前缀加到4： 1. SELECT count(DISTINCT(concat(first_name, left(last_name, 4))))/count(*) AS Selectivity FROM employees.employees; 2. +-------------+ 3. | Selectivity | 4. +-------------+ 5. | 0.9007 | 6. +-------------+ 这时选择性已经很理想了，而这个索引的长度只有18，比&lt;first_name, last_name&gt;短了接近一半，我们把这个前缀索引 建上： 1. ALTER TABLE employees.employees 2. ADD INDEX `first_name_last_name4` (first_name, last_name(4)); 此时再执行一遍按名字查询，比较分析一下与建索引前的结果： 1. SHOW PROFILES; 2. +----------+------------+---------------------------------------------------------------------------------+ 3. | Query_ID | Duration | Query | 4. +----------+------------+---------------------------------------------------------------------------------+ 5. | 87 | 0.11941700 | SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido' | 6. | 90 | 0.00092400 | SELECT * FROM employees.employees WHERE first_name='Eric' AND last_name='Anido' | 7. +----------+------------+---------------------------------------------------------------------------------+ 性能的提升是显著的，查询速度提高了120多倍。 前缀索引兼顾索引大小和查询速度，但是其缺点是不能用于ORDER BY和GROUP BY操作，也不能用于Covering index（即当索引本身包含查询所需全部数据时，不再访问数据文件本身）。 InnoDB的主键选择与插入优化在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 经常看到有帖子或博客讨论主键选择问题，有人建议使用业务无关的自增主键，有人觉得没有必要，完全可以使用如学号或身份证号这种唯一字段作为主键。不论支持哪种论点，大多数论据都是业务层面的。如果从数据库索引优化角度看，使用InnoDB引擎而不使用自增主键绝对是一个糟糕的主意。 上文讨论过InnoDB的索引实现，InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。如下图所示： 图13 这样就会形成一个紧凑的索引结构，近似顺序填满。由于每次插入时也不需要移动已有数据，因此效率很高，也不会增加很多开销在维护索引上。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置： 图14 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 因此，只要可以，请尽量在InnoDB上采用自增字段做主键。 后记这篇文章断断续续写了半个月，主要内容就是上面这些了。不可否认，这篇文章在一定程度上有纸上谈兵之嫌，因为我本人对MySQL的使用属于菜鸟级别，更没有太多数据库调优的经验，在这里大谈数据库索引调优有点大言不惭。就当是我个人的一篇学习笔记了。 其实数据库索引调优是一项技术活，不能仅仅靠理论，因为实际情况千变万化，而且MySQL本身存在很复杂的机制，如查询优化策略和各种引擎的实现差异等都会使情况变得更加复杂。但同时这些理论是索引调优的基础，只有在明白理论的基础上，才能对调优策略进行合理推断并了解其背后的机制，然后结合实践中不断的实验和摸索，从而真正达到高效使用MySQL索引的目的。 另外，MySQL索引及其优化涵盖范围非常广，本文只是涉及到其中一部分。如与排序（ORDER BY）相关的索引优化及覆盖索引（Covering index）的话题本文并未涉及，同时除B-Tree索引外MySQL还根据不同引擎支持的哈希索引、全文索引等等本文也并未涉及。如果有机会，希望再对本文未涉及的部分进行补充吧。 参考文献[1] Baron Scbwartz等 著，王小东等 译；高性能MySQL（High Performance MySQL）；电子工业出版社，2010 [2] Michael Kofler 著，杨晓云等 译；MySQL5权威指南（The Definitive Guide to MySQL5）；人民邮电出版社，2006 [3] 姜承尧 著；MySQL技术内幕-InnoDB存储引擎；机械工业出版社，2011 [4] D Comer, Ubiquitous B-tree; ACM Computing Surveys (CSUR), 1979 [5] Codd, E. F. (1970). “A relational model of data for large shared data banks”. Communications of the ACM, , Vol. 13, No. 6, pp. 377-387 [6] MySQL5.1参考手册 - http://dev.mysql.com/doc/refman/5.1/zh/index.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', });]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列3：一文搞懂String常见面试题，从基础到实战，更有原理分析和源码解析！]]></title>
    <url>%2F2019%2F09%2F03%2F3string%E5%92%8C%E5%8C%85%E8%A3%85%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 [TOC] string基础Java String 类字符串广泛应用 在 Java 编程中，在 Java 中字符串属于对象，Java 提供了 String 类来创建和操作字符串。 创建字符串创建字符串最简单的方式如下: String greeting = “菜鸟教程”; 在代码中遇到字符串常量时，这里的值是 “菜鸟教程“”，编译器会使用该值创建一个 String 对象。 和其它对象一样，可以使用关键字和构造方法来创建 String 对象。 String 类有 11 种构造方法，这些方法提供不同的参数来初始化字符串，比如提供一个字符数组参数: StringDemo.java 文件代码：public class StringDemo{ public static void main(String args[]){ char[] helloArray = { &apos;r&apos;, &apos;u&apos;, &apos;n&apos;, &apos;o&apos;, &apos;o&apos;, &apos;b&apos;}; String helloString = new String(helloArray); System.out.println( helloString ); } }以上实例编译运行结果如下： 1runoob 注意:String 类是不可改变的，所以你一旦创建了 String 对象，那它的值就无法改变了（详看笔记部分解析）。 如果需要对字符串做很多修改，那么应该选择使用 StringBuffer &amp; StringBuilder 类。 String基本用法创建String对象的常用方法（1） String s1 = “mpptest” (2) String s2 = new String(); (3) String s3 = new String(“mpptest”) String中常用的方法，用法如图所示，具体问度娘 三个方法的使用： lenth() substring() charAt()package com.mpp.string; public class StringDemo1 { public static void main(String[] args) { //定义一个字符串"晚来天欲雪 能饮一杯无" String str = "晚来天欲雪 能饮一杯无"; System.out.println("字符串的长度是："+str.length()); //字符串的雪字打印输出 charAt(int index) System.out.println(str.charAt(4)); //取出子串 天欲 System.out.println(str.substring(2)); //取出从index2开始直到最后的子串，包含2 System.out.println(str.substring(2,4)); //取出index从2到4的子串，包含2不包含4 顾头不顾尾 } } 两个方法的使用,求字符或子串第一次/最后一次在字符串中出现的位置： indexOf() lastIndexOf() package com.mpp.string; public class StringDemo2 { public static void main(String[] args) { String str = new String("赵客缦胡缨 吴钩胡缨霜雪明"); //查找胡在字符串中第一次出现的位置 System.out.println("\"胡\"在字符串中第一次出现的位置："+str.indexOf("胡")); //查找子串"胡缨"在字符串中第一次出现的位置 System.out.println("\"胡缨\"在字符串中第一次出现的位置"+str.indexOf("胡缨")); //查找胡在字符串中最后一次次出现的位置 System.out.println(str.lastIndexOf("胡")); //查找子串"胡缨"在字符串中最后一次出现的位置 System.out.println(str.lastIndexOf("胡缨")); //从indexof为5的位置，找第一次出现的"吴" System.out.println(str.indexOf("吴",5)); } } 字符串与byte数组间的相互转换package com.mpp.string; import java.io.UnsupportedEncodingException; public class StringDemo3 { public static void main(String[] args) throws UnsupportedEncodingException { //字符串和byte数组之间的相互转换 String str = new String("hhhabc银鞍照白马 飒沓如流星"); //将字符串转换为byte数组，并打印输出 byte[] arrs = str.getBytes("GBK"); for(int i=0;i){ System.out.print(arrs[i]); } //将byte数组转换成字符串 System.out.println(); String str1 = new String(arrs,"GBK"); //保持字符集的一致，否则会出现乱码 System.out.println(str1); } } ==运算符和equals之间的区别：引用指向的内容和引用指向的地址 package com.mpp.string; public class StringDemo5 { public static void main(String[] args) { String str1 = "mpp"; String str2 = "mpp"; String str3 = new String("mpp"); System.out.println(str1.equals(str2)); //true 内容相同 System.out.println(str1.equals(str3)); //true 内容相同 System.out.println(str1==str2); //true 地址相同 System.out.println(str1==str3); //false 地址不同 } } 字符串的不可变性String的对象一旦被创建，则不能修改，是不可变的 所谓的修改其实是创建了新的对象，所指向的内存空间不变 上图中，s1不再指向imooc所在的内存空间，而是指向了hello,imooc String的连接@Test public void contact () { //1连接方式 String s1 = &quot;a&quot;; String s2 = &quot;a&quot;; String s3 = &quot;a&quot; + s2; String s4 = &quot;a&quot; + &quot;a&quot;; String s5 = s1 + s2; //表达式只有常量时，编译期完成计算 //表达式有变量时，运行期才计算，所以地址不一样 System.out.println(s3 == s4); //f System.out.println(s3 == s5); //f System.out.println(s4 == &quot;aa&quot;); //t }String、String builder和String buffer的区别String是Java中基础且重要的类，并且String也是Immutable类的典型实现，被声明为final class，除了hash这个属性其它属性都声明为final,因为它的不可变性，所以例如拼接字符串时候会产生很多无用的中间对象，如果频繁的进行这样的操作对性能有所影响。 StringBuffer就是为了解决大量拼接字符串时产生很多中间对象问题而提供的一个类，提供append和add方法，可以将字符串添加到已有序列的末尾或指定位置，它的本质是一个线程安全的可修改的字符序列，把所有修改数据的方法都加上了synchronized。但是保证了线程安全是需要性能的代价的。 在很多情况下我们的字符串拼接操作不需要线程安全，这时候StringBuilder登场了，StringBuilder是JDK1.5发布的，它和StringBuffer本质上没什么区别，就是去掉了保证线程安全的那部分，减少了开销。 StringBuffer 和 StringBuilder 二者都继承了 AbstractStringBuilder ，底层都是利用可修改的char数组(JDK 9 以后是 byte数组)。 所以如果我们有大量的字符串拼接，如果能预知大小的话最好在new StringBuffer 或者StringBuilder 的时候设置好capacity，避免多次扩容的开销。扩容要抛弃原有数组，还要进行数组拷贝创建新的数组。 我们平日开发通常情况下少量的字符串拼接其实没太必要担心，例如 String str = “aa”+”bb”+”cc”; 像这种没有变量的字符串，编译阶段就直接合成”aabbcc”了，然后看字符串常量池（下面会说到常量池）里有没有，有也直接引用，没有就在常量池中生成，返回引用。 如果是带变量的，其实影响也不大，JVM会帮我们优化了。 1、在字符串不经常发生变化的业务场景优先使用String(代码更清晰简洁)。如常量的声明，少量的字符串操作(拼接，删除等)。 2、在单线程情况下，如有大量的字符串操作情况，应该使用StringBuilder来操作字符串。不能使用String”+”来拼接而是使用，避免产生大量无用的中间对象，耗费空间且执行效率低下（新建对象、回收对象花费大量时间）。如JSON的封装等。 3、在多线程情况下，如有大量的字符串操作情况，应该使用StringBuffer。如HTTP参数解析和封装等。 String类的源码分析String类型的internpublic void intern () { //2：string的intern使用 //s1是基本类型，比较值。s2是string实例，比较实例地址 //字符串类型用equals方法比较时只会比较值 String s1 = &quot;a&quot;; String s2 = new String(&quot;a&quot;); //调用intern时,如果s2中的字符不在常量池，则加入常量池并返回常量的引用 String s3 = s2.intern(); System.out.println(s1 == s2); System.out.println(s1 == s3); }String类型的equals//字符串的equals方法 // public boolean equals(Object anObject) { // if (this == anObject) { // return true; // } // if (anObject instanceof String) { // String anotherString = (String)anObject; // int n = value.length; // if (n == anotherString.value.length) { // char v1[] = value; // char v2[] = anotherString.value; // int i = 0; // while (n-- != 0) { // if (v1[i] != v2[i]) // return false; // i++; // } // return true; // } // } // return false; // }StringBuffer和Stringbuilder底层是继承父类的可变字符数组value /** - The value is used for character storage. */ char[] value; 初始化容量为16 /** - Constructs a string builder with no characters in it and an - initial capacity of 16 characters. */ public StringBuilder() { super(16); } 这两个类的append方法都是来自父类AbstractStringBuilder的方法 public AbstractStringBuilder append(String str) { if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this; } @Override public StringBuilder append(String str) { super.append(str); return this; } @Override public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this; }append方法Stringbuffer在大部分涉及字符串修改的操作上加了synchronized关键字来保证线程安全，效率较低。 String类型在使用 + 运算符例如 String a = “a” a = a + a;时，实际上先把a封装成stringbuilder，调用append方法后再用tostring返回，所以当大量使用字符串加法时，会大量地生成stringbuilder实例，这是十分浪费的，这种时候应该用stringbuilder来代替string。 扩容#注意在append方法中调用到了一个函数 ensureCapacityInternal(count + len);该方法是计算append之后的空间是否足够，不足的话需要进行扩容 public void ensureCapacity(int minimumCapacity) { if (minimumCapacity &gt; 0) ensureCapacityInternal(minimumCapacity); } private void ensureCapacityInternal(int minimumCapacity) { // overflow-conscious code if (minimumCapacity - value.length &gt; 0) { value = Arrays.copyOf(value, newCapacity(minimumCapacity)); } }如果新字符串长度大于value数组长度则进行扩容 扩容后的长度一般为原来的两倍 + 2； 假如扩容后的长度超过了jvm支持的最大数组长度MAX_ARRAY_SIZE。 考虑两种情况 如果新的字符串长度超过int最大值，则抛出异常，否则直接使用数组最大长度作为新数组的长度。 private int hugeCapacity(int minCapacity) { if (Integer.MAX_VALUE - minCapacity &lt; 0) { // overflow throw new OutOfMemoryError(); } return (minCapacity &gt; MAX_ARRAY_SIZE) ? minCapacity : MAX_ARRAY_SIZE; }删除这两个类型的删除操作： 都是调用父类的delete方法进行删除 public AbstractStringBuilder delete(int start, int end) { if (start &lt; 0) throw new StringIndexOutOfBoundsException(start); if (end &gt; count) end = count; if (start &gt; end) throw new StringIndexOutOfBoundsException(); int len = end - start; if (len &gt; 0) { System.arraycopy(value, start+len, value, start, count-end); count -= len; } return this; }事实上是将剩余的字符重新拷贝到字符数组value。 这里用到了system.arraycopy来拷贝数组，速度是比较快的 system.arraycopy方法转自知乎： 在主流高性能的JVM上（HotSpot VM系、IBM J9 VM系、JRockit系等等），可以认为System.arraycopy()在拷贝数组时是可靠高效的——如果发现不够高效的情况，请报告performance bug，肯定很快就会得到改进。 java.lang.System.arraycopy()方法在Java代码里声明为一个native方法。所以最naïve的实现方式就是通过JNI调用JVM里的native代码来实现。 String的不可变性关于String的不可变性，这里转一个不错的回答 什么是不可变？String不可变很简单，如下图，给一个已有字符串”abcd”第二次赋值成”abcedl”，不是在原内存地址上修改数据，而是重新指向一个新对象，新地址。 String和JVM的关系下面我们了解下Java栈、Java堆、方法区和常量池： Java栈（线程私有数据区）： 1每个Java虚拟机线程都有自己的Java虚拟机栈，Java虚拟机栈用来存放栈帧，每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 Java堆（线程共享数据区）： 1在虚拟机启动时创建，此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配。 方法区（线程共享数据区）： 1方法区在虚拟机启动的时候被创建，它存储了每一个类的结构信息，例如运行时常量池、字段和方法数据、构造函数和普通方法的字节码内容、还包括在类、实例、接口初始化时用到的特殊方法。在JDK8之前永久代是方法区的一种实现，而JDK8元空间替代了永久代，永久代被移除，也可以理解为元空间是方法区的一种实现。 常量池（线程共享数据区）： 12345常量池常被分为两大类：静态常量池和运行时常量池。静态常量池也就是Class文件中的常量池，存在于Class文件中。运行时常量池（Runtime Constant Pool）是方法区的一部分，存放一些运行时常量数据。 下面重点了解的是字符串常量池： 12345字符串常量池存在运行时常量池之中（在JDK7之前存在运行时常量池之中，在JDK7已经将其转移到堆中）。字符串常量池的存在使JVM提高了性能和减少了内存开销。使用字符串常量池，每当我们使用字面量（String s=”1”;）创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么就将此字符串对象的地址赋值给引用s（引用s在Java栈中）。如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中，并将此字符串对象的地址赋值给引用s（引用s在Java栈中）。 1使用字符串常量池，每当我们使用关键字new（String s=new String(”1”);）创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么不再在字符串常量池创建该字符串对象，而直接堆中复制该对象的副本，然后将堆中对象的地址赋值给引用s，如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中，然后在堆中复制该对象的副本，然后将堆中对象的地址赋值给引用s。 String为什么不可变？翻开JDK源码，java.lang.String类起手前三行，是这样写的： public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence { /** String本质是个char数组. 而且用final关键字修饰.*/ private final char value[]; ... ... } 首先String类是用final关键字修饰，这说明String不可继承。再看下面，String类的主力成员字段value是个char[]数组，而且是用final修饰的。 final修饰的字段创建以后就不可改变。 有的人以为故事就这样完了，其实没有。因为虽然value是不可变，也只是value这个引用地址不可变。挡不住Array数组是可变的事实。 Array的数据结构看下图。 也就是说Array变量只是stack上的一个引用，数组的本体结构在heap堆。 String类里的value用final修饰，只是说stack里的这个叫value的引用地址不可变。没有说堆里array本身数据不可变。看下面这个例子， final int[] value={1,2,3} ； int[] another={4,5,6}; value=another; //编译器报错，final不可变 value用final修饰，编译器不允许我把value指向堆区另一个地址。 但如果我直接对数组元素动手，分分钟搞定。 final int[] value={1,2,3}; value[2]=100; //这时候数组里已经是{1,2,100} 所以String是不可变，关键是因为SUN公司的工程师。 在后面所有String的方法里很小心的没有去动Array里的元素，没有暴露内部成员字段。private final char value[]这一句里，private的私有访问权限的作用都比final大。而且设计师还很小心地把整个String设成final禁止继承，避免被其他人继承后破坏。所以String是不可变的关键都在底层的实现，而不是一个final。考验的是工程师构造数据类型，封装数据的功力。 不可变有什么好处？这个最简单地原因，就是为了安全。看下面这个场景（有评论反应例子不够清楚，现在完整地写出来），一个函数appendStr( )在不可变的String参数后面加上一段“bbb”后返回。appendSb( )负责在可变的StringBuilder后面加“bbb”。 总结以下String的不可变性。 1 首先final修饰的类只保证不能被继承，并且该类的对象在堆内存中的地址不会被改变。 2 但是持有String对象的引用本身是可以改变的，比如他可以指向其他的对象。 3 final修饰的char数组保证了char数组的引用不可变。但是可以通过char[0] = ‘a’来修改值。不过String内部并不提供方法来完成这一操作，所以String的不可变也是基于代码封装和访问控制的。 举个例子 final class Fi { int a; final int b = 0; Integer s; } final char[]a = {&apos;a&apos;}; final int[]b = {1}; @Test public void final修饰类() { //引用没有被final修饰，所以是可变的。 //final只修饰了Fi类型，即Fi实例化的对象在堆中内存地址是不可变的。 //虽然内存地址不可变，但是可以对内部的数据做改变。 Fi f = new Fi(); f.a = 1; System.out.println(f); f.a = 2; System.out.println(f); //改变实例中的值并不改变内存地址。12345678Fi ff = f;//让引用指向新的Fi对象，原来的f对象由新的引用ff持有。//引用的指向改变也不会改变原来对象的地址f = new Fi();System.out.println(f);System.out.println(ff);&#125; 这里的对f.a的修改可以理解为char[0] = ‘a’这样的操作。只改变数据值，不改变内存值。 String常用工具类问题描述很多时候我们需要对字符串进行很多固定的操作,而这些操作在JDK/JRE中又没有预置,于是我们想到了apache-commons组件,但是它也不能完全覆盖我们的业务需求,所以很多时候还是要自己写点代码的,下面就是基于apache-commons组件写的部分常用方法: MAVEN依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;${commons-lang3.version}&lt;/version&gt; &lt;/dependency&gt;代码成果 public class StringUtils extends org.apache.commons.lang3.StringUtils { /** 值为&quot;NULL&quot;的字符串 */ private static final String NULL_STRING = &quot;NULL&quot;; private static final char SEPARATOR = &apos;_&apos;; /** * 满足一下情况返回true&lt;br/&gt; * ①.入参为空 * ②.入参为空字符串 * ③.入参为&quot;null&quot;字符串 * * @param string 需要判断的字符型 * @return boolean */ public static boolean isNullOrEmptyOrNULLString(String string) { return isBlank(string) || NULL_STRING.equalsIgnoreCase(string); } /** * 把字符串转为二进制码&lt;br/&gt; * 本方法不会返回null * * @param str 需要转换的字符串 * @return 二进制字节码数组 */ public static byte[] toBytes(String str) { return isBlank(str) ? new byte[]{} : str.getBytes(); } /** * 把字符串转为二进制码&lt;br/&gt; * 本方法不会返回null * * @param str 需要转换的字符串 * @param charset 编码类型 * @return 二进制字节码数组 * @throws UnsupportedEncodingException 字符串转换的时候编码不支持时出现 */ public static byte[] toBytes(String str, Charset charset) throws UnsupportedEncodingException { return isBlank(str) ? new byte[]{} : str.getBytes(charset.displayName()); } /** * 把字符串转为二进制码&lt;br/&gt; * 本方法不会返回null * * @param str 需要转换的字符串 * @param charset 编码类型 * @param locale 编码类型对应的地区 * @return 二进制字节码数组 * @throws UnsupportedEncodingException 字符串转换的时候编码不支持时出现 */ public static byte[] toBytes(String str, Charset charset, Locale locale) throws UnsupportedEncodingException { return isBlank(str) ? new byte[]{} : str.getBytes(charset.displayName(locale)); } /** * 二进制码转字符串&lt;br/&gt; * 本方法不会返回null * * @param bytes 二进制码 * @return 字符串 */ public static String bytesToString(byte[] bytes) { return bytes == null || bytes.length == 0 ? EMPTY : new String(bytes); } /** * 二进制码转字符串&lt;br/&gt; * 本方法不会返回null * * @param bytes 二进制码 * @param charset 编码集 * @return 字符串 * @throws UnsupportedEncodingException 当前二进制码可能不支持传入的编码 */ public static String byteToString(byte[] bytes, Charset charset) throws UnsupportedEncodingException { return bytes == null || bytes.length == 0 ? EMPTY : new String(bytes, charset.displayName()); } /** * 二进制码转字符串&lt;br/&gt; * 本方法不会返回null * * @param bytes 二进制码 * @param charset 编码集 * @param locale 本地化 * @return 字符串 * @throws UnsupportedEncodingException 当前二进制码可能不支持传入的编码 */ public static String byteToString(byte[] bytes, Charset charset, Locale locale) throws UnsupportedEncodingException { return bytes == null || bytes.length == 0 ? EMPTY : new String(bytes, charset.displayName(locale)); } /** * 把对象转为字符串 * * @param object 需要转化的字符串 * @return 字符串, 可能为空 */ public static String parseString(Object object) { if (object == null) { return null; } if (object instanceof byte[]) { return bytesToString((byte[]) object); } return object.toString(); } /** * 把字符串转为int类型 * * @param str 需要转化的字符串 * @return int * @throws NumberFormatException 字符串格式不正确时抛出 */ public static int parseInt(String str) throws NumberFormatException { return isBlank(str) ? 0 : Integer.parseInt(str); } /** * 把字符串转为double类型 * * @param str 需要转化的字符串 * @return double * @throws NumberFormatException 字符串格式不正确时抛出 */ public static double parseDouble(String str) throws NumberFormatException { return isBlank(str) ? 0D : Double.parseDouble(str); } /** * 把字符串转为long类型 * * @param str 需要转化的字符串 * @return long * @throws NumberFormatException 字符串格式不正确时抛出 */ public static long parseLong(String str) throws NumberFormatException { return isBlank(str) ? 0L : Long.parseLong(str); } /** * 把字符串转为float类型 * * @param str 需要转化的字符串 * @return float * @throws NumberFormatException 字符串格式不正确时抛出 */ public static float parseFloat(String str) throws NumberFormatException { return isBlank(str) ? 0L : Float.parseFloat(str); } /** * 获取i18n字符串 * * @param code * @param args * @return */ public static String getI18NMessage(String code, Object[] args) { //LocaleResolver localLocaleResolver = (LocaleResolver) SpringContextHolder.getBean(LocaleResolver.class); //HttpServletRequest request = ((ServletRequestAttributes)RequestContextHolder.getRequestAttributes()).getRequest(); //Locale locale = localLocaleResolver.resolveLocale(request); //return SpringContextHolder.getApplicationContext().getMessage(code, args, locale); return &quot;&quot;; } /** * 获得用户远程地址 * * @param request 请求头 * @return 用户ip */ public static String getRemoteAddr(HttpServletRequest request) { String remoteAddr = request.getHeader(&quot;X-Real-IP&quot;); if (isNotBlank(remoteAddr)) { remoteAddr = request.getHeader(&quot;X-Forwarded-For&quot;); } else if (isNotBlank(remoteAddr)) { remoteAddr = request.getHeader(&quot;Proxy-Client-IP&quot;); } else if (isNotBlank(remoteAddr)) { remoteAddr = request.getHeader(&quot;WL-Proxy-Client-IP&quot;); } return remoteAddr != null ? remoteAddr : request.getRemoteAddr(); } /** * 驼峰命名法工具 * * @return toCamelCase(&quot; hello_world &quot;) == &quot;helloWorld&quot; * toCapitalizeCamelCase(&quot;hello_world&quot;) == &quot;HelloWorld&quot; * toUnderScoreCase(&quot;helloWorld&quot;) = &quot;hello_world&quot; */ public static String toCamelCase(String s, Locale locale, char split) { if (isBlank(s)) { return &quot;&quot;; } s = s.toLowerCase(locale); StringBuilder sb = new StringBuilder(); for (char c : s.toCharArray()) { sb.append(c == split ? Character.toUpperCase(c) : c); } return sb.toString(); } public static String toCamelCase(String s) { return toCamelCase(s, Locale.getDefault(), SEPARATOR); } public static String toCamelCase(String s, Locale locale) { return toCamelCase(s, locale, SEPARATOR); } public static String toCamelCase(String s, char split) { return toCamelCase(s, Locale.getDefault(), split); } public static String toUnderScoreCase(String s, char split) { if (isBlank(s)) { return &quot;&quot;; } StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; s.length(); i++) { char c = s.charAt(i); boolean nextUpperCase = (i &lt; (s.length() - 1)) &amp;&amp; Character.isUpperCase(s.charAt(i + 1)); boolean upperCase = (i &gt; 0) &amp;&amp; Character.isUpperCase(c); sb.append((!upperCase || !nextUpperCase) ? split : &quot;&quot;).append(Character.toLowerCase(c)); } return sb.toString(); } public static String toUnderScoreCase(String s) { return toUnderScoreCase(s, SEPARATOR); } /** * 把字符串转换为JS获取对象值的三目运算表达式 * * @param objectString 对象串 * 例如：入参:row.user.id/返回：!row?&apos;&apos;:!row.user?&apos;&apos;:!row.user.id?&apos;&apos;:row.user.id */ public static String toJsGetValueExpression(String objectString) { StringBuilder result = new StringBuilder(); StringBuilder val = new StringBuilder(); String[] fileds = split(objectString, &quot;.&quot;); for (int i = 0; i &lt; fileds.length; i++) { val.append(&quot;.&quot; + fileds[i]); result.append(&quot;!&quot; + (val.substring(1)) + &quot;?&apos;&apos;:&quot;); } result.append(val.substring(1)); return result.toString(); } }参考文章https://blog.csdn.net/qq_34490018/article/details/82110578https://www.runoob.com/java/java-string.htmlhttps://www.cnblogs.com/zhangyinhua/p/7689974.htmlhttps://blog.csdn.net/sinat_21925975/article/details/86493248https://www.cnblogs.com/niew/p/9597379.html 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库3：Mysql存储引擎与数据存储原理]]></title>
    <url>%2F2019%2F09%2F03%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%933%EF%BC%9AMysql%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言作为一名开发人员，在日常的工作中会难以避免地接触到数据库，无论是基于文件的 sqlite 还是工程上使用非常广泛的 MySQL、PostgreSQL，但是一直以来也没有对数据库有一个非常清晰并且成体系的认知，所以最近两个月的时间看了几本数据库相关的书籍并且阅读了 MySQL 的官方文档，希望对各位了解数据库的、不了解数据库的有所帮助。 本文中对于数据库的介绍以及研究都是在 MySQL 上进行的，如果涉及到了其他数据库的内容或者实现会在文中单独指出。 数据库的定义很多开发者在最开始时其实都对数据库有一个比较模糊的认识，觉得数据库就是一堆数据的集合，但是实际却比这复杂的多，数据库领域中有两个词非常容易混淆，也就是_数据库和实例_： 数据库：物理操作文件系统或其他形式文件类型的集合； 实例：MySQL 数据库由后台线程以及一个共享内存区组成； 对于数据库和实例的定义都来自于 MySQL 技术内幕：InnoDB 存储引擎 一书，想要了解 InnoDB 存储引擎的读者可以阅读这本书籍。 数据库和实例在 MySQL 中，实例和数据库往往都是一一对应的，而我们也无法直接操作数据库，而是要通过数据库实例来操作数据库文件，可以理解为数据库实例是数据库为上层提供的一个专门用于操作的接口。 在 Unix 上，启动一个 MySQL 实例往往会产生两个进程，mysqld 就是真正的数据库服务守护进程，而 mysqld_safe 是一个用于检查和设置 mysqld 启动的控制程序，它负责监控 MySQL 进程的执行，当 mysqld 发生错误时，mysqld_safe 会对其状态进行检查并在合适的条件下重启。 MySQL 的架构MySQL 从第一个版本发布到现在已经有了 20 多年的历史，在这么多年的发展和演变中，整个应用的体系结构变得越来越复杂： 最上层用于连接、线程处理的部分并不是 MySQL 『发明』的，很多服务都有类似的组成部分；第二层中包含了大多数 MySQL 的核心服务，包括了对 SQL 的解析、分析、优化和缓存等功能，存储过程、触发器和视图都是在这里实现的；而第三层就是 MySQL 中真正负责数据的存储和提取的存储引擎，例如：InnoDB、MyISAM 等，文中对存储引擎的介绍都是对 InnoDB 实现的分析。 数据的存储在整个数据库体系结构中，我们可以使用不同的存储引擎来存储数据，而绝大多数存储引擎都以二进制的形式存储数据；这一节会介绍 InnoDB 中对数据是如何存储的。 在 InnoDB 存储引擎中，所有的数据都被逻辑地存放在表空间中，表空间（tablespace）是存储引擎中最高的存储逻辑单位，在表空间的下面又包括段（segment）、区（extent）、页（page）： 同一个数据库实例的所有表空间都有相同的页大小；默认情况下，表空间中的页大小都为 16KB，当然也可以通过改变 innodb_page_size 选项对默认大小进行修改，需要注意的是不同的页大小最终也会导致区大小的不同： 从图中可以看出，在 InnoDB 存储引擎中，一个区的大小最小为 1MB，页的数量最少为 64 个。 如何存储表MySQL 使用 InnoDB 存储表时，会将表的定义和数据索引等信息分开存储，其中前者存储在 .frm 文件中，后者存储在 .ibd 文件中，这一节就会对这两种不同的文件分别进行介绍。 .frm 文件无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 .frm 文件用来描述表的格式或者说定义；.frm 文件的格式在不同的平台上都是相同的。 1234CREATE TABLE test_frm( column1 CHAR(5), column2 INTEGER); 当我们使用上面的代码创建表时，会在磁盘上的 datadir 文件夹中生成一个 test_frm.frm 的文件，这个文件中就包含了表结构相关的信息： MySQL 官方文档中的 11.1 MySQL .frm File Format 一文对于 .frm 文件格式中的二进制的内容有着非常详细的表述，在这里就不展开介绍了。 .ibd 文件InnoDB 中用于存储数据的文件总共有两个部分，一是系统表空间文件，包括 ibdata1、ibdata2 等文件，其中存储了 InnoDB 系统信息和用户数据库表数据和索引，是所有表公用的。 当打开 innodb_file_per_table 选项时，.ibd 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。 如何存储记录与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。 当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式： Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant，它最开始并没有名字；Antelope 的名字是在新的文件格式 Barracuda 出现后才起的，Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。 两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储： Compact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看，Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。 行溢出数据当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将行数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页。 但是当我们使用新的行记录格式 Compressed 或者 Dynamic 时都只会在行记录中保存 20 个字节的指针，实际的数据都会存放在溢出页面中。 当然在实际存储中，可能会对不同长度的 TEXT 和 BLOB 列进行优化，不过这就不是本文关注的重点了。 想要了解更多与 InnoDB 存储引擎中记录的数据格式的相关信息，可以阅读 InnoDB Record Structure 数据页结构页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分： 每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。 在页的头部和尾部之间就是用户记录和空闲空间了，每一个数据页中都包含 Infimum 和 Supremum 这两个虚拟的记录（可以理解为占位符），Infimum 记录是比该页中任何主键值都要小的值，Supremum 是该页中的最大值： User Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 next_record 这一指针控制的。 B+ 树在查找对应的记录时，并不会直接从树中找出对应的行记录，它只能获取记录所在的页，将整个页加载到内存中，再通过 Page Directory 中存储的稀疏索引和 n_owned、next_record 属性取出对应的记录，不过因为这一操作是在内存中进行的，所以通常会忽略这部分查找的耗时。 InnoDB 存储引擎中对数据的存储是一个非常复杂的话题，这一节中也只是对表、行记录以及页面的存储进行一定的分析和介绍，虽然作者相信这部分知识对于大部分开发者已经足够了，但是想要真正消化这部分内容还需要很多的努力和实践。 索引索引是数据库中非常非常重要的概念，它是存储引擎能够快速定位记录的秘密武器，对于提升数据库的性能、减轻数据库服务器的负担有着非常重要的作用；索引优化是对查询性能优化的最有效手段，它能够轻松地将查询的性能提高几个数量级。 索引的数据结构在上一节中，我们谈了行记录的存储和页的存储，在这里我们就要从更高的层面看 InnoDB 中对于数据是如何存储的；InnoDB 存储引擎在绝大多数情况下使用 B+ 树建立索引，这是关系型数据库中查找最为常用和有效的索引，但是 B+ 树索引并不能找到一个给定键对应的具体值，它只能找到数据行对应的页，然后正如上一节所提到的，数据库把整个页读入到内存中，并在内存中查找具体的数据行。 B+ 树是平衡树，它查找任意节点所耗费的时间都是完全相同的，比较的次数就是 B+ 树的高度；在这里，我们并不会深入分析或者动手实现一个 B+ 树，只是对它的特性进行简单的介绍。 聚集索引和辅助索引数据库中的 B+ 树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），它们之间的最大区别就是，聚集索引中存放着一条行记录的全部信息，而辅助索引中只包含索引列和一个用于查找对应行记录的『书签』。 聚集索引InnoDB 存储引擎中的表都是使用索引组织的，也就是按照键的顺序存放；聚集索引就是按照表中主键的顺序构建一颗 B+ 树，并在叶节点中存放表中的行记录数据。 123456789CREATE TABLE users( id INT NOT NULL, first_name VARCHAR(20) NOT NULL, last_name VARCHAR(20) NOT NULL, age INT NOT NULL, PRIMARY KEY(id), KEY(last_name, first_name, age) KEY(first_name)); 如果使用上面的 SQL 在数据库中创建一张表，B+ 树就会使用 id 作为索引的键，并在叶子节点中存储一条记录中的所有信息。 图中对 B+ 树的描述与真实情况下 B+ 树中的数据结构有一些差别，不过这里想要表达的主要意思是：聚集索引叶节点中保存的是整条行记录，而不是其中的一部分。 聚集索引与表的物理存储方式有着非常密切的关系，所有正常的表应该有且仅有一个聚集索引（绝大多数情况下都是主键），表中的所有行记录数据都是按照聚集索引的顺序存放的。 当我们使用聚集索引对表中的数据进行检索时，可以直接获得聚集索引所对应的整条行记录数据所在的页，不需要进行第二次操作。 辅助索引数据库将所有的非聚集索引都划分为辅助索引，但是这个概念对我们理解辅助索引并没有什么帮助；辅助索引也是通过 B+ 树实现的，但是它的叶节点并不包含行记录的全部数据，仅包含索引中的所有键和一个用于查找对应行记录的『书签』，在 InnoDB 中这个书签就是当前记录的主键。 辅助索引的存在并不会影响聚集索引，因为聚集索引构成的 B+ 树是数据实际存储的形式，而辅助索引只用于加速数据的查找，所以一张表上往往有多个辅助索引以此来提升数据库的性能。 一张表一定包含一个聚集索引构成的 B+ 树以及若干辅助索引的构成的 B+ 树。 如果在表 users 中存在一个辅助索引 (first_name, age)，那么它构成的 B+ 树大致就是上图这样，按照 (first_name, age) 的字母顺序对表中的数据进行排序，当查找到主键时，再通过聚集索引获取到整条行记录。 上图展示了一个使用辅助索引查找一条表记录的过程：通过辅助索引查找到对应的主键，最后在聚集索引中使用主键获取对应的行记录，这也是通常情况下行记录的查找方式。 索引的设计索引的设计其实是一个非常重要的内容，同时也是一个非常复杂的内容；索引的设计与创建对于提升数据库的查询性能至关重要，不过这不是本文想要介绍的内容，有关索引的设计与优化可以阅读 数据库索引设计与优化 一书，书中提供了一种非常科学合理的方法能够帮助我们在数据库中建立最适合的索引，当然作者也可能会在之后的文章中对索引的设计进行简单的介绍和分析。 锁我们都知道锁的种类一般分为乐观锁和悲观锁两种，InnoDB 存储引擎中使用的就是悲观锁，而按照锁的粒度划分，也可以分成行锁和表锁。 并发控制机制乐观锁和悲观锁其实都是并发控制的机制，同时它们在原理上就有着本质的差别； 乐观锁是一种思想，它其实并不是一种真正的『锁』，它会先尝试对资源进行修改，在写回时判断资源是否进行了改变，如果没有发生改变就会写回，否则就会进行重试，在整个的执行过程中其实都没有对数据库进行加锁； 悲观锁就是一种真正的锁了，它会在获取资源前对资源进行加锁，确保同一时刻只有有限的线程能够访问该资源，其他想要尝试获取资源的操作都会进入等待状态，直到该线程完成了对资源的操作并且释放了锁后，其他线程才能重新操作资源； 虽然乐观锁和悲观锁在本质上并不是同一种东西，一个是一种思想，另一个是一种真正的锁，但是它们都是一种并发控制机制。 乐观锁不会存在死锁的问题，但是由于更新后验证，所以当冲突频率和重试成本较高时更推荐使用悲观锁，而需要非常高的响应速度并且并发量非常大的时候使用乐观锁就能较好的解决问题，在这时使用悲观锁就可能出现严重的性能问题；在选择并发控制机制时，需要综合考虑上面的四个方面（冲突频率、重试成本、响应速度和并发量）进行选择。 锁的种类对数据的操作其实只有两种，也就是读和写，而数据库在实现锁时，也会对这两种操作使用不同的锁；InnoDB 实现了标准的行级锁，也就是共享锁（Shared Lock）和互斥锁（Exclusive Lock）；共享锁和互斥锁的作用其实非常好理解： 共享锁（读锁）：允许事务对一条行数据进行读取； 互斥锁（写锁）：允许事务对一条行数据进行删除或更新； 而它们的名字也暗示着各自的另外一个特性，共享锁之间是兼容的，而互斥锁与其他任意锁都不兼容： 稍微对它们的使用进行思考就能想明白它们为什么要这么设计，因为共享锁代表了读操作、互斥锁代表了写操作，所以我们可以在数据库中并行读，但是只能串行写，只有这样才能保证不会发生线程竞争，实现线程安全。 锁的粒度无论是共享锁还是互斥锁其实都只是对某一个数据行进行加锁，InnoDB 支持多种粒度的锁，也就是行锁和表锁；为了支持多粒度锁定，InnoDB 存储引擎引入了意向锁（Intention Lock），意向锁就是一种表级锁。 与上一节中提到的两种锁的种类相似的是，意向锁也分为两种： 意向共享锁：事务想要在获得表中某些记录的共享锁，需要在表上先加意向共享锁； 意向互斥锁：事务想要在获得表中某些记录的互斥锁，需要在表上先加意向互斥锁； 随着意向锁的加入，锁类型之间的兼容矩阵也变得愈加复杂： 意向锁其实不会阻塞全表扫描之外的任何请求，它们的主要目的是为了表示是否有人请求锁定表中的某一行数据。 有的人可能会对意向锁的目的并不是完全的理解，我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。 锁的算法到目前为止已经对 InnoDB 中锁的粒度有一定的了解，也清楚了在对数据库进行读写时会获取不同的锁，在这一小节将介绍锁是如何添加到对应的数据行上的，我们会分别介绍三种锁的算法：Record Lock、Gap Lock 和 Next-Key Lock。 Record Lock记录锁（Record Lock）是加到索引记录上的锁，假设我们存在下面的一张表 users： 123456789CREATE TABLE users( id INT NOT NULL AUTO_INCREMENT, last_name VARCHAR(255) NOT NULL, first_name VARCHAR(255), age INT, PRIMARY KEY(id), KEY(last_name), KEY(age)); 如果我们使用 id 或者 last_name 作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 first_name 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。 Gap Lock记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15 的记录，因为整个范围都被间隙锁锁定了。 间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。 虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是其他事务向这个范围中添加新的记录。 Next-Key LockNext-Key 锁相比前两者就稍微有一些复杂，它是记录锁和记录前的间隙锁的结合，在 users 表中有以下记录： 123456789+------|-------------|--------------|-------+| id | last_name | first_name | age ||------|-------------|--------------|-------|| 4 | stark | tony | 21 || 1 | tom | hiddleston | 30 || 3 | morgan | freeman | 40 || 5 | jeff | dean | 50 || 2 | donald | trump | 80 |+------|-------------|--------------|-------+ 如果使用 Next-Key 锁，那么 Next-Key 锁就可以在需要的时候锁定以下的范围： 123456(-∞, 21](21, 30](30, 40](40, 50](50, 80](80, ∞) 既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。 当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条记录后面的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定。 Next-Key 锁的作用其实是为了解决幻读的问题，我们会在下一节谈事务的时候具体介绍。 死锁的发生既然 InnoDB 中实现的锁是悲观的，那么不同事务之间就可能会互相等待对方释放锁造成死锁，最终导致事务发生错误；想要在 MySQL 中制造死锁的问题其实非常容易： 两个会话都持有一个锁，并且尝试获取对方的锁时就会发生死锁，不过 MySQL 也能在发生死锁时及时发现问题，并保证其中的一个事务能够正常工作，这对我们来说也是一个好消息。 事务与隔离级别在介绍了锁之后，我们再来谈谈数据库中一个非常重要的概念 —— 事务；相信只要是一个合格的软件工程师就对事务的特性有所了解，其中被人经常提起的就是事务的原子性，在数据提交工作时，要么保证所有的修改都能够提交，要么就所有的修改全部回滚。 但是事务还遵循包括原子性在内的 ACID 四大特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）；文章不会对这四大特性全部展开进行介绍，相信你能够通过 Google 和数据库相关的书籍轻松获得有关它们的概念，本文最后要介绍的就是事务的四种隔离级别。 几种隔离级别事务的隔离性是数据库处理数据的几大基础之一，而隔离级别其实就是提供给用户用于在性能和可靠性做出选择和权衡的配置项。 ISO 和 ANIS SQL 标准制定了四种事务隔离级别，而 InnoDB 遵循了 SQL:1992 标准中的四种隔离级别：READ UNCOMMITED、READ COMMITED、REPEATABLE READ 和 SERIALIZABLE；每个事务的隔离级别其实都比上一级多解决了一个问题： RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）； READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）； REPEATABLE READ：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）； SERIALIZABLE：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题； MySQL 中默认的事务隔离级别就是 REPEATABLE READ，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。 接下来，我们将数据库中创建如下的表并通过个例子来展示在不同的事务隔离级别之下，会发生什么样的问题： 1234CREATE TABLE test( id INT NOT NULL, UNIQUE(id)); 脏读 在一个事务中，读取了其他事务未提交的数据。 当事务的隔离级别为 READ UNCOMMITED 时，我们在 SESSION 2 中插入的未提交数据在 SESSION 1 中是可以访问的。 不可重复读 在一个事务中，同一行记录被访问了两次却得到了不同的结果。 当事务的隔离级别为 READ COMMITED 时，虽然解决了脏读的问题，但是如果在 SESSION 1 先查询了一行数据，在这之后 SESSION 2 中修改了同一行数据并且提交了修改，在这时，如果 SESSION 1 中再次使用相同的查询语句，就会发现两次查询的结果不一样。 不可重复读的原因就是，在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录。 幻读 在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。 重新开启了两个会话 SESSION 1 和 SESSION 2，在 SESSION 1 中我们查询全表的信息，没有得到任何记录；在 SESSION 2 中向表中插入一条数据并提交；由于 REPEATABLE READ 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。 这种现象在数据库中就被称作幻读，虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样。 在标准的事务隔离级别中，幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决： REPEATABLE READ 和 READ UNCOMMITED 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 REPEATABLE READ 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决。 总结 文章中的内容大都来自于 高性能 MySQL、MySQL 技术内幕：InnoDB 存储引擎、数据库索引设计与优化 以及 MySQL 的 官方文档。 由于篇幅所限仅能对数据库中一些重要内容进行简单的介绍和总结，文中内容难免有所疏漏，如果对文章内容的有疑问，可以在博客下面评论留言。 Reference mysqld_safe version different than mysqld? File Space Management Externally Stored Fields in InnoDB InnoDB Record Structure InnoDB Page Structure Difference between clustered and nonclustered index InnoDB Locking 乐观锁与悲观锁的区别 Optimistic concurrency control MySQL 四种事务隔离级的说明 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列2：Java自动拆装箱里隐藏的秘密]]></title>
    <url>%2F2019%2F09%2F02%2F2Java%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 [TOC] Java 基本数据类型变量就是申请内存来存储值。也就是说，当创建变量的时候，需要在内存中申请空间。 内存管理系统根据变量的类型为变量分配存储空间，分配的空间只能用来储存该类型数据。 因此，通过定义不同类型的变量，可以在内存中储存整数、小数或者字符。 Java 的两大数据类型: 内置数据类型 引用数据类型 * 内置数据类型Java语言提供了八种基本类型。六种数字类型（四个整数型，两个浮点型），一种字符类型，还有一种布尔型。 byte： byte 数据类型是8位、有符号的，以二进制补码表示的整数； 最小值是 -128（-2^7）； 最大值是 127（2^7-1）； 默认值是 0； byte 类型用在大型数组中节约空间，主要代替整数，因为 byte 变量占用的空间只有 int 类型的四分之一； 例子：byte a = 100，byte b = -50。 short： short 数据类型是 16 位、有符号的以二进制补码表示的整数 最小值是 -32768（-2^15）； 最大值是 32767（2^15 - 1）； Short 数据类型也可以像 byte 那样节省空间。一个short变量是int型变量所占空间的二分之一； 默认值是 0； 例子：short s = 1000，short r = -20000。 int： int 数据类型是32位、有符号的以二进制补码表示的整数； 最小值是 -2,147,483,648（-2^31）； 最大值是 2,147,483,647（2^31 - 1）； 一般地整型变量默认为 int 类型； 默认值是 0 ； 例子：int a = 100000, int b = -200000。 long： long 数据类型是 64 位、有符号的以二进制补码表示的整数； 最小值是 -9,223,372,036,854,775,808（-2^63）； 最大值是 9,223,372,036,854,775,807（2^63 -1）； 这种类型主要使用在需要比较大整数的系统上； 默认值是 0L； 例子： long a = 100000L，Long b = -200000L。“L”理论上不分大小写，但是若写成”l”容易与数字”1”混淆，不容易分辩。所以最好大写。 float： float 数据类型是单精度、32位、符合IEEE 754标准的浮点数； float 在储存大型浮点数组的时候可节省内存空间； 默认值是 0.0f； 浮点数不能用来表示精确的值，如货币； 例子：float f1 = 234.5f。 double： double 数据类型是双精度、64 位、符合IEEE 754标准的浮点数； 浮点数的默认类型为double类型； double类型同样不能表示精确的值，如货币； 默认值是 0.0d； 例子：double d1 = 123.4。 boolean： boolean数据类型表示一位的信息； 只有两个取值：true 和 false； 这种类型只作为一种标志来记录 true/false 情况； 默认值是 false； 例子：boolean one = true。 char： char类型是一个单一的 16 位 Unicode 字符； 最小值是 \u0000（即为0）； 最大值是 \uffff（即为65,535）； char 数据类型可以储存任何字符； 例子：char letter = ‘A’;。 123456789101112131415161718192021222324//8位byte bx = Byte.MAX_VALUE;byte bn = Byte.MIN_VALUE;//16位short sx = Short.MAX_VALUE;short sn = Short.MIN_VALUE;//32位int ix = Integer.MAX_VALUE;int in = Integer.MIN_VALUE;//64位long lx = Long.MAX_VALUE;long ln = Long.MIN_VALUE;//32位float fx = Float.MAX_VALUE;float fn = Float.MIN_VALUE;//64位double dx = Double.MAX_VALUE;double dn = Double.MIN_VALUE;//16位char cx = Character.MAX_VALUE;char cn = Character.MIN_VALUE;//1位boolean bt = Boolean.TRUE;boolean bf = Boolean.FALSE; `127` `-128` `32767` `-32768` `2147483647` `-2147483648` `9223372036854775807` `-9223372036854775808` `3.4028235E38` `1.4E-45` `1.7976931348623157E308` `4.9E-324` `￿` `true` `false`引用类型 在Java中，引用类型的变量非常类似于C/C++的指针。引用类型指向一个对象，指向对象的变量是引用变量。这些变量在声明时被指定为一个特定的类型，比如 Employee、Puppy 等。变量一旦声明后，类型就不能被改变了。 对象、数组都是引用数据类型。 所有引用类型的默认值都是null。 一个引用变量可以用来引用任何与之兼容的类型。 例子：Site site = new Site(“Runoob”)。 Java 常量常量在程序运行时是不能被修改的。 在 Java 中使用 final 关键字来修饰常量，声明方式和变量类似： 1final double PI = 3.1415927; 虽然常量名也可以用小写，但为了便于识别，通常使用大写字母表示常量。 字面量可以赋给任何内置类型的变量。例如： 12byte a = 68;char a = &apos;A&apos; 自动拆箱和装箱（详解）Java 5增加了自动装箱与自动拆箱机制，方便基本类型与包装类型的相互转换操作。在Java 5之前，如果要将一个int型的值转换成对应的包装器类型Integer，必须显式的使用new创建一个新的Integer对象，或者调用静态方法Integer.valueOf()。 //在Java 5之前，只能这样做 Integer value = new Integer(10); //或者这样做 Integer value = Integer.valueOf(10); //直接赋值是错误的 //Integer value = 10;`在Java 5中，可以直接将整型赋给Integer对象，由编译器来完成从int型到Integer类型的转换，这就叫自动装箱。 `//在Java 5中，直接赋值是合法的，由编译器来完成转换` `Integer value = 10;` `与此对应的，自动拆箱就是可以将包装类型转换为基本类型，具体的转换工作由编译器来完成。` `//在Java 5 中可以直接这么做` `Integer value = new Integer(10);` `int i = value;`自动装箱与自动拆箱为程序员提供了很大的方便，而在实际的应用中，自动装箱与拆箱也是使用最广泛的特性之一。自动装箱和自动拆箱其实是Java编译器提供的一颗语法糖（语法糖是指在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是更方便程序员使用。通过可提高开发效率，增加代码可读性，增加代码的安全性）。 实现在八种包装类型中，每一种包装类型都提供了两个方法： 静态方法valueOf(基本类型)：将给定的基本类型转换成对应的包装类型； 实例方法xxxValue()：将具体的包装类型对象转换成基本类型；下面我们以int和Integer为例，说明Java中自动装箱与自动拆箱的实现机制。看如下代码： class Auto //code1 { public static void main(String[] args) { //自动装箱 Integer inte = 10; //自动拆箱 int i = inte; //再double和Double来验证一下 Double doub = 12.40; double d = doub; } }上面的代码先将int型转为Integer对象，再讲Integer对象转换为int型，毫无疑问，这是可以正确运行的。可是，这种转换是怎么进行的呢？使用反编译工具，将生成的Class文件在反编译为Java文件，让我们看看发生了什么： class Auto//code2 { public static void main(String[] paramArrayOfString) { Integer localInteger = Integer.valueOf(10);​​ int i = localInteger.intValue(); ​​​ Double localDouble = Double.valueOf(12.4D);​ double d = localDouble.doubleValue();​ } } 我们可以看到经过javac编译之后，code1的代码被转换成了code2，实际运行时，虚拟机运行的就是code2的代码。也就是说，虚拟机根本不知道有自动拆箱和自动装箱这回事；在将Java源文件编译为class文件的过程中，javac编译器在自动装箱的时候，调用了Integer.valueOf()方法，在自动拆箱时，又调用了intValue()方法。我们可以看到，double和Double也是如此。实现总结：其实自动装箱和自动封箱是编译器为我们提供的一颗语法糖。在自动装箱时，编译器调用包装类型的valueOf()方法；在自动拆箱时，编译器调用了相应的xxxValue()方法。 自动装箱与拆箱中的“坑”在使用自动装箱与自动拆箱时，要注意一些陷阱，为了避免这些陷阱，我们有必要去看一下各种包装类型的源码。 Integer源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public final class Integer extends Number implements Comparable&lt;Integer&gt; &#123; private final int value; /*Integer的构造方法，接受一个整型参数,Integer对象表示的int值，保存在value中*/ public Integer(int value) &#123; this.value = value; &#125; /*equals()方法判断的是:所代表的int型的值是否相等*/ public boolean equals(Object obj) &#123; if (obj instanceof Integer) &#123; return value == ((Integer)obj).intValue(); &#125; return false;&#125; /*返回这个Integer对象代表的int值，也就是保存在value中的值*/ public int intValue() &#123; return value; &#125; /** * 首先会判断i是否在[IntegerCache.low,Integer.high]之间 * 如果是，直接返回Integer.cache中相应的元素 * 否则，调用构造方法，创建一个新的Integer对象 */ public static Integer valueOf(int i) &#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125;/** * 静态内部类，缓存了从[low,high]对应的Integer对象 * low -128这个值不会被改变 * high 默认是127，可以改变，最大不超过：Integer.MAX_VALUE - (-low) -1 * cache 保存从[low,high]对象的Integer对象 */ private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); &#125; private IntegerCache() &#123;&#125;&#125; 以上是Oracle(Sun)公司JDK 1.7中Integer源码的一部分，通过分析上面的代码，得到： 1）Integer有一个实例域value，它保存了这个Integer所代表的int型的值，且它是final的，也就是说这个Integer对象一经构造完成，它所代表的值就不能再被改变。 2）Integer重写了equals()方法，它通过比较两个Integer对象的value，来判断是否相等。 3）重点是静态内部类IntegerCache，通过类名就可以发现：它是用来缓存数据的。它有一个数组，里面保存的是连续的Integer对象。 (a) low：代表缓存数据中最小的值，固定是-128。 (b) high：代表缓存数据中最大的值，它可以被该改变，默认是127。high最小是127，最大是Integer.MAX_VALUE-(-low)-1，如果high超过了这个值，那么cache[ ]的长度就超过Integer.MAX_VALUE了，也就溢出了。 (c) cache[]：里面保存着从[low,high]所对应的Integer对象，长度是high-low+1(因为有元素0，所以要加1)。 4）调用valueOf(inti)方法时，首先判断i是否在[low,high]之间，如果是，则复用Integer.cache[i-low]。比如，如果Integer.valueOf(3)，直接返回Integer.cache[131]；如果i不在这个范围，则调用构造方法，构造出一个新的Integer对象。 5）调用intValue()，直接返回value的值。通过3）和4）可以发现，默认情况下，在使用自动装箱时，VM会复用[-128,127]之间的Integer对象。 Integer a1 = 1; Integer a2 = 1; Integer a3 = new Integer(1); //会打印true，因为a1和a2是同一个对象,都是Integer.cache[129] System.out.println(a1 == a2); //false，a3构造了一个新的对象，不同于a1,a2 System.out.println(a1 == a3);了解基本类型缓存（常量池）的最佳实践1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071//基本数据类型的常量池是-128到127之间。// 在这个范围中的基本数据类的包装类可以自动拆箱，比较时直接比较数值大小。public static void main(String[] args) &#123; //int的自动拆箱和装箱只在-128到127范围中进行，超过该范围的两个integer的 == 判断是会返回false的。 Integer a1 = 128; Integer a2 = -128; Integer a3 = -128; Integer a4 = 128; System.out.println(a1 == a4); System.out.println(a2 == a3); Byte b1 = 127; Byte b2 = 127; Byte b3 = -128; Byte b4 = -128; //byte都是相等的，因为范围就在-128到127之间 System.out.println(b1 == b2); System.out.println(b3 == b4); // Long c1 = 128L; Long c2 = 128L; Long c3 = -128L; Long c4 = -128L; System.out.println(c1 == c2); System.out.println(c3 == c4); //char没有负值 //发现char也是在0到127之间自动拆箱 Character d1 = 128; Character d2 = 128; Character d3 = 127; Character d4 = 127; System.out.println(d1 == d2); System.out.println(d3 == d4); `结果` `false` `true` `true` `true` `false` `true` `false` `true` Integer i = 10; Byte b = 10; //比较Byte和Integer.两个对象无法直接比较，报错 //System.out.println(i == b); System.out.println(&quot;i == b &quot; + i.equals(b)); //答案是false,因为包装类的比较时先比较是否是同一个类，不是的话直接返回false. int ii = 128; short ss = 128; long ll = 128; char cc = 128; System.out.println(&quot;ii == bb &quot; + (ii == ss)); System.out.println(&quot;ii == ll &quot; + (ii == ll)); System.out.println(&quot;ii == cc &quot; + (ii == cc)); 结果 i == b false ii == bb true ii == ll true ii == cc true //这时候都是true，因为基本数据类型直接比较值，值一样就可以。 总结：通过上面的代码，我们分析一下自动装箱与拆箱发生的时机： （1）当需要一个对象的时候会自动装箱，比如Integer a = 10;equals(Object o)方法的参数是Object对象，所以需要装箱。 （2）当需要一个基本类型时会自动拆箱，比如int a = new Integer(10);算术运算是在基本类型间进行的，所以当遇到算术运算时会自动拆箱，比如代码中的 c == (a + b); （3） 包装类型 == 基本类型时，包装类型自动拆箱； 需要注意的是：“==”在没遇到算术运算时，不会自动拆箱；基本类型只会自动装箱为对应的包装类型，代码中最后一条说明的内容。 在JDK 1.5中提供了自动装箱与自动拆箱，这其实是Java 编译器的语法糖，编译器通过调用包装类型的valueOf()方法实现自动装箱，调用xxxValue()方法自动拆箱。自动装箱和拆箱会有一些陷阱，那就是包装类型复用了某些对象。 （1）Integer默认复用了[-128,127]这些对象，其中高位置可以修改； （2）Byte复用了全部256个对象[-128,127]； （3）Short服用了[-128,127]这些对象； （4）Long服用了[-128,127]; （5）Character复用了[0,127],Charater不能表示负数; Double和Float是连续不可数的，所以没法复用对象，也就不存在自动装箱复用陷阱。 Boolean没有自动装箱与拆箱，它也复用了Boolean.TRUE和Boolean.FALSE，通过Boolean.valueOf(boolean b)返回的Blooean对象要么是TRUE，要么是FALSE，这点也要注意。 本文介绍了“真实的”自动装箱与拆箱，为了避免写出错误的代码，又从包装类型的源码入手，指出了各种包装类型在自动装箱和拆箱时存在的陷阱，同时指出了自动装箱与拆箱发生的时机。 基本数据类型的存储方式上面自动拆箱和装箱的原理其实与常量池有关。 存在栈中public void(int a){int i = 1;int j = 1;}方法中的i 存在虚拟机栈的局部变量表里，i是一个引用，j也是一个引用，它们都指向局部变量表里的整型值 1.int a是传值引用，所以a也会存在局部变量表。 存在堆里class A{int i = 1;A a = new A();}i是类的成员变量。类实例化的对象存在堆中，所以成员变量也存在堆中，引用a存的是对象的地址，引用i存的是值，这个值1也会存在堆中。可以理解为引用i指向了这个值1。也可以理解为i就是1. 3 包装类对象怎么存其实我们说的常量池也可以叫对象池。比如String a= new String(“a”).intern()时会先在常量池找是否有“a”对象如果有的话直接返回“a”对象在常量池的地址，即让引用a指向常量”a”对象的内存地址。public native String intern();Integer也是同理。 下图是Integer类型在常量池中查找同值对象的方法。 1234567891011121314151617181920212223242526272829303132333435363738public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125;private static class IntegerCache &#123; static final int low = -128; static final int high; static final Integer cache[]; static &#123; // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(&quot;java.lang.Integer.IntegerCache.high&quot;); if (integerCacheHighPropValue != null) &#123; try &#123; int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); &#125; catch( NumberFormatException nfe) &#123; // If the property cannot be parsed into an int, ignore it. &#125; &#125; high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; &#125; private IntegerCache() &#123;&#125;&#125; 所以基本数据类型的包装类型可以在常量池查找对应值的对象，找不到就会自动在常量池创建该值的对象。 而String类型可以通过intern来完成这个操作。 JDK1.7后，常量池被放入到堆空间中，这导致intern()函数的功能不同，具体怎么个不同法，且看看下面代码，这个例子是网上流传较广的一个例子，分析图也是直接粘贴过来的，这里我会用自己的理解去解释这个例子： 123456789101112131415[java] view plain copyString s = new String(&quot;1&quot;); s.intern(); String s2 = &quot;1&quot;; System.out.println(s == s2); String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;); s3.intern(); String s4 = &quot;11&quot;; System.out.println(s3 == s4); 输出结果为：[java] view plain copyJDK1.6以及以下：false false JDK1.7以及以上：false true JDK1.6查找到常量池存在相同值的对象时会直接返回该对象的地址。 JDK 1.7后，intern方法还是会先去查询常量池中是否有已经存在，如果存在，则返回常量池中的引用，这一点与之前没有区别，区别在于，如果在常量池找不到对应的字符串，则不会再将字符串拷贝到常量池，而只是在常量池中生成一个对原字符串的引用。 那么其他字符串在常量池找值时就会返回另一个堆中对象的地址。 下一节详细介绍String以及相关包装类。 具体请见：https://blog.csdn.net/a724888/article/details/80042298 关于Java面向对象三大特性，请参考： https://blog.csdn.net/a724888/article/details/80033043 参考文章https://www.runoob.com/java/java-basic-datatypes.html https://www.cnblogs.com/zch1126/p/5335139.html https://blog.csdn.net/jreffchen/article/details/81015884 https://blog.csdn.net/yuhongye111/article/details/31850779 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java基本数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库2：『浅入浅出』MySQL 和 InnoDB]]></title>
    <url>%2F2019%2F09%2F02%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%932%EF%BC%9A%E3%80%8E%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA%E3%80%8FMySQL%20%E5%92%8C%20InnoDB%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 作为一名开发人员，在日常的工作中会难以避免地接触到数据库，无论是基于文件的 sqlite 还是工程上使用非常广泛的 MySQL、PostgreSQL，但是一直以来也没有对数据库有一个非常清晰并且成体系的认知，所以最近两个月的时间看了几本数据库相关的书籍并且阅读了 MySQL 的官方文档，希望对各位了解数据库的、不了解数据库的有所帮助。 本文中对于数据库的介绍以及研究都是在 MySQL 上进行的，如果涉及到了其他数据库的内容或者实现会在文中单独指出。 数据库的定义很多开发者在最开始时其实都对数据库有一个比较模糊的认识，觉得数据库就是一堆数据的集合，但是实际却比这复杂的多，数据库领域中有两个词非常容易混淆，也就是_数据库和实例_： 数据库：物理操作文件系统或其他形式文件类型的集合； 实例：MySQL 数据库由后台线程以及一个共享内存区组成； 对于数据库和实例的定义都来自于 MySQL 技术内幕：InnoDB 存储引擎 一书，想要了解 InnoDB 存储引擎的读者可以阅读这本书籍。 数据库和实例在 MySQL 中，实例和数据库往往都是一一对应的，而我们也无法直接操作数据库，而是要通过数据库实例来操作数据库文件，可以理解为数据库实例是数据库为上层提供的一个专门用于操作的接口。 在 Unix 上，启动一个 MySQL 实例往往会产生两个进程，mysqld 就是真正的数据库服务守护进程，而 mysqld_safe 是一个用于检查和设置 mysqld 启动的控制程序，它负责监控 MySQL 进程的执行，当 mysqld 发生错误时，mysqld_safe 会对其状态进行检查并在合适的条件下重启。 MySQL 的架构MySQL 从第一个版本发布到现在已经有了 20 多年的历史，在这么多年的发展和演变中，整个应用的体系结构变得越来越复杂： 最上层用于连接、线程处理的部分并不是 MySQL 『发明』的，很多服务都有类似的组成部分；第二层中包含了大多数 MySQL 的核心服务，包括了对 SQL 的解析、分析、优化和缓存等功能，存储过程、触发器和视图都是在这里实现的；而第三层就是 MySQL 中真正负责数据的存储和提取的存储引擎，例如：InnoDB、MyISAM 等，文中对存储引擎的介绍都是对 InnoDB 实现的分析。 数据的存储在整个数据库体系结构中，我们可以使用不同的存储引擎来存储数据，而绝大多数存储引擎都以二进制的形式存储数据；这一节会介绍 InnoDB 中对数据是如何存储的。 在 InnoDB 存储引擎中，所有的数据都被逻辑地存放在表空间中，表空间（tablespace）是存储引擎中最高的存储逻辑单位，在表空间的下面又包括段（segment）、区（extent）、页（page）： 同一个数据库实例的所有表空间都有相同的页大小；默认情况下，表空间中的页大小都为 16KB，当然也可以通过改变 innodb_page_size 选项对默认大小进行修改，需要注意的是不同的页大小最终也会导致区大小的不同： 从图中可以看出，在 InnoDB 存储引擎中，一个区的大小最小为 1MB，页的数量最少为 64 个。 如何存储表MySQL 使用 InnoDB 存储表时，会将表的定义和数据索引等信息分开存储，其中前者存储在 .frm 文件中，后者存储在 .ibd 文件中，这一节就会对这两种不同的文件分别进行介绍。 .frm 文件 无论在 MySQL 中选择了哪个存储引擎，所有的 MySQL 表都会在硬盘上创建一个 .frm 文件用来描述表的格式或者说定义；.frm 文件的格式在不同的平台上都是相同的。 1CREATE TABLE test_frm( column1 CHAR(5), column2 INTEGER); 当我们使用上面的代码创建表时，会在磁盘上的 datadir 文件夹中生成一个 test_frm.frm 的文件，这个文件中就包含了表结构相关的信息： MySQL 官方文档中的 11.1 MySQL .frm File Format 一文对于 .frm文件格式中的二进制的内容有着非常详细的表述，在这里就不展开介绍了。 .ibd 文件 InnoDB 中用于存储数据的文件总共有两个部分，一是系统表空间文件，包括 ibdata1、ibdata2 等文件，其中存储了 InnoDB 系统信息和用户数据库表数据和索引，是所有表公用的。 当打开 innodb_file_per_table 选项时，.ibd 文件就是每一个表独有的表空间，文件存储了当前表的数据和相关的索引数据。 如何存储记录与现有的大多数存储引擎一样，InnoDB 使用页作为磁盘管理的最小单位；数据在 InnoDB 存储引擎中都是按行存储的，每个 16KB 大小的页中可以存放 2-200 行的记录。 当 InnoDB 存储数据时，它可以使用不同的行格式进行存储；MySQL 5.7 版本支持以下格式的行存储方式： Antelope 是 InnoDB 最开始支持的文件格式，它包含两种行格式 Compact 和 Redundant，它最开始并没有名字；Antelope 的名字是在新的文件格式 Barracuda 出现后才起的，Barracuda 的出现引入了两种新的行格式 Compressed 和 Dynamic；InnoDB 对于文件格式都会向前兼容，而官方文档中也对之后会出现的新文件格式预先定义好了名字：Cheetah、Dragon、Elk 等等。 两种行记录格式 Compact 和 Redundant 在磁盘上按照以下方式存储： Compact 和 Redundant 格式最大的不同就是记录格式的第一个部分；在 Compact 中，行记录的第一部分倒序存放了一行数据中列的长度（Length），而 Redundant 中存的是每一列的偏移量（Offset），从总体上上看，Compact 行记录格式相比 Redundant 格式能够减少 20% 的存储空间。 行溢出数据 当 InnoDB 使用 Compact 或者 Redundant 格式存储极长的 VARCHAR 或者 BLOB 这类大对象时，我们并不会直接将所有的内容都存放在数据页节点中，而是将行数据中的前 768 个字节存储在数据页中，后面会通过偏移量指向溢出页。 但是当我们使用新的行记录格式 Compressed 或者 Dynamic 时都只会在行记录中保存 20 个字节的指针，实际的数据都会存放在溢出页面中。 当然在实际存储中，可能会对不同长度的 TEXT 和 BLOB 列进行优化，不过这就不是本文关注的重点了。 想要了解更多与 InnoDB 存储引擎中记录的数据格式的相关信息，可以阅读 InnoDB Record Structure 数据页结构页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分： 每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。 在页的头部和尾部之间就是用户记录和空闲空间了，每一个数据页中都包含 Infimum 和 Supremum 这两个虚拟的记录（可以理解为占位符），Infimum 记录是比该页中任何主键值都要小的值，Supremum 是该页中的最大值： User Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 next_record 这一指针控制的。 B+ 树在查找对应的记录时，并不会直接从树中找出对应的行记录，它只能获取记录所在的页，将整个页加载到内存中，再通过 Page Directory 中存储的稀疏索引和 n_owned、next_record 属性取出对应的记录，不过因为这一操作是在内存中进行的，所以通常会忽略这部分查找的耗时。 InnoDB 存储引擎中对数据的存储是一个非常复杂的话题，这一节中也只是对表、行记录以及页面的存储进行一定的分析和介绍，虽然作者相信这部分知识对于大部分开发者已经足够了，但是想要真正消化这部分内容还需要很多的努力和实践。 索引索引是数据库中非常非常重要的概念，它是存储引擎能够快速定位记录的秘密武器，对于提升数据库的性能、减轻数据库服务器的负担有着非常重要的作用；索引优化是对查询性能优化的最有效手段，它能够轻松地将查询的性能提高几个数量级。 索引的数据结构在上一节中，我们谈了行记录的存储和页的存储，在这里我们就要从更高的层面看 InnoDB 中对于数据是如何存储的；InnoDB 存储引擎在绝大多数情况下使用 B+ 树建立索引，这是关系型数据库中查找最为常用和有效的索引，但是 B+ 树索引并不能找到一个给定键对应的具体值，它只能找到数据行对应的页，然后正如上一节所提到的，数据库把整个页读入到内存中，并在内存中查找具体的数据行。 B+ 树是平衡树，它查找任意节点所耗费的时间都是完全相同的，比较的次数就是 B+ 树的高度；在这里，我们并不会深入分析或者动手实现一个 B+ 树，只是对它的特性进行简单的介绍。 聚集索引和辅助索引数据库中的 B+ 树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），它们之间的最大区别就是，聚集索引中存放着一条行记录的全部信息，而辅助索引中只包含索引列和一个用于查找对应行记录的『书签』。 聚集索引 InnoDB 存储引擎中的表都是使用索引组织的，也就是按照键的顺序存放；聚集索引就是按照表中主键的顺序构建一颗 B+ 树，并在叶节点中存放表中的行记录数据。 1CREATE TABLE users( id INT NOT NULL, first_name VARCHAR(20) NOT NULL, last_name VARCHAR(20) NOT NULL, age INT NOT NULL, PRIMARY KEY(id), KEY(last_name, first_name, age) KEY(first_name)); 如果使用上面的 SQL 在数据库中创建一张表，B+ 树就会使用 id 作为索引的键，并在叶子节点中存储一条记录中的所有信息。 图中对 B+ 树的描述与真实情况下 B+ 树中的数据结构有一些差别，不过这里想要表达的主要意思是：聚集索引叶节点中保存的是整条行记录，而不是其中的一部分。 聚集索引与表的物理存储方式有着非常密切的关系，所有正常的表应该有且仅有一个聚集索引（绝大多数情况下都是主键），表中的所有行记录数据都是按照聚集索引的顺序存放的。 当我们使用聚集索引对表中的数据进行检索时，可以直接获得聚集索引所对应的整条行记录数据所在的页，不需要进行第二次操作。 辅助索引 数据库将所有的非聚集索引都划分为辅助索引，但是这个概念对我们理解辅助索引并没有什么帮助；辅助索引也是通过 B+ 树实现的，但是它的叶节点并不包含行记录的全部数据，仅包含索引中的所有键和一个用于查找对应行记录的『书签』，在 InnoDB 中这个书签就是当前记录的主键。 辅助索引的存在并不会影响聚集索引，因为聚集索引构成的 B+ 树是数据实际存储的形式，而辅助索引只用于加速数据的查找，所以一张表上往往有多个辅助索引以此来提升数据库的性能。 一张表一定包含一个聚集索引构成的 B+ 树以及若干辅助索引的构成的 B+ 树。 如果在表 users 中存在一个辅助索引 (first_name, age)，那么它构成的 B+ 树大致就是上图这样，按照 (first_name, age) 的字母顺序对表中的数据进行排序，当查找到主键时，再通过聚集索引获取到整条行记录。 上图展示了一个使用辅助索引查找一条表记录的过程：通过辅助索引查找到对应的主键，最后在聚集索引中使用主键获取对应的行记录，这也是通常情况下行记录的查找方式。 索引的设计索引的设计其实是一个非常重要的内容，同时也是一个非常复杂的内容；索引的设计与创建对于提升数据库的查询性能至关重要，不过这不是本文想要介绍的内容，有关索引的设计与优化可以阅读 数据库索引设计与优化 一书，书中提供了一种非常科学合理的方法能够帮助我们在数据库中建立最适合的索引，当然作者也可能会在之后的文章中对索引的设计进行简单的介绍和分析。 锁我们都知道锁的种类一般分为乐观锁和悲观锁两种，InnoDB 存储引擎中使用的就是悲观锁，而按照锁的粒度划分，也可以分成行锁和表锁。 并发控制机制乐观锁和悲观锁其实都是并发控制的机制，同时它们在原理上就有着本质的差别； 乐观锁是一种思想，它其实并不是一种真正的『锁』，它会先尝试对资源进行修改，在写回时判断资源是否进行了改变，如果没有发生改变就会写回，否则就会进行重试，在整个的执行过程中其实都没有对数据库进行加锁； 悲观锁就是一种真正的锁了，它会在获取资源前对资源进行加锁，确保同一时刻只有有限的线程能够访问该资源，其他想要尝试获取资源的操作都会进入等待状态，直到该线程完成了对资源的操作并且释放了锁后，其他线程才能重新操作资源； 虽然乐观锁和悲观锁在本质上并不是同一种东西，一个是一种思想，另一个是一种真正的锁，但是它们都是一种并发控制机制。 乐观锁不会存在死锁的问题，但是由于更新后验证，所以当冲突频率和重试成本较高时更推荐使用悲观锁，而需要非常高的响应速度并且并发量非常大的时候使用乐观锁就能较好的解决问题，在这时使用悲观锁就可能出现严重的性能问题；在选择并发控制机制时，需要综合考虑上面的四个方面（冲突频率、重试成本、响应速度和并发量）进行选择。 锁的种类对数据的操作其实只有两种，也就是读和写，而数据库在实现锁时，也会对这两种操作使用不同的锁；InnoDB 实现了标准的行级锁，也就是共享锁（Shared Lock）和互斥锁（Exclusive Lock）；共享锁和互斥锁的作用其实非常好理解： 共享锁（读锁）：允许事务对一条行数据进行读取； 互斥锁（写锁）：允许事务对一条行数据进行删除或更新； 而它们的名字也暗示着各自的另外一个特性，共享锁之间是兼容的，而互斥锁与其他任意锁都不兼容： 稍微对它们的使用进行思考就能想明白它们为什么要这么设计，因为共享锁代表了读操作、互斥锁代表了写操作，所以我们可以在数据库中并行读，但是只能串行写，只有这样才能保证不会发生线程竞争，实现线程安全。 锁的粒度无论是共享锁还是互斥锁其实都只是对某一个数据行进行加锁，InnoDB 支持多种粒度的锁，也就是行锁和表锁；为了支持多粒度锁定，InnoDB 存储引擎引入了意向锁（Intention Lock），意向锁就是一种表级锁。 与上一节中提到的两种锁的种类相似的是，意向锁也分为两种： 意向共享锁：事务想要在获得表中某些记录的共享锁，需要在表上先加意向共享锁； 意向互斥锁：事务想要在获得表中某些记录的互斥锁，需要在表上先加意向互斥锁； 随着意向锁的加入，锁类型之间的兼容矩阵也变得愈加复杂： 意向锁其实不会阻塞全表扫描之外的任何请求，它们的主要目的是为了表示是否有人请求锁定表中的某一行数据。 有的人可能会对意向锁的目的并不是完全的理解，我们在这里可以举一个例子：如果没有意向锁，当已经有人使用行锁对表中的某一行进行修改时，如果另外一个请求要对全表进行修改，那么就需要对所有的行是否被锁定进行扫描，在这种情况下，效率是非常低的；不过，在引入意向锁之后，当有人使用行锁对表中的某一行进行修改之前，会先为表添加意向互斥锁（IX），再为行记录添加互斥锁（X），在这时如果有人尝试对全表进行修改就不需要判断表中的每一行数据是否被加锁了，只需要通过等待意向互斥锁被释放就可以了。 锁的算法到目前为止已经对 InnoDB 中锁的粒度有一定的了解，也清楚了在对数据库进行读写时会获取不同的锁，在这一小节将介绍锁是如何添加到对应的数据行上的，我们会分别介绍三种锁的算法：Record Lock、Gap Lock 和 Next-Key Lock。 Record Lock 记录锁（Record Lock）是加到索引记录上的锁，假设我们存在下面的一张表 users： 1CREATE TABLE users( id INT NOT NULL AUTO_INCREMENT, last_name VARCHAR(255) NOT NULL, first_name VARCHAR(255), age INT, PRIMARY KEY(id), KEY(last_name), KEY(age)); 如果我们使用 id 或者 last_name 作为 SQL 中 WHERE 语句的过滤条件，那么 InnoDB 就可以通过索引建立的 B+ 树找到行记录并添加索引，但是如果使用 first_name 作为过滤条件时，由于 InnoDB 不知道待修改的记录具体存放的位置，也无法对将要修改哪条记录提前做出判断就会锁定整个表。 Gap Lock 记录锁是在存储引擎中最为常见的锁，除了记录锁之外，InnoDB 中还存在间隙锁（Gap Lock），间隙锁是对索引记录中的一段连续区域的锁；当使用类似 SELECT * FROM users WHERE id BETWEEN 10 AND 20 FOR UPDATE; 的 SQL 语句时，就会阻止其他事务向表中插入 id = 15 的记录，因为整个范围都被间隙锁锁定了。 间隙锁是存储引擎对于性能和并发做出的权衡，并且只用于某些事务隔离级别。 虽然间隙锁中也分为共享锁和互斥锁，不过它们之间并不是互斥的，也就是不同的事务可以同时持有一段相同范围的共享锁和互斥锁，它唯一阻止的就是其他事务向这个范围中添加新的记录。 Next-Key Lock Next-Key 锁相比前两者就稍微有一些复杂，它是记录锁和记录前的间隙锁的结合，在 users 表中有以下记录： 1+------|-------------|--------------|-------+| id | last_name | first_name | age ||------|-------------|--------------|-------|| 4 | stark | tony | 21 || 1 | tom | hiddleston | 30 || 3 | morgan | freeman | 40 || 5 | jeff | dean | 50 || 2 | donald | trump | 80 |+------|-------------|--------------|-------+ 如果使用 Next-Key 锁，那么 Next-Key 锁就可以在需要的时候锁定以下的范围： 1(-∞, 21](21, 30](30, 40](40, 50](50, 80](80, ∞) 既然叫 Next-Key 锁，锁定的应该是当前值和后面的范围，但是实际上却不是，Next-Key 锁锁定的是当前值和前面的范围。 当我们更新一条记录，比如 SELECT * FROM users WHERE age = 30 FOR UPDATE;，InnoDB 不仅会在范围 (21, 30] 上加 Next-Key 锁，还会在这条记录后面的范围 (30, 40] 加间隙锁，所以插入 (21, 40] 范围内的记录都会被锁定。 Next-Key 锁的作用其实是为了解决幻读的问题，我们会在下一节谈事务的时候具体介绍。 死锁的发生既然 InnoDB 中实现的锁是悲观的，那么不同事务之间就可能会互相等待对方释放锁造成死锁，最终导致事务发生错误；想要在 MySQL 中制造死锁的问题其实非常容易： 两个会话都持有一个锁，并且尝试获取对方的锁时就会发生死锁，不过 MySQL 也能在发生死锁时及时发现问题，并保证其中的一个事务能够正常工作，这对我们来说也是一个好消息。 事务与隔离级别在介绍了锁之后，我们再来谈谈数据库中一个非常重要的概念 —— 事务；相信只要是一个合格的软件工程师就对事务的特性有所了解，其中被人经常提起的就是事务的原子性，在数据提交工作时，要么保证所有的修改都能够提交，要么就所有的修改全部回滚。 但是事务还遵循包括原子性在内的 ACID 四大特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）；文章不会对这四大特性全部展开进行介绍，相信你能够通过 Google 和数据库相关的书籍轻松获得有关它们的概念，本文最后要介绍的就是事务的四种隔离级别。 几种隔离级别事务的隔离性是数据库处理数据的几大基础之一，而隔离级别其实就是提供给用户用于在性能和可靠性做出选择和权衡的配置项。 ISO 和 ANIS SQL 标准制定了四种事务隔离级别，而 InnoDB 遵循了 SQL:1992 标准中的四种隔离级别：READ UNCOMMITED、READ COMMITED、REPEATABLE READ 和 SERIALIZABLE；每个事务的隔离级别其实都比上一级多解决了一个问题： RAED UNCOMMITED：使用查询语句不会加锁，可能会读到未提交的行（Dirty Read）； READ COMMITED：只对记录加记录锁，而不会在记录之间加间隙锁，所以允许新的记录插入到被锁定记录的附近，所以再多次使用查询语句时，可能得到不同的结果（Non-Repeatable Read）； REPEATABLE READ：多次读取同一范围的数据会返回第一次查询的快照，不会返回不同的数据行，但是可能发生幻读（Phantom Read）； SERIALIZABLE：InnoDB 隐式地将全部的查询语句加上共享锁，解决了幻读的问题； MySQL 中默认的事务隔离级别就是 REPEATABLE READ，但是它通过 Next-Key 锁也能够在某种程度上解决幻读的问题。 接下来，我们将数据库中创建如下的表并通过个例子来展示在不同的事务隔离级别之下，会发生什么样的问题： 1CREATE TABLE test( id INT NOT NULL, UNIQUE(id)); 脏读 在一个事务中，读取了其他事务未提交的数据。 当事务的隔离级别为 READ UNCOMMITED 时，我们在 SESSION 2 中插入的未提交数据在 SESSION 1 中是可以访问的。 不可重复读 在一个事务中，同一行记录被访问了两次却得到了不同的结果。 当事务的隔离级别为 READ COMMITED 时，虽然解决了脏读的问题，但是如果在 SESSION 1 先查询了一行数据，在这之后 SESSION 2 中修改了同一行数据并且提交了修改，在这时，如果 SESSION 1 中再次使用相同的查询语句，就会发现两次查询的结果不一样。 不可重复读的原因就是，在 READ COMMITED 的隔离级别下，存储引擎不会在查询记录时添加行锁，锁定 id = 3 这条记录。 幻读 在一个事务中，同一个范围内的记录被读取时，其他事务向这个范围添加了新的记录。 重新开启了两个会话 SESSION 1 和 SESSION 2，在 SESSION 1 中我们查询全表的信息，没有得到任何记录；在 SESSION 2 中向表中插入一条数据并提交；由于 REPEATABLE READ 的原因，再次查询全表的数据时，我们获得到的仍然是空集，但是在向表中插入同样的数据却出现了错误。 这种现象在数据库中就被称作幻读，虽然我们使用查询语句得到了一个空的集合，但是插入数据时却得到了错误，好像之前的查询是幻觉一样。 在标准的事务隔离级别中，幻读是由更高的隔离级别 SERIALIZABLE 解决的，但是它也可以通过 MySQL 提供的 Next-Key 锁解决： REPERATABLE READ 和 READ UNCOMMITED 其实是矛盾的，如果保证了前者就看不到已经提交的事务，如果保证了后者，就会导致两次查询的结果不同，MySQL 为我们提供了一种折中的方式，能够在 REPERATABLE READ 模式下加锁访问已经提交的数据，其本身并不能解决幻读的问题，而是通过文章前面提到的 Next-Key 锁来解决。 总结 文章中的内容大都来自于 高性能 MySQL、MySQL 技术内幕：InnoDB 存储引擎、数据库索引设计与优化 以及 MySQL 的 官方文档。 由于篇幅所限仅能对数据库中一些重要内容进行简单的介绍和总结，文中内容难免有所疏漏，如果对文章内容的有疑问，可以在博客下面评论留言。 Innodb与Myisam引擎的区别与应用场景1. 区别： （1）事务处理： MyISAM是非事务安全型的，而InnoDB是事务安全型的（支持事务处理等高级处理）； （2）锁机制不同： MyISAM是表级锁，而InnoDB是行级锁； （3）select ,update ,insert ,delete 操作： MyISAM：如果执行大量的SELECT，MyISAM是更好的选择 InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表 （4）查询表的行数不同： MyISAM：select count() from table,MyISAM只要简单的读出保存好的行数，注意的是，当count()语句包含 where条件时，两种表的操作是一样的 InnoDB ： InnoDB 中不保存表的具体行数，也就是说，执行select count(*) from table时，InnoDB要扫描一遍整个表来计算有多少行 （5）外键支持： mysiam表不支持外键，而InnoDB支持 为什么MyISAM会比Innodb 的查询速度快。 INNODB在做SELECT的时候，要维护的东西比MYISAM引擎多很多；1）数据块，INNODB要缓存，MYISAM只缓存索引块， 这中间还有换进换出的减少；2）innodb寻址要映射到块，再到行，MYISAM 记录的直接是文件的OFFSET，定位比INNODB要快3）INNODB还需要维护MVCC一致；虽然你的场景没有，但他还是需要去检查和维护 MVCC ( Multi-Version Concurrency Control )多版本并发控制 3. 应用场景 MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。 InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夯实Java基础系列1：Java面向对象三大特性（基础篇）]]></title>
    <url>%2F2019%2F09%2F01%2F1%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《夯实Java基础系列博文》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。该系列博文会告诉你如何从入门到进阶，一步步地学习Java基础知识，并上手进行实战，接着了解每个Java知识点背后的实现原理，更完整地了解整个Java技术体系，形成自己的知识框架。为了更好地总结和检验你的学习成果，本系列文章也会提供每个知识点对应的面试题以及参考答案。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 Java面向对象三大特性（基础篇）面向对象简称 OO（Object Oriented），20 世纪 80 年代以后，有了面向对象分析（OOA）、 面向对象设计（OOD）、面向对象程序设计（OOP）等新的系统开发方式模型的研究。 对语言来说，一切皆是对象。把现实世界中的对象抽象地体现在编程世界中，一个对象代表了某个具体的操作。一个个对象最终组成了完整的程序设计，这些对象可以是独立存在的，也可以是从别的对象继承过来的。对象之间通过相互作用传递信息，实现程序开发。 对象的概念Java 是面向对象的编程语言，对象就是面向对象程序设计的核心。所谓对象就是真实世界中的实体，对象与实体是一一对应的，也就是说现实世界中每一个实体都是一个对象，它是一种具体的概念。对象有以下特点： 对象具有属性和行为。 对象具有变化的状态。 对象具有唯一性。 对象都是某个类别的实例。 一切皆为对象，真实世界中的所有事物都可以视为对象。 面向对象和面向过程的区别 面向过程：一种较早的编程思想，顾名思义就是该思想是站着过程的角度思考问题，强调的就是功能行为，功能的执行过程，即先后顺序，而每一个功能我们都使用函数（类似于方法）把这些步骤一步一步实现。使用的时候依次调用函数就可以了。 面向过程的设计：最小的程序单元是函数，每个函数负责完成某一个功能，用于接受输入数据，函数对输入数据进行处理，然后输出结果数据，整个软件系统由一个个的函数组成，其中作为程序入口的函数称之为主函数，主函数依次调用其他函数，普通函数之间可以相互调用，从而实现整个系统功能。 面向过程最大的问题在于随着系统的膨胀，面向过程将无法应付，最终导致系统的崩溃。为了解决这一种软件危机，我们提出面向对象思想。 面向过程的缺陷：是采用指定而下的设计模式，在设计阶段就需要考虑每一个模块应该分解成哪些子模块，每一个子模块又细分为更小的子模块，如此类推，直到将模块细化为一个个函数。 存在的问题 ​ 设计不够直观，与人类的思维习惯不一致系统软件适应新差，可拓展性差，维护性低 面向对象： ​ 一种基于面向过程的新编程思想，顾名思义就是该思想是站在对象的角度思考问题，我们把多个功能合理放到不同对象里，强调的是具备某些功能的对象。 具备某种功能的实体，称为对象。面向对象最小的程序单元是：类。面向对象更加符合常规的思维方式，稳定性好，可重用性强，易于开发大型软件产品，有良好的可维护性。 在软件工程上，面向对象可以使工程更加模块化，实现更低的耦合和更高的内聚。 面向对象的三大核心特性简介面向对象开发模式更有利于人们开拓思维，在具体的开发过程中便于程序的划分，方便程序员分工合作，提高开发效率。 该开发模式之所以使程序设计更加完善和强大，主要是因为面向对象具有继承、封装和多态 3 个核心特性。 1、继承的概念 继承是java面向对象编程技术的一块基石，因为它允许创建分等级层次的类。 继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 兔子和羊属于食草动物类，狮子和豹属于食肉动物类。 食草动物和食肉动物又是属于动物类。 所以继承需要符合的关系是：is-a，父类更通用，子类更具体。 虽然食草动物和食肉动物都是属于动物，但是两者的属性和行为上有差别，所以子类会具有父类的一般特性也会具有自身的特性。 2、Java 多态 多态是同一个行为具有多个不同表现形式或形态的能力。 多态就是同一个接口，使用不同的实例而执行不同操作，如图所示： 多态性是对象多种表现形式的体现。 现实中，比如我们按下 F1 键这个动作： 如果当前在 Flash 界面下弹出的就是 AS 3 的帮助文档； 如果当前在 Word 下弹出的就是 Word 帮助； 在 Windows 下弹出的就是 Windows 帮助和支持。 同一个事件发生在不同的对象上会产生不同的结果。 3、Java 封装 在面向对象程式设计方法中，封装（英语：Encapsulation）是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。 封装可以被认为是一个保护屏障，防止该类的代码和数据被外部类定义的代码随机访问。 要访问该类的代码和数据，必须通过严格的接口控制。 封装最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 适当的封装可以让程式码更容易理解与维护，也加强了程式码的安全性。 面向对象编程三大特性详解面向对象编程是利用 类和对象编程的一种思想。万物可归类，类是对于世界事物的高度抽象 ，不同的事物之间有不同的关系 ，一个类自身与外界的封装关系，一个父类和子类的继承关系， 一个类和多个类的多态关系。万物皆对象，对象是具体的世界事物，面向对象的三大特征封装，继承，多态，封装，封装说明一个类行为和属性与其他类的关系，低耦合，高内聚；继承是父类和子类的关系，多态说的是类与类的关系。 一、继承1、继承的概念如同生活中的子女继承父母拥有的所有财产，程序中的继承性是指子类拥有父类数据结构的方法和机制，这是类之间的一种关系；继承只能是单继承。 例如定义一个语文老师类和数学老师类，如果不采用继承方式，那么两个类中需要定义的属性和方法如图 1 所示。 图1 语文老师类和数学老师类中的属性和方法 从图 1 能够看出，语文老师类和数学老师类中的许多属性和方法相同，这些相同的属性和方法可以提取出来放在一个父类中，这个父类用于被语文老师类和数学老师类继承。当然父类还可以继承别的类，如图 2 所示。 图2 父类继承示例图 总结图 2 的继承关系，可以用概括的树形关系来表示，如图 3 所示。 图3 类继承示例图 从图 3 中可以看出，学校主要人员是一个大的类别，老师和学生是学校主要人员的两个子类，而老师又可以分为语文老师和数学老师两个子类，学生也可以分为班长和组长两个子类。 使用这种层次形的分类方式，是为了将多个类的通用属性和方法提取出来，放在它们的父类中，然后只需要在子类中各自定义自己独有的属性和方法，并以继承的形式在父类中获取它们的通用属性和方法即可。 继承是类与类的一种关系，是一种“is a”的关系。比如“狗”继承“动物”，这里动物类是狗类的父类或者基类，狗类是动物类的子类或者派生类。如下图所示： 注：java中的继承是单继承，即一个类只有一个父类。 补充：Java中的继承只能单继承，但是可以通过内部类继承其他类来实现多继承。 123456789101112131415161718192021222324public class Son extends Father&#123;public void go () &#123;System.out.println(&quot;son go&quot;);&#125;public void eat () &#123;System.out.println(&quot;son eat&quot;);&#125;public void sleep() &#123;System.out.println(&quot;zzzzzz&quot;);&#125;public void cook() &#123;//匿名内部类实现的多继承new Mother().cook();//内部类继承第二个父类来实现多继承Mom mom = new Mom();mom.cook();&#125;private class Mom extends Mother &#123;@Overridepublic void cook() &#123;System.out.println(&quot;mom cook&quot;);&#125;&#125;&#125; 2、继承的好处 子类拥有父类的所有属性和方法（除了private修饰的属性不能拥有）从而实现了实现代码的复用； 3、语法规则 A、方法的重写 子类如果对继承的父类的方法不满意（不适合），可以自己编写继承的方法，这种方式就称为方法的重写。当调用方法时会优先调用子类的方法。 重写要注意： a、返回值类型 b、方法名 c、参数类型及个数 都要与父类继承的方法相同，才叫方法的重写。 重载和重写的区别： 方法重载：在同一个类中处理不同数据的多个相同方法名的多态手段。 方法重写：相对继承而言，子类中对父类已经存在的方法进行区别化的修改。 B、继承的初始化顺序 1、初始化父类再初始化子类 2、先执行初始化对象中属性，再执行构造方法中的初始化。 基于上面两点，我们就知道实例化一个子类，java程序的执行顺序是： 父类对象属性初始化—-&gt;父类对象构造方法—-&gt;子类对象属性初始化—&gt;子类对象构造方法 下面有个形象的图： C、final关键字 使用final关键字做标识有“最终的”含义。 final 修饰类，则该类不允许被继承。 final 修饰方法，则该方法不允许被覆盖(重写)。 final 修饰属性，则该类的该属性不会进行隐式的初始化，所以 该final 属性的初始化属性必须有值，或在构造方法中赋值(但只能选其一，且必须选其一，因为没有默认值！)，且初始化之后就不能改了，只能赋值一次。 final 修饰变量，则该变量的值只能赋一次值，在声明变量的时候才能赋值，即变为常量。 D、super关键字 在对象的内部使用，可以代表父类对象。 1、访问父类的属性：super.age 2、访问父类的方法：super.eat() super的应用： 首先我们知道子类的构造的过程当中必须调用父类的构造方法。其实这个过程已经隐式地使用了我们的super关键字。 这是因为如果子类的构造方法中没有显示调用父类的构造方法，则系统默认调用父类无参的构造方法。 那么如果自己用super关键字在子类里调用父类的构造方法，则必须在子类的构造方法中的第一行。 要注意的是：如果子类构造方法中既没有显示调用父类的构造方法，而父类没有无参的构造方法，则编译出错。 （补充说明，虽然没有显示声明父类的无参的构造方法，系统会自动默认生成一个无参构造方法，但是，如果你声明了一个有参的构造方法，而没有声明无参的构造方法，这时系统不会动默认生成一个无参构造方法，此时称为父类有没有无参的构造方法。） 二、封装1、封装的概念封装是将代码及其处理的数据绑定在一起的一种编程机制，该机制保证了程序和数据都不受外部干扰且不被误用。封装的目的在于保护信息，使用它的主要优点如下。 保护类中的信息，它可以阻止在外部定义的代码随意访问内部代码和数据。 隐藏细节信息，一些不需要程序员修改和使用的信息，比如取款机中的键盘，用户只需要知道按哪个键实现什么操作就可以，至于它内部是如何运行的，用户不需要知道。 有助于建立各个系统之间的松耦合关系，提高系统的独立性。当一个系统的实现方式发生变化时，只要它的接口不变，就不会影响其他系统的使用。例如 U 盘，不管里面的存储方式怎么改变，只要 U 盘上的 USB 接口不变，就不会影响用户的正常操作。 提高软件的复用率，降低成本。每个系统都是一个相对独立的整体，可以在不同的环境中得到使用。例如，一个 U 盘可以在多台电脑上使用。 Java 语言的基本封装单位是类。由于类的用途是封装复杂性，所以类的内部有隐藏实现复杂性的机制。Java 提供了私有和公有的访问模式，类的公有接口代表外部的用户应该知道或可以知道的每件东西，私有的方法数据只能通过该类的成员代码来访问，这就可以确保不会发生不希望的事情。 2、封装的优点在面向对象程式设计方法中，封装（英语：Encapsulation）是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。 封装可以被认为是一个保护屏障，防止该类的代码和数据被外部类定义的代码随机访问。 要访问该类的代码和数据，必须通过严格的接口控制。 封装最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 适当的封装可以让程式码更容易理解与维护，也加强了程式码的安全性。 封装的优点 良好的封装能够减少耦合。 类内部的结构可以自由修改。 可以对成员变量进行更精确的控制。 隐藏信息，实现细节。 Java 封装，说白了就是将一大坨公共通用的实现逻辑玩意，装到一个盒子里（class），出入口都在这个盒子上。你要用就将这个盒子拿来用，连接出入口，就能用了，不用就可以直接扔，对你代码没什么影响。 对程序员来说，使用封装的目的： 偷懒，辛苦一次，后面都能少敲很多代码，增强了代码得复用性 简化代码，看起来更容易懂 隐藏核心实现逻辑代码，简化外部逻辑，并且不让其他人修改，jar 都这么干 一对一，一个功能就只为这个功能服务；避免头发绳子一块用，导致最后一团糟 3、封装的实现步骤 需要注意：对封装的属性不一定要通过get/set方法，其他方法也可以对封装的属性进行操作。当然最好使用get/set方法，比较标准。 A、访问修饰符 从表格可以看出从上到下封装性越来越差。 B、this关键字 1.this关键字代表当前对象 this.属性 操作当前对象的属性 this.方法 调用当前对象的方法。 2.封装对象的属性的时候，经常会使用this关键字。 3.当getter和setter函数参数名和成员函数名重合的时候，可以使用this**区别。如：** C、Java 中的内部类 内部类（ Inner Class ）就是定义在另外一个类里面的类。与之对应，包含内部类的类被称为外部类。 那么问题来了：那为什么要将一个类定义在另一个类里面呢？清清爽爽的独立的一个类多好啊！！ 答：内部类的主要作用如下： 内部类提供了更好的封装，可以把内部类隐藏在外部类之内，不允许同一个包中的其他类访问该类。 内部类的方法可以直接访问外部类的所有数据，包括私有的数据。 内部类所实现的功能使用外部类同样可以实现，只是有时使用内部类更方便。 内部类可分为以下几种： 成员内部类 静态内部类 方法内部类 匿名内部类 三、多态1、多态的概念面向对象的多态性，即“一个接口，多个方法”。多态性体现在父类中定义的属性和方法被子类继承后，可以具有不同的属性或表现方式。多态性允许一个接口被多个同类使用，弥补了单继承的不足。多态概念可以用树形关系来表示，如图 4 所示。 图4 多态示例图 从图 4 中可以看出，老师类中的许多属性和方法可以被语文老师类和数学老师类同时使用，这样也不易出错。 2、多态的好处可替换性（substitutability）。多态对已存在代码具有可替换性。例如，多态对圆Circle类工作，对其他任何圆形几何体，如圆环，也同样工作。 可扩充性（extensibility）。多态对代码具有可扩充性。增加新的子类不影响已存在类的多态性、继承性，以及其他特性的运行和操作。实际上新加子类更容易获得多态功能。例如，在实现了圆锥、半圆锥以及半球体的多态基础上，很容易增添球体类的多态性。 接口性（interface-ability）。多态是超类通过方法签名，向子类提供了一个共同接口，由子类来完善或者覆盖它而实现的。 灵活性（flexibility）。它在应用中体现了灵活多样的操作，提高了使用效率。 简化性（simplicity）。多态简化对应用软件的代码编写和修改过程，尤其在处理大量对象的运算和操作时，这个特点尤为突出和重要。 子代父类实例化，然后就相当于一个父亲有很多儿子，送快递的给这个父亲的儿子送东西，他只需要送到父亲的家就行了，至于具体是那个儿子的，父亲还会分不清自己的儿子么，所以你就不用操心了。 使用多态是一种好习惯多态方式声明是一种好的习惯。当我们创建的类，使用时，只用到它的超类或接口定义的方法时，我们可以将其索引声明为它的超类或接口类型。 它的好处是，如果某天我们对这个接口方法的实现方式变了，对这个接口又有一个新的实现类，我们的程序也需要使用最新的实现方式，此时只要将对象实现修改一下，索引无需变化。 比如Map&lt; String,String&gt; map = new HashMap &lt; String,String&gt;(); 想换成HashTable实现，可以Map&lt; String,String&gt; map = new HashTable &lt; String,String&gt;(); 比如写一个方法，参数要求传递List类型，你就可以用List list = new ArrayList()中的list传递，但是你写成ArrayList list = new ArrayList()是传递不进去的。尽管方法处理时都一样。另外，方法还可以根据你传递的不同list（ArrayList或者LinkList）进行不同处理。 3、Java中的多态java里的多态主要表现在两个方面： A、引用多态 父类的引用可以指向本类的对象； 父类的引用可以指向子类的对象； 这两句话是什么意思呢，让我们用代码来体验一下，首先我们创建一个父类Animal和一个子类Dog，在主函数里如下所示： 注意：我们不能使用一个子类的引用来指向父类的对象，如：。 这里我们必须深刻理解引用多态的意义，才能更好记忆这种多态的特性。为什么子类的引用不能用来指向父类的对象呢？我在这里通俗给大家讲解一下：就以上面的例子来说，我们能说“狗是一种动物”，但是不能说“动物是一种狗”，狗和动物是父类和子类的继承关系，它们的从属是不能颠倒的。当父类的引用指向子类的对象时，该对象将只是看成一种特殊的父类（里面有重写的方法和属性），反之，一个子类的引用来指向父类的对象是不可行的！！ B、方法多态 根据上述创建的两个对象：本类对象和子类对象，同样都是父类的引用，当我们指向不同的对象时，它们调用的方法也是多态的。 创建本类对象时，调用的方法为本类方法； 创建子类对象时，调用的方法为子类重写的方法或者继承的方法； 使用多态的时候要注意：如果我们在子类中编写一个独有的方法（没有继承父类的方法），此时就不能通过父类的引用创建的子类对象来调用该方法！！！ 注意： 继承是多态的基础。 C、引用类型转换 了解了多态的含义后，我们在日常使用多态的特性时经常需要进行引用类型转换。 引用类型转换： 1.向上类型转换(隐式/自动类型转换)，是小类型转换到大类型 就以上述的父类Animal和一个子类Dog来说明，当父类的引用可以指向子类的对象时，就是向上类型转换。如： 2. 向下类型转换(强制类型转换)，是大类型转换到小类型(有风险,可能出现数据溢出)。 将上述代码再加上一行，我们再次将父类转换为子类引用，那么会出现错误，编译器不允许我们直接这么做，虽然我们知道这个父类引用指向的就是子类对象，但是编译器认为这种转换是存在风险的。如： 那么我们该怎么解决这个问题呢，我们可以在animal前加上（Dog）来强制类型转换。如： 但是如果父类引用没有指向该子类的对象，则不能向下类型转换，虽然编译器不会报错，但是运行的时候程序会出错，如： 其实这就是上面所说的子类的引用指向父类的对象，而强制转换类型也不能转换！！ 还有一种情况是父类的引用指向其他子类的对象，则不能通过强制转为该子类的对象。如： 这是因为我们在编译的时候进行了强制类型转换，编译时的类型是我们强制转换的类型，所以编译器不会报错，而当我们运行的时候，程序给animal开辟的是Dog类型的内存空间，这与Cat类型内存空间不匹配，所以无法正常转换。这两种情况出错的本质是一样的，所以我们在使用强制类型转换的时候要特别注意这两种错误！！下面有个更安全的方式来实现向下类型转换。。。。 3. instanceof运算符，来解决引用对象的类型，避免类型转换的安全性问题。 instanceof是Java的一个二元操作符，和==，&gt;，&lt;是同一类东东。由于它是由字母组成的，所以也是Java的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回boolean类型的数据。 我们来使用instanceof运算符来规避上面的错误，代码修改如下： 利用if语句和instanceof运算符来判断两个对象的类型是否一致。 补充说明：在比较一个对象是否和另一个对象属于同一个类实例的时候，我们通常可以采用instanceof和getClass两种方法通过两者是否相等来判断，但是两者在判断上面是有差别的。Instanceof进行类型检查规则是:你属于该类吗？或者你属于该类的派生类吗？而通过getClass获得类型信息采用==来进行检查是否相等的操作是严格的判断,不会存在继承方面的考虑； 总结：在写程序的时候，如果要进行类型转换，我们最好使用instanceof运算符来判断它左边的对象是否是它右边的类的实例，再进行强制转换。 D、重写和重载 多态一般可以分为两种，一个是重写override，一个是重载overload。 1234567重写是由于继承关系中的子类有一个和父类同名同参数的方法，会覆盖掉父类的方法。重载是因为一个同名方法可以传入多个参数组合。注意，同名方法如果参数相同，即使返回值不同也是不能同时存在的，编译会出错。从jvm实现的角度来看，重写又叫运行时多态，编译时看不出子类调用的是哪个方法，但是运行时操作数栈会先根据子类的引用去子类的类信息中查找方法，找不到的话再到父类的类信息中查找方法。而重载则是编译时多态，因为编译期就可以确定传入的参数组合，决定调用的具体方法是哪一个了。 1. 向上转型和向下转型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public static void main(String[] args) &#123; Son son = new Son(); //首先先明确一点，转型指的是左侧引用的改变。 //father引用类型是Father，指向Son实例，就是向上转型，既可以使用子类的方法，也可以使用父类的方法。 //向上转型,此时运行father的方法 Father father = son; father.smoke(); //不能使用子类独有的方法。 // father.play();编译会报错 father.drive(); //Son类型的引用指向Father的实例，所以是向下转型，不能使用子类非重写的方法，可以使用父类的方法。 //向下转型，此时运行了son的方法 Son son1 = (Son) father; //转型后就是一个正常的Son实例 son1.play(); son1.drive(); son1.smoke(); //因为向下转型之前必须先经历向上转型。 //在向下转型过程中，分为两种情况： //情况一：如果父类引用的对象如果引用的是指向的子类对象， //那么在向下转型的过程中是安全的。也就是编译是不会出错误的。 //因为运行期Son实例确实有这些方法 Father f1 = new Son(); Son s1 = (Son) f1; s1.smoke(); s1.drive(); s1.play(); //情况二：如果父类引用的对象是父类本身，那么在向下转型的过程中是不安全的，编译不会出错， //但是运行时会出现java.lang.ClassCastException错误。它可以使用instanceof来避免出错此类错误。 //因为运行期Father实例并没有这些方法。 Father f2 = new Father(); Son s2 = (Son) f2; s2.drive(); s2.smoke(); s2.play(); //向下转型和向上转型的应用，有些人觉得这个操作没意义，何必先向上转型再向下转型呢，不是多此一举么。其实可以用于方法参数中的类型聚合，然后具体操作再进行分解。 //比如add方法用List引用类型作为参数传入，传入具体类时经历了向下转型 add(new LinkedList()); add(new ArrayList()); //总结 //向上转型和向下转型都是针对引用的转型，是编译期进行的转型，根据引用类型来判断使用哪个方法 //并且在传入方法时会自动进行转型（有需要的话）。运行期将引用指向实例，如果是不安全的转型则会报错。 //若安全则继续执行方法。&#125;public static void add(List list) &#123; System.out.println(list); //在操作具体集合时又经历了向上转型// ArrayList arr = (ArrayList) list;// LinkedList link = (LinkedList) list;&#125; 总结：向上转型和向下转型都是针对引用的转型，是编译期进行的转型，根据引用类型来判断使用哪个方法。并且在传入方法时会自动进行转型（有需要的话）。运行期将引用指向实例，如果是不安全的转型则会报错，若安全则继续执行方法。 2. 编译期的静态分派 其实就是根据引用类型来调用对应方法。 12345678910111213public static void main(String[] args) &#123; Father father = new Son(); 静态分派 a= new 静态分派(); //编译期确定引用类型为Father。 //所以调用的是第一个方法。 a.play(father); //向下转型后，引用类型为Son，此时调用第二个方法。 //所以，编译期只确定了引用，运行期再进行实例化。 a.play((Son)father); //当没有Son引用类型的方法时，会自动向上转型调用第一个方法。 a.smoke(father); // 123456789101112&#125;public void smoke(Father father) &#123; System.out.println(&quot;father smoke&quot;);&#125;public void play (Father father) &#123; System.out.println(&quot;father&quot;); //father.drive();&#125;public void play (Son son) &#123; System.out.println(&quot;son&quot;); //son.drive();&#125; 3. 方法重载优先级匹配 123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args) &#123; 方法重载优先级匹配 a = new 方法重载优先级匹配(); //普通的重载一般就是同名方法不同参数。 //这里我们来讨论当同名方法只有一个参数时的情况。 //此时会调用char参数的方法。 //当没有char参数的方法。会调用int类型的方法，如果没有int就调用long //即存在一个调用顺序char -&gt; int -&gt; long -&gt;double -&gt; ..。 //当没有基本类型对应的方法时，先自动装箱，调用包装类方法。 //如果没有包装类方法，则调用包装类实现的接口的方法。 //最后再调用持有多个参数的char...方法。 a.eat(&apos;a&apos;); a.eat(&apos;a&apos;,&apos;c&apos;,&apos;b&apos;);&#125;public void eat(short i) &#123; System.out.println(&quot;short&quot;);&#125;public void eat(int i) &#123; System.out.println(&quot;int&quot;);&#125;public void eat(double i) &#123; System.out.println(&quot;double&quot;);&#125;public void eat(long i) &#123; System.out.println(&quot;long&quot;);&#125;public void eat(Character c) &#123; System.out.println(&quot;Character&quot;);&#125;public void eat(Comparable c) &#123; System.out.println(&quot;Comparable&quot;);&#125;public void eat(char ... c) &#123; System.out.println(Arrays.toString(c)); System.out.println(&quot;...&quot;);&#125;// public void eat(char i) &#123;// System.out.println(&quot;char&quot;);// &#125; 参考文章https://segmentfault.com/a/1190000009707894 https://www.cnblogs.com/hysum/p/7100874.html http://c.biancheng.net/view/939.html https://www.runoob.com/ https://blog.csdn.net/android_hl/article/details/53228348 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>面向对象</tag>
        <tag>继承</tag>
        <tag>封装</tag>
        <tag>多态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据库1：无废话MySQL入门]]></title>
    <url>%2F2019%2F09%2F01%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE%E5%BA%931%EF%BC%9A%E6%97%A0%E5%BA%9F%E8%AF%9DMySQL%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 前言开始使用 我下面所有的SQL语句是基于MySQL 5.6+运行。 MySQL 为关系型数据库(Relational Database Management System)，一个关系型数据库由一个或数个表格组成, 如图所示的一个表格： 表头(header): 每一列的名称; 列(col): 具有相同数据类型的数据的集合; 行(row): 每一行用来描述某个人/物的具体信息; 值(value): 行的具体信息, 每个值必须与该列的数据类型相同; 键(key): 表中用来识别某个特定的人物的方法, 键的值在当前列中具有唯一性。 登录MySQL1mysql -h 127.0.0.1 -u 用户名 -pmysql -D 所选择的数据库名 -h 主机名 -u 用户名 -pmysql&gt; exit # 退出 使用 “quit;” 或 “\q;” 一样的效果mysql&gt; status; # 显示当前mysql的version的各种信息mysql&gt; select version(); # 显示当前mysql的version信息mysql&gt; show global variables like &apos;port&apos;; # 查看MySQL端口号 创建数据库对于表的操作需要先进入库use 库名; 1-- 创建一个名为 samp_db 的数据库，数据库字符编码指定为 gbkcreate database samp_db character set gbk;drop database samp_db; -- 删除 库名为samp_db的库show databases; -- 显示数据库列表。use samp_db; -- 选择创建的数据库samp_dbshow tables; -- 显示samp_db下面所有的表名字describe 表名; -- 显示数据表的结构delete from 表名; -- 清空表中记录 创建数据库表 使用 create table 语句可完成对表的创建, create table 的常见形式:语法：create table 表名称(列声明); 1CREATE TABLE `user_accounts` ( `id` int(100) unsigned NOT NULL AUTO_INCREMENT primary key, `password` varchar(32) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;用户密码&apos;, `reset_password` tinyint(32) NOT NULL DEFAULT 0 COMMENT &apos;用户类型：0－不需要重置密码；1-需要重置密码&apos;, `mobile` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;手机&apos;, `create_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), `update_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), -- 创建唯一索引，不允许重复 UNIQUE INDEX idx_user_mobile(`mobile`))ENGINE=InnoDB DEFAULT CHARSET=utf8COMMENT=&apos;用户表信息&apos;; 数据类型的属性解释 NULL：数据列可包含NULL值； NOT NULL：数据列不允许包含NULL值； DEFAULT：默认值； PRIMARY：KEY 主键； AUTO_INCREMENT：自动递增，适用于整数类型； UNSIGNED：是指数值类型只能为正数； CHARACTER SET name：指定一个字符集； COMMENT：对表或者字段说明； 增删改查SELECT SELECT 语句用于从表中选取数据。语法：SELECT 列名称 FROM 表名称语法：SELECT * FROM 表名称 1-- 表station取个别名叫s，表station中不包含 字段id=13或者14 的，并且id不等于4的 查询出来，只显示idSELECT s.id from station s WHERE id in (13,14) and user_id not in (4); -- 从表 Persons 选取 LastName 列的数据SELECT LastName FROM Persons -- 结果集中会自动去重复数据SELECT DISTINCT Company FROM Orders -- 表 Persons 字段 Id_P 等于 Orders 字段 Id_P 的值，-- 结果集显示 Persons表的 LastName、FirstName字段，Orders表的OrderNo字段SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P -- gbk 和 utf8 中英文混合排序最简单的办法 -- ci是 case insensitive, 即 “大小写不敏感”SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci;SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci; UPDATE Update 语句用于修改表中的数据。语法：UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值 1-- update语句设置字段值为另一个结果取出来的字段update user set name = (select name from user1 where user1 .id = 1 )where id = (select id from user2 where user2 .name=&apos;小苏&apos;);-- 更新表 orders 中 id=1 的那一行数据更新它的 title 字段UPDATE `orders` set title=&apos;这里是标题&apos; WHERE id=1; INSERT INSERT INTO 语句用于向表格中插入新的行。语法：INSERT INTO 表名称 VALUES (值1, 值2,....)语法：INSERT INTO 表名称 (列1, 列2,...) VALUES (值1, 值2,....) 1-- 向表 Persons 插入一条字段 LastName = JSLite 字段 Address = shanghaiINSERT INTO Persons (LastName, Address) VALUES (&apos;JSLite&apos;, &apos;shanghai&apos;);-- 向表 meeting 插入 字段 a=1 和字段 b=2INSERT INTO meeting SET a=1,b=2;-- -- SQL实现将一个表的数据插入到另外一个表的代码-- 如果只希望导入指定字段，可以用这种方法：-- INSERT INTO 目标表 (字段1, 字段2, ...) SELECT 字段1, 字段2, ... FROM 来源表;INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1; DELETE DELETE 语句用于删除表中的行。语法：DELETE FROM 表名称 WHERE 列名称 = 值 1-- 在不删除table_name表的情况下删除所有的行，清空表。DELETE FROM table_name-- 或者DELETE * FROM table_name-- 删除 Person表字段 LastName = &apos;JSLite&apos; DELETE FROM Person WHERE LastName = &apos;JSLite&apos; -- 删除 表meeting id 为2和3的两条数据DELETE from meeting where id in (2,3); WHERE WHERE 子句用于规定选择的标准。语法：SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 1-- 从表 Persons 中选出 Year 字段大于 1965 的数据SELECT * FROM Persons WHERE Year&gt;1965 AND 和 OR AND - 如果第一个条件和第二个条件都成立；OR - 如果第一个条件和第二个条件中只要有一个成立； AND1-- 删除 meeting 表字段 -- id=2 并且 user_id=5 的数据 和-- id=3 并且 user_id=6 的数据 DELETE from meeting where id in (2,3) and user_id in (5,6); -- 使用 AND 来显示所有姓为 &quot;Carter&quot; 并且名为 &quot;Thomas&quot; 的人：SELECT * FROM Persons WHERE FirstName=&apos;Thomas&apos; AND LastName=&apos;Carter&apos;; OR1-- 使用 OR 来显示所有姓为 &quot;Carter&quot; 或者名为 &quot;Thomas&quot; 的人：SELECT * FROM Persons WHERE firstname=&apos;Thomas&apos; OR lastname=&apos;Carter&apos; ORDER BY 语句默认按照升序对记录进行排序。ORDER BY - 语句用于根据指定的列对结果集进行排序。DESC - 按照降序对记录进行排序。ASC - 按照顺序对记录进行排序。 1-- Company在表Orders中为字母，则会以字母顺序显示公司名称SELECT Company, OrderNumber FROM Orders ORDER BY Company -- 后面跟上 DESC 则为降序显示SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC -- Company以降序显示公司名称，并OrderNumber以顺序显示SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC IN IN - 操作符允许我们在 WHERE 子句中规定多个值。IN - 操作符用来指定范围，范围中的每一条，都进行匹配。IN取值规律，由逗号分割，全部放置括号中。语法：SELECT &quot;字段名&quot;FROM &quot;表格名&quot;WHERE &quot;字段名&quot; IN (&#39;值一&#39;, &#39;值二&#39;, ...); 1-- 从表 Persons 选取 字段 LastName 等于 Adams、CarterSELECT * FROM Persons WHERE LastName IN (&apos;Adams&apos;,&apos;Carter&apos;) NOT NOT - 操作符总是与其他操作符一起使用，用在要过滤的前面。 1SELECT vend_id, prod_name FROM Products WHERE NOT vend_id = &apos;DLL01&apos; ORDER BYprod_name; UNION UNION - 操作符用于合并两个或多个 SELECT 语句的结果集。 1-- 列出所有在中国表（Employees_China）和美国（Employees_USA）的不同的雇员名SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA -- 列出 meeting 表中的 pic_url，-- station 表中的 number_station 别名设置成 pic_url 避免字段不一样报错-- 按更新时间排序SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; AS as - 可理解为：用作、当成，作为；别名一般是重命名列名或者表名。语法：select column_1 as 列1,column_2 as 列2 from table as 表 1SELECT * FROM Employee AS emp-- 这句意思是查找所有Employee 表里面的数据，并把Employee表格命名为 emp。-- 当你命名一个表之后，你可以在下面用 emp 代替 Employee.-- 例如 SELECT * FROM emp. SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders-- 列出表 Orders 字段 OrderPrice 列最大值，-- 结果集列不显示 OrderPrice 显示 LargestOrderPrice -- 显示表 users_profile 中的 name 列SELECT t.name from (SELECT * from users_profile a) AS t; -- 表 user_accounts 命名别名 ua，表 users_profile 命名别名 up-- 满足条件 表 user_accounts 字段 id 等于 表 users_profile 字段 user_id-- 结果集只显示mobile、name两列SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.user_id; JOIN 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。 JOIN: 如果表中有至少一个匹配，则返回行 INNER JOIN:在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 1SELECT Persons.LastName, Persons.FirstName, Orders.OrderNoFROM PersonsINNER JOIN OrdersON Persons.Id_P = Orders.Id_PORDER BY Persons.LastName; SQL 函数COUNT COUNT 让我们能够数出在表格中有多少笔资料被选出来。语法：SELECT COUNT(&quot;字段名&quot;) FROM &quot;表格名&quot;; 1-- 表 Store_Information 有几笔 store_name 栏不是空白的资料。-- &quot;IS NOT NULL&quot; 是 &quot;这个栏位不是空白&quot; 的意思。SELECT COUNT (Store_Name) FROM Store_Information WHERE Store_Name IS NOT NULL; -- 获取 Persons 表的总数SELECT COUNT(1) AS totals FROM Persons;-- 获取表 station 字段 user_id 相同的总数select user_id, count(*) as totals from station group by user_id; MAX MAX 函数返回一列中的最大值。NULL 值不包括在计算中。语法：SELECT MAX(&quot;字段名&quot;) FROM &quot;表格名&quot; 1-- 列出表 Orders 字段 OrderPrice 列最大值，-- 结果集列不显示 OrderPrice 显示 LargestOrderPriceSELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders 触发器 语法：create trigger &lt;触发器名称&gt;{ before | after} # 之前或者之后出发insert | update | delete # 指明了激活触发程序的语句的类型on &lt;表名&gt; # 操作哪张表for each row # 触发器的执行间隔，for each row 通知触发器每隔一行执行一次动作，而不是对整个表执行一次。&lt;触发器SQL语句&gt; 1DELIMITER $ -- 自定义结束符号CREATE TRIGGER set_userdate BEFORE INSERT on `message`for EACH ROWBEGIN UPDATE `user_accounts` SET status=1 WHERE openid=NEW.openid;END$DELIMITER ; -- 恢复结束符号 OLD和NEW不区分大小写 NEW 用NEW.col_name，没有旧行。在DELETE触发程序中，仅能使用OLD.col_name，没有新行。 OLD 用OLD.col_name来引用更新前的某一行的列 添加索引普通索引(INDEX) 语法：ALTER TABLE 表名字 ADD INDEX 索引名字 ( 字段名字 ) 1-- –直接创建索引CREATE INDEX index_user ON user(title)-- –修改表结构的方式添加索引ALTER TABLE table_name ADD INDEX index_name ON (column(length))-- 给 user 表中的 name字段 添加普通索引(INDEX)ALTER TABLE `table` ADD INDEX index_name (name)-- –创建表的时候同时创建索引CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length)))-- –删除索引DROP INDEX index_name ON table 主键索引(PRIMARY key) 语法：ALTER TABLE 表名字 ADD PRIMARY KEY ( 字段名字 ) 1-- 给 user 表中的 id字段 添加主键索引(PRIMARY key)ALTER TABLE `user` ADD PRIMARY key (id); 唯一索引(UNIQUE) 语法：ALTER TABLE 表名字 ADD UNIQUE (字段名字) 1-- 给 user 表中的 creattime 字段添加唯一索引(UNIQUE)ALTER TABLE `user` ADD UNIQUE (creattime); 全文索引(FULLTEXT) 语法：ALTER TABLE 表名字 ADD FULLTEXT (字段名字) 1-- 给 user 表中的 description 字段添加全文索引(FULLTEXT)ALTER TABLE `user` ADD FULLTEXT (description); 添加多列索引 语法： ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3) 1-- 给 user 表中的 name、city、age 字段添加名字为name_city_age的普通索引(INDEX)ALTER TABLE user ADD INDEX name_city_age (name(10),city,age); 建立索引的时机在WHERE和JOIN中出现的列需要建立索引，但也不完全如此： MySQL只对&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN使用索引 某些时候的LIKE也会使用索引。 在LIKE以通配符%和_开头作查询时，MySQL不会使用索引。 1-- 此时就需要对city和age建立索引，-- 由于mytable表的userame也出现在了JOIN子句中，也有对它建立索引的必要。SELECT t.Name FROM mytable t LEFT JOIN mytable m ON t.Name=m.username WHERE m.age=20 AND m.city=&apos;上海&apos;; SELECT * FROM mytable WHERE username like&apos;admin%&apos;; -- 而下句就不会使用：SELECT * FROM mytable WHEREt Name like&apos;%admin&apos;; -- 因此，在使用LIKE时应注意以上的区别。 索引的注意事项 索引不会包含有NULL值的列 使用短索引 不要在列上进行运算 索引会失效 创建后表的修改添加列 语法：alter table 表名 add 列名 列数据类型 [after 插入位置]; 示例: 1-- 在表students的最后追加列 address: alter table students add address char(60);-- 在名为 age 的列后插入列 birthday: alter table students add birthday date after age;-- 在名为 number_people 的列后插入列 weeks: alter table students add column `weeks` varchar(5) not null default &quot;&quot; after `number_people`; 修改列 语法：alter table 表名 change 列名称 列新名称 新数据类型; 1-- 将表 tel 列改名为 telphone: alter table students change tel telphone char(13) default &quot;-&quot;;-- 将 name 列的数据类型改为 char(16): alter table students change name name char(16) not null;-- 修改 COMMENT 前面必须得有类型属性alter table students change name name char(16) COMMENT &apos;这里是名字&apos;;-- 修改列属性的时候 建议使用modify,不需要重建表-- change用于修改列名字，这个需要重建表alter table meeting modify `weeks` varchar(20) NOT NULL DEFAULT &quot;&quot; COMMENT &quot;开放日期 周一到周日：0~6，间隔用英文逗号隔开&quot;; 删除列 语法：alter table 表名 drop 列名称; 1-- 删除表students中的 birthday 列: alter table students drop birthday; 重命名表 语法：alter table 表名 rename 新表名; 1-- 重命名 students 表为 workmates: alter table students rename workmates; 清空表数据 方法一：delete from 表名;方法二：truncate from &quot;表名&quot;; DELETE:1. DML语言;2. 可以回退;3. 可以有条件的删除; TRUNCATE:1. DDL语言;2. 无法回退;3. 默认所有的表内容都删除;4. 删除速度比delete快。 1-- 清空表为 workmates 里面的数据，不删除表。 delete from workmates;-- 删除workmates表中的所有数据，且无法恢复truncate from workmates; 删除整张表 语法：drop table 表名; 1-- 删除 workmates 表: drop table workmates; 删除整个数据库 语法：drop database 数据库名; 1-- 删除 samp_db 数据库: drop database samp_db; 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重新学习Mysql数据13：Mysql主从复制，读写分离，分表分库策略与实践]]></title>
    <url>%2F2019%2F09%2F01%2FMySQL%2F%E9%87%8D%E6%96%B0%E5%AD%A6%E4%B9%A0Mysql%E6%95%B0%E6%8D%AE13%EF%BC%9AMysql%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%EF%BC%8C%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%EF%BC%8C%E5%88%86%E8%A1%A8%E5%88%86%E5%BA%93%E7%AD%96%E7%95%A5%E4%B8%8E%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[本文转自互联网 本系列文章将整理到我在GitHub上的《Java面试指南》仓库，更多精彩内容请到我的仓库里查看 https://github.com/h2pl/Java-Tutorial 喜欢的话麻烦点下Star哈 文章首发于我的个人博客： www.how2playlife.com 本文是微信公众号【Java技术江湖】的《重新学习MySQL数据库》其中一篇，本文部分内容来源于网络，为了把本文主题讲得清晰透彻，也整合了很多我认为不错的技术博客内容，引用其中了一些比较好的博客文章，如有侵权，请联系作者。 该系列博文会告诉你如何从入门到进阶，从sql基本的使用方法，从MySQL执行引擎再到索引、事务等知识，一步步地学习MySQL相关技术的实现原理，更好地了解如何基于这些知识来优化sql，减少SQL执行时间，通过执行计划对SQL性能进行分析，再到MySQL的主从复制、主备部署等内容，以便让你更完整地了解整个MySQL方面的技术体系，形成自己的知识框架。 如果对本系列文章有什么建议，或者是有什么疑问的话，也可以关注公众号【Java技术江湖】联系作者，欢迎你参与本系列博文的创作和修订。 一、MySQL扩展具体的实现方式 随着业务规模的不断扩大，需要选择合适的方案去应对数据规模的增长，以应对逐渐增长的访问压力和数据量。 关于数据库的扩展主要包括：业务拆分、主从复制、读写分离、数据库分库与分表等。这篇文章主要讲述数据库分库与分表 （1）业务拆分 在 大型网站应用之海量数据和高并发解决方案总结一二 一篇文章中也具体讲述了为什么要对业务进行拆分。 业务起步初始，为了加快应用上线和快速迭代，很多应用都采用集中式的架构。随着业务系统的扩大，系统变得越来越复杂，越来越难以维护，开发效率变得越来越低，并且对资源的消耗也变得越来越大，通过硬件提高系统性能的方式带来的成本也越来越高。 因此，在选型初期，一个优良的架构设计是后期系统进行扩展的重要保障。 例如：电商平台，包含了用户、商品、评价、订单等几大模块，最简单的做法就是在一个数据库中分别创建users、shops、comment、order四张表。 但是，随着业务规模的增大，访问量的增大，我们不得不对业务进行拆分。每一个模块都使用单独的数据库来进行存储，不同的业务访问不同的数据库，将原本对一个数据库的依赖拆分为对4个数据库的依赖，这样的话就变成了4个数据库同时承担压力，系统的吞吐量自然就提高了。 （2）主从复制 一般是主写从读，一主多从 1、MySQL5.6 数据库主从（Master/Slave）同步安装与配置详解 2、MySQL主从复制的常见拓扑、原理分析以及如何提高主从复制的效率总结 3、使用mysqlreplicate命令快速搭建 Mysql 主从复制 上述三篇文章中，讲述了如何配置主从数据库，以及如何实现数据库的读写分离，这里不再赘述，有需要的选择性点击查看。 上图是网上的一张关于MySQL的Master和Slave之间数据同步的过程图。 主要讲述了MySQL主从复制的原理：数据复制的实际就是Slave从Master获取Binary log文件，然后再本地镜像的执行日志中记录的操作。由于主从复制的过程是异步的，因此Slave和Master之间的数据有可能存在延迟的现象，此时只能保证数据最终的一致性。 （3）数据库分库与分表 我们知道每台机器无论配置多么好它都有自身的物理上限，所以当我们应用已经能触及或远远超出单台机器的某个上限的时候，我们惟有寻找别的机器的帮助或者继续升级的我们的硬件，但常见的方案还是通过添加更多的机器来共同承担压力。 我们还得考虑当我们的业务逻辑不断增长，我们的机器能不能通过线性增长就能满足需求？因此，使用数据库的分库分表，能够立竿见影的提升系统的性能，关于为什么要使用数据库的分库分表的其他原因这里不再赘述，主要讲具体的实现策略。请看下边章节。 二、分表实现策略关键字：用户ID、表容量 对于大部分数据库的设计和业务的操作基本都与用户的ID相关，因此使用用户ID是最常用的分库的路由策略。用户的ID可以作为贯穿整个系统用的重要字段。因此，使用用户的ID我们不仅可以方便我们的查询，还可以将数据平均的分配到不同的数据库中。（当然，还可以根据类别等进行分表操作，分表的路由策略还有很多方式） 接着上述电商平台假设，订单表order存放用户的订单数据，sql脚本如下（只是为了演示，省略部分细节）： 12345CREATE TABLE `order` ( `order_id` bigint(32) primary key auto_increment, `user_id` bigint(32), ...) 当数据比较大的时候，对数据进行分表操作，首先要确定需要将数据平均分配到多少张表中，也就是：表容量。 这里假设有100张表进行存储，则我们在进行存储数据的时候，首先对用户ID进行取模操作，根据 user_id%100 获取对应的表进行存储查询操作，示意图如下： 例如，user_id = 101 那么，我们在获取值的时候的操作，可以通过下边的sql语句： 1select * from order_1 where user_id= 101 其中，order_1是根据 101%100 计算所得，表示分表之后的第一章order表。 注意： 在实际的开发中，如果你使用MyBatis做持久层的话，MyBatis已经提供了很好得支持数据库分表的功能，例如上述sql用MyBatis实现的话应该是： 接口定义： 12345678/** * 获取用户相关的订单详细信息 * @param tableNum 具体某一个表的编号 * @param userId 用户ID * @return 订单列表 */public List&lt;Order&gt; getOrder(@Param(&quot;tableNum&quot;) int tableNum,@Param(&quot;userId&quot;) int userId); xml配置映射文件： 1234&lt;select id=&quot;getOrder&quot; resultMap=&quot;BaseResultMap&quot;&gt; select * from order_$&#123;tableNum&#125; where user_id = #&#123;userId&#125; &lt;/select&gt; 其中${tableNum} 含义是直接让参数加入到sql中，这是MyBatis支持的特性。 注意： 1另外，在实际的开发中，我们的用户ID更多的可能是通过UUID生成的，这样的话，我们可以首先将UUID进行hash获取到整数值，然后在进行取模操作。 三、分库实现策略数据库分表能够解决单表数据量很大的时候数据查询的效率问题，但是无法给数据库的并发操作带来效率上的提高，因为分表的实质还是在一个数据库上进行的操作，很容易受数据库IO性能的限制。 因此，如何将数据库IO性能的问题平均分配出来，很显然将数据进行分库操作可以很好地解决单台数据库的性能问题。 分库策略与分表策略的实现很相似，最简单的都是可以通过取模的方式进行路由。 还是上例，将用户ID进行取模操作，这样的话获取到具体的某一个数据库，同样关键字有： 用户ID、库容量 路由的示意图如下： 上图中库容量为100。 同样，如果用户ID为UUID请先hash然后在进行取模。 四、分库与分表实现策略上述的配置中，数据库分表可以解决单表海量数据的查询性能问题，分库可以解决单台数据库的并发访问压力问题。 有时候，我们需要同时考虑这两个问题，因此，我们既需要对单表进行分表操作，还需要进行分库操作，以便同时扩展系统的并发处理能力和提升单表的查询性能，就是我们使用到的分库分表。 分库分表的策略相对于前边两种复杂一些，一种常见的路由策略如下： 123１、中间变量 ＝ user_id%（库数量*每个库的表数量）;２、库序号 ＝ 取整（中间变量／每个库的表数量）;３、表序号 ＝ 中间变量％每个库的表数量; 例如：数据库有256 个，每一个库中有1024个数据表，用户的user_id＝262145，按照上述的路由策略，可得： 123１、中间变量 ＝ 262145%（256*1024）= 1;２、库序号 ＝ 取整（1／1024）= 0;３、表序号 ＝ 1％1024 = 1; 这样的话，对于user_id＝262145，将被路由到第０个数据库的第１个表中。 示意图如下： 五、分库分表总结关于分库分表策略的选择有很多种，上文中根据用户ID应该是比较简单的一种。其他方式比如使用号段进行分区或者直接使用hash进行路由等。有兴趣的可以自行查找学习。 关于上文中提到的，如果用户的ID是通过UUID的方式生成的话，我们需要单独的进行一次hash操作，然后在进行取模操作等，其实hash本身就是一种分库分表的策略，使用hash进行路由策略的时候，我们需要知道的是，也就是hash路由策略的优缺点，优点是：数据分布均匀；缺点是：数据迁移的时候麻烦，不能按照机器性能分摊数据。 上述的分库和分表操作，查询性能和并发能力都得到了提高，但是还有一些需要注意的就是，例如：原本跨表的事物变成了分布式事物；由于记录被切分到不同的数据库和不同的数据表中，难以进行多表关联查询，并且不能不指定路由字段对数据进行查询。分库分表之后，如果我们需要对系统进行进一步的扩阵容（路由策略变更），将变得非常不方便，需要我们重新进行数据迁移。 最后需要指出的是，分库分表目前有很多的中间件可供选择，最常见的是使用淘宝的中间件Cobar。 GitHub地址：https://github.com/alibaba/cobara 文档地址为：https://github.com/alibaba/cobar/wiki 关于淘宝的中间件Cobar本篇内容不具体介绍，会在后边的学习中在做介绍。 另外Spring也可以实现数据库的读写分离操作，后边的文章，会进一步学习。 六、总结上述中，我们学到了如何进行数据库的读写分离和分库分表，那么，是不是可以实现一个可扩展、高性能、高并发的网站那？很显然还不可以!一个大型的网站使用到的技术远不止这些，可以说，这些都是其中的最基础的一个环节，因为还有很多具体的细节我们没有掌握到，比如：数据库的集群控制，集群的负载均衡，灾难恢复，故障自动切换，事务管理等等技术。因此，还有很多需要去学习去研究的地方。 总之： 1路漫漫其修远兮，吾将上下而求索。 前方道路美好而光明，2017年新征程，不泄步！ Mycat实现主从复制，读写分离，以及分库分表的实践Mycat是什么一个彻底开源的，面向企业应用开发的大数据库集群 支持事务、ACID、可以替代MySQL的加强版数据库 一个可以视为MySQL集群的企业级数据库，用来替代昂贵的Oracle集群 一个融合内存缓存技术、NoSQL技术、HDFS大数据的新型SQL Server 结合传统数据库和新型分布式数据仓库的新一代企业级数据库产品 一个新颖的数据库中间件产品 以上内容来自Mycat官网，简单来说，Mycat就是一个数据库中间件，对于我们开发来说，就像是一个代理，当我们需要使用到多个数据库和需要进行分库分表的时候，我们只需要在mycat里面配置好相关规则，程序无需做任何修改，只是需要将原本的数据源链接到mycat而已，当然如果以前有多个数据源，需要将数据源切换为单个数据源，这样有个好处就是当我们的数据量已经很大的时候，需要开始分库分表或者做读写分离的时候，不用修改代码（只需要改一下数据源的链接地址） 使用Mycat分表分库实践 haha,首先这不是一篇入门Mycat的博客但小编感觉又很入门的博客!这篇博客主要讲解Mycat中数据分片的相关知识，同时小编将会在本机数据库上进行测试验证，图文并茂展示出来。 数据库分区分表，咋一听非常地高大上，总有一种高高在上，望尘莫及的感觉，但小编想说的是，其实，作为一个开发人员，该来的总是会来，该学的东西你还是得学，区别只是时间先后顺序的问题。 一、分区分表分区就是把一个数据表的文件和索引分散存储在不同的物理文件中。 mysql支持的分区类型包括Range、List、Hash、Key，其中Range比较常用： RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区。 LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择。 HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。 KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值。 分表是指在逻辑上将一个表拆分成多个逻辑表，在整体上看是一张表，分表有水平拆分和垂直拆分两种,举个例子，将一张大的存储商户信息的表按照商户号的范围进行分表，将不同范围的记录分布到不同的表中。 二、Mycat 数据分片的种类Mycat 的分片其实和分表差不多意思，就是当数据库过于庞大，尤其是写入过于频繁且很难由一台主机支撑是，这时数据库就会面临瓶颈。我们将存放在同一个数据库实例中的数据分散存放到多个数据库实例（主机）上，进行多台设备存取以提高性能，在切分数据的同时可以提高系统的整体性。 数据分片是指将数据全局地划分为相关的逻辑片段，有水平切分、垂直切分、混合切分三种类型，下面主要讲下Mycat的水平和垂直切分。有一点很重要，那就是Mycat是分布式的，因此分出来的数据片分布到不同的物理机上是正常的，靠网络通信进行协作。 水平切分 就是按照某个字段的某种规则分散到多个节点库中，每个节点中包含一部分数据。可以将数据水平切分简单理解为按照数据行进行切分，就是将表中的某些行切分到一个节点，将另外某些行切分到其他节点，从分布式的整体来看它们是一个整体的表。 垂直切分 一个数据库由很多表构成，每个表对应不同的业务，垂直切分是指按照业务将表进行分类并分不到不同的节点上。垂直拆分简单明了，拆分规则明确，应用程序模块清晰、明确、容易整合，但是某个表的数据量达到一定程度后扩展起来比较困难。 混合切分 为水平切分和垂直切分的结合。 三、Mycat 垂直切分、水平切分实战1、垂直切分上面说到，垂直切分主要是根据具体业务来进行拆分的，那么，我们可以想象这么一个场景，假设我们有一个非常大的电商系统，那么我们需要将订单表、流水表、用户表、用户评论表等分别分不到不同的数据库中来提高吞吐量，架构图大概如下： 由于小编是在一台机器上测试，因此就只有host1这个节点，但不同的表还是依旧对应不同的数据库，只不过是所有数据库属于同一个数据库实例（主机）而已，后期不同主机只需增加&lt;dataHost&gt;节点即可。 mycat配置文件如下： server.xml 12345&lt;user name=&quot;root&quot;&gt; &lt;property name=&quot;password&quot;&gt;root&lt;/property&gt; // 对应四个逻辑库 &lt;property name=&quot;schemas&quot;&gt;order,trade,user,comment&lt;/property&gt;&lt;/user&gt; schema.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;!-- 4个逻辑库，对应4个不同的分片节点 --&gt; &lt;schema name=&quot;order&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;database1&quot; /&gt; &lt;schema name=&quot;trade&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;database2&quot; /&gt; &lt;schema name=&quot;user&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;database3&quot; /&gt; &lt;schema name=&quot;comment&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;database4&quot; /&gt; &lt;!-- 四个分片，对应四个不同的数据库 --&gt; &lt;dataNode name=&quot;database1&quot; dataHost=&quot;localhost1&quot; database=&quot;database1&quot; /&gt; &lt;dataNode name=&quot;database2&quot; dataHost=&quot;localhost1&quot; database=&quot;database2&quot; /&gt; &lt;dataNode name=&quot;database3&quot; dataHost=&quot;localhost1&quot; database=&quot;database3&quot; /&gt; &lt;dataNode name=&quot;database4&quot; dataHost=&quot;localhost1&quot; database=&quot;database4&quot; /&gt; &lt;!-- 实际物理主机，只有这一台 --&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; 登陆本机mysql，创建order,trade,user,comment4个数据库: 1234create database database1 character set utf8;create database database2 character set utf8;create database database3 character set utf8;create database database4 character set utf8; 执行bin目录下的startup_nowrap.bat文件，如果输出下面内容，则说明已经启动mycat成功，如果没有，请检查order,trade,user,comment4个数据库是否已经创建。 采用下面语句登陆Mycat服务器： mysql -uroot -proot -P8066 -h127.0.0.1 在comment数据库中创建Comment表，并插入一条数据 上图1处新建一个Comment表，2处插入一条记录，3处查看记录插入到哪个数据节点中，即database4。 2、水平切分server.xml 1234&lt;user name=&quot;root&quot;&gt; &lt;property name=&quot;password&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt;&lt;/user&gt; schema.xml 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot;?&gt;&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt; &lt;table name=&quot;travelrecord&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; /&gt; &lt;/schema&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;0&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;localhost:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; rule.xml 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mycat:rule SYSTEM &quot;rule.dtd&quot;&gt;&lt;mycat:rule xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;tableRule name=&quot;auto-sharding-long&quot;&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; rang-long &lt;/rule&gt; &lt;/tableRule&gt; &lt;function name=&quot;rang-long&quot; class=&quot;io.mycat.route.function.AutoPartitionByLong&quot;&gt; &lt;property name=&quot;mapFile&quot;&gt;autopartition-long.txt&lt;/property&gt; &lt;/function&gt;&lt;/mycat:rule&gt; conf目录下的autopartition-long.txt 12345# range start-end ,data node index# K=1000,M=10000.0-500M=0500M-1000M=11000M-1500M=2 上面的配置创建了一个名为TESTDB的逻辑库，并指定了需要切分的表&lt;table&gt;标签，表名为travelrecord,分区的策略采用rang-long算法，即根据id数据列值的范围进行切分，具体的规则在autopartition-long.txt文件中定义，即id在0-500*10000范围内的记录存放在db1的travelrecord表中，id在500*10000 - 1000*10000范围内的记录存放在db2数据库的travelrecord表中，下面我们插入两条数据，验证是否和分片规则一致。 创建db1,db2,db3数据库 123create database db1 character set utf8;create database db2 character set utf8;create database db3 character set utf8; 确实是这样的，到此我们就完成了mycat数据库的水平切分，这个例子只是演示按照id列值得范围进行切分，mycat还支持很多的分片算法，如取模、一致性哈希算法、按日期分片算法等等，大家可以看《分布式数据库架构及企业实战—-基于Mycat中间件》这本书深入学习。 为什么需要读写分离至于为什么需要读写分离，在我之前的文章有介绍过了，相信看到这篇文章的人也知道为什么需要读写分离了，当然如果你也需要了解一下，那么欢迎查看我之前的文章SpringBoot Mybatis 读写分离配置,顺便也可以了解一下怎么通过代码进行读写分离的 MySQL主从复制主从复制是读写分离的关键，不管通过什么方式进行读写分离，前提就是MySQL有主从复制，当前双机主从也行，但是关键的关键，是要能保证2个库的数据能一致（出掉刚写入主库从库还未能及时反应过来的情况），如果2个库的数据不一致，那么读写分离也有没有任何意义了，具体MySQL怎么做主从复制可以查看我之前的文章MySQL主从复制搭建，基于日志（binlog） Mycat读写分离设置配置Mycat用户Mycat的用户就跟MySQL用户是同一个意思，主要配置链接到Mycat的用户名以及密码，以及能使用的逻辑库，用户信息主要在server.xml中配置的，具体如下 12345678910111213141516171819202122232425262728293031323334353637383940&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- - - Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); - you may not use this file except in compliance with the License. - You may obtain a copy of the License at - - http://www.apache.org/licenses/LICENSE-2.0 - - Unless required by applicable law or agreed to in writing, software - distributed under the License is distributed on an &quot;AS IS&quot; BASIS, - WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. - See the License for the specific language governing permissions and - limitations under the License. --&gt;&lt;!DOCTYPE mycat:server SYSTEM &quot;server.dtd&quot;&gt; &lt;mycat:server xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;system&gt; &lt;property name=&quot;defaultSqlParser&quot;&gt;druidparser&lt;/property&gt; &lt;!-- &lt;property name=&quot;useCompression&quot;&gt;1&lt;/property&gt;--&gt; &lt;!--1为开启mysql压缩协议--&gt; &lt;!-- &lt;property name=&quot;processorBufferChunk&quot;&gt;40960&lt;/property&gt; --&gt; &lt;!-- &lt;property name=&quot;processors&quot;&gt;1&lt;/property&gt; &lt;property name=&quot;processorExecutor&quot;&gt;32&lt;/property&gt; --&gt; &lt;!--默认是65535 64K 用于sql解析时最大文本长度 --&gt; &lt;!--&lt;property name=&quot;maxStringLiteralLength&quot;&gt;65535&lt;/property&gt;--&gt; &lt;!--&lt;property name=&quot;sequnceHandlerType&quot;&gt;0&lt;/property&gt;--&gt; &lt;!--&lt;property name=&quot;backSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt; &lt;!--&lt;property name=&quot;frontSocketNoDelay&quot;&gt;1&lt;/property&gt;--&gt; &lt;!--&lt;property name=&quot;processorExecutor&quot;&gt;16&lt;/property&gt;--&gt; &lt;!-- &lt;property name=&quot;mutiNodeLimitType&quot;&gt;1&lt;/property&gt; 0：开启小数量级（默认） ；1：开启亿级数据排序 &lt;property name=&quot;mutiNodePatchSize&quot;&gt;100&lt;/property&gt; 亿级数量排序批量 &lt;property name=&quot;processors&quot;&gt;32&lt;/property&gt; &lt;property name=&quot;processorExecutor&quot;&gt;32&lt;/property&gt; &lt;property name=&quot;serverPort&quot;&gt;8066&lt;/property&gt; &lt;property name=&quot;managerPort&quot;&gt;9066&lt;/property&gt; &lt;property name=&quot;idleTimeout&quot;&gt;300000&lt;/property&gt; &lt;property name=&quot;bindIp&quot;&gt;0.0.0.0&lt;/property&gt; &lt;property name=&quot;frontWriteQueueSize&quot;&gt;4096&lt;/property&gt; &lt;property name=&quot;processors&quot;&gt;32&lt;/property&gt; --&gt; &lt;/system&gt; &lt;user name=&quot;raye&quot;&gt; &lt;property name=&quot;password&quot;&gt;rayewang&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;separate&lt;/property&gt; &lt;/user&gt; &lt;/host&gt; &lt;/mycat:server&gt; 其中&lt;user name=&quot;raye&quot;&gt;定义了一个名为raye的用户，标签user中的&lt;property name=&quot;password&quot;&gt;rayewang&lt;/property&gt;定义了用户的密码，&lt;property name=&quot;schemas&quot;&gt;separate&lt;/property&gt;定义了用户可以使用的逻辑库 配置Mycat逻辑库Mycat的配置有很多，不过因为我们只是使用Mycat的读写分类的功能，所以用到的配置并不多，只需要配置一些基本的，当然本文也只是会介绍到读写分离相关的配置，其他配置建议读者自己查看一下文档，或者通过其他方式了解，逻辑库是在schema.xml中配置的 首先介绍Mycat逻辑库中的一些配置标签 schemaschema 标签是用来定义逻辑库的，schema有四个属性dataNode,checkSQLschema,sqlMaxLimit,name dataNode 标签属性用于绑定逻辑库到某个具体的 database 上，1.3 版本如果配置了 dataNode，则不可以配置分片表，1.4 可以配置默认分片，只需要配置需要分片的表即可 name是定义当前逻辑库的名字的，方便server.xml中定义用户时的引用 checkSQLschema当该值设置为 true 时，如果我们执行语句select * from separate.users;则 MyCat 会把语句修改 为select * from users;。即把表示 schema 的字符去掉，避免发送到后端数据库执行时报（ERROR 1146 (42S02): Table ‘separate.users’ doesn’t exist）。 不过，即使设置该值为 true ，如果语句所带的是并非是 schema 指定的名字，例如：select * from db1.users; 那么 MyCat 并不会删除 db1 这个字段，如果没有定义该库的话则会报错，所以在提供 SQL语句的最好是不带这个字段。 sqlMaxLimit当该值设置为某个数值时。每条执行的 SQL 语句，如果没有加上 limit 语句，MyCat 也会自动的加上所对应的值。例如设置值为 100，执行select * from users;的效果为和执行select * from users limit 100;相同。设置该值的话，MyCat 默认会把查询到的信息全部都展示出来，造成过多的输出。所以，在正常使用中，还是建议加上一个值，用于减少过多的数据返回。当然 SQL 语句中也显式的指定 limit 的大小，不受该属性的约束。需要注意的是，如果运行的 schema 为非拆分库的，那么该属性不会生效。需要手动添加 limit 语句。 schema标签中有标签table用于定义不同的表分片信息，不过我们只是做读写分离，并不会用到，所以这里就不多介绍了 dataNodedataNodedataNode 标签定义了 MyCat 中的数据节点，也就是我们通常说所的数据分片。一个 dataNode 标签就是一个独立的数据分片,dataNode有3个属性:name,dataHost,database。 name定义数据节点的名字，这个名字需要是唯一的，此名字是用于table标签和schema标签中引用的 dataHost该属性用于定义该分片属于哪个数据库实例的，属性值是引用 dataHost 标签上定义的 name 属性 database该属性用于定义该分片属性哪个具体数据库实例上的具体库，因为这里使用两个纬度来定义分片，就是：实例+具体的库。因为每个库上建立的表和表结构是一样的。所以这样做就可以轻松的对表进行水平拆分 dataHostdataHost是定义真实的数据库连接的标签，该标签在 mycat 逻辑库中也是作为最底层的标签存在，直接定义了具体的数据库实例、读写分离配置和心跳语句，dataHost有7个属性：name,maxCon,minCon,balance,writeType,dbType,dbDriver,有2个标签heartbeat,writeHost,其中writeHost标签中又包含一个readHost标签 name唯一标识 dataHost 标签，供dataNode标签使用 maxCon指定每个读写实例连接池的最大连接。也就是说，标签内嵌套的 writeHost、readHost 标签都会使用这个属性的值来实例化出连接池的最大连接数 minCon指定每个读写实例连接池的最小连接，初始化连接池的大小 balance 读取负载均衡类型 balance=”0”, 不开启读写分离机制，所有读操作都发送到当前可用的 writeHost 上。 balance=”1”，全部的 readHost 与 stand by writeHost 参与 select 语句的负载均衡，简单的说，当双主双从模式(M1-&gt;S1，M2-&gt;S2，并且 M1 与 M2 互为主备)，正常情况下，M2,S1,S2 都参与 select 语句的负载均衡。 balance=”2”，所有读操作都随机的在 writeHost、readhost 上分发。 balance=”3”，所有读请求随机的分发到 wiriterHost 对应的 readhost 执行，writerHost 不负担读压力 writeType写入负载均衡类型，目前的取值有 3 种： writeType=”0”, 所有写操作发送到配置的第一个 writeHost，第一个挂了切到还生存的第二个writeHost，重新启动后已切换后的为准，切换记录在配置文件中:dnindex.properties . writeType=”1”，所有写操作都随机的发送到配置的 writeHost dbType 指定后端连接的数据库类型，目前支持二进制的 mysql 协议，还有其他使用 JDBC 连接的数据库。例如：mongodb、oracle、spark 等 dbDriver指定连接后端数据库使用的 Driver，目前可选的值有 native 和 JDBC。使用 native 的话，因为这个值执行的 是二进制的 mysql 协议，所以可以使用 mysql 和 maridb。其他类型的数据库则需要使用 JDBC 驱动来支持。从 1.6 版本开始支持 postgresql 的 native 原始协议。 如果使用 JDBC 的话需要将符合 JDBC 4 标准的驱动 JAR 包放到 MYCAT\lib 目录下，并检查驱动 JAR 包中包括如下目录结构的文件：META-INF\services\java.sql.Driver。在这个文件内写上具体的 Driver 类名，例如： com.mysql.jdbc.Driver。 heartbeat这个标签内指明用于和后端数据库进行心跳检查的语句。例如,MYSQL 可以使用 select user()，Oracle 可以使用 select 1 from dual 等。 这个标签还有一个 connectionInitSql 属性，主要是当使用 Oracla 数据库时，需要执行的初始化 SQL 语句就这个放到这里面来。例如：alter session set nlsdateformat=’yyyy-mm-dd hh24:mi:ss’ writeHost，readHost这两个标签都指定后端数据库的相关配置给 mycat，用于实例化后端连接池。唯一不同的是，writeHost 指定写实例、readHost 指定读实例，组着这些读写实例来满足系统的要求。 在一个 dataHost 内可以定义多个 writeHost 和 readHost。但是，如果 writeHost 指定的后端数据库宕机，那么这个 writeHost 绑定的所有 readHost 都将不可用。另一方面，由于这个 writeHost 宕机系统会自动的检测到，并切换到备用的 writeHost 上去,这2个标签属性都一致，拥有host,url,password,user,weight,usingDecrypt等属性 host用于标识不同实例，一般 writeHost 我们使用M1，readHost 我们用S1 url真实数据库的实例的链接地址，如果是使用 native 的 dbDriver，则一般为 address:port 这种形式。用 JDBC 或其他的dbDriver，则需要特殊指定。当使用 JDBC 时则可以这么写：jdbc:mysql://localhost:3306/ user真实数据库实例的链接用户名 password真实数据库实例的链接密码 weight权重 配置在 readhost 中作为读节点的权重,主要用于多台读取的数据库实例机器配置不同的情况，可以根据权重调整访问量 usingDecrypt是否对密码加密默认 0 否 如需要开启配置 1，同时使用加密程序对密码加密 注意，readHost是在writeHost标签内的，不是单独的 以下是我的读写分离配置文件 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot;?&gt; &lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt; &lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt; &lt;schema name=&quot;separate&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;/&gt; &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;test&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=&quot;hostM1&quot; url=&quot;192.168.1.126:3307&quot; user=&quot;root&quot; password=&quot;123456&quot;&gt; &lt;!-- can have multi read hosts --&gt; &lt;readHost host=&quot;hostS2&quot; url=&quot;192.168.1.126:3308&quot; user=&quot;root&quot; password=&quot;123456&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; 前面已经差不多都解释清楚了，因为我只是用的基本的主从复制，所以我的将dataHost的balance设置成了3 启动mycat，然后用数据库连接工具连接到mycat，可以测试是否配置成功，最简单的就是通过修改从库的数据，这样方便查看到底是运行到哪个库上面了，另外由于我是基于docker启动的mycat，所以如果是直接在系统中运行的mycat的，可以去看官方文档，看看到底怎么启动mycat 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>Java技术江湖</category>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的2018秋招总结]]></title>
    <url>%2F2018%2F09%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2Falldone%2F</url>
    <content type="text"><![CDATA[本文是我在2018年7月到9月份参加校园招聘的一篇总结。主要包括以下内容： 1 秋招小结 2 面经分享 3 Java后端技术专栏 4 Java工程师书单 5 学习资源分享 6 大牛博客推荐 7 笔试经验 8 面试经验 秋招小结 从七月初第一次投递简历，到九月初，整整两个月的时间，大大小小投了几十家公司，其中很多都是提前批，内推，也经历了许多的笔试，面试。 期间也拿了几个offer，包括百度，蚂蚁金服，华为，网易（网易云音乐没给offer，调到了其他部门）。有几家直接收到拒信的，包括拼多多，深信服。还有几家在等待结果，包括腾讯，头条（头条今天刚刚收到意向书），快手，斗鱼等。 当然也有一些还没面试完的公司以及待安排面试的公司，这里就不展开说了。 八月底基本上提前批就已经结束了，所以一般这段时间正式校招也开始了，各种大规模的笔试也很多，所以大家即使没有拿到offer也不要灰心，毕竟校招是一场持久战，基本上要到九月十月才能下结论。 我之前分享了很多公司的面经，其实大部分都是提前批的，很多都是直接免笔试的，因为我对算法并不是很在行，所以感觉还是比较幸运的。 包括头条拿到了白金码，也很感谢那位给我白金码的牛友。另外牛客上的招聘信息，笔经面经也让我受益匪浅，所以还是很感谢牛客这个平台，我也希望能够写一些东西来回馈牛客网和各位牛友。 号外号外： 1234567891011九月份秋招刚刚开始，很多同学也在紧锣密鼓地准备笔试面试。在这里我搞了一个交流群，希望能够帮助到各位，大家如果有问题也可以一起交流探讨。关注公众号：程序员江湖，然后点击公众号的“联系方式”即可获取群聊二维码。如果过期或者满人也可以加我微信，我可以拉你进群。当然，如果有什么问题想要与我交流也可以加我微信。微信二维码如下： 面经分享 具体的面经都比较长，这里大概介绍一下面试的情况，然后我会放上面经的链接供大家查阅。 1 阿里面经 阿里中间件：https://www.nowcoder.com/discuss/8786 蚂蚁金服：https://www.nowcoder.com/discuss/91738 岗位是研发工程师，直接找蚂蚁金服的大佬进行内推。 我参与了阿里巴巴中间件部门的提前批面试，一共经历了四次面试，拿到了口头offer。 然后我也参加了蚂蚁金服中间件部门的面试，经历了三次面试，但是没有走流程，所以面试中止了。 最后我走的是蚂蚁金服财富事业群的流程，经历了四次面试，包括一次交叉面，最终拿到了蚂蚁金服的意向书，评级为A。 阿里的面试体验还是比较好的，至少不要求手写算法，但是非常注重Java基础，中间件部门还会特别安排Java基础笔试。 2 腾讯面经 https://www.nowcoder.com/discuss/100383 岗位是后台开发工程师，我没有选择意向事业群。 SNG的部门捞了我的简历，开始了面试，他们的技术栈主要是Java，所以比较有的聊。 一共经历了四次技术面试和一次HR面试，目前正在等待结果。 腾讯的面试一如既往地注重考查网络和操作系统，并且喜欢问Linux底层的一些知识，在这方面我还是有很多不足的。 3 百度面经 https://www.nowcoder.com/discuss/90112 https://www.nowcoder.com/discuss/89690 岗位是研发工程师岗位，部门包括百度智能云的三个分部门以及大搜索部门。 百度的提前批面试不走流程，所以可以同时面试好多个部门，所以我参加百度面试的次数大概有12次左右，最终应该是拿了两个部门的offer。 百度的面试风格非常统一，每次面试基本都要到电脑上写算法，所以那段时间写算法写的头皮发麻。 4 网易面经 https://www.nowcoder.com/discuss/98494 面试部门是网易云音乐，岗位是Java开发工程师。 网易是唯一一家我去外地面试的公司，也是我最早去实习的老东家。 一共三轮面试，耗时一个下午。 网易的面试比我想象中的要难，面试官会问的问题都比较深，并且会让你写一些结合实践的代码。 5 头条面经 https://www.nowcoder.com/discuss/94233 岗位是后台研发工程师，地点选择了上海。 我参加的是字节跳动的内推面试，当时找了一个牛友要到了白金码，再次感谢这位头条大佬。 然后就开始了一下午的视频面试，一共三轮技术面试，每一轮都要写代码，问问题的风格有点像腾讯，也喜欢问一些底层知识，让我有点懵逼。 目前还在等待结果。 6 快手面经 https://www.nowcoder.com/discuss/99283 岗位是Java开发工程师，面试我的部门好像是基础架构部门。 快手是两轮视频面试加上一轮hr面试。 7 拼多多面经 https://www.nowcoder.com/discuss/95942 岗位是业务平台研发工程师。 当时在学校里参加了面试，过程是比较顺利的，问的问题也都比较有难度。 自我感觉良好，但是最后却收到了拒信，还是挺可惜的。 Java后端技术专栏 对于校园招聘来说，最重要的还是基础知识。下面的博客专栏出自我的技术博客 https://blog.csdn.net/a724888 这些专栏中有一些文章是我自己原创的，也有一些文章是转载自技术大牛的，基本都是是我在学习Java后端的两年时间内陆续完成的。 总的来说算是比较全面了，做后端方向的同学可以参考一下。 深入浅出Java核心技术 本专栏主要介绍Java基础，并且会结合实现原理以及具体实例来讲解。同时还介绍了Java集合类，设计模式以及Java8的相关知识。 深入理解JVM虚拟机 带你走进JVM的世界，整合高质量文章以阐述虚拟机的原理及相关技术，让开发者更好地了解Java的底层运行原理以及相应的调优方法。 Java并发指南 本专栏主要介绍Java并发编程相关的基本原理以及进阶知识。主要包括Java多线程基础，Java并发编程基本原理以及JUC并发包的使用和源码解析。 Java网络编程与NIO Java网络编程一直是很重要的一部分内容，其中涉及了socket的使用，以及Java网络编程的IO模型，譬如BIO,NIO,AIO，当然也包括Linux的网络编程模型。 了解这部分知识对于理解网络编程有很多帮助。另外还补充了两个涉及NIO的重要技术：Tomcat和Netty。 JavaWeb技术世界 从这里开始打开去往JavaWeb世界的大门。什么是J2EE，什么是JavaWeb，以及这个生态中常用的一些技术：Maven，Spring，Tomcat，Junit，log4j等等。 我们不仅要了解怎么使用它们，更要去了解它们为什么出现，其中一些技术的实现原理是什么。 Spring与SpringMVC源码解析 本专栏主要讲解Spring和SpringMVC的实现原理。Spring是最流行的Java框架之一。 本专栏文章主要包括IOC的实现原理分析，AOP的实现原理分析，事务的实现源码分析等，当然也有SpringMVC的源码解析文章。 重新学习MySQL与Redis 本专栏介绍MySQL的基本知识，比如基本架构，存储引擎，索引原理，主从复制，事务等内容。当然也会讲解一些和sql语句优化有关的知识。 同时本专栏里也介绍了Redis的基本实现原理，包括数据结构，主从复制，集群方案，分布式锁等实现。 分布式系统理论与实践 本专栏介绍分布式的基本理论和相关技术，比如CAP和BASE理论，一致性算法，以及ZooKeeper这类的分布式协调服务。 在分布式实践方面，我们会讲到负载均衡，缓存，分布式事务，分布式锁，以及Dubbo这样的微服务，也包括消息队列，数据库中间件等等。 后端开技术杂谈 本专栏涵盖了大后端的众多技术文章，当你在Java后端方面有一定基础以后，再多了解一些相关技术总是有好处的。 除了Java后端的文章以外，还会涉及Hadoop生态，云计算技术，搜索引擎，甚至包括一些数据挖掘和AI的文章。 总的来说选取了一些不错的基础类文章，能让你对大后端有一个更直观的认识。 Java工程师书单 我之前专门写了一篇文章介绍了Java工程师的书单，可以这里重点列举一些好书，推荐给大家。 完整内容可以参考这个帖子：https://www.nowcoder.com/discuss/99317 《计算机网络：自顶向下》这本从应用层讲到物理层，感觉这种方式学起来更轻松。 《图解算法》《啊哈算法》这两部书籍非常适合学习算法的入门，前者主要用图解的形式覆盖了大部分常用算法，包括dp，贪心等等，可以作为入门书，后者则把很多常用算法都进行了实现，包括搜索，图，树等一些比较高级的常用算法。 《剑指offer》这本书还是要强烈推荐的，毕竟是面试题经常参考的书籍，当然最好有前面基本的铺垫再看，可能收获更大，这本书在面试之前一般都要嚼烂。如果想看Java版本的代码，可以到我的Github仓库中查看。 《Java编程思想》这本书也是被誉为Java神书的存在了，但是对新手不友好，适合有些基础再看，当然要选择性地看。我当时大概只看了1/3 《Java核心技术卷一》这本书还是比较适合入门的，当然，这种厚皮书要看完还是很有难度的，不过比起上面那本要简单一些 《深入理解JVM虚拟机》这本书是Java开发者必须看的书，很多jvm的文章都是提取这本书的内容。JVM是Java虚拟机，赋予了Java程序生命，所以好好看看把，我自己就已经看了三遍了。 《Java并发编程艺术》这本书是国内作者写的Java并发书籍，比上面那一本更简单易懂，适合作为并发编程的入门书籍，当然，学习并发原理之前，还是先把Java的多线程搞懂吧。 《深入JavaWeb技术内幕》这本书是Java Web的集大成之作，涵盖了大部分Java Web开发的知识点，不过一本书显然无法把所有细节都讲完，但是作为Java Web的入门或者进阶书籍来看的话还是很不错的。 《Redis设计与实现》该书全面而完整地讲解了 Redis 的内部运行机制,对 Redis 的大多数单机功能以及所有多机功能的实现原理进行了介绍。这本书把Redis的基本原理讲的一清二楚，包括数据结构，持久化，集群等内容，有空应该看看。 《大型网站技术架构》这本淘宝系技术指南还是非常值得推崇的，可以说是把大型网站的现代架构进行了一次简单的总结，内容涵盖了各方面，主要讲的是概念，很适合没接触过架构的同学入门。看完以后你会觉得后端技术原来这么博大精深。 《分布式服务框架原理与实践》上面那本书讲的是分布式架构的实践，而这本书更专注于分布式服务的原理讲解和对应实践，很好地讲述了分布式服务的基本概念，相关技术，以及解决方案等，对于想要学习分布式服务框架的同学来说是本好书。 《从Paxos到Zookeeper分布式一致性原理与实践》说起分布式系统，我们需要了解它的原理，相关理论及技术，这本书也是从这个角度出发，讲解了分布式系统的一些常用概念，并且带出了分布式一哥zookeeper，可以说是想学分布式技术的同学必看的书籍。 《大数据技术原理与应用》作为大数据方面的一本教材，厦大教授写的这本书还是非常赞的，从最基础的原理方面讲解了Hadoop的生态系统，并且把每个组件的原理都讲得比较清楚，另外也加入了spark，storm等内容，可以说是大数据入门非常好的一本书了。 技术大牛推荐 1 江南白衣这位大大绝对是我的Java启蒙导师，他推荐的Java后端书架让我受益匪浅。 2 码农翻身刘欣，一位工作15年的IBM架构师，用最浅显易懂的文章讲解技术的那些事，力荐，他的文章帮我解决了很多困惑。 3 CoolShell陈皓老师的博客相信大家都看过，干货很多，酷壳应该算是国内最有影响力的个人博客了。 4 廖雪峰学习Git和Python，看它的博客就够了。 5 HollisChuang阿里一位研发大佬的博客，主要分享Java技术文章，内容还不错。 6 梁桂钊阿里另一位研发大佬，博客里的后端技术文章非常丰富。 7 chenssy这位大佬分享的Java技术文章也很多，并且有很多基础方面的文章，新手可以多看看。 8 Java Doop一位魔都Java开发者的技术博客，里面有一些不错的讲解源码的文章，数量不是很多，但是质量都挺不错的。 学习资源分享 学习Java后端两年的时间里，接触过很多的资料，网站和课程，也走了不少弯路，所以这里也总结一些比较好的资源推荐给大家。 0 CSDN和博客园，主流的技术交流平台，虽然广告越打越多了，但是还是有很多不错的博文的。 首先还是安利一下我的CSDN博客：https://blog.csdn.net/a724888 1 importnew 专注Java学习资源分享，适合Java初学者。 2 并发编程网，主要分享Java相关进阶内容，适合Java提高。 3 推酷 一个不错的技术分享社区。 4 segmentfault，有点像国内的Stack Overflow，适合交流代码问题的地方。 5 掘金，一个很有极客范的技术社区，强推，有很多技术大牛分享优质文章。 6 开发者头条，一个整合优质技术博客的社区，里面基本上都是精选的高质量博文，适合技术学习提升。 7 v2ex，一个极客社区，除了交流技术以外还会有很多和程序员生活相关的话题分享。 8 知乎这个就不必多说了。我在知乎上也有Java技术和校招的专栏，有兴趣的同学可以看看：https://www.zhihu.com/people/h2pl 9 简书简书上有些技术文章也很不错，有空大家也可以去看看。 10 Github 有一些GitHub的项目还是非常不错的，其中也有仓库会分享技术文章。 我的GitHub：包括Java技术仓库，刷题指南以及项目记录https://github.com/h2pl 笔试经验 提前批的笔试其实不是很多，我参加了网易，网易游戏，拼多多等公司的笔试，应该都是低分飘过。 我的算法基础比较一般，读研之前0基础，所以这方面不是很有发言权，大概说几点我的学习经验。 1234567891 打好数据结构基础2 先易后难，看一些基础的算法书籍，比如《图结算法》，《啊哈算法》等等。3 剑指offer刷起来，两到三遍，做到胸有成竹4 LeetCode刷个200题左右，记得二刷，做好总结。5 到牛客网做公司的历年真题，熟悉题型，保持手感。 剑指offer指南和LeetCode刷题指南可以在我的GitHub中参阅： https://github.com/h2pl/Java-Tutorial 其中LeetCode指南是参考@CyC2018大佬的文章。 更详细的内容后续会在我的公众号更新：程序员江湖 面试经验 面试主要考的还是你的基础知识，需要你对Java后端技术栈有一个全局上的把握，具体说起来就太多了。 Java后端技术专栏可以参考我的博客：https://blog.csdn.net/a724888 有关技术总结和项目经验的内容可以查看我的GitHub：https://github.com/h2pl 我个人也总结了一些面试方面的经验，主要是一些技巧。 1234567891 做好自我介绍和项目总结，把握你发言的主动权2 搞清楚简历上的技术点，兵来将挡水来土掩3 注意分点答题，思路清晰，也更容易讲清楚原理。4 压力面下保持冷静，不要回怼面试官5 HR面试注意常用技巧，可以提前准备。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>校园招聘</category>
      </categories>
      <tags>
        <tag>校园招聘</tag>
        <tag>秋招总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心技术学习总结]]></title>
    <url>%2F2018%2F07%2F10%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文主要是我最近复习Java基础原理过程中写的Java基础学习总结。Java的知识点其实非常多，并且有些知识点比较难以理解，有时候我们自以为理解了某些内容，其实可能只是停留在表面上，没有理解其底层实现原理。 纸上得来终觉浅，绝知此事要躬行。笔者之前对每部分的内容对做了比较深入的学习以及代码实现，基本上比较全面地讲述了每一个Java基础知识点，当然可能有些遗漏和错误，还请读者指正。 这里先把整体的学习大纲列出来，让大家对知识框架有个基本轮廓，具体每个部分的内容，笔者都对应写了一篇博文来加以讲解和剖析，并且发表在我的个人博客和csdn技术专栏里，下面给出地址 专栏：深入理解Java原理 https://blog.csdn.net/column/details/21930.html 相关代码实现在我的GitHub里： https://github.com/h2pl/MyTech 喜欢的话麻烦star一下哈 本系列技术文章首发于我的个人博客： https://h2pl.github.io 更多关于Java后端学习的内容请到我的CSDN博客上查看： https://blog.csdn.net/a724888 Java基础学习总结每部分内容会重点写一些常见知识点，方便复习和记忆，但是并不是全部内容，详细的内容请参见具体的文章地址。 面向对象三大特性继承：一般类只能单继承，内部类实现多继承，接口可以多继承 封装：访问权限控制public &gt; protected &gt; 包 &gt; private 内部类也是一种封装 多态：编译时多态，体现在向上转型和向下转型，通过引用类型判断调用哪个方法（静态分派）。 运行时多态，体现在同名函数通过不同参数实现多种方法（动态分派）。基本数据类型基本类型位数，自动装箱，常量池 例如byte类型是1byte也就是8位，可以表示的数字是-128到127，因为还有一个0，加起来一共是256，也就是2的八次方。 32位和64位机器的int是4个字节也就是32位，char是1个字节就是8位，float是4个字节，double是8个字节，long是8个字节。 所以它们占有字节数是相同的，这样的话两个版本才可以更好地兼容。（应该） 基本数据类型的包装类只在数字范围-128到127中用到常量池，会自动拆箱装箱，其余数字范围的包装类则会新建实例String及包装类String类型是final类型，在堆中分配空间后内存地址不可变。 底层是final修饰的char[]数组，数组的内存地址同样不可变。 但实际上可以通过修改char[n] = &apos;a&apos;来进行修改，不会改变String实例的内存值，不过在jdk中，用户无法直接获取char[]，也没有方法能操作该数组。 所以String类型的不可变实际上也是理论上的不可变。所以我们在分配String对象以后，如果将其 = &quot;abc&quot;，那也只是改变了引用的指向，实际上没有改变原来的对象。 StringBuffer和StringBuilder底层是可变的char[]数组，继承父类AbstractStringBuilder的各种成员和方法，实际上的操作都是由父类方法来完成的。final关键字final修饰基本数据类型保证不可变 final修饰引用保证引用不能指向别的对象，否则会报错。 final修饰类，类的实例分配空间后地址不可变，子类不能重写所有父类方法。因此在cglib动态代理中，不能为一个类的final修饰的函数做代理，因为cglib要将被代理的类设置为父类，然后再生成字节码。 final修饰方法，子类不能重写该方法。抽象类和接口1 抽象类可以有方法实现。 抽象类可以有非final成员变量。 抽象方法要用abstract修饰。 抽象类可以有构造方法，但是只能由子类进行实例化。 2 接口可以用extends加多个接口实现多继承。 接口只能有public final类型的成员变量。 接口只能有抽象方法，不能有方法体、 接口不能实例化，但是可以作为引用类型。代码块和加载顺序假设该类是第一次进行实例化。那么有如下加载顺序 静态总是比非静态优先，从早到晚的顺序是： 1 静态代码块 和 静态成员变量的顺序根据代码位置前后来决定。 2 代码块和成员变量的顺序也根据代码位置来决定 3 最后才调用构造方法构造方法包、内部类、外部类1 Java项目一般从src目录开始有com.*.*.A.java这样的目录结构。这就是包结构。所以一般编译后的结构是跟包结构一模一样的，这样的结构保证了import时能找到正确的class引用包访问权限就是指同包下的类可见。 import 一般加上全路径，并且使用.*时只包含当前目录的所有类文件，不包括子目录。 2 外部类只有public和default两种修饰，要么全局可访问，要么包内可访问。 3 内部类可以有全部访问权限，因为它的概念就是一个成员变量，所以访问权限设置与一般的成员变量相同。 非静态内部类是外部类的一个成员变量，只跟外部类的实例有关。 静态内部类是独立于外部类存在的一个类，与外部类实例无关，可以通过外部类.内部类直接获取Class类型。异常1 异常体系的最上层是Throwable类 子类有Error和Exception Exception的子类又有RuntimeException和其他具体的可检查异常。 2 Error是jvm完全无法处理的系统错误，只能终止运行。 运行时异常指的是编译正确但运行错误的异常，如数组越界异常，一般是人为失误导致的，这种异常不用try catch，而是需要程序员自己检查。 可检查异常一般是jvm处理不了的一些异常，但是又经常会发生，比如Ioexception，Sqlexception等，是外部实现带来的异常。 3 多线程的异常流程是独立的，互不影响。 大型模块的子模块异常一般需要重新封装成外部异常再次抛出，否则只能看到最外层异常信息，难以进行调试。 日志框架是异常报告的最好帮手，log4j，slf4j中，在工作中必不可少。泛型Java中的泛型是伪泛型，只在编译期生效，运行期自动进行泛型擦除，将泛型替换为实际上传入的类型。 泛型类用class &lt;T&gt; A { }这样的形式表示，里面的方法和成员变量都可以用T来表示类型。泛型接口也是类似的，不过泛型类实现泛型接口时可以选择注入实际类型或者是继续使用泛型。 泛型方法可以自带泛型比如void &lt;E&gt; E go(); 泛型可以使用?通配符进行泛化 Object&lt;?&gt;可以接受任何类型 也可以使用 &lt;? extends Number&gt; &lt;? super Integer&gt;这种方式进行上下边界的限制。Class类和Object类Java反射的基础是Class类，该类封装所有其他类的类型信息，并且在每个类加载后在堆区生成每个类的一个Class&lt;类名&gt;实例，用于该类的实例化。 Java中可以通过多种方式获取Class类型，比如A.class,new A().getClass()方法以及Class.forName(&quot;com.?.?.A&quot;)方法。 Object是所有类的父类，有着自己的一些私有方法，以及被所有类继承的9大方法。 有人讨论Object和Class类型谁先加载谁后加载，因为每个类都要继承Object，但是又得先被加载到堆区，事实上，这个问题在JVM初始化时就解决了，没必要多想。javac和javajavac 是编译一个java文件的基本命令，通过不同参数可以完成各种配置，比如导入其他类，指定编译路径等。 java是执行一个java文件的基本命令，通过参数配置可以以不同方式执行一个java程序或者是一个jar包。 javap是一个class文件的反编译程序，可以获取class文件的反编译结果，甚至是jvm执行程序的每一步代码实现。​ 反射Java反射包reflection提供对Class，Method，field，constructor等信息的封装类型。 通过这些api可以轻易获得一个类的各种信息并且可以进行实例化，方法调用等。 类中的private参数可以通过setaccessible方法强制获取。 反射的作用可谓是博大精深，JDK动态代理生成代理类的字节码后，首先把这个类通过defineclass定义成一个类，然后用class.for(name)会把该类加载到jvm，之后我们就可以通过，A.class.GetMethod()获取其方法，然后通过invoke调用其方法，在调用这个方法时，实际上会通过被代理类的引用再去调用原方法。枚举类枚举类继承Enum并且每个枚举类的实例都是唯一的。 枚举类可以用于封装一组常量，取值从这组常量中取，比如一周的七天，一年的十二个月。 枚举类的底层实现其实是语法糖，每个实例可以被转化成内部类。并且使用静态代码块进行初始化，同时保证内部成员变量不可变。序列化序列化的类要实现serializable接口 transient修饰符可以保证某个成员变量不被序列化 readObject和writeOject来实现实例的写入和读取。 待更新。 事实上，一些拥有数组变量的类都会把数组设为transient修饰，这样的话不会对整个数组进行序列化，而是利用专门的方法将有数据的数组范围进行序列化，以便节省空间。动态代理jdk自带的动态代理可以代理一个已经实现接口的类。 cglib代理可以代理一个普通的类。 动态代理的基本实现原理都是通过字节码框架动态生成字节码，并且在用defineclass加载类后，获取代理类的实例。 一般需要实现一个代理处理器，用来处理被代理类的前置操作和后置操作。在JDK动态代理中，这个类叫做invocationHandler。 JDK动态代理首先获取被代理类的方法，并且只获取在接口中声明的方法，生成代理类的字节码后，首先把这个类通过defineclass定义成一个类，然后把该类加载到jvm，之后我们就可以通过，A.class.GetMethod()获取其方法，然后通过invoke调用其方法，在调用这个方法时，实际上会通过被代理类的引用再去调用原方法。 而对于cglib动态代理，一般会把被代理类设为代理类的父类，然后获取被代理类中所有非final的方法，通过asm字节码框架生成代理类的字节码，这个代理类很神奇，他会保留原来的方法以及代理后的方法，通过方法数组的形式保存。 cglib的动态代理需要实现一个enhancer和一个interceptor，在interceptor中配置我们需要的代理内容。如果没有配置interceptor，那么代理类会调用被代理类自己的方法，如果配置了interceptor，则会使用代理类修饰过的方法。多线程这里先不讲juc包里的多线程类。juc相关内容会在Java并发专题讲解。 线程的实现可以通过继承Thread类和实现Runable接口 也可以使用线程池。callable配合future可以实现线程中的数据获取。 Java中的线程有7种状态，new runable running blocked waiting time_waiting terminate blocked是线程等待其他线程锁释放。 waiting是wait以后线程无限等待其他线程使用notify唤醒 time_wating是有限时间地等待被唤醒，也可能是sleep固定时间。 Thread的join是实例方法，比如a.join(b),则说明a线程要等b线程运行完才会运行。 o.wait方法会让持有该对象o的线程释放锁并且进入阻塞状态，notify则是持有o锁对象的线程通知其他等待锁的线程获取锁。notify方法并不会释放锁。注意这两个方法都只能在synchronized同步方法或同步块里使用。 synchronized方法底层使用系统调用的mutex锁，开销较大，jvm会为每个锁对象维护一个等待队列，让等待该对象锁的线程在这个队列中等待。当线程获取不到锁时则让线程阻塞，而其他检查notify以后则会通知任意一个线程，所以这个锁时非公平锁。 Thread.sleep()，Thread.interrupt()等方法都是类方法，表示当前调用该方法的线程的操作。​ 一个线程实例连续start两次会抛异常,这是因为线程start后会设置标识，如果再次start则判断为错误。 IO流IO流也是Java中比较重要的一块，Java中主要有字节流，字符流，文件等。其中文件也是通过流的方式打开，读取和写入的。 IO流的很多接口都使用了装饰者模式，即将原类型通过传入装饰类构造函数的方式，增强原类型，以此获得像带有缓冲区的字节流，或者将字节流封装成字符流等等，其中需要注意的是编码问题，后者打印出来的结果可能是乱码哦。 IO流与网络编程息息相关，一个socket接入后，我们可以获取它的输入流和输出流，以获取TCP数据包的内容，并且可以往数据报里写入内容，因为TCP协议也是按照流的方式进行传输的，实际上TCP会将这些数据进行分包处理，并且通过差错检验，超时重传，滑动窗口协议等方式，保证了TCP数据包的高效和可靠传输。网络编程承接IO流的内容 IO流与网络编程息息相关，一个socket接入后，我们可以获取它的输入流和输出流，以获取TCP数据包的内容，并且可以往数据报里写入内容，因为TCP协议也是按照流的方式进行传输的，实际上TCP会将这些数据进行分包处理，并且通过差错检验，超时重传，滑动窗口协议等方式，保证了TCP数据包的高效和可靠传输。 除了使用socket来获取TCP数据包外，还可以使用UDP的DatagramPacket来封装UDP数据包，因为UDP数据包的大小是确定的，所以不是使用流方式处理，而是需要事先定义他的长度，源端口和目标端口等信息。 为了方便网络编程，Java提供了一系列类型来支持网络编程的api，比如URL类，InetAddress类等。 后续文章会带来NIO相关的内容，敬请期待。​ Java8接口中的默认方法，接口终于可以有方法实现了，使用注解即可标识出默认方法。 lambda表达式实现了函数式编程，通过注解可以声明一个函数式接口，该接口中只能有一个方法，这个方法正是使用lambda表达式时会调用到的接口。 Option类实现了非空检验 新的日期API 各种api的更新，包括chm，hashmap的实现等 Stream流概念，实现了集合类的流式访问，可以基于此使用map和reduce并行计算。微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式学习总结]]></title>
    <url>%2F2018%2F07%2F09%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[设计模式基础学习总结这篇总结主要是基于我之前设计模式基础系列文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：设计模式学习https://blog.csdn.net/a724888/article/category/6780980 设计模式创建型模式创建型模式创建型模式的作用就是创建对象，说到创建一个对象，最熟悉的就是 new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 单例单例模式保证全局的单例类只有一个实例，这样的话使用的时候直接获取即可，比如数据库的一个连接，Spring里的bean，都可以是单例的。 单例模式一般有5种写法。 第一种是饿汉模式，先把单例进行实例化，获取的时候通过静态方法直接获取即可。缺点是类加载后就完成了类的实例化，浪费部分空间。 第二种是饱汉模式，先把单例置为null，然后通过静态方法获取单例时再进行实例化，但是可能有多线程同时进行实例化，会出现并发问题。 第三种是逐步改进的方法，一开始可以用synchronized关键字进行同步，但是开销太大，而后改成使用volatile修饰单例，然后通过一次检查判断单例是否已初始化，如果未初始化就使用synchronized代码块，再次检查单例防止在这期间被初始化，而后才真正进行初始化。 第四种是使用静态内部类来实现，静态内部类只在被使用的时候才进行初始化，所以在内部类中进行单例的实例化，只有用到的时候才会运行实例化代码。然后外部类再通过静态方法返回静态内部类的单例即可。 第五种是枚举类，枚举类的底层实现其实也是内部类。枚举类确保每个类对象在全局是唯一的。所以保证它是单例，这个方法是最简单的。工厂模式简单工厂一般是用一个工厂创建多个类的实例。 工厂模式一般是指一个工厂服务一个接口，为这个接口的实现类进行实例化 抽象工厂模式是指一个工厂服务于一个产品族，一个产品族可能包含多个接口，接口又会包含多个实现类，通过一个工厂就可以把这些绑定在一起，非常方便。原型模式一般通过一个实例进行克隆从而获得更多同一原型的实例。使用实例的clone方法即可完成。建造者模式建造者模式中有一个概念叫做链式调用，链式调用为一个类的实例化提供便利，一般提供系列的方法进行实例化，实际上就是将set方法改造一下，将原本返回为空的set方法改为返回this实例，从而实现链式调用。 建造者模式在此基础上加入了builder方法，提供给外部进行调用，同样使用链式调用来完成参数注入。结构型模式结构型模式前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 桥接模式有点复杂。建议参考原文 适配器模式适配器模式用于将两个不同的类进行适配。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上， 它们很相似，都需要一个具体的实现类的实例。 但是它们的目的不一样，代理模式做的是增强原方法的活； 适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”， 而鸡和鸭它们之间原本没有继承关系。 适配器模式可以分为类适配器，对象适配器等。 类适配器通过继承父类就可以把自己适配成父类了。 而对象适配器则需要把对象传入另一个对象的构造方法中，以便进行包装。 享元模式/ 享元模式的核心在于享元工厂类，// 享元工厂类的作用在于提供一个用于存储享元对象的享元池，// 用户需要对象时，首先从享元池中获取，// 如果享元池中不存在，则创建一个新的享元对象返回给用户，// 在享元池中保存该新增对象。 //享元模式// 英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。//// 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。//// 这种简单的代码我就不演示了。 代理模式// 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。// 在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，// 其实就是动态代理的过程。比如 Spring 中，// 我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，// 然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 外观模式外观模式一般封装具体的实现细节，为用户提供一个更加简单的接口。 通过一个方法调用就可以获取需要的内容。 组合模式//组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 //直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，// 同时还有下属员工集合（虽然可能集合为空），// 而下属员工和自己的结构是一样的，// 也有姓名、部门这些属性，// 同时也有他们的下属员工集合。 class Employee { private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 }装饰者模式装饰者装饰者模式把每个增强类都继承最高级父类。然后需要功能增强时把类实例传入增强类即可，然后增强类在使用时就可以增强原有类的功能了。 和代理模式不同的是，装饰者模式每个装饰类都继承父类，并且可以进行多级封装。 行为型模式行为型模式行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式策略模式一般把一个策略作为一个类，并且在需要指定策略的时候传入实例，于是我们可以在需要使用算法的地方传入指定算法。 命令模式命令模式一般分为命令发起者，命令以及命令接受者三个角色。 命令发起者在使用时需要注入命令实例。然后执行命令调用。 命令调用实际上会调用命令接收者的方法进行实际调用。 比如遥控器按钮相当于一条命令，点击按钮时命令运行，自动调用电视机提供的方法即可。 模板方法模式模板方法一般指提供了一个方法模板，并且其中有部分实现类和部分抽象类，并且规定了执行顺序。 实现类是模板提供好的方法。而抽象类则需要用户自行实现。 模板方法规定了一个模板中方法的执行顺序，非常适合一些开发框架，于是模板方法也广泛运用在开源框架中。 状态模式少见。 观察者模式和事件监听机制观察者模式一般用于订阅者和消息发布者之间的数据订阅。 一般分为观察者和主题，观察者订阅主题，把实例注册到主题维护的观察者列表上。 而主题更新数据时自动把数据推给观察者或者通知观察者数据已经更新。 但是由于这样的方式消息推送耦合关系比较紧。并且很难在不打开数据的情况下知道数据类型是什么。 知道后来为了使数据格式更加灵活，使用了事件和事件监听器的模式，事件包装的事件类型和事件数据，从主题和观察者中解耦。 主题当事件发生时，触发该事件的所有监听器，把该事件通过监听器列表发给每个监听器，监听得到事件以后，首先根据自己支持处理的事件类型中找到对应的事件处理器，再用处理器处理对应事件。 责任链模式责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内核与基础命令学习总结]]></title>
    <url>%2F2018%2F07%2F09%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FLinux%E5%86%85%E6%A0%B8%E4%B8%8E%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这部分内容主要是基于一些关于Linux系统的内核基础和基本命令的学习总结，内容不全面，只讲述了其中的一小部分，后续会再补充，如有错误，还请见谅。 Linux操作系统 Linux操作系统博大精深，其中对线程，IO，文件系统等概念的实现都很有借鉴意义。 ​ 文件系统和VFS文件系统的inode上面讲过了。VFS主要用于屏蔽底层的不同文件系统，比如接入网络中的nfs文件系统，亦或是windows文件系统，正常情况下难以办到，而vfs通过使用IO操作的posix规范来规定所有文件读写操作，每个文件系统只需要实现这些操作就可以接入VFS，不需要重新安装文件系统。 进程和线程&gt; 进程、程序与线程 &gt; &gt; 程序 &gt; &gt; 程序，简单的来说就是存在磁盘上的二进制文件，是可以内核所执行的代码 &gt; &gt; 进程 &gt; &gt; 当一个用户启动一个程序，将会在内存中开启一块空间，这就创造了一个进程，一个进程包含一个独一无二的PID，和执行者的权限属性参数，以及程序所需代码与相关的资料。 &gt; 进程是系统分配资源的基本单位。 &gt; 一个进程可以衍生出其他的子进程，子进程的相关权限将会沿用父进程的相关权限。 &gt; &gt; 线程 &gt; &gt; 每个进程包含一个或多个线程，线程是进程内的活动单元，是负责执行代码和管理进程运行状态的抽象。 &gt; 线程是独立运行和调度的基本单位。 子进程和父进程进程的层次结构（父进程与子进程）在进程执行的过程中可能会衍生出其他的进程，称之为子进程，子进程拥有一个指明其父进程PID的PPID。子进程可以继承父进程的环境变量和权限参数。 于是，linux系统中就诞生了进程的层次结构——进程树。进程树的根是第一个进程（init进程）。 过程调用的流程： fork &amp; exec一个进程生成子进程的过程是，系统首先复制(fork)一份父进程，生成一个暂存进程，这个暂存进程和父进程的区别是pid不一样，而且拥有一个ppid，这时候系统再去执行(exec)这个暂存进程，让他加载实际要运行的程序，最终成为一个子进程的存在。 服务与进程 简单的说服务(daemon)就是常驻内存的进程，通常服务会在开机时通过init.d中的一段脚本被启动。 进程通信 进程通信的几种基本方式：管道，信号量，消息队列，共享内存，快速用户控件互斥。 fork方法 一个进程，包括代码、数据和分配给进程的资源。fork（）函数通过系统调用创建一个与原来进程几乎完全相同的进程， 也就是两个进程可以做完全相同的事，但如果初始参数或者传入的变量不同，两个进程也可以做不同的事。 一个进程调用fork（）函数后，系统先给新的进程分配资源，例如存储数据和代码的空间。然后把原来的进程的所有值都复制到新的新进程中，只有少数值与原来的进程的值不同。相当于克隆了一个自己。 fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值： 1）在父进程中，fork返回新创建子进程的进程ID； 2）在子进程中，fork返回0； 3）如果出现错误，fork返回一个负值；如何理解pid在父子进程中不同？ 其实就相当于链表，进程形成了链表，父进程的pid指向了子进程的pid，因为子进程没有子进程，所以pid为0。 写时复制传统的fork机制是，调用fork时，内核会复制所有的内部数据结构，复制进程的页表项，然后把父进程的地址空间按页复制给子进程（非常耗时）。 现代的fork机制采用了一种惰性算法的优化策略。 为了避免复制时系统开销，就尽可能的减少“复制”操作，当多个进程需要读取他们自己那部分资源的副本时，并不复制多个副本出来，而是为每个进程设定一个文件指针，让它们读取同一个实际文件。 显然这样的方式会在写入时产生冲突（类似并发），于是当某个进程想要修改自己的那个副本时，再去复制该资源，（只有写入时才复制，所以叫写时复制）这样就减少了复制的频率。父子进程，僵尸进程，孤儿进程，守护进程父进程通过fork产生子进程。 孤儿进程：当子进程未结束时父进程异常退出，原本需要由父进程进行处理的子进程变成了孤儿进程，init系统进程会把这些进程领养，避免他们成为孤儿。 僵尸进程：当子进程结束时，会在内存中保留一部分数据结构等待父亲进程显式结束，如果父进程没有执行结束操作，则会导致子进程的剩余结构无法被释放，占用空间造成严重后果。 守护进程：守护进程用于监控其他进程，当发现大量僵尸进程时，会找到他们的父节点并杀死，同时让init线程认养他们以便释放这些空间。 僵尸进程是有害的，孤儿进程由于内核进程的认养不会造成危害。 进程组和会话 会话和进程组进程组每个进程都属于某个进程组，进程组就是由一个或者多个为了实现作业控制而相互关联的进程组成的。 一个进程组的id是进程组首进程的pid（如果一个进程组只有一个进程，那进程组和进程其实没啥区别）。 进程组的意义在于，信号可以发送给进程组中的所有进程。这样可以实现对多个进程的同时操作。会话会话是一个或者多个进程组的集合。 一般来说，会话(session)和shell没有什么本质上的区别。我们通常使用用户登录一个终端进行一系列操作这样的例子来描述一次会话。 举例 $cat ship-inventory.txt | grep booty|sort上面就是在某次会话中的一个shell命令，它会产生一个由3个进程组成的进程组。 守护进程守护进程（服务）守护进程(daemon)运行在后台，不与任何控制终端相关联。通常在系统启动时通过init脚本被调用而开始运行。 在linux系统中，守护进程和服务没有什么区别。对于一个守护进程，有两个基本的要求：其一：必须作为init进程的子进程运行，其二：不与任何控制终端交互。 硬连接和软连接硬链接指的是不同的文件名指向同一个inode节点，比如某个目录下的a和另一个目录下的b，建立一个软连接让a指向b，则a和b共享同一个inode。 软连接是指一个文件的inode节点不存数据，而是存储着另一个文件的绝对路径，访问文件内容时实际上是去访问对应路径下的文件inode，这样的话文件发生改动或者移动都会导致软连接失效。 线程线程基础概念线程是进程内的执行单元（比进程更低一层的概念），具体包括 虚拟处理器，堆栈，程序状态等。可以认为 线程是操作系统调度的最小执行单元。 现代操作系统对用户空间做两个基础抽象:虚拟内存和虚拟处理器。这使得进程内部“感觉”自己独占机器资源。 虚拟内存系统会为每个进程分配独立的内存空间，这会让进程以为自己独享全部的RAM。 但是同一个进程内的所有线程共享该进程的内存空间。虚拟处理器这是一个针对线程的概念，它让每个线程都“感觉”自己独享CPU。实际上对于进程也是一样的。 线程模型线程模型线程的概念同时存在于内核和用户空间中。下面介绍三种线程模型。 内核级线程模型每个内核线程直接转换成用户空间的线程。即内核线程：用户空间线程=1：1 用户级线程模型这种模型下，一个保护了n个线程的用户进程只会映射到一个内核进程。即n:1。 可以减少上下文切换的成本，但在linux下没什么意义，因为linux下进程间的上下文切换本身就没什么消耗，所以很少使用。 混合式线程模型上述两种模型的混合，即n:m型。 很难实现。内核线程实现系统线程实现：PThreads原始的linux系统调用中，没有像C++11或者是Java那样完整的线程库。 整体看来pthread的api比较冗余和复杂，但是基本操作也主要是 创建、退出等。 1.创建线程 int pthread_create (若线程创建成功，则返回0。若线程创建失败，则返回出错编号) 注意:线程创建者和新建线程之间没有fork()调用那样的父子关系，它们是对等关系。调用pthread_create()创建线程后，线程创建者和新建线程哪个先运行是不确定的，特别是在多处理机器上。2.终止线程 void pthread_exit(void *value_ptr); 线程调用pthread_exit()结束自己，参数value_ptr作为线程的返回值被调用pthread_join的线程使用。由于一个进程中的多个线程是共享数据段的，因此通常在线程退出之后，退出线程所占用的资源并不会随着线程的终止而得到释放，但是可以用pthread_join()函数来同步并释放资源3.取消线程 int pthread_cancel(pthread_t thread); 注意：若是在整个程序退出时，要终止各个线程，应该在成功发送 CANCEL指令后，使用 pthread_join函数，等待指定的线程已经完全退出以后，再继续执行；否则，很容易产生 “段错误”。4.连接线程（阻塞） int pthread_join(pthread_t thread, void **value_ptr); 等待线程thread结束，并设置*value_ptr为thread的返回值。pthread_join阻塞调用者，一直到线程thread结束为止。当函数返回时，被等待线程的资源被收回。如果进程已经结束，那么该函数会立即返回。并且thread指定的线程必须是joinable的。 需要留意的一点是linux机制下，线程存在一个被称为joinable的状态。下面简要了解一下：Join和Detach这块的概念，非常类似于之前父子进程那部分，等待子进程退出的内容（一系列的wait函数）。 linux机制下，线程存在两种不同的状态：joinable和unjoinable。 如果一个线程被标记为joinable时，即便它的线程函数执行完了，或者使用了pthread_exit()结束了该线程，它所占用的堆栈资源和进程描述符都不会被释放（类似僵尸进程），这种情况应该由线程的创建者调用pthread_join()来等待线程的结束并回收其资源（类似wait系函数）。默认情况下创建的线程都是这种状态。 如果一个线程被标记成unjoinable，称它被分离(detach)了，这时候如果该线程结束，所有它的资源都会被自动回收。省去了给它擦屁股的麻烦。 因为创建的线程默认都是joinable的，所以要么在父线程调用pthread_detach(thread_id)将其分离，要么在线程内部，调用pthread_detach(pthread_self())来把自己标记成分离的。文件系统文件描述符在linux内核中，文件是用一个整数来表示的，称为 文件描述符，通俗的来说，你可以理解它是文件的id（唯一标识符） 普通文件 普通文件就是字节流组织的数据。 文件并不是通过和文件名关联来实现的，而是通过关联索引节点来实现的，文件节点拥有文件系统为普通文件分配的唯一整数值(ino)，并且存放着一些文件的相关元数据。 目录与链接 正常情况下文件是通过文件名来打开的。 目录是可读名称到索引编号之间的映射，名称和索引节点之间的配对称为链接。 可以把目录看做普通文件，只是它包含着文件名称到索引节点的映射（链接）文件系统是基于底层存储建立的一个树形文件结构。比较经典的是Linux的文件系统，首先在硬盘的超级块中安装文件系统，磁盘引导时会加载文件系统的信息。 linux使用inode来标识任意一个文件。inode存储除了文件名以外的文件信息，包括创建时间，权限，以及一个指向磁盘存储位置的指针，那里才是真正存放数据的地方。 一个目录也是一个inode节点。 详细阐述一次文件访问的过程： 首先用户ls查看目录。由于一个目录也是一个文件，所以相当于是看目录文件下有哪些东西。 实际上目录文件是一个特殊的inode节点，它不需要存储实际数据，而只是维护一个文件名到inode的映射表。 于是我们ls到另一个目录。同理他也是一个inode。我们在这个inode下执行vi操作打开某个文件，于是linux通过inode中的映射表找到了我们请求访问的文件名对应的inode。 然后寻道到对应的磁盘位置，读取内容到缓冲区，通过系统调用把内容读到内存中，最后进行访问。IO操作文件描述符 对于内核而言，所有打开的文件都通过文件描述符引用。文件描述符是一个非负整数。当打开一个现有文件或创建一个新文件时，内核向进程返回一个文件描述符。当读或写一个文件时，使用open或create返回的文件描述符表示该文件，将其作为参数传给read或write函数。 write函数 write函数定义如下： #include ssize_t write(int filedes, void *buf, size_t nbytes); // 返回：若成功则返回写入的字节数，若出错则返回-1 // filedes：文件描述符 // buf:待写入数据缓存区 // nbytes:要写入的字节数 同样，为了保证写入数据的完整性，在《UNIX网络编程 卷1》中，作者将该函数进行了封装，具体程序如下： 1 ssize_t /* Write "n" bytes to a descriptor. */ 2 writen(int fd, const void *vptr, size_t n) 3 { 4 size_t nleft; 5 ssize_t nwritten; 6 const char *ptr; 7 8 ptr = vptr; 9 nleft = n; 10 while (nleft > 0) { 11 if ( (nwritten = write(fd, ptr, nleft))]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统学习总结]]></title>
    <url>%2F2018%2F07%2F09%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这部分内容主要是基于一些关于操作系统基础的学习总结，内容不全面，只讲述了其中的一小部分，后续会再补充，如有错误，还请见谅。操作系统 CPUcpu是中央处理器，他是计算机的核心。cpu通过和寄存器，高速缓存，以及内存交互来执行程序。 主板分为南桥和北桥，北桥主要是内存总线，通往内存。而南桥主要是慢速设备的IO总线，包括硬盘，网卡等IO设备。 32位cpu最多寻址4g内存，而64位cpu目前来说没有上限。 程序执行过程cpu发出指令，将硬盘上的一段程序读入内存，由于cpu和硬盘的速度差距更大，一般使用中断，dma等方式来将硬盘数据载入内存。然后cpu通过寄存器以及指令集执行指令，cpu读取内存上的代码，内存上的方法执行是一个栈调用的过程。 高速缓存 读写缓冲区为了弥补cpu和内存的速度差，cpu配有多级缓存。 一般有一级缓存和二级缓存，缓存根据局部性原理把经常使用的代码加载如缓存，能比直接访问内存快上几百倍。 同样的，内存和硬盘间的速度差距也很大，需要通过读写缓冲区来进行速度匹配，内存写入磁盘时先写入缓冲区，读数据时从缓冲区读取硬盘准备好的数据。 内存管理和虚拟内存由于程序的大小越来越大，而计算机想要支持多道程序，当一个程序遇到IO操作时转而去执行另一个程序，这就要求内存中装有多个程序的代码了。 然而程序的内存消耗与日俱增，同时装载多个程序越来越困难，所以人们提出了，只在程序需要使用到的时候再把他装入内存，平时把代码放在硬盘中即可。 分页由于内存需要装载硬盘中的数据，所以需要约定一个存储单元，操作系统把它叫做页，一个页一般长度是8kb或者16kb。内存从硬盘读取数据时读取的是一个页或多个页。 虚拟内存由于这些代码持久化在硬盘中，占用了一部分空间，并且需要运行时会加载到内存中，所以这部分的空间一般也成为虚拟内存的空间（大小）。 页表和页面置换算法为了知道每个程序对应的代码存在硬盘中的位置，操作系统需要维护一个程序到页面的映射表，cpu要内存加载一个页面时，首先要访问页表，来得知页面在硬盘中的位置，然后让内存去该位置把对应的页面调入内存中。 为了提升页面调入的效率，也使用了多种的页面置换算法，比如lru最近最久未使用，fifo先进先出，时钟法，多级队列法等。 当然，为了进一步提高效率，还会使用多级页表的方式，不过一般需要硬件维护一个页表映射页面的快速转换结构，以便能迅速地完成页面解析和调度。 中断和缺页中断计算机提供一系列中断指令与硬件进行交互，操作系统可以使用这些中断去控制硬件，比如使用中断通知cpu硬盘的IO操作已经准备好，键盘通过一次又一次的中断来输入字符。 中断一般适用于快速返回的字符设备，比如鼠标键盘，而硬盘这类耗时IO操作，使用中断后cpu仍然需要等待硬盘把数据装入内存。是非常缓慢的。 于是才会使用DMA来完成IO操作，DMA额外提供一个处理器去处理IO操作，当硬盘数据加载到内存中后再去通知CPU操作已完成。 缺页中断就是因为内存中没有cpu需要访问的页面，必须根据页表到硬盘中获取页面，并根据置换算法进行页面调度。如果找不到对应页面，则程序执行会报错。分页和分段分页是上述所说的，通过内存和硬盘的约定，将调度单元设定为一个页面。 分段则不同，分段并不是物理意义上的分段，而是逻辑上把代码使用到的空间划分成多个部分，比如受保护部分，公开部分，核心部分等，这样可以更好地描述每个段的代码信息，为使用者提供便利，为了支持程序员的这种需求，操作系统加入了分段的概念，将代码对应的一段虚拟内存划分为不同的逻辑段。 同时为了根据不同段来以不同方式访问内存，操作系统需要另外维护一个段表，以用于段的映射。由于分段只是逻辑上的概念，所以底层的内存分页仍然是必不可少的，因此在逻辑段的基础上，物理上又会划分为多个页，一个段中可能包含了多个页面。 因此，完善的虚拟内存管理器需要支持段页表，先映射段，再映射页面。 进程与线程进程进程是资源分配的资本单位，操作系统为进程开辟一段内存空间，内存空间从高位向低位，包括函数调用栈，变量以及其他区域。cpu根据这些信息配合寄存器进行函数调用和程序执行。 多进程由于计算机是分时系统，所以多进程的使用不可避免，操作系统需要进行进程的切换，方法是内存指针指向新位置，保存原来的进程信息，同时刷新寄存器等数据。然后开始执行新的进程. 一般操作系统会使用pcb结构来记录进程的信息和上下文。通过他和cpu配合来完成进程切换。 线程线程是系统调度的基本单位，没有线程以前，一般会使用多进程模型，一个程序往往需要多个进程配合使用，但是多进程之间并没有共享内存，导致进程间通信非常麻烦。 比如文本输入工具，一边键入文字一边需要保存数据，如果是单进程执行，则每次输入都触发IO操作，非常费时，如果一个进程负责输入展示一个进程负责输入保存，确实速度很快，但是两个进程没办法共享数据。除非使用额外的通讯手段。 多线程而多线程就好办了，多线程都是基于进程产生的，线程被创建后只需要分配少量空间支持堆栈操作即可，同时线程还共享进程中的内存，所以一般使用进程分配资源，线程进行调度的方式。 操作系统对线程的支持： 一般情况下操作系统都支持线程，并且可以创建内核级线程，内核可以识别线程并且为其分配空间，进行线程调度。 但是内核级线程实现比较复杂，使用起来也不甚方便。 所以往往开发人员会使用用户级线程，用户级线程比如Java的多线程，通过简单的api实现，当需要操作系统支持时，才使用底层调用api接口进行内核级的系统调用。 但是一般情况下用户级线程是不被内核识别的，也就是说，用户级线程会被内核认为是一个进程，从而执行进程的调度。这样的话就没有意义了。 所以一般情况下用户级线程会映射到对应的内核级线程中，内核为进程创建一定数量的内核级线程以供使用。Java中的线程基本上都是使用这种方式实现的。线程通信和进程通信线程通信一般只需要使用共享内存的方式即可实现。 而进程通信则需要额外的通信机制。 1 信号量，一般多进程间的同步使用信号量来完成，系统为临界区添加支持并发量为n的信号量,多进程访问临界区资源时，首先需要执行p操作来减少信号量，如果信号量等于0则操作失败，并且挂起进程，否则成功进入临界区执行。 当进程退出临界区时，执行v操作，将信号量加一，并唤醒挂起的进程进行操作。 2 管程，管程是对信号量的一个包装，避免使用信号量时出错。 3 管道，直接连接两个进程，一个进程写入管道，另一个进程可以读取管道，但是他不支持全双工，并且只能在父子进程间使用，所以局限性比较大 4 消息队列 操作系统维护一个消息队列，进程将消息写入队列中，其他进程轮询消息队列看是否有自己的消息，增加了轮询的开销，但是提高了消息的可靠性和易用性，同时支持了订阅消息。 5 socket socket一般用于不同主机上的进程通信，双方通过ip和port的方式寻址到对方主机并找到监听该端口的进程，为了完成通信，他们先建立tcp连接，在此基础上交换数据，也就完成了进程间的通信。 进程调度不小心把这茬忘了，进程调度算法有几种，fifo先来先服务，短作业优先，时间片轮转，优先级调度，多级反馈队列等。基本上可以通过名字知道算法的大概实现。 死锁死锁的必要条件： 1互斥：资源必须是互斥的，只能给一个进程使用 2占有和等待：占有资源时可以请求其他资源 3不可抢占：资源占有时不会被抢 4环路等待：有两个以上的进程组成一个环路，每个进程都在等待下一个进程的资源释放。死锁的处理方法： 1鸵鸟 2死锁预防 在程序运行之前预防发生死锁。 （一）破坏互斥条件 例如假脱机打印机技术允许若干个进程同时输出，唯一真正请求物理打印机的进程是打印机守护进程。 这样子就破坏了互斥条件，转而使用单个队列串行执行操作。 （二）破坏占有和等待条件 一种实现方式是规定所有进程在开始执行前请求所需要的全部资源。 分配了全部资源就不需要去等待其他资源。 （三）破坏不可抢占条件 允许抢占式调度。 （四）破坏环路等待 给资源统一编号，进程只能按编号顺序来请求资源。 按正确顺序请求资源，就不会发生死锁。3死锁避免 ==银行家算法用于在程序运行时避免发生死锁。== 银行家算法用于在程序运行时判断资源的分配情况是否是安全状态，如果某一步骤使程序可能发生死锁，银行家算法会拒绝该操作执行，从而避免进入不安全状态。 （一）安全状态 图 a 的第二列 Has 表示已拥有的资源数，第三列 Max 表示总共需要的资源数，Free 表示还有可以使用的资源数。从图 a 开始出发，先让 B 拥有所需的所有资源（图 b），运行结束后释放 B，此时 Free 变为 5（图 c）；接着以同样的方式运行 C 和 A，使得所有进程都能成功运行，因此可以称图 a 所示的状态时安全的。 定义：如果没有死锁发生，并且即使所有进程突然请求对资源的最大需求，也仍然存在某种调度次序能够使得每一个进程运行完毕，则称该状态是安全的。 （二）单个资源的银行家算法 一个小城镇的银行家，他向一群客户分别承诺了一定的贷款额度，算法要做的是判断对请求的满足是否会进入不安全状态，如果是，就拒绝请求；否则予以分配。 上图 c 为不安全状态，因此算法会拒绝之前的请求，从而避免进入图 c 中的状态。 4死锁检测和恢复 ==银行家算法检测程序并且阻止死锁发生，而死锁检测和恢复则不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。== （一）每种类型一个资源的死锁检测 上图为==资源分配图==，其中方框表示资源，圆圈表示进程。资源指向进程表示该资源已经分配给该进程，进程指向资源表示进程请求获取该资源。 如果有环则说明有死锁。 （二）每种类型多个资源的死锁检测 ==资源分配矩阵== 上图中，有三个进程四个资源，每个数据代表的含义如下： E 向量：资源总量 A 向量：资源剩余量 C 矩阵：每个进程所拥有的资源数量，每一行都代表一个进程拥有资源的数量 R 矩阵：每个进程请求的资源数量 如果存在请求数量无法被满足时，就会出现死锁。 ==（三）死锁恢复== 利用抢占恢复，允许其他进程抢占资源。 利用回滚恢复，回滚进程操作，释放其资源。 通过杀死进程恢复，杀死进程后释放资源。 IO和磁盘磁盘是块设备，键盘是字符设备，网卡是网络设备。他们都接在IO总线上，属于慢速设备。 磁盘和寻址磁盘的结构比较复杂，主要通过扇区，盘面和磁头位置决定当前访问的磁盘位置。 cpu为了能够访问磁盘内容，首先要把磁盘的内容载入内存中，于是要和内存约定统一的寻址单元，cpu指定一个起始位置，访问该位置以后的n个存储单元，一般存储单元是一个页，16K或者8K。 这一操作中，指向起始位置需要随机寻址，而接下来的访问操作是顺序访问，磁盘的随机读写和顺序读写的速度差距是很大的。所以一般会通过缓冲区来缓存IO数据。 磁盘内部一般也会分为很多部分，比如操作系统会将磁盘做一个分区，使用磁盘的一些位置存储元数据信息，以保证磁盘能够支持操作系统以及文件系统。 一般在物理分区的起始位置会有一个引导区和分区表,BIOS自动将磁盘中引导区的内核程序载入内存，此时操作系统才开始运行，并且根据分区表操作系统可以知道每个分区的起始位置在哪。读写一个磁盘块的时间的影响因素有： 旋转时间（主轴旋转磁盘，使得磁头移动到适当的扇区上）寻道时间（制动手臂移动，使得磁头移动到适当的磁道上）实际的数据传输时间其中，寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。 先来先服务FCFS, First Come First Served 按照磁盘请求的顺序进行调度。 优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长。 最短寻道时间优先SSTF, Shortest Seek Time First 优先调度与当前磁头所在磁道距离最近的磁道。 虽然平均寻道时间比较低，但是不够公平。如果新到达的磁道请求总是比一个在等待的磁道请求近，那么在等待的磁道请求会一直等待下去，也就是出现饥饿现象。具体来说，两边的磁道请求更容易出现饥饿现象。 电梯算法SCAN 电梯总是保持一个方向运行，直到该方向没有请求为止，然后改变运行方向。 电梯算法（扫描算法）和电梯的运行过程类似，总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向。 因为考虑了移动方向，因此所有的磁盘请求都会被满足，解决了 SSTF 的饥饿问题。 IO设备除了硬盘以外，还有键盘，网卡等IO设备，这些设备需要操作系统通过驱动程序来进行交互，驱动程序用于适配这些设备。 为了执行IO操作，内核一般要为IO设备提供一个缓存区，比如网卡的IO操作，会为socket提供一个缓存区，当多个socket使用一个缓冲区进行通信，就是复用了缓冲区，也就是IO复用的一种方式。 同时，内核还会维护一个IO请求的列表，当IO请求就绪时，让几个线程去执行IO操作，实现了线程的复用。 文件系统文件系统是基于底层存储建立的一个树形文件结构。比较经典的是Linux的文件系统，首先在硬盘的超级块中安装文件系统，磁盘引导时会加载文件系统的信息。 linux使用inode来标识任意一个文件。inode存储除了文件名以外的文件信息，包括创建时间，权限，以及一个指向磁盘存储位置的指针，那里才是真正存放数据的地方。 一个目录也是一个inode节点。 详细阐述一次文件访问的过程： 首先用户ls查看目录。由于一个目录也是一个文件，所以相当于是看目录文件下有哪些东西。 实际上目录文件是一个特殊的inode节点，它不需要存储实际数据，而只是维护一个文件名到inode的映射表。 于是我们ls到另一个目录。同理他也是一个inode。我们在这个inode下执行vi操作打开某个文件，于是linux通过inode中的映射表找到了我们请求访问的文件名对应的inode。 然后寻道到对应的磁盘位置，读取内容到缓冲区，通过系统调用把内容读到内存中，最后进行访问。微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络学习总结]]></title>
    <url>%2F2018%2F07%2F09%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这部分内容主要是基于一些关于计算机网络基础的学习总结，内容不全面，只讲述了其中的一小部分，后续会再补充，如有错误，还请见谅。 计算机网络常见概念 网卡和路由器网卡是一个有mac地址的物理设备，通过mac地址与局域网内的交换机通信，交换机可以识别mac地址。 而单纯的中继器，集线器，双绞线等设备只识别物理层设备。 路由器则工作在3层ip层，必须要有ip才能工作，所以路由器每一个接口都对应一个ip，维护一个可以识别ip的路由表，进行ip数据报转发。 交换机交换机具有自学习能力，学习的是交换表的内容。交换表中存储着 MAC 地址到接口的映射。 以太网以太网是一种星型拓扑结构局域网。 早期使用集线器进行连接，它是一种物理层设备，作用于比特而不是帧，当一个比特到达接口时，集线器重新生成这个比特，并将其能量强度放大，从而扩大网络的传输距离。之后再将这个比特向其它所有接口。特别是，如果集线器同时收到同时从两个不同接口的帧，那么就发生了碰撞。 目前以太网使用交换机替代了集线器，它不会发生碰撞，能根据 MAC 地址进行存储转发。 虚拟局域网VLAN正常情况下，局域网中的链路层广播在整个局域网可达，而vlan可以在物理局域网中划分虚拟局域网，使广播帧只有在vlan当中的主机才能收到。 虚拟局域网可以建立与物理位置无关的逻辑组，只有在同一个虚拟局域网中的成员才会收到链路层广播信息，例如下图中 (A1, A2, A3, A4) 属于一个虚拟局域网，A1 发送的广播会被 A2、A3、A4 收到，而其它站点收不到。 DHCP协议(动态主机配置协议)首先DHCP是为了让主机获得一个ip地址，所以主机会发一个0.0.0.0为发送方，255.255.255.255为接收方的ip数据报，也就是广播数据报，并且广播数据包只在局域网中有效，然后链路层解析为数据帧，发送给局域网内的DHCP服务器。 ARP协议arp负责把ip地址解析成局域网内的一个mac地址，只在局域网中有效。逆arp则把mac地址解析成ip地址。 网络层实现主机之间的通信，而链路层实现具体每段链路之间的通信。因此在通信过程中，IP 数据报的源地址和目的地址始终不变，而 MAC 地址随着链路的改变而改变。 ARP 实现由 IP 地址得到 MAC 地址。 每个主机都有一个 ARP 高速缓存，里面有本局域网上的各主机和路由器的 IP 地址到硬件地址的映射表。 如果主机 A 知道主机 B 的 IP 地址，但是 ARP 高速缓存中没有该 IP 地址到 MAC 地址的映射，此时主机 A 通过广播的方式发送 ARP 请求分组，主机 B 收到该请求后会发送 ARP 响应分组给主机 A 告知其 MAC 地址，随后主机 A 向其高速缓存中写入主机 B 的 IP 地址到 MAC 地址的映射。 网关和NAT当需要和外部局域网访问时，需要经过网关服务器以便兼容不同协议栈。局域网内部使用内网ip，经过网关时要转成外网ip，网关会帮你完成改写操作，当收到数据报时，网关又会帮你把ip改为内网ip。这种修改ip隐藏内部网络的方式叫做NAT。 nat穿透的方式是主机和网关服务器协定一个ip地址作为主机服务的ip，所以主机可以通过这个ip和外网交流。 DNS协议和http请求过程访问一个域名时，会发送dns报文请求（应用层）给本地的DNS服务器，解析出域名对应的ip，然后三次握手建立连接，（当然TCP数据报由本地局域网经过网关转给外网，再经过多次路由才到达目标主机），然后发送http请求获得响应报文 ICMPICMP 是为了更有效地转发 IP 数据报和提高交付成功的机会。它封装在 IP 数据报中，但是不属于高层协议。 ICMP 报文分为差错报告报文和询问报文。 1. Ping Ping 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。 Ping 发送的 IP 数据报封装的是无法交付的 UDP 用户数据报。 2. Traceroute Traceroute 是 ICMP 的另一个应用，用来跟踪一个分组从源点到终点的路径，事实上，traceroute也封装着无法交付的udp，和ping类似。。源主机向目的主机发送一连串的 IP 数据报，每个数据包的ttl时间不同，所以可以跟踪每一跳路由的信息。 ==但是因为数据报封装的是无法交付的UDP报文，因此目的主机要向源主机发送 ICMP终点不可达差错报告报文。之后源主机知道了到达目的主机所经过的路由器 IP地址以及到达每个路由器的往返时间。== 虚拟专用网VPN和内网ip由于 IP 地址的紧缺，一个机构能申请到的 IP 地址数往往远小于本机构所拥有的主机数。并且一个机构并不需要把所有的主机接入到外部的互联网中，机构内的计算机可以使用仅在本机构有效的 IP 地址（专用地址）。 有三个专用地址块： 10.0.0.0 ~ 10.255.255.255 172.16.0.0 ~ 172.31.255.255 192.168.0.0 ~ 192.168.255.255 这些ip也称为内网ip，用于局域网间的通信，只能通过网关抵达公网。 使用隧道技术实现vpn。 原理是；普通的内网ip无法被访问到，一般可以使用nat技术让网关作为中转人，而ip数据报也会改写成网关服务器的地址。 如果想让数据报保留内网地址，并且实现跨公网访问，那么只能通过隧道技术，把内网数据报加密包装在公网ip数据报中，然后通过公网ip抵达对方的专用网络，进行拆包和发送。 为什么vpn能翻墙呢，因为我们通过对vpn服务器的连接，可以将内网ip数据报装在里面，发送给vpn，vpn解析后再发送给真正的服务器。 由于本地网关阻拦了某些网站的请求，所以我们要把这个请求加密封装，然后通过隧道把数据发给一个海外服务器，让他真正完成请求。应用层应用层的协议主要是http，ftp这类协议，http访问超文本html，而ftp访问文件系统。 http通过浏览器可以方便地进行dns解析，建立tcp连接，发送http请求，得到http响应，这些工作都是浏览器完成的。 http1.0 1.1和2.01.0和1.1的主要变化1 http1.0经过多年发展，在1.1提出了改进。 首先是提出了长连接，http请求可以在一次tcp连接中不断发送。 2 然后是http1.1支持只发送header而不发送body。原因是先用header判断能否成功，再发数据，节约带宽，事实上，post请求默认就是这样做的。 3 http1.1的host字段。由于虚拟主机可以支持多个域名，所以一般将域名解析后得到host。http1.0和http2.0的区别。http2.0变化巨大。 1 http支持多路复用，同一个连接可以并发处理多个请求，方法是把http数据包拆为多个帧，并发有序的发送，根据序号在另一端进行重组，而不需要一个个http请求顺序到达。 2 http2.0支持服务端推送，就是服务端在http请求到达后，除了返回数据之外，还推送了额外的内容给客户端。 3HTTP2.0压缩了请求头，同时基本单位是二进制帧流，这样的数据占用空间更少。 4http2.0只适用于https场景，因为其在http和tcp中间加了一层ssl层。get和postget和post本质都是http请求，只不过对他们的作用做了界定和适配，并且让他们适应各自的场景。 1本质区别是get只是一次http请求，post先发请求体再发请求体，实际上是两次请求 2表面区别： get可以cache而post不能，因为浏览器是这么安排的 一般设计get是幂等的而post不是 get的参数放在url传递，而post放在请求体里，因为get没有请求体。 所以get请求不安全，并且有长度限制（url不能太长），而post几乎没有限制，请求体可以很大。### session和cookie并且浏览器还维护了cookie以便记录用于对网站的一些信息，下次请求时在http报文中带上这些数据，服务器接收以后根据cookie中的sessionid获取对应的session即可 tokensession一般维护在内存中，有时候也会持久化到数据库，但是如果session由单点维护可能出现宕机等情况，于是一般会采用分布式的方案。 session存放的几种方案。 0 存在内存中。用sessionid标识用户。 这样的session十分依赖于cookie。如果浏览器禁用了cookie则session无用武之地。 当然也可以把内容存在数据库里，缺点是数据库访问压力较大。 1有做法会将session内容存在cookie中，但前提是经过了加密，然后下次服务器对其进行解密，但是这样浏览器需要维护太多内容了。 2当用户登录或者执行某些操作，则使用用户的一部分字段信息进行加密算法得到一串字符串成为token，用于唯一标识用户，或者是某些操作，比如登录，支付，服务端生成该token返回给用户，用户提交请求时必须带上这个token，就可以确认用户信息以及操作是否合法了。 这样我们不需要存session，只需要在想得到用户信息时解密token即可。 token还有一个好处就是可以在移动端和pc端兼容，因为移动端不支持cookie。 3token和oauth。经常有第三方授权登录的例子，本质就是使用token。首先我们打开授权登录页，登陆后服务端返回token，我们提交第三方的请求时，带上这个token，第三方不知道他是啥意思，并且token过段时间就过期了。cas单点登录单点登录是为了多个平台之间公用一个授权系统，做法是，所有登录都要指向统一登录服务，登陆成功以后在认证中心建立session，并且得到ticket，然后重定向页面，此时页面也会向认证中心确认ticket是否合法，然后就可以访问其他系统的页面了。 从而访问其他系统时，由于已经有了认证中心的cookie，所以直接带上ticket访问即可。 每次访问新系统时需要在认证中心注册session，然后单点退出时再把这些session退出，才能实现用户登出。 web安全和https密码加密MD5等加密方法可以用来对密码进行加密。一般还会加盐 xss跨站脚本攻击利用有输入功能网站的输入框来注入JavaScript脚本代码，用户访问该页面时会自动执行某些脚本代码，导致cookie等个人信息泄露，可能会被转发到其他网站。 解决办法是对输入进行检验，利用一个些工具类就可以做到。 跨站点请求伪造csrf首先用户访问了一个网站并登陆，会把cookie保留在浏览器，然后某些网站用一些隐性链接诱导用户点击，点击时发送请求会携带浏览器中的cookie，比如支付宝的账号密码，通过该cookie再去伪造一个支付宝支付请求，达到伪造请求的目的。 解决这个问题的办法就是禁止js请求跨域名。但是他为ajax提供了特殊定制。 SQL 注入攻击 概念服务器上的数据库运行非法的 SQL 语句，主要通过拼接来完成。 防范手段（一）使用参数化查询 以下以 Java 中的 PreparedStatement 为例，它是预先编译的 SQL 语句，可以传入适当参数并且多次执行。由于没有拼接的过程，因此可以防止 SQL 注入的发生。 （二）单引号转换 将传入的参数中的单引号转换为连续两个单引号，PHP 中的 Magic quote 可以完成这个功能。 拒绝服务攻击拒绝服务攻击（denial-of-service attack，DoS），亦称洪水攻击，其目的在于使目标电脑的网络或系统资源耗尽，使服务暂时中断或停止，导致其正常用户无法访问。 分布式拒绝服务攻击（distributed denial-of-service attack，DDoS），指攻击者使用网络上两个或以上被攻陷的电脑作为“僵尸”向特定的目标发动“拒绝服务”式攻击。 DDoS攻击通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 这种攻击方式可分为以下几种： 通过使网络过载来干扰甚至阻断正常的网络通讯； 通过向服务器提交大量请求，使服务器超负荷； 阻断某一用户访问服务器； 阻断某服务与特定系统或个人的通讯。攻击现象 被攻击主机上有大量等待的TCP连接； 网络中充斥着大量的无用的数据包； 源地址为假 制造高流量无用数据，造成网络拥塞，使受害主机无法正常和外界通讯； 利用受害主机提供的传输协议上的缺陷反复高速的发出特定的服务请求，使主机无法处理所有正常请求； 严重时会造成系统死机。总体来说，对DoS和DDoS的防范主要从下面几个方面考虑： 尽可能对系统加载最新补丁，并采取有效的合规性配置，降低漏洞利用风险； 采取合适的安全域划分，配置防火墙、入侵检测和防范系统，减缓攻击。 采用分布式组网、负载均衡、提升系统容量等可靠性措施，增强总体服务能力。httpshttps博大精深，首先先来看看他的基础知识 1对称加密和非对称加密 对称加密两方使用同一把密钥加密和解密，传输密钥时如果丢失就会被破解。 2非对称加密两方各有一把私钥，而公钥公开，A用私钥加密，把公钥和数据传给B，B用公钥解密。同理，B用私钥对数据进行加密，返回给A，A也用公钥进行解密。 3非对称加密只要私钥不丢就很安全，但是效率比较低，所以一般使用非对称加密传输对称加密的密钥，使用对称加密完成数据传输。 4数字签名，为了避免数据在传输过程中被替换，比如黑客修改了你的报文内容，但是你并不知道，所以我们让发送端做一个数字签名，把数据的摘要消息进行一个加密，比如MD5，得到一个签名，和数据一起发送。然后接收端把数据摘要进行md5加密，如果和签名一样，则说明数据确实是真的。 5数字证书，对称加密中，双方使用公钥进行解密。虽然数字签名可以保证数据不被替换，但是数据是由公钥加密的，如果公钥也被替换，则仍然可以伪造数据，因为用户不知道对方提供的公钥其实是假的。 所以为了保证发送方的公钥是真的，CA证书机构会负责颁发一个证书，里面的公钥保证是真的，用户请求服务器时，服务器将证书发给用户，这个证书是经由系统内置证书的备案的。​​ 6 https过程 用户发送请求，服务器返回一个数字证书。 用户在浏览器端生成一个随机数，使用证书中的公钥加密，发送给服务端。 服务端使用公钥解密该密文，得到随机数。 往后两者使用该随机数作为公钥进行对称加密。 番外：关于公钥加密私钥解密与私钥加密公钥解密说明 第一种是签名,使用私钥加密,公钥解密,用于让所有公钥所有者验证私钥所有者的身份并且用来防止私钥所有者发布的内容被篡改.但是不用来保证内容不被他人获得. 第二种是加密,用公钥加密,私钥解密,用于向公钥所有者发布信息,这个信息可能被他人篡改,但是无法被他人获得.搜索 传输层UDP 和 TCP 的特点 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。TCP是传输层最重要的协议。 由于网络层只提供最大交付的服务，尽可能地完成路由转发，以及把链路层报文传送给任意一台主机。他做的工作很专注，所以不会提供其他的可靠性保证。 但是真实网络环境下随时会发生丢包，乱序，数据内容出错等情况，这些情况必须得到处理，于是我们使用传输层tcp来解决这些问题。 UDP报文伪首部的意义：伪首部并非TCP&amp;UDP数据报中实际的有效成分。伪首部是一个虚拟的数据结构，其中的信息是从数据报所在IP分组头的分组头中提取的，既不向下传送也不向上递交，而仅仅是为计算校验和。 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP 首部格式 序号 ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 ：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 三次握手和四次挥手为了保证tcp的可靠传输，需要建立起一条通路，也就是所谓连接。这条通路必须保证有效并且能正确结束。 ​ 三次握手 1 首先客户端发送连接请求syn，携带随机数x。 2 服务端返回请求ack，x + 1,说明服务端对x进行了回复。 3 客户端返回请求ack，y，说明接受到了信息并且开始传输数据，起始数据为y。 客户端状态时syn_send和establish 服务端则是从listen到syn_rcvd，再到establish 四次挥手 1 首先客户端请求断开连接，发送fin请求，服务端返回fin的ack，继续处理断开前需要处理完的数据。 2 过了一会，服务端处理完数据发送给客户端ack，表明已经关闭，客户端最后再发一个ack给服务端，如果服务端已关闭则无反应，客户端经过两个ttl后挥手完毕，确认服务端断开。这两个ttl成为time wait状态，用于确定服务端真的关闭。 3 客户端发完fin后的状态从establish变为fin1——wait，服务端发完ack后的状态从establish变为closewait。 4 客户端收到第一个ack后进入fin_2wait状态，服务端过了一会发送last——ack给客户端，说明关闭好了，客户端收到ack后进入timewait，然后发送ack。双方都closed。半连接syn和洪泛法攻击黑客开启大量的syn请求而不发送ack，服务端开启半连接等待ack，直到资源耗尽，所以必须检测来访ip 为什么要三次握手三次握手的原因 第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 也就是说，如果只有两次握手，服务端返回ack后直接通信，那么如果客户端因为网络问题没有收到ack，可能会再次请求连接，但时服务端不知道这其实是同一个请求，于是又打开了一个连接，相当于维护了很多的无用连接。 time wait的作用1 需要服务端可靠地终止连接，如果处于time_wait客户端发给服务端的ack报文丢失，则服务端会再发一次fin，此时客户端不应该关闭。 2 保证迟来的tcp报文有时间被丢弃，因为2msl里超时抵达的报文都会被丢弃。 可靠传输协议TCP协议有三个重要属性。 可靠传输，主要通过有序接收，确认后发送，以及超时重传来实现，并且使用分片来提高发送效率，通过检验和避免错误。 流量控制，主要通过窗口限制接收和发送速率。 拥塞控制，主要通过不同拥塞状态的算法来处理拥塞，一开始发的比较慢，然后指数增加，当丢包时再降低速度，重新开始第一阶段，避免拥塞。总结以下就是几个特点： TCP 可靠传输 TCP 使用超时重传来实现可靠传输： 1 如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。​ 2 滑动窗口可以连续发送多个数据再统一进行确认。 因为发送端希望在收到确认前，继续发送其它报文段。比如说在收到0号报文的确认前还发出了1-3号的报文，这样提高了信道的利用率。 3 滑动窗口只重传丢失的数据报 但可以想想，0-4发出去后可能要重传，所以需要一个缓冲区维护这些报文，所以就有了窗口。 4每当完成一个确认窗口往前滑动一格，可以传新的一个数据，因此可以顺序发送顺序确认TCP 流量控制 流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。TCP 拥塞控制 如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接受，而拥塞控制是为了降低整个网络的拥塞程度。 TCP 主要通过四种算法来进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 一般刚开始时慢开始，然后拥塞避免，出现个别丢包时（连续三个包序号不对）， 则执行快重传，然后进入快恢复阶段，接着继续拥塞避免。如果发生多次超时也就是拥塞时，直接进入慢开始。 这种情况下，只是丢失个别报文段，而不是网络拥塞，因此执行快恢复，令 ssthresh = cwnd/2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。 ==发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口==。 滑动窗口协议综合实现了上述这一些内容： 为什么要使用滑动窗口，因为滑动窗口可以实现可靠传输，流量控制和拥塞控制（拥塞控制用的是拥塞窗口变量） tcp的粘包拆包tcp报文是流式的数据，没有标识数据结束，只有序号等字段，tcp协议自动完成数据报的切分。由于tcp使用缓冲区发送，又没有标识结束，当缓冲区的数据没清空又有新数据进来，就会发生粘包，如果数据太大存装不下，就会被拆包。 网络层IP 数据报格式 版本 : 有 4（IPv4）和 6（IPv6）两个值； 首部长度 : 占 4 位，因此最大值为 15。 总长度 : 包括首部长度和数据部分长度。 生存时间 ：TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL 为 0 时就丢弃数据报。 ==* 协议 ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。== 首部检验和 ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。 片偏移 : 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。 总结： ip层只保证尽最大努力交付，他所承载的一切都是对路由，转发，已经网络传输最友好的设计。 路由器负责记录路由表和转发ip数据报，路由表记录着ip地址和下一跳路由的端口的对应关系。 由于路由聚合的缘故，一般用170.177.233.0/24就可以标识好几个网络了。 以前会使用A，B，C类地址，和子网，现在直接使用地址聚合，前24位是网络号，后面8位是主机号。 ## 某个聚合路由地址划分网络给n台机器，是否符合要求。。 要看这个网络中的主机号能否达到n个。IP 地址编址方式IP 地址的编址方式经历了三个历史阶段： 分类 子网划分 无分类 1. 分类由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 主机号 &gt;} 子网划分 通过在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址。注意，外部网络看不到子网的存在。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 子网号 &gt;, &lt; 主机号 &gt;} 要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。 无分类 无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用网络前缀和主机号来对 IP 地址进行编码，网络前缀的长度可以根据需要变化。 IP 地址 ::= {&lt; 网络前缀号 &gt;, &lt; 主机号 &gt;} CIDR 的记法上采用在 IP 地址后面加上网络前缀长度的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。 CIDR 的地址掩码可以继续称为子网掩码，子网掩码首 1 长度为网络前缀的长度。一个 CIDR 地址块中有很多地址，一个 CIDR 表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。 把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为 构成超网 。 在路由表中的项目由“网络前缀”和“下一跳地址”组成，在查找时可能会得到不止一个匹配结果，应当采用最长前缀匹配来确定应该匹配哪一个。 总结 使用分类法的ip必须标识是哪一类地址，比较麻烦，而且一旦设置为某类地址它就只能使用那一部分地址空间了。 使用子网掩码可以避免使用分类并且更灵活地决定网络号和主机号的划分。但是需要配置子网掩码，比较复杂。 CIDR 138.1.2.11/24 使用CIDR避免了子网划分，直接使用后n位作为网络号，简化了子网的配置（实际上用n代替了子网掩码）。并且在路由器中可以使用地址聚合，一个ip可以聚合多个网络号。ip分片详谈在TCP/IP分层中，数据链路层用MTU（Maximum Transmission Unit，最大传输单元）来限制所能传输的数据包大小，MTU是指一次传送的数据最大长度，不包括数据链路层数据帧的帧头，如以太网的MTU为1500字节，实际上数据帧的最大长度为1512字节，其中以太网数据帧的帧头为12字节。 当发送的IP数据报的大小超过了MTU时，IP层就需要对数据进行分片，否则数据将无法发送成功。 IP分片的实现 IP分片发生在IP层，不仅源端主机会进行分片，中间的路由器也有可能分片，因为不同的网络的MTU是不一样的，如果传输路径上的某个网络的MTU比源端网络的MTU要小，路由器就可能对IP数据报再次进行分片。而分片数据的重组只会发生在目的端的IP层。 ==避免IP分片== 在网络编程中，我们要避免出现IP分片，那么为什么要避免呢？原因是IP层是没有超时重传机制的，如果IP层对一个数据包进行了分片，只要有一个分片丢失了，只能依赖于传输层进行重传，结果是所有的分片都要重传一遍，这个代价有点大。由此可见，IP分片会大大降低传输层传送数据的成功率，所以我们要避免IP分片。 对于UDP包，我们需要在应用层去限制每个包的大小，一般不要超过1472字节，即以太网MTU（1500）—UDP首部（8）—IP首部（20）。 对于TCP数据，应用层就不需要考虑这个问题了，因为传输层已经帮我们做了。在建立连接的三次握手的过程中，连接双方会相互通告MSS（Maximum Segment =Size，最大报文段长度），MSS一般是MTU—IP首部（20）—TCP首部（20），每次发送的TCP数据都不会超过双方MSS的最小值，所以就保证了IP数据报不会超过MTU，避免了IP分片。 外部网关协议 BGP BGP（Border Gateway Protocol，边界网关协议） AS 之间的路由选择很困难，主要是因为互联网规模很大。并且各个 AS 内部使用不同的路由选择协议，就无法准确定义路径的度量。并且 AS 之间的路由选择必须考虑有关的策略，比如有些 AS 不愿意让其它 AS 经过。 BGP 只能寻找一条比较好的路由，而不是最佳路由。 每个 AS 都必须配置 BGP 发言人，通过在两个相邻 BGP 发言人之间建立 TCP 连接来交换路由信息。路由选择协议和算法 路由选择协议路由选择协议都是自适应的，能随着网络通信量和拓扑结构的变化而自适应地进行调整。 互联网可以划分为许多较小的自治系统 AS，一个 AS 可以使用一种和别的 AS 不同的路由选择协议。 可以把路由选择协议划分为两大类： 自治系统内部的路由选择：RIP 和 OSPF自治系统间的路由选择：BGP 总结： 1. 内部网关协议 RIP RIP 是一种基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1，跳数最多为 15，超过 15 表示不可达。 RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。 2. 内部网关协议 OSPF 开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。 开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。OSPF 具有以下特点： 计算出最短路径，然后向本自治系统中的所有路由器发送信息，这种方法是洪泛法。 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。 变化时，路由器才会发送信息。 所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。总结： AS是一个自治域，一般是指相似度很大公用一个协议的路由器族，比如同一个运营商的网络。 因特网中AS之间的路由选择协议是BGP。 AS内的路由选择协议有RIP和OSPF。 RIP两两交换，最后大家都同步。 OSPF找到最短路径。告诉大家。链路层链路层最主要是指局域网内的网络交互了，使用mac地址通过交换机进行通信，其中用得最多的局域网协议就是以太网。 链路层使用MTU表示最大传输帧长度，报文长度不能超过MTU,否则会进行分片，比如比较大的IP数据报就会被分片，为了避免被分片。一般要控制IP报文长度。 广播： 要理解什么是广播风暴，就必须先理解网络通信技术。 网络上的一个节点，它发送一个数据帧或包，被传输到由广播域定义的本地网段上的每个节点就是广播。 网络广播分为第2层广播和第3层广播。第2层广播也称硬件广播，用于在局域网内向所有的结点发送数据，通常不会穿过局域网的边界（路由器），除非它变成一个单播。广播将是一个二进制的全1或者十六进制全F的地址。而第3层广播用于在这个网络内向所有的结点发送数据。 帧的传输方式，即单播帧（Unicast Frame）、多播帧（Multicast Frame）和广播帧（Broadcast Frame）。 1、单播帧 单播帧也称“点对点”通信。此时帧的接收和传递只在两个节点之间进行，帧的目的MAC地址就是对方的MAC地址，网络设备（指交换机和路由器）根据帧中的目的MAC地址，将帧转发出去。 2、多播帧 多播帧可以理解为一个人向多个人（但不是在场的所有人）说话，这样能够提高通话的效率。多播占网络中的比重并不多，主要应用于网络设备内部通信、网上视频会议、网上视频点播等。 3、广播帧 广播帧可以理解为一个人对在场的所有人说话，这样做的好处是通话效率高，信息一下子就可以传递到全体。在广播帧中，帧头中的目的MAC地址是“FF.FF.FF.FF.FF.FF”，代表网络上所有主机网卡的MAC地址。 广播帧在网络中是必不可少的，如客户机通过DHCP自动获得IP地址的过程就是通过广播帧来实现的。而且，由于设备之间也需要相互通信，因此在网络中即使没有用户人为地发送广播帧，网络上也会出现一定数量的广播帧。 同单播和多播相比，广播几乎占用了子网内网络的所有带宽。网络中不能长时间出现大量的广播帧，否则就会出现所谓的“广播风暴”（每秒的广播帧数在1000以上）。拿开会打一个比方，在会场上只能有一个人发言，如果所有人都同时发言的话，会场上就会乱成一锅粥。广播风暴就是网络长时间被大量的广播数据包所占用，使正常的点对点通信无法正常进行，其外在表现为网络速度奇慢无比。出现广播风暴的原因有很多，一块故障网卡就可能长时间地在网络上发送广播包而导致广播风暴。 使用路由器或三层交换机能够实现在不同子网间隔离广播风暴的作用。当路由器或三层交换机收到广播帧时并不处理它，使它无法再传递到其他子网中，从而达到隔离广播风暴的目的。因此在由几百台甚至上千台电脑构成的大中型局域网中，为了隔离广播风暴，都要进行子网划分。 使用vlan完全可以隔离广播风暴。 在交换以太网上运行TCP/IP环境下：二层广播是在数据链路层的广播，它 的广播范围是二层交换机连接的所有端口；二层广播不能通过路由器。 三层广播就是在网络层的广播，它的范围是同一IP子网内的设备，子网广播也不能通过路由器。 第三层的数据必须通过第二层的封装再发送，所以三层广播必然通过二层广播来实现。 设想在同一台二层交换机上连接2个ip子网的设备，所有的设备都可以接收到二层广播，但三层广播只对本子网设备有效，非本子网的设备也会接收到广播包，但会被丢弃。 广播风暴（broadcast storm） 简单的讲是指当广播数据充斥网络无法处理，并占用大量网络带宽，导致正常业务不能运行，甚至彻底瘫痪，这就发生了“广播风暴” 。一个数据帧或包被传输到本地网段 （由广播域定义）上的每个节点就是广播；由于网络拓扑的设计和连接问题，或其他原因导致广播在网段内大量复制，传播数据帧，导致网络性能下降，甚至网络瘫痪，这就是广播风暴。 要避免广播风暴，可以采用恰当划分VLAN、缩小广播域、隔离广播风暴，还可在千兆以太网口上启用广播风暴控制，最大限度地避免网络再次陷入瘫痪。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer算法学习总结]]></title>
    <url>%2F2018%2F07%2F09%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E5%89%91%E6%8C%87offer%2F</url>
    <content type="text"><![CDATA[节选剑指offer比较经典和巧妙的一些题目，以便复习使用。一部分题目给出了完整代码，一部分题目比较简单直接给出思路。但是不保证我说的思路都是正确的，个人对算法也不是特别在行，只不过这本书的算法多看了几遍多做了几遍多了点心得体会。于是想总结一下。如果有错误也希望能指出，谢谢。 具体代码可以参考我的GitHub仓库： https://github.com/h2pl/SwordToOffer 数论和数字规律从1到n整数中1出现的次数题目描述求出113的整数中1出现的次数,并算出1001300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数。 1暴力办法，把整数转为字符串，依次枚举相加。复杂度是O（N * k）k为数字长度。 2第二种办法看不懂，需要数学推导，太长不看 排数组排成最小的数输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 解析：本题的关键是，两个数如何排成最小的，答案是，如果把数字看成字符串a,b那么如果a+b&gt;b+a，则a应该放在b后面。例如 3和32 3 + 32 = 332,32 + 3 = 323,332&gt;323,所以32要放在前面。 根据这个规律，构造一个比较器，使用排序方法即可。 丑数题目描述把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 解析 1 暴力枚举每个丑数，找出第N个即可。 2 这个思路比较巧妙，由于丑数一定是由2,3,5三个因子构成的，所以我们每次构造出一个比前面丑数大但是比后面小的丑数，构造N次即可。 public class Solution { public static int GetUglyNumber_Solution(int index) { if (index == 0) return 0; int []res = new int[index]; res[0] = 1; int i2,i3,i5; i2 = i3 = i5 = 0; for (int i = 1;i &lt; index;i ++) { res[i] = Math.min(res[i2] * 2, Math.min(res[i3] * 3, res[i5] * 5)); if (res[i] == res[i2] * 2) i2 ++; if (res[i] == res[i3] * 3) i3 ++; if (res[i] == res[i5] * 5) i5 ++; } return res[index - 1]; } } } i2,i3,i5分别代表目前有几个2,3,5的因子，每次选一个最小的丑数，然后开始找下一个。当然i2，i3，i5也要跟着变。数组和矩阵二维数组的查找/** * Created by 周杰伦 on 2018/2/25. * 题目描述 在一个二维数组中，每一行都按照从左到右递增的顺序排序， 每一列都按照从上到下递增的顺序排序。请完成一个函数， 输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 1 2 3 2 3 4 3 4 5 */ 解析：比较经典的一题，解法也比较巧妙，由于数组从左向右和从上到下的都是递增的，所以找一个数可以先从最右开始找。 假设最右值为a，待查数为x，那么如果x &lt; a说明x在a的左边，往左找即可，如果x &gt; a，说明x 在 a的下面一行，到下面一行继续按照该规则查找，就可以遍历所有数。 算法的时间复杂度是O(M * N) public class 二维数组中的查找 { public static boolean Find(int target, int[][] array) { if(array[0][0] &gt; target) { return false; } int row = 0; int col = 0; while (row &lt; array.length &amp;&amp; col &gt;0) { if (target == array[row][col]) { return true; } else if (target &lt;array[row][col]) { col --; } else if (target &gt; array[row][col]) { col ++; } else row++; } return false; } }顺时针打印矩阵。输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 这题还是有点麻烦的，因为要顺时针打印，所以实际上是由外向内打印，边界的处理和递归调用需要谨慎。 这题我自己没写出标准答案。参考一个答案吧。关键在于四个循环中的分界点设置。 //主体循环部分才5行。其实是有规律可循的。将每一层的四个边角搞清楚就可以打印出来了 import java.util.ArrayList; public class Solution { public ArrayList&lt;Integer&gt; printMatrix(int [][] array) { ArrayList&lt;Integer&gt; result = new ArrayList&lt;Integer&gt; (); if(array.length==0) return result; int n = array.length,m = array[0].length; if(m==0) return result; int layers = (Math.min(n,m)-1)/2+1;//这个是层数 for(int i=0;i&lt;layers;i++){ for(int k = i;k&lt;m-i;k++) result.add(array[i][k]);//左至右 for(int j=i+1;j&lt;n-i;j++) result.add(array[j][m-i-1]);//右上至右下 for(int k=m-i-2;(k&gt;=i)&amp;&amp;(n-i-1!=i);k--) result.add(array[n-i-1][k]);//右至左 for(int j=n-i-2;(j&gt;i)&amp;&amp;(m-i-1!=i);j--) result.add(array[j][i]);//左下至左上 } return result; } }调整数组中数字的顺序，使正数在负数的前面双指针即可以解决，变式有正负，奇偶等等。 数组中出现次数超过一半的数字本题有很多种解法。 1 最笨的解法，统计每个数的出现次数，O（n2) 2 使用hashmap，空间换时间O（n) 3 由于出现超过一半的数字一定也是中位数，所以可以先排序，再找到第n/2位置上的节点。 4 使用快速排序的复杂度是O（nlogn)，基于快排的特性，每一轮的过程都会把一个数放到最终位置，所以我们可以判断一下这个数的位置是不是n/2，如果是的话，那么就直接返回即可。这样就优化了快排的步骤。 4.5事实上，上述办法的复杂度仍然是O（nlogn） 快速排序的partition函数将一个数组分为左右两边，并且我们可以知道，如果flag值在k位置左边，那么往左找，如果在k位置右边，那么往左找。 这里科普一下经典快排中的一个方法partition，剑指offer书中直接跳过了这部分，让我摸不着头脑。 虽然快排用到了经典的分而治之的思想，但是快排实现的前提还是在于 partition 函数。正是有了 partition 的存在，才使得可以将整个大问题进行划分，进而分别进行处理。 除了用来进行快速排序，partition 还可以用 O(N) 的平均时间复杂度从无序数组中寻找第K大的值。和快排一样，这里也用到了分而治之的思想。首先用 partition 将数组分为两部分，得到分界点下标 pos，然后分三种情况： pos == k-1，则找到第 K 大的值，arr[pos]； pos &gt; k-1，则第 K 大的值在左边部分的数组。 pos &lt; k-1，则第 K 大的值在右边部分的数组。 下面给出基于迭代的实现（用来寻找第 K 小的数）： int find_kth_number(vector&lt;int&gt; &amp;arr, int k){ int begin = 0, end = arr.size(); assert(k&gt;0 &amp;&amp; k&lt;=end); int target_num = 0; while (begin &lt; end){ int pos = partition(arr, begin, end); if(pos == k-1){ target_num = arr[pos]; break; } else if(pos &gt; k-1){ end = pos; } else{ begin = pos + 1; } } return target_num; } 该算法的时间复杂度是多少呢？考虑最坏情况下，每次 partition 将数组分为长度为 N-1 和 1 的两部分，然后在长的一边继续寻找第 K 大，此时时间复杂度为 O(N^2 )。不过如果在开始之前将数组进行随机打乱，那么可以尽量避免最坏情况的出现。而在最好情况下，每次将数组均分为长度相同的两半，运行时间 T(N) = N + T(N/2)，时间复杂度是 O(N)。所以也就是说，本题用这个方法解的话，复杂度只需要O（n),因为第一次交换需要N/2,j接下来的交换的次数越来越少，最后加起来就是O（N）了。 5 由于数字出现次数超过长度的一半，也就是平均每两个数字就有一个该数字，但他们不一定连续，所以变量time保存一个数的出现次数，然后变量x代表目前选择的数字，遍历中，如果x与后一位不相等则time–，time=0时x改为后一位，time重新变为1。最终x指向的数字就是出现次数最多的。 举两个例子，比如1,2,3,4,5,6,6,6,6,6,6。明显符合。1,6,2,6,3,6,4,6,5,6,6 遍历到最后得到x=6，以此类推，可以满足要求。 找出前k小的数 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 / 解析： 1如果允许改变数组，那么则可以继承上一题的思想。，使用快速排序中的partition方法，只需要O（N）的复杂度 2使用堆排序 解析：用前k个数构造一个大小为k的大顶堆，然后遍历余下数字，如果比堆顶大，则跳过，如果比堆顶小，则替换掉堆顶元素，然后执行一次堆排序（即根节点向下调整)。此时的堆顶元素已被替换， 然后遍历完所有元素，堆中的元素就是最小的k个元素了。 如果要求最大的k个元素，则构造小顶堆就可以了。 构造堆的方法是，数组的第N/2号元素到0号元素依次向下调整，此时数组就构成了堆。 实际上我们可以使用现成的集合类，红黑树是一棵搜索树，他是排序的，所以可以得到最大和最小值，那么我们每次和最小值比较，符合条件就进行替换即可。复杂度是O（nlogn) public ArrayList&lt;Integer&gt; GetLeastNumbers_Solution(int [] input, int k) { ArrayList&lt;Integer&gt;arrayList=new ArrayList&lt;&gt;(); if(input==null || input.length==0 ||k==0 ||k&gt;input.length)return arrayList; TreeSet&lt;Integer&gt; treeSet=new TreeSet&lt;&gt;();​ for(int i=0;i&lt;input.length;i++){ if(treeSet.size()&lt;k){ treeSet.add(input[i]); } else { if(input[i]&lt;treeSet.last()){ treeSet.pollLast(); treeSet.add(input[i]); } } } for(Integer x:treeSet){ arrayList.add(x); } return arrayList; }连续子数组的最大和** Created by 周杰伦 on 2017/3/23. 题目描述HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。你会不会被他忽悠住？(子向量的长度至少是1) 解析：笨办法需要O（n2）的复杂度。 1 但是实际上只需要一次遍历即可解决。通过sum保存当前和，然后如果当前和为正，那么继续往后加，如果当前和为负，则直接丢弃，令当前和等于一个新值。并且每一步都要比较当前和与最大值。 */ public class 连续数字序列的最大和 { public int FindGreatestSumOfSubArray(int[] array) { if(array==null || array.length==0)return 0; int sum=0;int max=array[0]; for(int i=0;i&lt;array.length;i++){ //如果当前和&lt;0，那就不加，直接赋新值 if(sum&lt;=0){ sum=array[i]; }//如果当前和大于零，则继续加。 else { sum+=array[i]; } if(max&lt;sum){ max=sum; } } return max; }2 本题也可以使用DP解法 DP数组代表以i为结尾元素的连续最大和 DP[i] = arr[i] (DP[i-1] &lt; 0) = DP[i] + arr[i] (DP[i -1] &gt; 0) 逆序对/** Created by 周杰伦 on 2017/3/23. 题目描述在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。即输出P%1000000007 / 解析：本题采用归并排序的框架，只是在归并的时候做出逆序对查找，具体参见下面代码。 核心点是，在归并两个有序数组时，如果a数组的元素a1比b数组的元素b1大时，说明有mid - i + 1个数都比b1大。i为a1元素的位置。 这样子我们就可以统计逆序对的个数了。经典巧妙。！ public class 逆序对 { public double Pairs = 0; public int InversePairs(int [] array) { if (array.length==0 ||array==null) return 0; mergesort(array,0,array.length-1); Pairs = Pairs + 1000000007; return (int) (Pairs % 1000000007); } public void merge(int []array,int left,int mid,int right){ //有一点很重要的是，归并分成两部分，其中一段是left到mid，第二段是mid+1到right。 //不能从0到mid-1，然后mid到right。因为这样左右不均分，会出错。千万注意。 //mid=(left+right)/2 if (array.length==0 ||array==null ||left&gt;=right) return ; int p=left,q=mid+1,k=0; int []temp=new int[right-left+1]; while (p&lt;=mid &amp;&amp; q&lt;=right){ if(array[p]&gt;array[q]){ temp[k++]=array[q++]; //当前半数组中有一个数p比后半个数组中的一个数q大时，由于两个数组 //已经分别有序，所以说明p到中间数之间的所有数都比q大。 Pairs+=mid-p+1; } else temp[k++]=array[p++]; } while (p&lt;=mid){ temp[k++]=array[p++];} while (q&lt;=right){ temp[k++]=array[q++];} for (int m = 0; m &lt; temp.length; m++) array[left + m] = temp[m]; } public void mergesort(int []arr,int left,int right){ if (arr.length==0 ||arr==null) return ; int mid=(right+left)/2; if(left&lt;right) { mergesort(arr, left, mid); mergesort(arr, mid + 1, right); merge(arr, left,mid, right); } }数字在排序数组中出现的次数1 顺序扫描 鲁迅说过：看到排序数组要想到二分法！ 2 通过二分查找找到数字k第一次出现的位置，即先比较k和中间值，再依次二分，如果中间值等于k并且中间值左边！=k，则是第一个k。反之可以找到最后一次出现k的位置。然后相减即可。复杂度是logn 和为s的两个整数，和为s的连续正数序列1 和为s的两个整数，双指针遍历即可 2 和为s的连续正数序列。维护一个范围，start到end表示目前的数字序列。大于S则start++,小于S则start– n个色子的点数求n个色子点数之和等于s的概率 1 递归实现 2 循环实现，所有可能值存成一个数组，大小为6n，然后把每个出现数字次数存入数组，遍历一遍即可得到概率。 扑克牌的顺子题目描述LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何。为了方便起见,你可以认为大小王是0。 把扑克牌存到数组中，并且A看作1,J为11,Q为12,K为13，然后进行排序，如果有不连续的数字，不存在顺子，如果都连续，则是顺子 Arrays.sort(arr); for (int i = 1 ;i &lt; arr.length;i ++) { if (arr[i] == arr[i - 1]) { return false; } if (arr[i] - arr[i - 1] == 1) { continue; }else if (arr[i] - arr[i - 1] - 1 &lt;= cnt) { cnt -= arr[i] - arr[i - 1] - 1; }else { return false; } } return true; }数组中重复的数字在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 数组中重复的数字在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 解析：一般使用hashmap即可达到O（n) 但剑指offer的解法可以只用O(1)的空间做到。 实际上当每个数字a和他们所在的位置n相同时，每个数字只出现一次，但如果n + 1的位置上的数也是a，那么a就是第一个重复出现的数字。 根据这个思路。我们在循环中当第i个数a与arr[a]相等时，不变，如果不相等，则两者互换，然后开始下一轮遍历。接下来继续交换，如果出现相等的情况，则就是第一个重复出现的数。 举例 2 3 1 0 2 5 31 arr[0] = 2 != 0,所以arr[0]与arr[2]做交换，得1 3 2 0 2 5 32 arr[0] = 1 != 0，所以arr[0]和arr[1]交换，的3 1 2 0 2 5 33 arr[0] = 3 != 0,所以arr[0]和arr[3]交换，得0 1 2 3 2 5 34 arr[0]到arr[3]都符合要求,arr[4] = 2 != 4，所以arr[4]和arr[2]交换，发现两者相等，所以他就是第一个重复的数。 public boolean duplicate(int array[],int length,int [] duplication) { if ( array==null ) return false; // key step for( int i=0; i&lt;length; i++ ){ while( i!=array[i] ){ if ( array[i] == array[array[i]] ) { duplication[0] = array[i]; return true; } int temp = array[i]; array[i] = array[array[i]]; array[array[i]] = temp; } } return false; }从上述例子可以看到，一个数最多被交换两次。所以复杂度为O（N) 构建乘积数组题目描述给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]*A[i+1]…*A[n-1]。不能使用除法。 不能用除法，那么就两个循环，一个从0乘到i - 1，一个从i + 1乘到n-1 数据流的中位数题目描述如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 解析，与字符流第一个不重复的字符类似，每次添加数字都要输出一次结果。 public class Solution { static ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public static void Insert(Integer num) { list.add(num); Collections.sort(list); } public static Double GetMedian() { if (list.size() % 2 == 0) { int l = list.get(list.size()/2); int r = list.get(list.size()/2 - 1); return (l + r)/2.0; } else { return list.get(list.size()/2)/1.0; } }​ } 滑动窗口中的最大值给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 解析：1 保持窗口为3进行右移，每次计算出一个最大值即可。 2 使用两个栈实现一个队列，复杂度O（N）,使用两个栈实现最大值栈，复杂度O（1）。两者结合可以完成本题。但是太麻烦了。 3 使用双端队列解决该问题。 import java.util.*; /** 用一个双端队列，队列第一个位置（队头）保存当前窗口的最大值，当窗口滑动一次 1.判断当前最大值是否过期（如果最大值所在的下标已经不在窗口范围内，则过期） 2.对于一个新加入的值，首先一定要先放入队列，即使他比队头元素小，因为队头元素可能过期。 3.新增加的值从队尾开始比较，把所有比他小的值丢掉(因为队列只存最大值，所以之前比他小的可以丢掉) */ public class Solution { public ArrayList&lt;Integer&gt; maxInWindows(int [] num, int size) { ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); if(size == 0) return res; int begin; ArrayDeque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); for(int i = 0; i &lt; num.length; i++){ begin = i - size + 1; if(q.isEmpty()) q.add(i); else if(begin &gt; q.peekFirst()) q.pollFirst(); while((!q.isEmpty()) &amp;&amp; num[q.peekLast()] &lt;= num[i]) q.pollLast(); q.add(i); if(begin &gt;= 0) res.add(num[q.peekFirst()]); } return res; } }字符串字符串的排列输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 解析：这是一个全排列问题，也就是N个不同的数排成所有不同的序列，只不过把数换成了字符串。 全排列的过程就是，第一个元素与后续的某个元素交换，然后第二个元素也这么做，直到最后一个元素为之，过程是一个递归的过程，也是一个dfs的过程。 注意元素也要和自己做一次交换，要不然会漏掉自己作为头部的情况。 然后再进行一次字典序的排序即可。 public static ArrayList&lt;String&gt; Permutation(String str) { char []arr = str.toCharArray(); List&lt;char []&gt; list = new ArrayList&lt;&gt;(); all(arr, 0, arr.length - 1, list); Collections.sort(list, (o1, o2) -&gt; String.valueOf(o1).compareTo(String.valueOf(o2))); ArrayList&lt;String&gt; res = new ArrayList&lt;&gt;(); for (char[] c : list) { if (!res.contains(String.valueOf(c))) res.add(String.valueOf(c)); } return res; } //注意要换完为之，因为每换一次可以去掉头部一个数字，这样可以避免重复 public static void all(char []arr, int cur, int end, List&lt;char[]&gt; list) { if (cur == end) { // System.out.println(Arrays.toString(arr)); list.add(Arrays.copyOf(arr, arr.length)); } for (int i = cur;i &lt;= end;i ++) { //这里的交换包括跟自己换，所以只有一轮换完才能确定一个结果 swap(arr, cur, i); all(arr, cur + 1, end, list); swap(arr, cur, i); } } public static void swap(char []arr, int i, int j) { if (i &gt; arr.length || j &gt; arr.length || i &gt;= j)return; char temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; }替换空格/** * Created by 周杰伦 on 2018/2/25. * 请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 */ 解析：如果单纯地按顺序替换空格，每次替换完还要将数组扩容，再右移，这部操作的时间复杂度就是O（2*N）=O（N)，所以总的复杂度是O（n^2)，所以应该采取先扩容的办法，统计出空格数，然后扩容，接下来按顺序添加字符，遇到空格直接改成添加%20即可，这样避免了右移操作和多次扩容，复杂度是O（N)​ public class 替换空格 { public static String replaceSpace(StringBuffer str) { int newlen = 0; for(int i = 0; i &lt; str.length(); i++) { if(str.charAt(i) == ‘ ‘) { newlen = newlen + 3; } else { newlen ++; } } char []newstr = new char[newlen]; int j = 0; for(int i = 0 ; i &lt; str.length(); i++) { if (str.charAt(i) == ‘ ‘) { newstr[j++] = ‘%’; newstr[j++] = ‘2’; newstr[j++] = ‘0’; }else { newstr[j++] = str.charAt(i); } } return String.valueOf(newstr); } 第一次只出现一次的字符哈希表可解 翻转单词顺序和左旋转字符串1题目描述牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ 这个解法很经典，先把每个单词逆序，再把整个字符串逆序，结果就是把每个单词都进行了翻转。2汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ 字符串循环左移N位的处理方法也很经典，先把前N位逆序，再把剩余字符串逆序，最后整体逆序。 abcXYZdef -&gt; cbafedZYX -&gt; XYZdefabc把字符串转换为整数题目描述将一个字符串转换成一个整数，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0 解析：首先需要判断正负号，然后判断每一位是否是数字，然后判断是否溢出，判断溢出可以通过加完第n位的和与未加第n位的和进行比较。最后可以得出结果。所以需要3-4步判断。 表示数值的字符串请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。 不得不说这种题型太恶心了，就是需要一直判断边界条件 参考一个答案。比较完整 bool isNumeric(char* str) { // 标记符号、小数点、e是否出现过 bool sign = false, decimal = false, hasE = false; for (int i = 0; i &lt; strlen(str); i++) { if (str[i] == &apos;e&apos; || str[i] == &apos;E&apos;) { if (i == strlen(str)-1) return false; // e后面一定要接数字 if (hasE) return false; // 不能同时存在两个e hasE = true; } else if (str[i] == &apos;+&apos; || str[i] == &apos;-&apos;) { // 第二次出现+-符号，则必须紧接在e之后 if (sign &amp;&amp; str[i-1] != &apos;e&apos; &amp;&amp; str[i-1] != &apos;E&apos;) return false; // 第一次出现+-符号，且不是在字符串开头，则也必须紧接在e之后 if (!sign &amp;&amp; i &gt; 0 &amp;&amp; str[i-1] != &apos;e&apos; &amp;&amp; str[i-1] != &apos;E&apos;) return false; sign = true; } else if (str[i] == &apos;.&apos;) { // e后面不能接小数点，小数点不能出现两次 if (hasE || decimal) return false; decimal = true; } else if (str[i] &lt; &apos;0&apos; || str[i] &gt; &apos;9&apos;) // 不合法字符 return false; } return true; }字符流中第一个不重复的字符题目描述请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。 本题主要要注意的是流。也就是说每次输入一个字符就要做一次判断。比如输入aaaabbbcd，输出就是a###b##cd StringBuilder sb = new StringBuilder(); int []map = new int[256]; public void Insert(char ch) { sb.append(ch); if (map[ch] == 0) { map[ch] = 1; }else { map[ch] ++; } System.out.println(FirstAppearingOnce()); } //return the first appearence once char in current stringstream public char FirstAppearingOnce() { for (int i = 0;i &lt; sb.length();i ++) { if (map[sb.charAt(i)] == 1) { return sb.charAt(i); } } return &apos;#&apos;; }链表从尾到头打印链表考查递归，递归可以使输出的顺序倒置 public static void printReverse(Node node) { if (node.next != null) { printReverse(node.next); } System.out.print(node.val + &quot; &quot;); }链表倒数第k个节点使用两个指针，一个先走k步。然后一起走即可。 反转链表老生常谈，但是容易写错。 public ListNode ReverseList(ListNode head) { if(head==null || head.next==null)return head; ListNode pre,next; pre=null; next=null; while(head!=null){ //保存下一个结点 next=head.next; //连接下一个结点 head.next=pre; pre=head; head=next; } return pre; } }合并两个排序链表与归并排序的合并类似 复杂链表的复制题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 这题比较恶心。 解析： 1 直接复制链表，然后再去复制特殊指针，复杂度是O(n2) 2 使用hash表保存特殊指针的映射关系，第二步简化操作，复杂度是O（n） 3 复制每个节点并且连成一个大链表A-A’-B-B’，然后从头到尾判断特殊指针，如果有特殊指针，则让后续节点的特殊指针指向原节点特殊指针指向的节点的后置节点，晕了吧，其实就是原来是A指向B，现在是A’指向B‘。 最后我们根据奇偶序号把链表拆开，复杂度是O（N)且不用额外空间。 两个链表的第一个公共节点1 逆置链表，反向找第一个不同节点，前一个就是公共节点 2 求长度并相减得n，短的链表先走n步，然后一起走即可。 孩子们的游戏(圆圈中最后剩下的数)这是一个约瑟夫环问题。 1 使用循环链表求解，每次走n步摘取一个节点，然后继续，直到最后一个节点就是剩下的数，空间复杂度为O（n) 2 使用数组来做public static int LastRemaining_Solution(int n, int m) { int []arr = new int[n]; for (int i = 0;i &lt; n;i ++) { arr[i] = i; } int cnt = 0; int sum = 0; for (int i = 0;i &lt; n;i = (i + 1) % n) { if (arr[i] == -1) { continue; } cnt ++; if (cnt == m) { arr[i] = -1; cnt = 0; sum ++; } if (sum == n) { return i; } } return n - 1; }3 使用余数法求解 int LastRemaining_Solution(int n, int m) { if (m == 0 || n == 0) { return -1; } ArrayList&lt;Integer&gt; data = new ArrayList&lt;Integer&gt;(); for (int i = 0; i &lt; n; i++) { data.add(i); } int index = -1; while (data.size() &gt; 1) { // System.out.println(data); index = (index + m) % data.size(); // System.out.println(data.get(index)); data.remove(index); index--; } return data.get(0); }链表的环的入口结点一个链表中包含环，请找出该链表的环的入口结点。 解析： 1 指定两个指针，一个一次走两步，一个一次走一步，然后当两个节点相遇时，这个节点必定在环中。既然这个节点在环中，那么让这个节点走一圈直到与自己相等为之，可以得到环的长度n。 2 得到了环的长度以后，根据数学推导的结果，我们可以指定两个指针，一个先走n步，然后两者同时走，这样的话，当慢节点到达入口节点时，快节点也转了一圈刚好又到达入口节点，所以也就是他们相等的时候就是入口节点了。 删除链表中重复的节点题目描述在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5 保留头结点，然后找到下一个不重复的节点，与他相连，重复的节点直接跳过即可。 #二叉树 二叉搜索树转换为双向链表输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 二叉搜索树要转换成有序的双向链表，实际上就是使用中序遍历把节点连入链表中，并且题目要求在原来节点上进行操作，也就是使用左指针和右指针表示链表的前置节点和后置节点。 使用栈实现中序遍历的非递归算法，便可以找出节点的先后关系，依次连接即可。 public TreeNode Convert(TreeNode root) { if(root==null) return null; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode p = root; TreeNode pre = null;// 用于保存中序遍历序列的上一节点 boolean isFirst = true; while(p!=null||!stack.isEmpty()){ while(p!=null){ stack.push(p); p = p.left; } p = stack.pop(); if(isFirst){ root = p;// 将中序遍历序列中的第一个节点记为root pre = root; isFirst = false; }else{ pre.right = p; p.left = pre; pre = p; } p = p.right; } return root; } }重建二叉树 * 题目描述 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 */ 解析：首先，头结点一定是先序遍历的首位，并且该节点把中序分为左右子树，根据这个规则，左子树由左边数组来完成，右子树由右边数组来完成，根节点由中间节点来构建，于是便有了如下的递归代码。该题的难点就在于边界的判断。 public TreeNode reConstructBinaryTree(int [] pre, int [] in) { if(pre.length == 0||in.length == 0){ return null; } TreeNode node = new TreeNode(pre[0]); for(int i = 0; i &lt; in.length; i++){ if(pre[0] == in[i]){ node.left = reConstructBinaryTree(Arrays.copyOfRange(pre, 1, i+1), Arrays.copyOfRange(in, 0, i));//为什么不是i和i-1呢，因为要避免出错，中序找的元素需要再用一次。 node.right = reConstructBinaryTree(Arrays.copyOfRange(pre, i+1, pre.length), Arrays.copyOfRange(in, i+1,in.length)); } } return node; }树的子结构/** * Created by 周杰伦 on 2018/3/27. * 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） */ 解析：本题还是有点难度的，子结构要求节点完全相同，所以先判断节点是否相同，然后使用先序遍历进行递判断，判断的依据是如果子树为空，则说明节点都找到了，如果原树节点为空，说明找不到对应节点,接着递归地判断该节点的左右子树是否符合要求. public class 树的子结构 { public boolean HasSubtree(TreeNode root1, TreeNode root2) { boolean res = false; if (root1 != null &amp;&amp; root2 != null) { if (root1.val == root2.val) { res = aHasb(root1, root2); } if (res == false) { res = HasSubtree(root1.left,root2); } if (res == false) { res = HasSubtree(root1.right,root2); } return res; } else return false; } public boolean aHasb(TreeNode t1, TreeNode t2){ if (t2 == null) return true; if (t1 == null) return false; if (t1.val != t2.val) return false; return aHasb(t1.left,t2.left) &amp;&amp; aHasb(t1.right,t2.right); } }​ 镜像二叉树/** * Created by 周杰伦 on 2017/3/19.操作给定的二叉树，将其变换为源二叉树的镜像。 输入描述: 二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 */ 解析：其实镜像二叉树就是交换所有节点的左右子树，所以使用遍历并且进行交换即可。 /** public class TreeNode { int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) { this.val = val; } } */ public class 镜像二叉树 { public void Mirror(TreeNode root) { if(root == null)return; if(root.left!=null || root.right!=null) { TreeNode temp=root.left; root.left=root.right; root.right=temp; } Mirror(root.left); Mirror(root.right); }​ 树的层次遍历也就是从上到下打印节点，使用队列即可完成。 二叉树的深度经典遍历。 判断是否平衡二叉树判断左右子树的高度差是否 &lt;= 1即可。 二叉搜索树的后序遍历题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 解析：这题其实也非常巧妙。二叉搜索树的特点就是他的左子树都比根节点小，右子树都比跟节点大。而后序遍历的根节点在最后，所以后续遍历的第1到N-1个节点应该是左右子树的节点（不一定左右子树都存在）。 后续遍历的序列是先左子树，再右子树，最后根节点，那么就要求，左半部分比根节点小，右半部分比根节点大，当然，左右部分不一定都存在。 所以，找出根节点后，首先找出左半部分，要求小于根节点，然后找出右半部分，要求大于根节点，如果符合，则递归地判断左右子树到的根节点（本步骤已经将左右部分划分，割据中间节点进行递归），如果不符合，直接返回false。 同理也可以判断前序遍历和中序遍历。 public class 二叉搜索树的后序遍历序列 { public static void main(String[] args) { int []a = {7,4,6,5}; System.out.println(VerifySquenceOfBST(a)); } public static boolean VerifySquenceOfBST(int [] sequence) { if (sequence == null || sequence.length == 0) { return false; } return isBST(sequence, 0, sequence.length - 1); } public static boolean isBST(int []arr, int start, int end) { if (start &gt;= end) return true; int root = arr[end]; int mid = start; for (mid = start;mid &lt; end &amp;&amp; arr[mid] &lt; root;mid ++) { } for (int i = mid;i &lt; end; i ++) { if (arr[i] &lt; root)return false; } return isBST(arr, start, mid - 1) &amp;&amp; isBST(arr, mid, end - 1); } }二叉树中和为某一值的路径/** Created by 周杰伦 on 2018/3/29. 题目描述输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 / 解析：由于要求从根节点到达叶子节点，并且要打印出所有路径，所以实际上用到了回溯的思想。 通过target跟踪当前和，进行先序遍历，当和满足要求时，加入集合，由于有多种结果，所以需要回溯，将访问过的节点弹出访问序列，才能继续访问下一个节点。 终止条件是和满足要求，并且节点是叶节点，或者已经访问到空节点也会返回。 public class 二叉树中和为某一值的路径 { private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; listAll = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); private ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root,int target) { if(root == null) return listAll; list.add(root.val); target -= root.val; if(target == 0 &amp;&amp; root.left == null &amp;&amp; root.right == null) listAll.add(new ArrayList&lt;Integer&gt;(list)); FindPath(root.left, target); FindPath(root.right, target); list.remove(list.size()-1); return listAll; } static int count = 0; static Stack&lt;Integer&gt; path = new Stack&lt;&gt;(); static Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); static ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists = new ArrayList&lt;&gt;(); }二叉树的下一个节点给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 解析：给出一个比较好懂的解法，中序遍历的结果存在集合中，找到根节点，进行中序遍历，然后找到该节点，下一个节点就是集合后一位 public TreeLinkNode GetNext(TreeLinkNode TreeLinkNode) { return findNextNode(TreeLinkNode); } public TreeLinkNode findNextNode(TreeLinkNode anynode) { if (anynode == null) return null; TreeLinkNode p = anynode; while (p.next != null) { p = p.next; } ArrayList&lt;TreeLinkNode&gt; list = inOrderSeq(p); for (int i = 0;i &lt; list.size();i ++) { if (list.get(i) == anynode) { if (i + 1 &lt; list.size()) { return list.get(i + 1); } else return null; } } return null; } static ArrayList&lt;TreeLinkNode&gt; list = new ArrayList&lt;&gt;(); public static ArrayList&lt;TreeLinkNode&gt; inOrderSeq(TreeLinkNode TreeLinkNode) { if (TreeLinkNode == null) return null; inOrderSeq(TreeLinkNode.left); list.add(TreeLinkNode); inOrderSeq(TreeLinkNode.right); return list; }对称的二叉树请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 解析，之前有一题是二叉树的镜像，递归交换左右子树即可求出镜像，然后递归比较两个树的每一个节点，则可以判断是否对称。 boolean isSymmetrical(TreeNode pRoot) { TreeNode temp = copyTree(pRoot); Mirror(pRoot); return isSameTree(temp, pRoot); }​ void Mirror(TreeNode root) { if(root == null)return; Mirror(root.left); Mirror(root.right); if(root.left!=null || root.right!=null) { TreeNode temp=root.left; root.left=root.right; root.right=temp; } ​ } boolean isSameTree(TreeNode t1,TreeNode t2){ if(t1==null &amp;&amp; t2==null)return true; else if(t1!=null &amp;&amp; t2!=null &amp;&amp; t1.val==t2.val) { boolean left = isSameTree(t1.left, t2.left); boolean right = isSameTree(t1.right, t2.right); return left &amp;&amp; right; } else return false; } TreeNode copyTree (TreeNode root) { if (root == null) return null; TreeNode t = new TreeNode(root.val); t.left = copyTree(root.left); t.right = copyTree(root.right); return t; }把二叉树打印成多行题目描述从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 解析：1 首先要知道到本题的基础思想，层次遍历。 2 然后是进阶的思想，按行打印二叉树并输出行号，方法是，一个节点last指向当前行的最后一个节点，一个节点nlast指向下一行最后一个节点。使用t表示现在遍历的节点，当t = last时，表示本行结束。此时last = nlast，开始下一行遍历。 同时，当t的左右子树不为空时，令nlast = t的左子树和右子树。每当last 赋值为nlast时，行号加一即可。 按之字形顺序打印二叉树请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 解析：1 首先要知道到本题的基础思想，层次遍历。 2 然后是进阶的思想，按行打印二叉树并输出行号，方法是，一个节点last指向当前行的最后一个节点，一个节点nlast指向下一行最后一个节点。使用t表示现在遍历的节点，当t = last时，表示本行结束。此时last = nlast，开始下一行遍历。 同时，当t的左右子树不为空时，令nlast = t的左子树和右子树。每当last 赋值为nlast时，行号加一即可。 3 基于第2步的思想，现在要z字型打印，只需把偶数行逆序即可。所以把每一行的数存起来，然后偶数行逆置即可。 ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) { LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); TreeNode root = pRoot; if(root == null) { return new ArrayList&lt;&gt;(); } TreeNode last = root; TreeNode nlast = root; queue.offer(root); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(root.val); ArrayList&lt;Integer&gt; one = new ArrayList&lt;&gt;(); one.addAll(list); ArrayList&lt;ArrayList&lt;Integer&gt;&gt; lists = new ArrayList&lt;&gt;(); lists.add(one); list.clear(); int row = 1; while (!queue.isEmpty()){ TreeNode t = queue.poll(); if(t.left != null) { queue.offer(t.left); list.add(t.left.val); nlast = t.left; } if(t.right != null) { queue.offer(t.right); list.add(t.right.val); nlast = t.right; } if(t == last) { if(!queue.isEmpty()) { last = nlast; row ++; ArrayList&lt;Integer&gt; temp = new ArrayList&lt;&gt;(); temp.addAll(list); list.clear(); if (row % 2 == 0) { Collections.reverse(temp); } lists.add(temp); } } } return lists; }序列化和反序列化二叉树解析：序列化和反序列化关键是要确定序列化方式。我么使用字符串来序列化。 用#代表空，用!分隔左右子树。 比如 1 2 3 4 5 使用先序遍历序列化结果是1!2!4!###3!#5!## 反序列化先让根节点指向第一位字符，然后左子树递归进行连接，右子树 public class Solution { public int index = -1; StringBuffer sb = new StringBuffer(); String Serialize(TreeNode root) { if(root == null) { sb.append(&quot;#!&quot;) ; } else { sb.append(root.val + &quot;!&quot;); Serialize(root.left); Serialize(root.right); } return sb.toString(); } TreeNode Deserialize(String str) { index ++; int len = str.length(); if(index &gt;= len) { return null; } String[] strr = str.split(&quot;!&quot;); TreeNode node = null; if(!strr[index].equals(&quot;#&quot;)) { node = new TreeNode(Integer.valueOf(strr[index])); node.left = Deserialize(str); node.right = Deserialize(str); } return node; } }二叉搜索树的第k个结点解析：二叉搜索树的中序遍历是有序的，只需要在中序中判断数字是否在第k个位置即可。 如果在左子树中发现了，那么递归返回该节点，如果在右子树出现，也递归返回该节点。注意必须要返回，否则结果会被递归抛弃掉。 TreeNode KthNode(TreeNode pRoot, int k) { count = 0; return inOrderSeq(pRoot, k); } static int count = 0; public TreeNode inOrderSeq(TreeNode treeNode, int k) { if (treeNode == null) return null; TreeNode left = inOrderSeq(treeNode.left, k); if (left != null) return left; if (++ count == k) return treeNode; TreeNode right = inOrderSeq(treeNode.right, k); if (right != null) return right; return null; }栈和队列用两个队列实现栈，用两个栈实现队列。简单说下思路 1 两个栈实现队列，要求先进先出，入队时节点先进入栈A，如果栈A满并且栈B空则把全部节点压入栈B。 出队时，如果栈B为空，那么直接把栈A节点全部压入栈B，再从栈B出栈，如果栈B不为空，则从栈B出栈。 2 两个队列实现栈，要求后进先出。入栈时，节点先加入队列A，出栈时，如果队列B不为空，则把头结点以后的节点出队并加入到队列B，然后自己出队。 如果出栈时队列B不为空，则把B头结点以后的节点移到队列A，然后出队头结点，以此类推。 包含min函数的栈/** 设计一个返回最小值的栈 定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。 Created by 周杰伦 on 2017/3/22. / 解析：这道题的解法也是非常巧妙的。因为每次进栈和出栈都有可能导致最小值发生改变。而我们要维护的是整个栈的最小值。 如果单纯使用一个数来保存最小值，会出现一种情况，最小值出栈时，你此时的最小值只能改成栈顶元素，但这个元素不一定时最小值。 所以需要一个数组来存放最小值，或者是一个栈。 使用另一个栈B存放最小值，每次压栈时比较节点值和栈B顶端节点值，如果比它小则压栈，否则不压栈，这样就可以从b的栈顶到栈顶依次访问最小值，次小值。以此类推。 当最小值节点出栈时，判断栈B顶部的节点和出栈节点是否相同，相同则栈B也出栈。 这样就可以维护一个最小值的函数了。 同理，最大值也是这样。 public class 包含min函数的栈 { Stack&lt;Integer&gt; stack=new Stack&lt;&gt;(); Stack&lt;Integer&gt; minstack=new Stack&lt;&gt;(); public void push(int node) { if(stack.isEmpty()) { stack.push(node); minstack.push(node); } else if(node&lt;stack.peek()){ stack.push(node); minstack.push(node); } else { stack.push(node); } } public void pop() { if(stack.isEmpty())return; if(stack.peek()==minstack.peek()){ stack.pop(); minstack.pop(); } else { stack.pop(); } } public int top() { return stack.peek(); } public int min() { if(minstack.isEmpty())return 0; return minstack.peek(); } }栈的压入和弹出序列 题目描述 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 解析：本题是比较抽象的，首先，根据入栈出栈的规则，我们可以建立一个栈A，用于保存压栈序列，然后压入第一个元素，比较出栈序列的第一个元素，如果不相等，继续压栈，直到二者相等，此时栈A元素出栈，然后重复上一步的操作。 如果在每次压栈过程中，入栈序列已经全部入栈A但是还是找不到出栈序列的第一个元素时，则说明不是出栈序列。 当栈A的元素全部压入并出栈后，如果出栈序列也出栈完毕，则满足题意。 public static boolean IsPopOrder(int[] pushA, int[] popA) { Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int j = 0; int i = 0; while (i &lt; pushA.length) { stack.push(pushA[i]); i++; while (!stack.empty() &amp;&amp; stack.peek() == popA[j]) { stack.pop(); j++; } if (i == pushA.length) { if (!stack.empty()) { return false; } else return true; } } return false; }排序和查找旋转数组的最小数字把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 解析：这题的思路很巧妙，如果直接遍历复杂度为O（N），但是使用二分查找可以加快速度，因为两边的数组都是递增的最小值一定在两边数组的边缘，于是通过二分查找，逐渐缩短左右指针的距离，知道左指针和右指针只差一步，那么右指针所在的数就是最小值了。 复杂度是O（logN) //这段代码忽略了三者相等的情况 public int minNumberInRotateArray(int [] array) { if (array.length == 0) return 0; if (array.length == 1) return array[0]; int min = 0; int left = 0, right = array.length - 1; //只有左边值大于右边值时，最小值才可能出现在中间 while (array[left] &gt; array[right]) { int mid = (left + right)/2; if (right - left == 1) { min = array[right]; break; } //如果左半部分递增，则最小值在右侧 if (array[left] &lt; array[mid]) { left = mid; } //如果右半部分递增，则最小值在左侧。 //由于左边值比右边值大，所以两种情况不会同时发生 else if (array[right] &gt; array[mid]) { right = mid ; } } return array[min]; } 注意：但是当arr[left] = arr[right] = arr[min]时。三个数都相等无法确定最小值，此时只能遍历。递归斐波那契数列1递归做法 2记忆搜索，用数组存放使用过的元素。 3DP，本题中dp就是记忆化搜索 青蛙跳台阶一次跳两步或者跳一步，问一共多少种跳法到达n级，所以和斐波那契数列是一样的。 变态跳台阶一次跳1到n步，问一共几种跳法，这题是找数字规律的，一共有2^(n-1)种方法 矩形覆盖和上题一样，也是找规律，答案也是2^(n-1) 位运算二进制中1的个数 Created by 周杰伦 on 2018/6/29. 题目描述 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 解析: 1 循环右移数字n，每次判断最低位是否为1，但是可能会导致死循环。 2 使用数字a = 1和n相与，a每次左移一位，再与n相与得到次低位，最多循环32次，当数字1左移32次也会等于0，所以结束循环。 3 非常奇葩的做法，把一个整数减去1，再与原整数相与，会把最右边的一个1变成0，于是统计可以完成该操作的次数即可知道有多少1了。 public class 二进制中1的个数 { public static int NumberOf1(int n) { int count = 0; while (n != 0) { ++count; n = (n - 1) &amp; n; } return count; } } 数组中只出现一次的数字题目描述一个整型数组里除了一个数字之外，其他的数字都出现了两次。请写程序找出这一个只出现一次的数字。 解析：左神称之为神仙题。 利用位运算的异或操作^。由于a^a = 0,0^b=b，所以。所有数执行异或操作，结果就是只出现一次的数。 不用加减乘除做加法解析：不用加减乘，那么只能用二进制了。 两个数a和b，如果不考虑进位，则0 + 1 = 1,1 + 1 = 0，0 + 0 = 0，这就相当于异或操作。如果考虑进位，则只有1 + 1有进位，所以使用相与左移的方法得到每一位的进位值，再通过异或操作和原来的数相加。当没有进位值的时候，运算结束。 public static int Add(int num1,int num2) { if( num2 == 0 )return num1; if( num1 == 0 )return num2; int temp = num2; while(num2!=0) { temp = num1 ^num2; num2 = (num1 &amp; num2)&lt;&lt;1; num1 = temp; } return num1; }回溯和DFS矩阵中的路径题目描述请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 解析：回溯法也就是特殊的dfs，需要找到所有的路径，所以每当到达边界条件或抵达目标时，递归返回，由于需要保存路径中的字母，所以递归返回时需要删除路径最后的节点，来保证路径合法。不过本题只有一个解，所以找到即可返回。 public class 矩阵中的路径 { public static void main(String[] args) { char[][]arr = {{'a','b','c','e'},{'s','f','c','s'},{'a','d','e','e'}}; char []str = {&apos;b&apos;,&apos;c&apos;,&apos;c&apos;,&apos;e&apos;,&apos;d&apos;}; System.out.println(hasPath(arr, arr.length, arr[0].length, str)); } static int flag = 0; public static boolean hasPath(char[][] matrix, int rows, int cols, char[] str) { int [][]visit = new int[rows][cols]; StringBuilder sb = new StringBuilder(); for (int i = 0;i &lt; rows;i ++) { for (int j = 0;j &lt; cols;j ++) { if (matrix[i][j] == str[0]) { visit[i][j] = 1; sb.append(str[0]); dfs(matrix, i, j, visit, str, 1, sb); visit[i][j] = 0; sb.deleteCharAt(sb.length() - 1); } } } return flag == 1; } public static void dfs(char [][]matrix, int row, int col, int [][]visit, char []str, int cur, StringBuilder sb) { if (sb.length() == str.length) { // System.out.println(sb.toString()); flag = 1; return; } int [][]pos = {{1,0},{-1,0},{0,1},{0,-1}}; for (int i = 0;i &lt; pos.length;i ++) { int x = row + pos[i][0]; int y = col + pos[i][1]; if (x &gt;= matrix.length || x &lt; 0 || y &gt;= matrix[0].length || y &lt; 0) { continue; } if (visit[x][y] == 0 &amp;&amp; matrix[x][y] == str[cur]) { sb.append(matrix[x][y]); visit[x][y] = 1; dfs(matrix, x, y, visit, str, cur + 1, sb); sb.deleteCharAt(sb.length() - 1); visit[x][y] = 0; } } }​ 机器人的运动范围题目描述地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 解析：这是一个可达性问题，使用dfs方法，走到的每一格标记为走过，走到无路可走时就是最终的结果。每次都有四个方向可以选择，所以写四个递归即可。 public class Solution { static int count = 0; public static int movingCount(int threshold, int rows, int cols) { count = 0; int [][]visit = new int[rows][cols]; dfs(0, 0, visit, threshold); return count; } public static void dfs(int row, int col, int[][]visit, int k) { if (row &gt;= visit.length || row &lt; 0 || col &gt;= visit[0].length || col &lt; 0) { return; } if (sum(row) + sum(col) &gt; k) { return; } if (visit[row][col] == 1){ return; } visit[row][col] = 1; count ++; dfs(row + 1,col,visit, k); dfs(row - 1,col,visit, k); dfs(row,col + 1,visit, k); dfs(row,col - 1,visit, k); } public static int sum(int num) { String s = String.valueOf(num); int sum = 0; for (int i = 0;i &lt; s.length();i ++) { sum += Integer.valueOf(s.substring(i, i + 1)); } return sum; } }微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop生态学习总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FHadoop%E7%94%9F%E6%80%81%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我之前Hadoop生态基础系列文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：Hadoop生态学习 https://blog.csdn.net/a724888/article/category/7779280 Hadoop生态hdfs架构hdfs是一个分布式文件系统。底层的存储采用廉价的磁盘阵列RAID，由于可以并发读写所以效率很高。 基本架构是一个namenode和多个dataNode。node的意思是节点，一般指主机，也可以是虚拟机。 每个文件都会有两个副本存放在datanode中。 读写客户端写入文件时，先把请求发送到namenode，namenode会返回datanode的服务地址，接着客户端去访问datanode，进行文件写入，然后通知namenode，namenode接收到写入完成的消息以后，会另外选两个datanode存放冗余副本。 读取文件时，从namenode获取一个datanode的地址，然后自己去读取即可。 当一个文件的副本不足两份时，namenode自动会完成副本复制。并且，由于datanode一般会放在各个机架。namenode一般会把副本一个放在同一机架，一个放在其他机架，防止某个机架出问题导致整个文件读写不可用。 高可用namenode节点是单点，所以宕机了就没救了，所以我们可以使用zookeeper来保证namenode的高可用。可以使用zookeeper选主来实现故障切换，namenode先注册一个节点在zk上，表示自己是主，宕机时zk会通知备份节点进行切换。 Hadoop2.0中加入了hdfs namenode高可用的方案，也叫HDFS HA。namenode和一个备份节点绑定在一起，并且通过一个共享数据区域进行数据同步。同时支持故障切换。 MapReduce架构和流程MapReduce是基于Hadoop集群的分布式计算方案。一般先编写map函数进行数据分片，然后通过shuffle进行相同分片的整合，最后通过reduce把所有的数据结果进行整理。 具体来说，用户提交一个MapReduce程序给namenode节点，namenode节点启动一个jobtracker进行子任务的调度和监控，然后派发每个子任务tasktracker到datanode进行任务执行，由于数据分布在各个节点，每个tasktracker只需要执行自己的那一部分即可。最后再将结果汇总给tasktracker。 wordcount首先是一个文本文件:hi hello good hello hi hi。三个节点，则进行三次map。hi hello，good hello，hi hi分别由三个节点处理。结果分别是hi 1 hello 1,good 1 hello 1,hi 1,hi 1。shuffle时进行combine操作，得到hi 1,hello 1,good 1 hello 1,hi 2。最终reduce的结果是hi 3 hello 2 good 1. hivehive是一个基于hdfs文件系统的数据仓库。可以通过hive sql语句执行对hdfs上文件的数据查询。原理是hive把hdfs上的数据文件看成一张张数据表，把表结构信息存在关系数据库如mysql中。然后执行sql时通过对表结构的解析再去hdfs上查询真正的数据，最后也会以结构化的形式返回。 hbase简介hbase是基于列的数据库。 他与传统关系数据库有很大不同。 首先在表结构上，hbase使用rowkey行键作为唯一主键，通过行键唯一确定一行数据。 同时，hbase使用列族的概念，每个表都有固定的列族，每一行的数据的列族都一样，但是每一行所在列族的实际列都可以不一样。比如列族是info，列可以是name age，也可以是sex address等。也就是说具体列可以在插入数据时再进行确认。 并且，hbase的每一行数据还可以有多个版本，通过时间戳来表示不同的数据版本。 存储一般情况下hbase使用hdfs作为底层存储，所以hdfs提供了数据的可靠性以及并发读写的高效率。 hbase一个表的 每n行数据会存在一个region中，并且，对于列来说，每一个列族都会用一个region来存储，假设有m个列族，那么就会有n * m个region需要存储在hdfs上。 同时hbase使用regionserver来管理这些region，他们可能存在不同的datanode里，所以通过regionserver可以找出每一个region的位置。 hbase使用zookeeper来保证regionserver的高可用，会自动进行故障切换。 zkzk在Hadoop的作用有几个，通过选主等机制保证主节点高可用。 使用zk进行配置资源的统一管理，保证服务器节点无状态，所有服务信息直接从zk获取即可。 使用zookeeper进行节点间的通信等，也可以使用zk的目录顺序节点实现分布式锁，以及服务器选主。不仅在Hadoop中，zk在分布式系统中总能有用武之地。 zookeeper本身的部署方式就是一个集群,一个master和多个slave。 使用zab协议保证一致性和高可用。 zab协议实现原理： 1 使用两段式提交的方式确保一个协议需要半数以上节点同意以后再进行广播执行。 2 使用基于机器编号加时间戳的id来表示每个事务，通过这个方式当初始选举或者主节点宕机时进行一轮选主，每个节点优先选择自己当主节点，在选举过程中节点优先采纳比较新的事务，将自己的选票更新，然后反馈个其他机器，最后当一个机器获得超过半数选票时当选为master。 3选主结束以后，主节点与slave进行主从同步，保证数据一致性，然后对外提供服务，并且写入只能通过master而读取可以通过任意一台机器。 sqoop将hive表中的内容导入到MySQL数据库，也可以将MySQL中的数据导入hive中。 yarn没有yarn之前，hdfs使用jobtracker和tasktracker来执行和跟踪任务，jobtracker的任务太重，又要执行又要监控还要获取结果。并且不同机器的资源情况没有被考虑在内。 yarn是一个资源调度系统。提供applicationmaster对一个调度任务进行封装，然后有一个resourcemanager专门负责各节点资源的管理和监控。同时nodemanager则运行每个节点中用于监控节点状态和向rm汇报。还有一个container则是对节点资源的一个抽象，applicationmaster任务将由节点上的一个container进行执行。rm会将他调度到最合适的机器上。 kafka架构 kafka是一个分布式的消息队列。 它组成一般包括kafka broker，每个broker中有多个的partition作为存储消息的队列。 并且向上提供服务时抽象为一个topic，我们访问topic时实际上执行的是对partition的写入和读取操作。 读写和高可用 partition支持顺序写入，效率比较高，并且支持零拷贝机制，通过内存映射磁盘mmap的方式，写入partition的数据顺序写入到映射的磁盘中，比传统的IO要快。 由于partition可能会宕机，所以一般也要支持partition的备份，1个broker ，master通常会有多个broker slave，是主从关系，通过zookeeper进行选主和故障切换。 当数据写入队列时，一般也会通过日志文件的方式进行数据备份，会把broker中的partition被分在各个slave中以便于均匀分布和恢复。 生产者和消费者 生产者消费者需要访问kafka的队列时，如果是写入，直接向zk发送请求，一般是向一个topic写入消息，broker会自动分配partition进行写入。然后zk会告诉生产者写入的partition所在的broker地址，然后进行写入。 如果是读取的话，也是通过zk获取partition所在位置，然后通过给定的offset进行读取，读取完后更新offset。 由于kafka的partition支持顺序读写。所以保证一个partition中的读取和写入时是顺序的，但是如果是多个partition则不保证顺序。 正常情况下kafka使用topic来实现消息点对点发送，并且每个consumer都要在一个consumer group中，而且comsumer group中每次只能有一个消费者能接受对应topic的消息。因为为了实现订阅也就是一对多发送，我们让每个consumer在一个单独的group，于是每个consumer都可以接受到该消息。 flumeflume用于数据的收集和分发，flume可以监听端口的数据流入，监视文件的变动以及各种数据形式的数据流入，然后再把数据重新转发到其他需要数据的节点或存储中。 1、Flume的概念 flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的。 2、Event的概念在这里有必要先介绍一下flume中event的相关概念：flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 在整个数据的传输的过程中，流动的是event，即事务保证是在event级别进行的。那么什么是event呢？—–event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。event从source，流向channel，再到sink，本身为一个字节数组，并可携带headers(头信息)信息。event代表着一个数据的最小完整单元，从外部数据源来，向外部的目的地去。 flume使用 ambariambari就是一个Hadoop的Web应用。 sparkspark和MapReduce不同的地方就是，把计算过程放在内存中运行。 spark提出了抽象的RDD分布式内存模型，把每一步的计算操作转换成一个RDD结构，然后形成一个RDD连接而成的有向图。 比如data.map().filter().reduce();程序提交到master以后，会解析成多个RDD，并且形成一个有向图，然后spark再根据这些RD结构在内存中执行对应的操作。当然这个拓扑结构会被拆分为各个子任务分发到各个spark节点上，然后计算完以后再形成下一个rdd。最后汇总结果即可。 由于是在内存中对数据进行操作，省去了不必要的IO操作，，不需要像Mapreduce一样还得先去hdfs读取文件再完成计算。 storm在运行一个Storm任务之前，需要了解一些概念： Topologies Streams Spouts Bolts Stream groupings Reliability Tasks Workers Configuration Storm集群和Hadoop集群表面上看很类似。但是Hadoop上运行的是MapReduce jobs，而在Storm上运行的是拓扑（topology），这两者之间是非常不一样的。一个关键的区别是： 一个MapReduce job最终会结束， 而一个topology永远会运行（除非你手动kill掉）。 在Storm的集群里面有两种节点： 控制节点（master node）和工作节点（worker node）。控制节点上面运行一个叫Nimbus后台程序，它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分发代码，分配计算任务给机器， 并且监控状态。 每一个工作节点上面运行一个叫做Supervisor的节点。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭工作进程。每一个工作进程执行一个topology的一个子集；一个运行的topology由运行在很多机器上的很多工作进程组成。 Nimbus和Supervisor之间的所有协调工作都是通过Zookeeper集群完成。另外，Nimbus进程和Supervisor进程都是快速失败（fail-fast)和无状态的。所有的状态要么在zookeeper里面， 要么在本地磁盘上。这也就意味着你可以用kill -9来杀死Nimbus和Supervisor进程， 然后再重启它们，就好像什么都没有发生过。这个设计使得Storm异常的稳定。 storm比起spark它的实时性能更高更强，storm可以做到亚秒级别的数据输入分析。而spark的方式是通过秒级的数据切分，来形成spark rdd数据集，然后再按照DAG有向图进行执行的。 storm则不然。 一：介绍Storm设计模型 1.Topology Storm对任务的抽象，其实 就是将实时数据分析任务 分解为 不同的阶段 点： 计算组件 Spout Bolt 边： 数据流向 数据从上一个组件流向下一个组件 带方向 2.tuple Storm每条记录 封装成一个tuple 其实就是一些keyvalue对按顺序排列 方便组件获取数据 3.Spout 数据采集器 源源不断的日志记录 如何被topology接收进行处理？ Spout负责从数据源上获取数据，简单处理 封装成tuple向后面的bolt发射 4.Bolt 数据处理器 二：开发wordcount案例 1.书写整个大纲的点线图 topology就是一个拓扑图，类似于spark中的dag有向图，只不过storm执行的流式的数据，比dag执行更加具有实时性。 topology包含了spout和bolt。spout负责获取数据，并且将数据发送给bolt，这个过程就是把任务派发到多个节点，bolt则负责对数据进行处理，比如splitbolt负责把每个单词提取出来，countbolt负责单词数量的统计，最后的printbolt将每个结果集tuple打印出来。 这就形成了一个完整的流程。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式技术实践总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文基于之前的分布式系统理论系列文章总结而成，本部分主要是实践内容，详细内容可见我的专栏：分布式系统理论与实践 https://blog.csdn.net/column/details/24090.html 本文主要是按照我自己的理解以及参考之前文章综合而成的，其中可能会有一些错误，还请见谅，也请指出。 分布式技术分布式数据和nosql分布式一般是指分布式部署的数据库。 比如Hbase基于HDFS分布式部署，所以他是一个分布式数据库。 当然MySQL也可以分布式部署，比如按照不同业务部署，或者把单表内容拆成多个表乃至多个库进行部署。 一般MySQL的扩展方式有： 1 主从复制 使用冗余保证可用 2 读写分离 主库负责写从库负责读，分担压力，并且保证数据一致性和备份。 3 分表分库，横向拆分数据表放到多个表中或者多个库中，一般多个表或者多个库会使用不同节点部署，也就是一种分布式方案，提高并发的读写量。 Nosql的话就比较多了，redis，memcache等。当然hbase也是，hbase按照region将数据文件分布在hdfs上，并且hdfs提供高可用和备份，同时hbase的regionserver也保证高可用，于是hbase的分布式方案也是比较成熟的。 缓存 分布式缓存一般作为缓存的软件有redis，memcache等。当然我本地写一个hashmap也可以作为缓存。 memcache提出了一致性哈希的算法，但是本身不支持数据持久化，也没有提供分布式方案，需要自己完成持久化以及分布式部署并且保证其可用性。 redis作为新兴的内存数据库，提供了比memcache更多的数据结构，以及各种分布式方案。当然它也支持持久化。 redis的部署方案： 1 redis的主从复制结构，和MySQL类似，使用日志aof或者持久化文件rdb进行主从同步。 2 读写分离，也可以做，但一般不需要。因为redis够快。 3 redis的哨兵方案，主节点配置哨兵，每当宕机时自动完成主从切换。 4 redis的集群方案，p2p的Redis Cluster部署了多台Redis服务器，每台Redis拥有全局的分片信息，所以任意节点都可以对外提供服务，当然每个节点只保存一部分分片，所以某台机器宕机时不会影响整个集群，当然每个节点也有slave，哨兵自动进行故障切换。 5 codis方案，codis屏蔽了集群的内部实现，可以不更改redis api的情况下使用代理的方式提供集群访问。并且使用 group的概念封装一组节点。 缓存需要解决的问题：命中：缓存有数据 不命中：去数据库读取 失效：过期 替换：缓存淘汰算法。 一般有lru，fifo，随机缓存等。缓存更新的方法缓存更新可以先更新数据库再更新缓存，也可以先更新缓存再更新数据库。 一般推荐先更新数据库，否则写一条数据时刚好有人读到缓存，把旧数据读到缓存中，此时新数据在数据库确不在缓存中。 还有一种方法，就是让缓存自己去完成数据库更新，而不是让应用去选择如何更新数据库，这样的话缓存和数据库的更新操作就是透明的了，我们只需要操作缓存即可。 缓存在springboot中的使用springboot支持将缓存的curd操作配置在注解中，只需要在对应方法上配置好键和更新策略。 则redis会根据该方法的操作类型执行对应操作，非常方便。一致性哈希分布式部署时，经常要面对的问题是，一个服务集群由谁来提供给这个客户度服务，需要一种算法来完成这一步映射。 如果直接使用hash显然分布非常不均匀。那如果使用余数法呢，一共有N台机器，我对N取余可以映射到任意一台机器上。 这种方法的缺点在于，当取余的值集中在某一范围时，就容易集中访问某些机器，导致热点问题。 于是memcache推出了一个叫做一致性哈希的算法，一个哈希环，环上支持2^32次方个节点，也就是包含了所有的ip。 然后我们把主机通过hash值分布到这个环上，请求到来时会映射到某一个节点，如果该节点没有主机，则顺时针寻找真正主机。 当节点加入或者节点删除时，并不会影响服务的可用性，只是某些请求会被映射到别的节点。 但是当请求集中到某个区域时，会产生倾斜，我们引入了虚拟节点来改善这个问题，虚拟节点对应到真实节点，所以加入虚拟节点可以更好地转移请求。 session和分布式sessionsession是web应用必备的一个结构。一般有几种方案来管理session。 1 web应用保存session到内存中，但是宕机会丢失 2 web应用持久化到数据库或者redis，增加数据库负担。 3 使用cookie保存加密后的session，浏览器压力大，可能被破解 4 使用单独的session服务集群提供session服务，并且本身也可以采用分布式部署，部署的时候可以主从。 保证session一致性的解决方法（客户端可以访问到自己的session）： 1 客户端cookie保存 2 多个webserver进行同步，效率低 3 反向代理绑定ip映射同一个服务器，但是宕机时出错 4 后端统一存储，比如redis，或则部署session服务。 负载均衡负载均衡一般可以分为七层，四层负载均衡。 Nginx 七层的负载均衡也就是http负载均衡，主要使用Nginx完成。 配置Nginx进行反向代理的url，然后转发请求到上游服务器，请求进来时自动转发到上游服务器，通过url进行负载均衡，所以是七层负载均衡。既然是七层负载，那么上游服务器提供了http服务，也可以解析该请求。 四层负载均衡主要是tcp请求的负载均衡，因为tcp请求是绑定到一个端口上的，所以我们根据端口进行请求转发到上游服务器的。既然是四层负载，上游服务器监听该端口的服务就可以处理该请求。 LVS LVS术语定义： DS：Director Server，前端负载均衡器节点（后文用Director称呼）； RS：Real Server，后端真实服务器； VIP：用户请求的目标的IP地址，一般是公网IP地址； DIP：Director Server IP，Director和Real Server通讯的内网IP地址； RIP：Real Server IP，Director和Real Server通讯的内网IP地址；LVS有三种实现负载均衡的方式 NAT 四层负载均衡 NAT支持四层负载均衡，NAT中只有DS提供公网ip，并且VIP绑定在DS的mac地址上，客户端只能访问DS。同时DS和RS通过内网ip进行网络连接。当TCP数据报到达DS时，DS修改数据报，指向RS的ip和port。进行转发即可。 同时，RS处理完请求后，由于网关时DS，所以仍然要返回给DS处理。 NAT模式中，RS返回数据包是返回给Director，Director再返回给客户端；事实上这跟NAT网络协议没什么关系。 DR 二层负载均衡 DR模式中，DS负责接收请求。接收请求后把数据报的mac地址改成指向RS的mac地址，并且由于三台机器拥有同样的vip地址。所以RS接收请求后认为该数据报应该由自己处理并相应。 同时为了避免RS再把相应转发会DS，我们禁用了对DS的arp，所以此时RS就会通过vip把响应通过vip网关返回给客户端。 Director通过修改请求中目标地址MAC为选定的RS实现数据转发，这就要求Diretor和Real Server必须在同一个广播域内，也就是他们的mac地址是可达的。DR（Direct Routing）模式中，RS返回数据是直接返回给客户端（通过额外的路由）； TUN TUN中使用了IP隧道技术，客户端请求发给DS时，DS会通过隧道技术把数据报通过隧道发给实际的RS，然后RS解析数据以后可以直接响应给客户端，因为他有客户端的ip地址。这就不要求DS和RS在同一网段了，当然前提是RS有公网ip。 TUN（IP Tunneling）模式中，RS返回的数据也是直接返回给客户端，这种模式通过Overlay协议（把一个IP数据包封装到另一个数据包内部叫Overlay）避免了DR的限制。 zookeeperzookeeper集群自身的特性： 1 一个zookeeper服务器集群，一开始就会进行选主，主节点挂掉后也会进行选主。 使用zab协议中的选主机制进行选主，也就是每个节点进行一次提议，刚开始提议自己，如果有新的提议则覆盖自己原来的提议，不断重复，直到有节点获得过半的投票。完成一轮选主。 2 选主结束后，开始进行消息广播和数据同步，保证每一台服务器的数据都和leader同步。 3 开始提供服务，客户端向leader发送请求，leader首先发出提议，当有半数以上节点响应时，leader会发送commit信息，于是所有节点执行该操作。当有机器宕机时重启后会和leader同步。这是一个类似2pc的提交方式。 zookeeper提供了分布式环境中常用的服务 1 配置服务，多个机器可以通过文件节点共享配置。 2 选主服务，通过添加顺序节点，可以进行选主。 3 分布式锁，顺序节点和watcher 4 全局id，使用机器号+时间戳可以生成一个transactionid，是全局唯一的。 数据库的分布式事务分布式事务的实现一般可以用2PC和3PC解决。 成熟的方案有： 1 TCC 补偿式事务，对每一个步骤都有一个补偿措施。 2 全局事务实现。 3 事务消息：rocketmq的事务实现，先发消息到队列中，然后本地执行事务并通知消息队列，若成功则消息主动推给另一个服务，直到服务二执行成功，消息从队列中删除。如果超时不成功，则消息要求事务A回滚。 如果过程中失败了，本地事务也会回滚。消息队列可以回调本地接口判断事务是否执行成功，防止超时。 4 本地实现消息表：本地实现消息表并且和事务记录存在一起，自己实现消息的轮询发送。首先把本地事务操作和消息增加放在一个事务里执行，然后轮询消息表进行发送，如果执行成功则消息达到服务B，通知其执行。执行成功后消息被删除，否则回滚事务删除消息。 分布式锁问题分布式锁用于分布式环境中的资源互斥，因为单机可以通过共享内存实现，而分布式环境只能通过网络实现。 MySQL实现分布式锁insert加锁，锁没有失效时间，容易产生死锁redis实现分布式锁1. 基于setnx、expire两个命令来实现 基于setnx（set if not exist）的特点，当缓存里key不存在时，才会去set，否则直接返回false。 如果返回true则获取到锁，否则获取锁失败，为了防止死锁，我们再用expire命令对这个key设置一个超时时间来避免。 但是这里看似完美，实则有缺陷，当我们setnx成功后，线程发生异常中断，expire还没来的及设置，那么就会产生死锁。 2 使用getset实现，可以判断自己是否获得了锁，但是可能会出现并发的原子性问题。拆分成两个操作。 3 避免原子性问题可以使用lua脚本保证事务的原子性。 4 上述都是单点的redis，如果是分布式环境的redis集群，可以使用redlock，要求节点向半数以上redis机器请求锁。才算成功。zookeeper实现分布式锁创建有序节点，最小的抢到锁，其他的监听他的上一个节点即可。并且抢到锁的节点释放时只会通知下一个节点。 小结 在分布式系统中，共享资源互斥访问问题非常普遍，而针对访问共享资源的互斥问题，常用的解决方案就是使用分布式锁，这里只介绍了几种常用的分布式锁，分布式锁的实现方式还有有很多种，根据业务选择合适的分布式锁，下面对上述几种锁进行一下比较： 数据库锁： 优点：直接使用数据库，使用简单。 缺点：分布式系统大多数瓶颈都在数据库，使用数据库锁会增加数据库负担。 缓存锁： 优点：性能高，实现起来较为方便，在允许偶发的锁失效情况，不影响系统正常使用，建议采用缓存锁。 缺点：通过锁超时机制不是十分可靠，当线程获得锁后，处理时间过长导致锁超时，就失效了锁的作用。 zookeeper锁： 优点：不依靠超时时间释放锁；可靠性高；系统要求高可靠性时，建议采用zookeeper锁。 缺点：性能比不上缓存锁，因为要频繁的创建节点删除节点。并且zookeeper只能单点写入。而Redis可以并发写入。消息队列适合场景： 1 服务之间解耦，比如淘宝的买家服务和物流服务，中间需要消息传递订单信息。但又不需要强耦合。便于服务的划分和独立部署 2 控制流量，大流量访问某服务时，避免服务出现问题，将其先存入队列，均匀释放流量。 3 削峰，当某一个服务如秒杀，如果直接集中访问，服务器可能会冲垮，所以先存到队列中，控制访问量，避免服务器冲击。 4 事务，消息事务 5 异步请求处理，比如一些不重要的服务可以延缓执行，比如卖家评价，站内信等。 常用消息队列： rabbitmq：使用consumer和producer的模型，并且使用了broker，broker中包含路由功能的exchanger，每个key绑定一个queue，应用通过key进行队列消费和生产。 一般是点对点的消息，也可以支持一对多的消息，当然也可以支持消息的订阅。还有就是主题模式，和key的区别就是主题模式是多级的key表示。 kafka： 微服务和Dubbo分布式架构意味着服务的拆分，最早的SOA架构已经进行了服务拆分，但是每个服务还是太过庞大，不适合扩展和修改。 微服务的拆分粒度更加细，服务可以独立部署和快速迭代，通知支持扩展。 服务之间一般使用rpc调用进行访问，可以使用自定义协议也可以使用http服务，当然通过netty 实现TCP服务并且搭配合理的序列化方案也可以完成rpc功能。rpc是微服务的基础。 微服务一般需要配置中心来进行服务注册和发现，以便服务信息更新和配置，dubbo中使用的是zookeeper，用于配置服务信息提供给生产者使用。 一般情况下微服务需要有监控中心，心跳检测每一台服务器，及时完成故障切换和通知。同时监控服务的性能和使用情况。 序列化方式一般可以使用protobuf，http服务一般使用json。 微服务还支持更多的包括权限控制，流量控制，灰度发布，服务降级等内容，这里就不再细谈。 全局id方法一：使用数据库的 auto_increment 来生成全局唯一递增ID 优点： 简单，使用数据库已有的功能 能够保证唯一性 能够保证递增性 步长固定​​ 缺点： 可用性难以保证：数据库常见架构是一主多从+读写分离，生成自增ID是写请求，主库挂了就玩不转了 扩展性差，性能有上限：因为写入是单点，数据库主库的写性能决定ID的生成性能上限，并且难以扩展方法三：uuid/guid 不管是通过数据库，还是通过服务来生成ID，业务方Application都需要进行一次远程调用，比较耗时。 有没有一种本地生成ID的方法，即高性能，又时延低呢？ uuid是一种常见的方案： string ID =GenUUID(); 优点： 本地生成ID，不需要进行远程调用，时延低 扩展性好，基本可以认为没有性能上限 缺点： 无法保证趋势递增 uuid过长，往往用字符串表示，作为主键建立索引查询效率低，常见优化方案为“转化为两个uint64整数存储”或者“折半存储”（折半后不能保证唯一性） 方法四：取当前毫秒数 uuid是一个本地算法，生成性能高，但无法保证趋势递增，且作为字符串ID检索效率低，有没有一种能保证递增的本地算法呢？ 取当前毫秒数是一种常见方案： uint64 ID = GenTimeMS(); 优点： 本地生成ID，不需要进行远程调用，时延低 生成的ID趋势递增 生成的ID是整数，建立索引后查询效率高 缺点： 如果并发量超过1000，会生成重复的ID 方法五：类snowflake算法 snowflake是twitter开源的分布式ID生成算法，其核心思想为，一个long型的ID： 41bit作为毫秒数 10bit作为机器编号 12bit作为毫秒内序列号 算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID，完全能满足业务的需求。 秒杀系统 第一层，客户端怎么优化（浏览器层，APP层）（a）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求； （b）JS层面，限制用户在x秒之内只能提交一次请求； 第二层，站点层面的请求拦截 怎么拦截？怎么防止程序员写for循环调用，有去重依据么？ip？cookie-id？…想复杂了，这类业务都需要登录，用uid即可。在站点层面，对uid进行请求计数和去重，甚至不需要统一存储计数，直接站点层内存存储（这样计数会不准，但最简单）。一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。 5s只透过一个请求，其余的请求怎么办？缓存，页面缓存，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面。同一个item的查询，例如车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面。如此限流，既能保证用户有良好的用户体验（没有返回404）又能保证系统的健壮性（利用页面缓存，把请求拦截在站点层了）。 好，这个方式拦住了写for循环发http请求的程序员，有些高端程序员（黑客）控制了10w个肉鸡，手里有10w个uid，同时发请求（先不考虑实名制的问题，小米抢手机不需要实名制），这下怎么办，站点层按照uid限流拦不住了。 第三层 服务层来拦截（反正就是不要让请求落到数据库上去）消息队列+缓存服务层怎么拦截？大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？没错，请求队列！ 对于写请求，做请求队列，每次只透有限的写请求去数据层（下订单，支付这样的写业务） 1w部手机，只透1w个下单请求去db 3k张火车票，只透3k个下单请求去db 如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”。 对于读请求，怎么优化？cache抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的。如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。 好了，最后是数据库层浏览器拦截了80%，站点层拦截了99.9%并做了页面缓存，服务层又做了写请求队列与数据缓存，每次透到数据库层的请求都是可控的。db基本就没什么压力了，闲庭信步，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，透这么多请求来数据库没有意义。 全部透到数据库，100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。 总结上文应该描述的非常清楚了，没什么总结了，对于秒杀系统，再次重复下我个人经验的两个架构优化思路： （1）尽量将请求拦截在系统上游（越上游越好）； （2）读多写少的常用多使用缓存（缓存抗读压力）； 浏览器和APP：做限速 站点层：按照uid做限速，做页面缓存 服务层：按照业务做写请求队列控制流量，做数据缓存 数据层：闲庭信步 并且：结合业务做优化 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式理论学习总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%90%86%E8%AE%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文基于之前的分布式系统理论系列文章总结而成,主要是理论部分，详细内容可见我的专栏：分布式系统理论与实践 https://blog.csdn.net/column/details/24090.html 本文主要是按照我自己的理解以及参考之前文章综合而成的，其中可能会有一些错误，还请见谅，也请指出。 分布式理论CAPCAP定理讲的是三个性。consistency数据一致性，availability可用性，partition tolerance分区容错性。 三者只能选其中两者。为什么呢，看看这三个性质意味着什么吧。 首先看看分区容错性，分区容错性指的是网络出现分区（丢包，断网，超时等情况都属于网络分区）时，整个服务仍然可用。 由于网络分区在实际环境下一定存在，所以必须首先被考虑。于是分区容错性是必须要保证的，否则一旦出现分区服务就不可用，那就没办法弄了。 所以实际上是2选1的问题。在可用性和一致性中做出选择。 在一个分布式环境下，多个节点一起对外提供服务，如果要保证可用性，那么一台机器宕机了仍然有其他机器能提供服务。但是宕机的机器重启以后就会发现数据和其他机器存在不一致，那么一致性就无法得到保证。 如果保证一致性，如果有机器宕机，那么其他节点就不能工作了，否则一定会产生数据不一致。 BASE在这么严苛的规定下，CAP一般很难实现一个健壮的系统。于是提出了BASE来削弱这些要求。 BASE是基本可用basically available，soft state软状态，eventually consistent最终一致性。 基本可用就是允许服务在某些时候降级，比如淘宝在高峰时期会关闭退货等服务。 软状态就是允许数据出现中间状态，比如支付宝提交转账以后并不是立刻到账，中间经过了多次消息传递和转发。 最终一致性就是指数据最终要是一致的，比如多个节点的数据需要定期同步，支付宝转账最终一定会到账。 分布式系统关键词时钟，时间，事件顺序分布式系统的一个问题在与缺少全局时钟，所以大家没有一个统一的时间，就很难用时间去确定各个节点事件的发生顺序，为了保证事件的顺序执行， Lamport timestampsLeslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)[3]。 分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport时间戳原理如下： 每个事件对应一个Lamport时间戳，初始值为0 如果事件在节点内发生，时间戳加1 如果事件属于发送事件，时间戳加1并在消息中带上该时间戳 如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1这样的话，节点内的事件有序，发送事件有序，接收事件一定在发送事件以后发生。再加上人为的一些规定，因此根据时间戳可以确定一个全序排列。 Vector clockLamport时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用Lamport时间戳很好地表示出来，那就是同时发生关系(concurrent)[4]。Vector clock是在Lamport时间戳基础上演进的另一种逻辑时钟方法，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳[5][6]。 如果 Tb[Q] &gt; Ta[Q] 并且 Tb[P] &lt; Ta[P]，则认为a、b同时发生，记作 a &lt;-&gt; b。例如图2中节点B上的第4个事件 (A:2，B:4，C:1) 与节点C上的第2个事件 (B:3，C:2) 没有因果关系、属于同时发生事件。 因为B4 &gt; B3并且 C1&lt;C2，说明两者之间没有顺序关系，否则不会出现一大一小，因此他们是同时发生的。 Version vector基于Vector clock我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突(detect conflict)。 分布式系统中数据一般存在多个副本(replication)，多个副本可能被同时更新，这会引起副本间数据不一致[7]，Version vector的实现与Vector clock非常类似[8]，目的用于发现数据冲突[9]。 当两个写入数据事件同时发生则发生了冲突，于是通过某些方法解决数据冲突。 Vector clock只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生[11]。 选主，租约，多数派 选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。 多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。 租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。 选举(electioin) 一致性问题(consistency)是独立的节点间如何达成决议的问题，选出大家都认可的leader本质上也是一致性问题，因而如何应对宕机恢复、网络分化等在leader选举中也需要考量。 在一致性算法Paxos、ZAB[2]、Raft[3]中，为提升决议效率均有节点充当leader的角色。 ZAB、Raft中描述了具体的leader选举实现，与Bully算法类似ZAB中使用zxid标识节点，具有最大zxid的节点表示其所具备的事务(transaction)最新、被选为leader。 多数派(quorum) 在网络分化的场景下以上Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader，这时候就需要引入多数派(quorum)[4]。多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一。 租约(lease) 选举中很重要的一个问题，以上尚未提到：怎么判断leader不可用、什么时候应该发起重新选举？ 最先可能想到会通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主。 租约(lease)是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题[6]，后面在分布式锁[7]等很多方面都有应用。 (a). 节点0、1、2在Z上注册自己，Z根据一定的规则(例如先到先得)颁发租约给节点，该租约同时对应一个有效时长；这里假设节点0获得租约、成为leader (b). leader宕机时，只有租约到期(timeout)后才重新发起选举，这里节点1获得租约、成为leader租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题。在实践应用中，zookeeper、ectd可用于租约颁发。 一致性，2pc和3pc一致性(consensus) 何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。 为了保证执行的一致性，可以使用2pc两段式提交和3pc三段式提交。 2PC 2PC(tow phase commit)两阶段提交[5]顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)： 举个例子，首先用户想要执行一个事务，于是提交给leader，leader先让各个节点执行该事务。 我们要知道，事务是通过日志来实现的。各个节点使用redo日志进行重做，使用undo日志进行回滚。 于是各个节点执行事务，并把执行结果是否成功返回给leader，当leader收到全部确认消息后，发送消息让所有节点commit。如果有节点执行失败，则leader要求所有节点回滚。 2pc可能出现的一些问题是： 1 leader必须等待所有节点结果，如果有节点宕机或超时，则拒绝该事务，并向节点发送回滚的信息。 2 如果leader宕机，则一般配置watcherdog自动切换成备用leader，然后进行下一次的请求提交。 3这两种情况单独发生时都没有关系，有对应的措施可以进行回滚，但是如果当一个节点宕机时leader正在等待所有节点消息，其他节点也在等待leader最后的消息。 此时leader也不幸宕机，切换之后leader并不知道一个节点宕机了，这样的话其他的节点也会被阻塞住导致无法回滚。 3PCcoordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令 。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。 participant如果在不同阶段宕机，我们来看看3PC如何应对： 阶段1: coordinator或watchdog未收到宕机participant的vote，直接中止事务；宕机的participant恢复后，读取logging发现未发出赞成vote，自行中止该次事务 阶段2: coordinator未收到宕机participant的precommit ACK，但因为之前已经收到了宕机participant的赞成反馈(不然也不会进入到阶段2)，coordinator进行commit；watchdog可以通过问询其他participant获得这些信息，过程同理；宕机的participant恢复后发现收到precommit或已经发出赞成vote，则自行commit该次事务 阶段3: 即便coordinator或watchdog未收到宕机participant的commit ACK，也结束该次事务；宕机的participant恢复后发现收到commit或者precommit，也将自行commit该次事务因为有了准备提交(prepare tocommit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。 总结一下就是：阶段一leader要求节点准备，节点返回ack或者fail。 如果节点都是ack，leader返回ack进入阶段二。（如果fail则回滚，因为节点没有接收到ack，所以最终都会回滚） 阶段二时节点执行事务并且发送结果给leader，leader返回ack或者fail。由于阶段二的节点已经有了一个确定的状态ack，如果leader超时或宕机不返回，成功执行节点也会进行commit操作，这样即使有节点宕机也不会影响到其他节点。 一致性算法paxosBasic Paxos 何为一致性问题？简单而言，一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)。 和2PC类似，Paxos先把节点分成两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。假如只有一个proposer发起提议，并且节点不宕机、消息不丢包，那么acceptor做到以下这点就可以确定一个值。 proposer发出提议，acceptor根据提议的id和值来决定是否接收提议，接受提议则替换为自己的提议，并且返回之前id最大的提议，当超过一半节点提议该值时，则该值被确定，这样既保证了时序，也保证了多数派。 Multi Paxos 通过以上步骤分布式系统已经能确定一个值，“只确定一个值有什么用？这可解决不了我面临的问题。” 你心中可能有这样的疑问。 其实不断地进行“确定一个值”的过程、再为每个过程编上序号，就能得到具有全序关系(total order)的系列值，进而能应用在数据库副本存储等很多场景。我们把单次“确定一个值”的过程称为实例(instance)，它由proposer/acceptor/learner组成。 Fast Paxos 在Multi Paxos中，proposer -&gt; leader -&gt; acceptor -&gt; learner，从提议到完成决议共经过3次通信，能不能减少通信步骤？ 对Multi Paxos phase2a，如果可以自由提议value，则可以让proposer直接发起提议、leader退出通信过程，变为proposer -&gt; acceptor -&gt; learner，这就是Fast Paxos[2]的由来。 多次paxos的确定值使用可以让多个proposer，acceptor一起运作。多个proposer提出提议，acceptor保留最大提议比返回之前提议，proposer当提议数量满足多数派则取出最大值向acceptor提议，于是过半数的acceptor比较提议后可以接受该提议，于是最终leader将提议写入acceptor，而acceptor再写入对应的learner。 raft和zabZab Zab[5][6]的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency，或叫线性一致性linearizable consistency)。 和Raft一样，Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段： discovery: 选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch(每次选举产生新Leader的同时产生新epoch，类似Raft的term) sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader) broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit) Raft： 单个 Candidate 的竞选有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。 下图表示一个分布式系统的最初阶段，此时只有 Follower，没有 Leader。Follower A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。 此时 A 发送投票请求给其它所有节点。 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。 多个 Candidate 竞选 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票，例如下图中 Candidate B 和 Candidate D 都获得两票，因此需要重新开始投票。 当重新开始投票时，由于每个节点设置的随机竞选超时时间不同，因此能下一次再次出现多个 Candidate 并获得同样票数的概率很低。 日志复制 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。 Leader 会把修改复制到所有 Follower。 Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。 zookeeperzookeeper在分布式系统中作为协调员的角色，可应用于Leader选举、分布式锁、配置管理等服务的实现。以下我们从zookeeper供的API、应用场景和监控三方面学习和了解zookeeper（以下简称ZK）。 ZK API ZK以Unix文件系统树结构的形式管理存储的数据，图示如下： 其中每个树节点被称为znode，每个znode类似一个文件，包含文件元信息(meta data)和数据。 以下我们用server表示ZK服务的提供方，client表示ZK服务的使用方，当client连接ZK时，相应创建session会话信息。 有两种类型的znode： Regular: 该类型znode只能由client端显式创建或删除 Ephemeral: client端可创建或删除该类型znode；当session终止时，ZK亦会删除该类型znode znode创建时还可以被打上sequential标志，被打上该标志的znode，将自行加上自增的数字后缀 ZK提供了以下API，供client操作znode和znode中存储的数据： create(path, data, flags)：创建路径为path的znode，在其中存储data[]数据，flags可设置为Regular或Ephemeral，并可选打上sequential标志。 delete(path, version)：删除相应path/version的znode exists(path,watch)：如果存在path对应znode，则返回true；否则返回false，watch标志可设置监听事件 getData(path, watch)：返回对应znode的数据和元信息（如version等） setData(path, data, version)：将data[]数据写入对应path/version的znode getChildren(path, watch)：返回指定znode的子节点集合K应用场景 基于以上ZK提供的znode和znode数据的操作，可轻松实现Leader选举、分布式锁、配置管理等服务。 Leader选举 利用打上sequential标志的Ephemeral，我们可以实现Leader选举。假设需要从三个client中选取Leader，实现过程如下： 1、各自创建Ephemeral类型的znode，并打上sequential标志： [zk: localhost:2181(CONNECTED) 4] ls /master[lock-0000000241, lock-0000000243, lock-0000000242] 2、检查 /master 路径下的所有znode，如果自己创建的znode序号最小，则认为自己是Leader；否则记录序号比自己次小的znode 3、非Leader在次小序号znode上设置监听事件，并重复执行以上步骤2 配置管理 znode可以存储数据，基于这一点，我们可以用ZK实现分布式系统的配置管理，假设有服务A，A扩容设备时需要将相应新增的ip/port同步到全网服务器的A.conf配置，实现过程如下： 1、A扩容时，相应在ZK上新增znode，该znode数据形式如下： [zk: localhost:2181(CONNECTED) 30] get /A/blk-0000340369{“svr_info”: [{“ip”: “1.1.1.1.”, “port”: “11000”}]}cZxid = 0x2ffdeda3be…… 2、全网机器监听 /A，当该znode下有新节点加入时，调用相应处理函数，将服务A的新增ip/port加入A.conf 3、完成步骤2后，继续设置对 /A监听 ZK监控 ZK自身提供了一些“四字命令”，通过这些四字命令，我们可以获得ZK集群中，某台ZK的角色、znode数、健康状态等信息： 小结 zookeeper以目录树的形式管理数据，提供znode监听、数据设置等接口，基于这些接口，我们可以实现Leader选举、配置管理、命名服务等功能。结合四字命令，加上模拟zookeeper client 创建/删除znode，我们可以实现对zookeeper的有效监控。在各种分布式系统中，我们经常可以看到zookeeper的身影。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis原理与实践总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FRedis%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文主要对Redis的设计和实现原理做了一个介绍很总结，有些东西我也介绍的不是很详细准确，尽量在自己的理解范围内把一些知识点和关键性技术做一个描述。如有错误，还望见谅，欢迎指出。这篇文章主要还是参考我之前的技术专栏总结而来的。欢迎查看： 重新学习Redis https://blog.csdn.net/column/details/21877.html 使用和基础数据结构（外观）redis的基本使用方式是建立在redis提供的数据结构上的。 字符串REDIS_STRING (字符串)是 Redis 使用得最为广泛的数据类型,它除了是 SET 、GET 等命令 的操作对象之外,数据库中的所有键,以及执行命令时提供给 Redis 的参数,都是用这种类型 保存的。 字符串类型分别使用 REDIS_ENCODING_INT 和 REDIS_ENCODING_RAW 两种编码 只有能表示为 long 类型的值,才会以整数的形式保存,其他类型 的整数、小数和字符串,都是用 sdshdr 结构来保存 哈希表REDIS_HASH (哈希表)是HSET 、HLEN 等命令的操作对象 它使用 REDIS_ENCODING_ZIPLIST和REDIS_ENCODING_HT 两种编码方式 Redis 中每个hash可以存储232-1键值对（40多亿） 列表REDIS_LIST(列表)是LPUSH 、LRANGE等命令的操作对象 它使用 REDIS_ENCODING_ZIPLIST和REDIS_ENCODING_LINKEDLIST 这两种方式编码 一个列表最多可以包含232-1 个元素(4294967295, 每个列表超过40亿个元素)。 集合REDIS_SET (集合) 是 SADD 、 SRANDMEMBER 等命令的操作对象 它使用 REDIS_ENCODING_INTSET 和 REDIS_ENCODING_HT 两种方式编码 Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员) 有序集REDIS_ZSET (有序集)是ZADD 、ZCOUNT 等命令的操作对象 它使用 REDIS_ENCODING_ZIPLIST和REDIS_ENCODING_SKIPLIST 两种方式编码 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的,但分数(score)却可以重复。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1 (4294967295, 每个集合可存储40多亿个成员) 下图说明了，外部数据结构和底层实际数据结构是通过realobject来连接的。一个外观类型里面必然存着一个realobject，通过它来访问底层数据结构。 底层数据结构下面讨论redis底层数据结构 1 SDS动态字符串 sds字符串是字符串的实现 动态字符串是一个结构体，内部有一个buf数组，以及字符串长度，剩余长度等字段，优点是通过长度限制写入，避免缓冲区溢出，另外剩余长度不足时会自动扩容，扩展性较好，不需要频繁分配内存。 并且sds支持写入二进制数据，而不一定是字符。 2 dict字典 dict字典是哈希表的实现。 dict字典与Java中的哈希表实现简直如出一辙，首先都是数组+链表组成的结构，通过dictentry保存节点。 其中dict同时保存两个entry数组，当需要扩容时，把节点转移到第二个数组即可，平时只使用一个数组。 3 压缩链表ziplist 3.1 ziplist是一个经过特殊编码的双向链表，它的设计目标就是为了提高存储效率。ziplist可以用于存储字符串或整数，其中整数是按真正的二进制表示进行编码的，而不是编码成字符串序列。它能以O(1)的时间复杂度在表的两端提供push和pop操作。 3.2 实际上，ziplist充分体现了Redis对于存储效率的追求。一个普通的双向链表，链表中每一项都占用独立的一块内存，各项之间用地址指针（或引用）连接起来。这种方式会带来大量的内存碎片，而且地址指针也会占用额外的内存。 3.3 而ziplist却是将表中每一项存放在前后连续的地址空间内，一个ziplist整体占用一大块内存。它是一个表（list），但其实不是一个链表（linked list）。 3.4 另外，ziplist为了在细节上节省内存，对于值的存储采用了变长的编码方式，大概意思是说，对于大的整数，就多用一些字节来存储，而对于小的整数，就少用一些字节来存储。 实际上。redis的字典一开始的数据比较少时，会使用ziplist的方式来存储，也就是key1，value1，key2，value2这样的顺序存储，对于小数据量来说，这样存储既省空间，查询的效率也不低。 当数据量超过阈值时，哈希表自动膨胀为之前我们讨论的dict。 4 quicklist quicklist是结合ziplist存储优势和链表灵活性与一身的双端链表。 quicklist的结构为什么这样设计呢？总结起来，大概又是一个空间和时间的折中： 4.1 双向链表便于在表的两端进行push和pop操作，但是它的内存开销比较大。 首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。 4.2 ziplist由于是一整块连续内存，所以存储效率很高。 但是，它不利于修改操作，每次数据变动都会引发一次内存的realloc。特别是当ziplist长度很长的时候，一次realloc可能会导致大批量的数据拷贝，进一步降低性能。 5 zsetzset其实是两种结构的合并。也就是dict和skiplist结合而成的。dict负责保存数据对分数的映射，而skiplist用于根据分数进行数据的查询（相辅相成） 6 skiplist sortset数据结构使用了ziplist+zset两种数据结构。 Redis里面使用skiplist是为了实现sorted set这种对外的数据结构。sorted set提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。 sortedset是由skiplist，dict和ziplist组成的。 当数据较少时，sorted set是由一个ziplist来实现的。当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 skiplist的节点中存着节点值和分数。并且跳表是根据节点的分数进行排序的，所以可以根据节点分数进行范围查找。 7inset inset是一个数字结合，他使用灵活的数据类型来保持数字。 新创建的intset只有一个header，总共8个字节。其中encoding = 2, length = 0。添加13, 5两个元素之后，因为它们是比较小的整数，都能使用2个字节表示，所以encoding不变，值还是2。当添加32768的时候，它不再能用2个字节来表示了（2个字节能表达的数据范围是-215~215-1，而32768等于215，超出范围了），因此encoding必须升级到INTSET_ENC_INT32（值为4），即用4个字节表示一个元素。 8总结 sds是一个灵活的字符串数组，并且支持直接存储二进制数据，同时提供长度和剩余空间的字段来保证伸缩性和防止溢出。 dict是一个字典结构，实现方式就是Java中的hashmap实现，同时持有两个节点数组，但只使用其中一个，扩容时换成另外一个。 ziplist是一个压缩链表，他放弃内存不连续的连接方式，而是直接分配连续内存进行存储，减少内存碎片。提高利用率，并且也支持存储二进制数据。 quicklist是ziplist和传统链表的中和形成的链表结果，每个链表节点都是一个ziplist。 skiplist一般有ziplist和zset两种实现方法，根据数据量来决定。zset本身是由skiplist和dict实现的。 inset是一个数字集合，他根据插入元素的数据类型来决定数组元素的长度。并自动进行扩容。 9 他们实现了哪些结构 字符串由sds实现 list由ziplist和quicklist实现 sortset由ziplist和zset实现 hash表由dict实现 集合由inset实现。 redis server结构和数据库redisDb1 redis服务器中维护着一个数据库名为redisdb，实际上他是一个dict结构。 Redis的数据库使用字典作为底层实现，数据库的增、删、查、改都是构建在字典的操作之上的。 2 redis服务器将所有数据库都保存在服务器状态结构redisServer(redis.h/redisServer)的db数组（应该是一个链表）里： 同理也有一个redis client结构，通过指针可以选择redis client访问的server是哪一个。 3 redisdb的键空间 typedef struct redisDb { // 数据库键空间，保存着数据库中的所有键值对 dict *dict; /* The keyspace for this DB */ // 键的过期时间，字典的键为键，字典的值为过期事件 UNIX 时间戳 dict *expires; /* Timeout of keys with a timeout set */ // 数据库号码 int id; /* Database ID */ // 数据库的键的平均 TTL ，统计信息 long long avg_ttl; /* Average TTL, just for stats */ //.. } redisDb这部分的代码说明了，redisdb除了维护一个dict组以外，还需要对应地维护一个expire的字典数组。 大的dict数组中有多个小的dict字典，他们共同负责存储redisdb的所有键值对。 同时，对应的expire字典则负责存储这些键的过期时间 4 过期键的删除策略 2、过期键删除策略通过前面的介绍，大家应该都知道数据库键的过期时间都保存在过期字典里，那假如一个键过期了，那么这个过期键是什么时候被删除的呢？现在来看看redis的过期键的删除策略： a、定时删除：在设置键的过期时间的同时，创建一个定时器，在定时结束的时候，将该键删除； b、惰性删除：放任键过期不管，在访问该键的时候，判断该键的过期时间是否已经到了，如果过期时间已经到了，就执行删除操作； c、定期删除：每隔一段时间，对数据库中的键进行一次遍历，删除过期的键。 redis的事件模型redis处理请求的方式基于reactor线程模型，即一个线程处理连接，并且注册事件到IO多路复用器，复用器触发事件以后根据不同的处理器去执行不同的操作。总结以下客户端到服务端的请求过程 总结 远程客户端连接到 redis 后，redis服务端会为远程客户端创建一个 redisClient 作为代理。 redis 会读取嵌套字中的数据，写入 querybuf 中。 解析 querybuf 中的命令，记录到 argc 和 argv 中。 根据 argv[0] 查找对应的 recommand。 执行 recommend 对应的执行函数。 执行以后将结果存入 buf &amp; bufpos &amp; reply 中。 返回给调用方。返回数据的时候，会控制写入数据量的大小，如果过大会分成若干次。保证 redis 的相应时间。 Redis 作为单线程应用，一直贯彻的思想就是，每个步骤的执行都有一个上限（包括执行时间的上限或者文件尺寸的上限）一旦达到上限，就会记录下当前的执行进度，下次再执行。保证了 Redis 能够及时响应不发生阻塞。备份方式快照（RDB）：就是我们俗称的备份，他可以在定期内对数据进行备份，将Redis服务器中的数据持久化到硬盘中； 只追加文件（AOF）：他会在执行写命令的时候，将执行的写命令复制到硬盘里面，后期恢复的时候，只需要重新执行一下这个写命令就可以了。类似于我们的MySQL数据库在进行主从复制的时候，使用的是binlog二进制文件，同样的是执行一遍写命令； appendfsync同步频率的区别如下图： redis主从复制Redis复制工作过程： slave向master发送sync命令。 master开启子进程来讲dataset写入rdb文件，同时将子进程完成之前接收到的写命令缓存起来。 子进程写完，父进程得知，开始将RDB文件发送给slave。 master发送完RDB文件，将缓存的命令也发给slave。 master增量的把写命令发给slave。 注意有两步操作，一个是写入rdb的时候要缓存写命令，防止数据不一致。发完rdb后还要发写命令给salve，以后增量发命令就可以了分布式锁实现使用setnx加expire实现加锁和时限 加锁时使用setnx设置key为1并设置超时时间，解锁时删除键 tryLock(){ SETNX Key 1 EXPIRE Key Seconds } release(){ DELETE Key }这个方案的一个问题在于每次提交一个Redis请求，如果执行完第一条命令后应用异常或者重启，锁将无法过期，一种改善方案就是使用Lua脚本（包含SETNX和EXPIRE两条命令），但是如果Redis仅执行了一条命令后crash或者发生主从切换，依然会出现锁没有过期时间，最终导致无法释放。 使用getset加锁和获取过期时间针对锁无法释放问题的一个解决方案基于GETSET命令来实现 思路： SETNX(Key,ExpireTime)获取锁 如果获取锁失败，通过GET(Key)返回的时间戳检查锁是否已经过期 GETSET(Key,ExpireTime)修改Value为NewExpireTime 检查GETSET返回的旧值，如果等于GET返回的值，则认为获取锁成功 注意：这个版本去掉了EXPIRE命令，改为通过Value时间戳值来判断过期​ 2.0的setnx可以配置过期时间。V2.0 基于SETNX tryLock(){ SETNX Key 1 Seconds } release(){ DELETE Key }Redis 2.6.12版本后SETNX增加过期时间参数，这样就解决了两条命令无法保证原子性的问题。但是设想下面一个场景： C1成功获取到了锁，之后C1因为GC进入等待或者未知原因导致任务执行过长，最后在锁失效前C1没有主动释放锁 2. C2在C1的锁超时后获取到锁，并且开始执行，这个时候C1和C2都同时在执行，会因重复执行造成数据不一致等未知情况 3. C1如果先执行完毕，则会释放C2的锁，此时可能导致另外一个C3进程获取到了锁 流程图如下 使用sentx将值设为时间戳，通过lua脚本进行cas比较和删除操作V3.0 tryLock(){ SETNX Key UnixTimestamp Seconds } release(){ EVAL( //LuaScript if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1]) else return 0 end ) }这个方案通过指定Value为时间戳，并在释放锁的时候检查锁的Value是否为获取锁的Value，避免了V2.0版本中提到的C1释放了C2持有的锁的问题；另外在释放锁的时候因为涉及到多个Redis操作，并且考虑到Check And Set 模型的并发问题，所以使用Lua脚本来避免并发问题。 如果在并发极高的场景下，比如抢红包场景，可能存在UnixTimestamp重复问题，另外由于不能保证分布式环境下的物理时钟一致性，也可能存在UnixTimestamp重复问题，只不过极少情况下会遇到。 分布式Redis锁：Redlockredlock的思想就是要求一个节点获取集群中N/2 + 1个节点上的锁才算加锁成功。 总结不论是基于SETNX版本的Redis单实例分布式锁，还是Redlock分布式锁，都是为了保证下特性 安全性：在同一时间不允许多个Client同时持有锁 活性 死锁：锁最终应该能够被释放，即使Client端crash或者出现网络分区（通常基于超时机制）容错性：只要超过半数Redis节点可用，锁都能被正确获取和释放 分布式方案 1 主从复制，优点是备份简易使用。缺点是不能故障切换，并且不易扩展。 2 使用sentinel哨兵工具监控和实现自动切换。 3 codis集群方案 首先codis使用代理的方式隐藏底层redis，这样可以完美融合以前的代码，不需要更改redis访问操作。 然后codis使用了zookeeper进行监控和自动切换。同时使用了redis-group的概念，保证一个group里是一主多从的主从模型，基于此来进行切换。 4 redis cluster集群 该集群是一个p2p方式部署的集群 Redis cluster是一个去中心化、多实例Redis间进行数据共享的集群。 每个节点上都保存着其他节点的信息，通过任一节点可以访问正常工作的节点数据，因为每台机器上的保留着完整的分片信息，某些机器不正常工作不影响整体集群的工作。并且每一台redis主机都会配备slave，通过sentinel自动切换。 redis事务事务MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 redis事务有一个特点，那就是在2.6以前，事务的一系列操作，如果有的成功有的失败，仍然会提交成功的那部分，后来改为全部不提交了。 但是Redis事务不支持回滚，提交以后不能执行回滚操作。 为什么 Redis 不支持回滚（roll back） 如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。​ redis脚本事务Redis 脚本和事务从定义上来说， Redis 中的脚本本身就是一种事务， 所以任何在事务里可以完成的事， 在脚本里面也能完成。 并且一般来说， 使用脚本要来得更简单，并且速度更快。 因为脚本功能是 Redis 2.6 才引入的， 而事务功能则更早之前就存在了， 所以 Redis 才会同时存在两种处理事务的方法。 redis事务的ACID特性在传统的关系型数据库中,尝尝用ACID特质来检测事务功能的可靠性和安全性。在redis中事务总是具有原子性(Atomicity),一致性(Consistency)和隔离性(Isolation),并且当redis运行在某种特定的持久化模式下,事务也具有耐久性(Durability). ①原子性 事务具有原子性指的是,数据库将事务中的多个操作当作一个整体来执行,服务器要么就执行事务中的所有操作,要么就一个操作也不执行。但是对于redis的事务功能来说,事务队列中的命令要么就全部执行,要么就一个都不执行,因此redis的事务是具有原子性的。 ②一致性 事务具有一致性指的是,如果数据库在执行事务之前是一致的,那么在事务执行之后,无论事务是否执行成功,数据库也应该仍然一致的。 ”一致“指的是数据符合数据库本身的定义和要求,没有包含非法或者无效的错误数据。redis通过谨慎的错误检测和简单的设计来保证事务一致性。③隔离性 事务的隔离性指的是,即使数据库中有多个事务并发在执行,各个事务之间也不会互相影响,并且在并发状态下执行的事务和串行执行的事务产生的结果完全 相同。 因为redis使用单线程的方式来执行事务(以及事务队列中的命令),并且服务器保证,在执行事务期间不会对事物进行中断,因此,redis的事务总是以串行 的方式运行的,并且事务也总是具有隔离性的④持久性 事务的耐久性指的是,当一个事务执行完毕时,执行这个事务所得的结果已经被保持到永久存储介质里面。 因为redis事务不过是简单的用队列包裹起来一组redis命令,redis并没有为事务提供任何额外的持久化功能,所以redis事务的耐久性由redis使用的模式 决定微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql原理与实践总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FMysql%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文根据自己对MySQL的学习和实践以及各类文章与书籍总结而来。囊括了MySQL数据库的基本原理和技术。本文主要是我的一个学习总结，基于之前的系列文章做了一个概括，如有错误，还望指出，谢谢。 详细内容请参考我的系列文章： 重新学习MySQL与Redis https://blog.csdn.net/column/details/21877.html 数据库原理Mysql是关系数据库。 范式 反范式范式设计主要是避免冗余，以及数据不一致。反范式设计主要是避免多表连接，增加了冗余。 主键 外键主键是一个表中一行数据的唯一标识。外键则是值某一列的键值是其他表的主键，外键的作用一般用来作为两表连接的键，并且保证数据的一致性。 锁 共享锁和排它锁数据库的锁用来进行并发控制，排它锁也叫写锁，共享锁也叫行锁，根据不同粒度可以分为行锁和表锁。 存储过程与视图存储过程是对sql语句进行预编译并且以文件形式包装为一个可以快速执行的程序。但是缺点是不易修改，稍微改动语句就需要重新开发储存过程，优点是执行效率快。视图就是对其他一个或多个表进行重新包装，是一个外观模式，对视图数据的改动也会影响到数据报本身。 事务与隔离级别事务的四个性质：原子性，一致性，持久性，隔离性。 原子性：一个事务中的操作要么全部成功要么全部失败。 一致性：事务执行成功的状态都是一致的，即使失败回滚了，也应该和事务执行前的状态是一致的。 隔离性：两个事务之间互不相干，不能互相影响。 事务的隔离级别读未提交：事务A和事务B，A事务中执行的操作，B也可以看得到，因为级别是未提交读，别人事务中还没提交的数据你也看得到。这是没有任何并发措施的级别，也是默认级别。这个问题叫做脏读，为了解决这个问题，提出了读已提交。 读已提交：事务A和B，A中的操作B看不到，只有A提交后，在B中才看得到。虽然A的操作B看不到，但是B可以修改A用到的数据，导致A读两次的数据结果不同。这就是不可重读问题。 可重复读：事务A和B，事务A和B，A在数据行上加读锁，B虽然看得到但是改不了。所以是可重复读的，但是A的其他行仍然会被B访问并修改，所以导致了幻读问题。 序列化：数据库强制事务A和B串行化操作，避免了并发问题，但是效率比较低。 后面可以看一下mysql对隔离级别的实现。 索引索引的作用就和书的目录类似，比如根据书名做索引，然后我们通过书名就可以直接翻到某一页。数据表中我们要找一条数据，也可以根据它的主键来找到对应的那一页。当然数据库的搜索不是翻书，如果一页一页翻书，就相当于是全表扫描了，效率很低，所以人翻书肯定也是跳着翻。数据库也会基于类似的原理”跳着”翻书，快速地找到索引行。 mysql原理MySQL是oracle公司的免费数据库，作为关系数据库火了很久了。所以我们要学他。 mysql客户端，服务端，存储引擎，文件系统MySQL数据库的架构可以分为客户端，服务端，存储引擎和文件系统。 详细可以看下架构图，我稍微总结下 最高层的客户端，通过tcp连接mysql的服务器，然后执行sql语句，其中涉及了查询缓存，执行计划处理和优化，接下来再到存储引擎层执行查询，底层实际上访问的是主机的文件系统。 mysql常用语法1 登录mysql mysql -h 127.0.0.1 -u 用户名 -p 2 创建表语法还是比较复杂的，之前有腾讯面试官问这个，然后答不上来。 CREATE TABLE `user_accounts` ( `id` int(100) unsigned NOT NULL AUTO_INCREMENT primary key, `password` varchar(32) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;用户密码&apos;, `reset_password` tinyint(32) NOT NULL DEFAULT 0 COMMENT &apos;用户类型：0－不需要重置密码；1-需要重置密码&apos;, `mobile` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;手机&apos;, `create_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), `update_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), -- 创建唯一索引，不允许重复 UNIQUE INDEX idx_user_mobile(`mobile`) ) ENGINE=InnoDB DEFAULT CHARSET=utf83 crud比较简单，不谈 4 join用于多表连接，查询的通常是两个表的字段。 union用于组合同一种格式的多个select查询。 6 聚合函数，一般和group by一起使用，比如查找某部门员工的工资平均值。就是select AVE(money) from departmentA group by department 7 建立索引 唯一索引(UNIQUE)语法：ALTER TABLE 表名字 ADD UNIQUE (字段名字) 添加多列索引语法： ALTER TABLE table_name ADD INDEX index_name ( column1, column2, column3) 8 修改添加列 添加列语法：alter table 表名 add 列名 列数据类型 [after 插入位置]; 删除列语法：alter table 表名 drop 列名称; 9 清空表数据方法一：delete from 表名;方法二：truncate from “表名”; DELETE:1. DML语言;2. 可以回退;3. 可以有条件的删除; TRUNCATE:1. DDL语言;2. 无法回退;3. 默认所有的表内容都删除;4. 删除速度比delete快。 MySQL的存储原理下面我们讨论的是innodb的存储原理 innodb的存储引擎将数据存储单元分为多层。按此不表 MySQL中的逻辑数据库只是一个shchme。事实上物理数据库只有一个。 mysql使用两个文件分别存储数据库的元数据和数据库的真正数据。 数据页page数据页结构页是 InnoDB 存储引擎管理数据的最小磁盘单位，而 B-Tree 节点就是实际存放表中数据的页面，我们在这里将要介绍页是如何组织和存储记录的；首先，一个 InnoDB 页有以下七个部分： 每一个页中包含了两对 header/trailer：内部的 Page Header/Page Directory 关心的是页的状态信息，而 Fil Header/Fil Trailer 关心的是记录页的头信息。 也就是说，外部的h-t对用来和其他页形成联系，而内部的h-t用来是保存内部记录的状态。 User Records 就是整个页面中真正用于存放行记录的部分，而 Free Space 就是空余空间了，它是一个链表的数据结构，为了保证插入和删除的效率，整个页面并不会按照主键顺序对所有记录进行排序，它会自动从左侧向右寻找空白节点进行插入，行记录在物理存储上并不是按照顺序的，它们之间的顺序是由 next_record 这一指针控制的。 也就是说，一个页中存了非常多行的数据，而每一行数据和相邻行使用指针进行链表连接。mysql的索引，b树，聚集索引1 MySQL的innodb支持聚簇索引，myisam不支持聚簇索引。 innodb在建表时自动按照第一个非空字段或者主键建立聚簇索引。mysql使用B+树建立索引。 每一个非叶子结点只存储主键值，而叶子节点则是一个数据页，这个数据页就是上面所说的存储数据的page页。 一个节点页对应着多行数据，每个节点按照顺序使用指针连成一个链表。mysql使用索引访问一行数据时，先通过log2n的时间访问到叶子节点，然后在数据页中按照行数链表执行顺序查找，直到找到那一行数据。 2 b+树索引可以很好地支持范围搜索，因为叶子节点通过指针相连。 mysql的explain 慢查询日志explain主要用于检查sql语句的执行计划，然后分析sql是否使用到索引，是否进行了全局扫描等等。 mysql慢查询日志可以在mysql的,my.cnf文件中配置开启，然后执行操作超过设置时间就会记录慢日志。 比如分析一个sql： explain查看执行计划 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N ALL votenum,vote \N \N \N 996507 50.00 Using where​​ 还是没用到索引，因为不符合最左前缀匹配。查询需要3.5秒左右 ​​ 最后修改一下sql语句 EXPLAIN SELECT * FROM vote_record WHERE id &gt; 0 AND vote_num &gt; 1000; id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N range PRIMARY,votenum,vote PRIMARY 4 \N 498253 50.00 Using where​​ 用到了索引，但是只用到了主键索引。再修改一次 ​​ EXPLAIN SELECT * FROM vote_record WHERE id &gt; 0 AND vote_num = 1000; ​​ id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE vote_record \N index_merge PRIMARY,votenum,vote votenum,PRIMARY 8,4 \N 51 100.00 Using intersect(votenum,PRIMARY); Using where​​ 用到了两个索引，votenum,PRIMARY。 mysql的binlog,redo log和undo log。binlog就是二进制日志，用于记录用户数据操作的日志。用于主从复制。 redolog负责事务的重做，记录事务中的每一步操作，记录完再执行操作，并且在数据刷入磁盘前刷入磁盘，保证可以重做成功。 undo日志负责事务的回滚，记录事务操作中的原值，记录完再执行操作，在事务提交前刷入磁盘，保证可以回滚成功。 这两个日志也是实现分布式事务的基础。 mysql的数据类型mysql一般提供多种数据类型，int，double，varchar，tinyint，datatime等等。文本的话有fulltext，mediumtext等。没啥好说的。 mysql的sql优化。sql能优化的点是在有点多。 比如基本的，不使用null判断，不使用&gt;&lt;分页的时候利用到索引，查询的时候注意顺序。 如果是基于索引的优化，则要注意索引列是否能够使用到 1 索引列不要使用&gt;&lt; != 以及 null，还有exists等。 2 索引列不要使用聚集函数。 3 如果是联合索引，排在第一位的索引一定要用到，否则后面的也会失效，为什么呢，因为第一列索引不同时才会找第二列，如果没有第一列索引，后续的索引页没有意义。 举个例子。联合索引A,B,C。查询时必须要用到A，但是A的位置无所谓，只要用到就行，A,B,C或者C,B,A都可以。 4 分页时直接limit n 5可能用不到索引，假设索引列是ID，那么我们使用where id &gt; n limit 5就可以实现上述操作了。MySQL的事务实现和锁innodb支持行级锁和事务，而myisam只支持表锁，它的所有操作都需要加锁。 1 锁 锁可以分为共享锁和排它锁，也叫读锁和写锁。 select操作默认不加锁，需要加锁时会用for update加排它锁，或者用in share mode表示加共享锁。 这里的锁都是行锁。 innodb会使用行锁配合mvcc一同完成事务的实现。 并且使用next-key lock来实现可重复读，而不必加表锁或者串行化执行。2 MVCC MVCC是多版本控制协议。 通过时间戳来判断先后顺序，并且是无锁的。但是需要额外存一个字段。 读操作比较自己的版本号，自动读取比自己版本号新的版本。不读。 写操作自动覆盖写版本号比自己的版本号早的版本。否则不写。 这样保证一定程度上的一致性。 MVCC比较好地支持读多写少的情景。 但是偶尔需要加锁时才会进行加锁。3 事务 所以看看innodb如何实现事务的。 首先，innodb的行锁是加在索引上的，因为innodb默认有聚簇索引，但实际上的行锁是对整个索引节点进行加锁，锁了该节点所有的行。 看看innodb如何实现隔离级别以及解决一致问题 未提交读，会导致脏读，没有并发措施 已提交读，写入时需要加锁，使用行级写锁锁加锁指定行，其他事务就看不到未提交事务的数据了。但是会导致不可重读， 可重复读：在原来基础上，在读取行时也需要加行级读锁，这样其他事务不能修改这些数据。就避免了不可重读。 但是这样会导致幻读。 序列化：序列化会串行化读写操作来避免幻读，事实上就是事务在读取数据时加了表级读锁。但是实际上。mysql的新版innodb引擎已经解决了幻读的问题，并且使用的是可重复读级别就能解决幻读了。 实现的原理是next-key lock。是gap lock的加强版。不会锁住全表，只会锁住被读取行前后的间隙行。 ​​ 分库分表分库分表的方案比较多，首先看下分表。 当一个大表没办法继续优化的时候，可以使用分表，横向拆分的方案就是把一个表的数据放到多个表中。一般可以按照某个键来分表。比如最常用的id，1-100w放在表一。100w-200w在表二，以此类推。 如果是纵向分表，则可以按列拆分，比如用户信息的字段放在一个表，用户使用数据放在另一个表，这其实就是一次性拆表了。 分库的话就是把数据表存到多个库中了，和横向分表的效果差不多。 如果只是单机的分表分库，其性能瓶颈在于主机。 我们需要考虑扩展性，所以需要使用分布式的数据库。 ==分布式数据库解决方案mycat== mycat是一款支持分库分表的数据库中间件，支持单机也支持分布式。 首先部署mycat，mycat的访问方式和一个mysqlserver是类似的。里面可以配置数据库和数据表。 然后在mycat的配置文件中，我们可以指定分片，比如按照id分片，然后在每个分片下配置mysql节点，可以是本地的数据库实例也可以是其他主机上的数据库。 这样的话，每个分片都能找到对应机器上的数据库和表了。 用户连接mycat执行数据库操作，实际上会根据id映射到对应的数据库和表中，主从复制，读写分离主从复制大法好，为了避免单点mysql宕机和丢失数据，我们一般使用主从部署，主节点将操作日志写入binlog，然后日志文件通过一个连接传给从节点的relaylog。从节点定时去relaylog读取日志，并且执行操作。这样保证了主从的同步。 读写分离大法好，为了避免主库的读写压力太大，由于业务以读操作为主，所以主节点一般作为主库，读节点作为从库，从库负责读，主库负责写，写入主库的数据通过日志同步给从库。这样的部署就是读写分离。 使用mycat中间件也可以配置读写分离，只需在分片时指定某个主机是读节点还是写节点即可。 分布式数据库分布式关系数据库无非就是关系数据库的分布式部署方案。 真正的分布式数据库应该是nosql数据库，比如基于hdfs的hbase数据库。底层就是分布式的。 redis的分布式部署方案也比较成熟。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring与SpringMVC源码解析总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FSpring%E4%B8%8ESpringMVC%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我之前Spring和SpringMVC源码系列文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：#Spring和SpringMVC源码解析https://blog.csdn.net/column/details/21851.html Spring和SpringMVCSpring是一个框架，除了提供IOC和AOP以外，还加入了web等众多内容。 1 IOC：控制反转，改变类实例化的方式，通过xml等配置文件指定接口的实现类，让实现类和代码解耦，通过配置文件灵活调整实现类。 2 AOP: 面向切面编程，将切面代码封装，比如权限验证，日志模块等，这些逻辑重复率大，通过一个增强器封装功能，然后定义需要加入这些功能的切面，切面一般用表达式或者注解去匹配方法，可以完成前置和后置的处理逻辑。 3 SpringMVC是一个web框架，基于Spring之上，实现了web相关的功能，使用dispatcherservlet作为一切请求的处理入口。通过配置viewresolver解析页面，通过配置管理静态文件，还可以注入其他的配置信息，除此之外，springmvc可以访问spring容器的所有bean。 Spring源码总结IOC: 1 Spring的bean容器也叫beanfactory，我们常用的applicationcontext实际上内部有一个listablebeanfactory实际存储bean的map。 2 bean加载过程：spring容器加载时先读取配置文件，一般是xml，然后解析xml，找到其中所有bean，依次解析，然后生成每个bean的beandefinition，存在一个map中，根据beanid映射实际bean的map。 3 bean初始化：加载完以后，如果不启用懒加载模式，则默认使用单例加载，在注册完bean以后，可以获取到beandefinition信息，然后根据该信息首先先检查依赖关系，如果依赖其他bean则先加载其他bean，然后通过反射的方式即newinstance创建一个单例bean。 为什么要用反射呢，因为实现类可以通过配置改变，但接口是一致的，使用反射可以避免实现类改变时无法自动进行实例化。 当然，bean也可以使用原型方式加载，使用原型的话，每次创建bean都会是全新的。 AOP: AOP的切面，切点，增强器一般也是配置在xml文件中的，所以bean容器在解析xml时会找到这些内容，并且首先创建增强器bean的实例。 基于上面创建bean的过程，AOP起到了什么作用呢，或者是是否有参与到其中呢，答案是有的。 在获得beandefinition的时候，spring容器会检查该bean是否有aop切面所修饰，是否有能够匹配切点表达式的方法，如果有的话，在创建bean之前，会将bean重新封装成一个动态代理的对象。 代理类会为bean增加切面中配置的advisor增强器，然后返回bean的时候实际上返回的是一个动态代理对象。 所以我们在调用bean的方法时，会自动织入切面的增强器，当然，动态代理既可以选择jdk增强器，也可以选择cglib增强器。 Spring事务： spring事务其实是一种特殊的aop方式。在spring配置文件中配置好事务管理器和声明式事务注解后，就可以使用@transactional进行事务方法的处理了。 事务管理器的bean中会配置基本的信息，然后需要配置事务的增强器，不同方法使用不同的增强器。当然如果使用注解的话就不用这么麻烦了。 然后和aop的动态代理方式类似，当Spring容器为bean生成代理时，会注入事务的增强器，其中实际上实现了事务中的begin和commit，所以执行方法的过程实际上就是在事务中进行的。 SpringMVC源码总结1 dispatcherservlet概述SpringMVC使用dispatcherservlet作为唯一如果，在web.xml中进行配置，他继承自frameworkservlet，向上继承自httpservletbean。 httpservletbean为dispatcherservlet加载了来自web.xml配置信息中的信息，保存在servletcontext上下文中，而frameworkservletbean则初始化了spring web的bean容器。 这个容器一般是配置在spring-mvc.xml中的，他独立于spring容器，但是把spring容器作为父容器，所以SpringMVC可以访问spring容器中的各种类。 而dispatcherservlet自己做了什么呢，因为springmvc中配置了很多例如静态文件目录，自动扫描bean注解，以及viewresovler和httpconverter等信息，所以它需要初始化这些策略，如果没有配置则会使用默认值。 2 dispatcherservlet的执行流程 首先web容器会加载指定扫描bean并进行初始化。 当请求进来后，首先执行service方法，然后到dodispatch方法执行请求转发，事实上，spring web容器已经维护了一个map，通过注解@requestmapping映射到对应的bean以及方法上。通过这个map可以获取一个handlerchain，真正要执行的方法被封装成一个handler，并且调用方法前要执行前置的一些过滤器。 最终执行handler方法时实际上就是去执行真正的方法了。 3 viewresolver 解析完请求和执行完方法，会把modelandview对象解析成一个view对象，让后使用view.render方法执行渲染，至于使用什么样的视图解析器，就是由你配置的viewresolver来决定的，一般默认是jspviewresolver。 4 httpmessageconverter 一般配合responsebody使用，可以将数据自动转换为json和xml，根据http请求中适配的数据类型来决定使用哪个转换器。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb技术总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJavaWeb%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我之前两个系列的文章而来。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章： JavaWeb技术世界 https://blog.csdn.net/column/details/21850.html Servlet及相关类servlet是一个接口，它的实现类有GenericServlet，而httpservlet是GenericServlet的一个子类，一般我们都会使用这个类。 servletconfig是用于保存servlet配置信息的数据结构，而servletcontext则负责保持servlet的上下文，web应用启动时加载web.xml信息于servletconfig中。 Jsp和ViewResolverjsp页面需要编译成class文件并通过tomcat的类加载器进行加载，形成servlet实例，请求到来时实际上执行的是servlet代码，然后最终再通过viewresolver渲染成页面。 filter，listenerfilter是过滤器，也需要在web.xml中配置，是责任链式的调用，在servlet执行service方法前执行。listener则是监听器，由于容器组件都实现了lifecycle接口，所以可以在组件上添加监听器来控制生命周期。 web.xmlweb.xml用来配置servlet和servlet的配置信息，listener和filter。也可以配置静态文件的目录等。 war包waWAR包WAR(Web Archive file)网络应用程序文件，是与平台无关的文件格式，它允许将许多文件组合成一个压缩文件。war专用在web方面 。 JAVA WEB工程，都是打成WAR包进行发布。 典型的war包内部结构如下： webapp.war | index.jsp | |— images |— META-INF |— WEB-INF | web.xml // WAR包的描述文件 | |— classes | action.class // java类文件 | |— lib other.jar // 依赖的jar包 share.jartomcat基础上一篇文章关于网络编程和NIO已经讲过了，这里按住不表。 log4jlog4j是非常常用的日志组件，不过现在为了使用更通用的日志组件，一般使用slf4j来配置日志管理器，然后再介入日志源，比如log4j这样的日志组件。 数据库驱动和连接池一般我们会使用class.forname加载数据库驱动，但是随着Spring的发展，现在一般会进行数据源DataSource这个bean的配置，bean里面填写你的数据来源信息即可，并且在实现类中可以选择支持连接池的数据源实现类，比如c3poDataSource，非常方便。 数据库连接池本身和线程池类似，就是为了避免频繁建立数据库连接，保存了一部分连接并存放在集合里，一般可以用队列来存放。 除此之外，还可以使用tomcat的配置文件来管理数据库连接池，只需要简单的一些配置，就可以让tomcat自动管理数据库的连接池了。应用需要使用的时候，通过jndi的方式访问即可，具体方法就是调用jndi命名服务的look方法。 单元测试单元测试是工程中必不可少的组件，maven项目在打包期间会自动运行所有单元测试。一般我们使用junit做单元测试，统一地在test包中分别测试service和dao层，并且使用mock方法来构造假的数据，以便跳过数据库或者其他外部资源来完成测试。 Mavenmaven是一个项目构建工具，基于约定大于配置的方式，规定了一个工程各个目录的用途，并且根据这些规则进行编译，测试和打包。同时他提供了方便的包管理方式，以及快速部署的优势。 Gitgit是分布式的代码管理工具，比起svn有着分布式的优势。太过常见了，略了。 Json和xml数据描述形式不同，json更简洁。 hibernate和mybatis由于jdbc方式的数据库连接和语句执行太过繁琐，重复代码太多，后来提出了jdbctemplate对数据进行bean转换。 但是还是差强人意，于是转而出现了hibernate这类的持久化框架。可以做到数据表和bean一一映射，程序只需要操作bean就可以完成数据库的curd。 mybatis比hibernate更轻量级，mybatis支持原生sql查询，并且也可以使用bean映射，同时还可以自定义地配置映射对象，更加灵活，并且在多表查询上更有优势。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>JavaWeb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM原理学习总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJVM%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我之前JVM系列文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：深入理解JVM虚拟机 https://blog.csdn.net/column/details/21960.html JVM介绍和源码首先JVM是一个虚拟机，当你安装了jre，它就包含了jvm环境。JVM有自己的内存结构，字节码执行引擎，因此class字节码才能在jvm上运行，除了Java以外，Scala，groovy等语言也可以编译成字节码而后在jvm中运行。JVM是用c开发的。 JVM内存模型内存模型老生常谈了，主要就是线程共享的堆区，方法区，本地方法栈。还有线程私有的虚拟机栈和程序计数器。 堆区存放所有对象，每个对象有一个地址，Java类jvm初始化时加载到方法区，而后会在堆区中生成一个Class对象，来负责这个类所有实例的实例化。 栈区存放的是栈帧结构，栈帧是一段内存空间，包括参数列表，返回地址，局部变量表等，局部变量表由一堆slot组成，slot的大小固定，根据变量的数据类型决定需要用到几个slot。 方法区存放类的元数据，将原来的字面量转换成引用，当然，方法区也提供常量池，常量池存放-128到127的数字类型的包装类。字符串常量池则会存放使用intern的字符串变量。 JVM OOM和内存泄漏这里指的是oom和内存泄漏这类错误。 oom一般分为三种，堆区内存溢出，栈区内存溢出以及方法区内存溢出。 堆内存溢出主要原因是创建了太多对象，比如一个集合类死循环添加一个数，此时设置jvm参数使堆内存最大值为10m，一会就会报oom异常。 栈内存溢出主要与栈空间和线程有关，因为栈是线程私有的，如果创建太多线程，内存值超过栈空间上限，也会报oom。 方法区内存溢出主要是由于动态加载类的数量太多，或者是不断创建一个动态代理，用不了多久方法区内存也会溢出，会报oom，这里在1.7之前会报permgem oom，1.8则会报meta space oom，这是因为1.8中删除了堆中的永久代，转而使用元数据区。 内存泄漏一般是因为对象被引用无法回收，比如一个集合中存着很多对象，可能你在外部代码把对象的引用置空了，但是由于对象还被集合给引用着，所以无法被回收，导致内存泄漏。测试也很简单，就在集合里添加对象，添加完以后把引用置空，循环操作，一会就会出现oom异常，原因是内存泄漏太多了，导致没有空间分配新的对象。 常见调试工具命令行工具有jstack jstat jmap 等，jstack可以跟踪线程的调用堆栈，以便追踪错误原因。 jstat可以检查jvm的内存使用情况，gc情况以及线程状态等。 jmap用于把堆栈快照转储到文件系统，然后可以用其他工具去排查。 visualvm是一款很不错的gui调试工具，可以远程登录主机以便访问其jvm的状态并进行监控。 class文件结构class文件结构比较复杂，首先jvm定义了一个class文件的规则，并且让jvm按照这个规则去验证与读取。 开头是一串魔数，然后接下来会有各种不同长度的数据，通过class的规则去读取这些数据，jvm就可以识别其内容，最后将其加载到方法区。 JVM的类加载机制jvm的类加载顺序是bootstrap类加载器，extclassloader加载器，最后是appclassloader用户加载器，分别加载的是jdk/bin ，jdk/ext以及用户定义的类目录下的类（一般通过ide指定），一般核心类都由bootstrap和ext加载器来加载，appclassloader用于加载自己写的类。 双亲委派模型，加载一个类时，首先获取当前类加载器，先找到最高层的类加载器bootstrap让他尝试加载，他如果加载不了再让ext加载器去加载，如果他也加载不了再让appclassloader去加载。这样的话，确保一个类型只会被加载一次，并且以高层类加载器为准，防止某些类与核心类重复，产生错误。 defineclass findclass和loadclass类加载classloader中有两个方法loadclass和findclass，loadclass遵从双亲委派模型，先调用父类加载的loadclass，如果父类和自己都无法加载该类，则会去调用findclass方法，而findclass默认实现为空，如果要自定义类加载方式，则可以重写findclass方法。 常见使用defineclass的情况是从网络或者文件读取字节码，然后通过defineclass将其定义成一个类，并且返回一个Class对象，说明此时类已经加载到方法区了。当然1.8以前实现方法区的是永久代，1.8以后则是元空间了。 JVM虚拟机字节码执行引擎jvm通过字节码执行引擎来执行class代码，他是一个栈式执行引擎。这部分内容比较高深，在这里就不献丑了。 编译期优化和运行期优化编译期优化主要有几种 1 泛型的擦除，使得泛型在编译时变成了实际类型，也叫伪泛型。 2 自动拆箱装箱，foreach循环自动变成迭代器实现的for循环。 3 条件编译，比如if(true)直接可得。 运行期优化主要有几种 1 JIT即时编译 Java既是编译语言也是解释语言，因为需要编译代码生成字节码，而后通过解释器解释执行。 但是，有些代码由于经常被使用而成为热点代码，每次都编译太过费时费力，干脆直接把他编译成本地代码，这种方式叫做JIT即时编译处理，所以这部分代码可以直接在本地运行而不需要通过jvm的执行引擎。 2 公共表达式擦除，就是一个式子在后面如果没有被修改，在后面调用时就会被直接替换成数值。 3 数组边界擦除，方法内联，比较偏，意义不大。 4 逃逸分析，用于分析一个对象的作用范围，如果只局限在方法中被访问，则说明不会逃逸出方法，这样的话他就是线程安全的，不需要进行并发加锁。 1 JVM的垃圾回收1 GC算法：停止复制，存活对象少时适用，缺点是需要两倍空间。标记清除，存活对象多时适用，但是容易产生随便。标记整理，存活对象少时适用，需要移动对象较多。 2 GC分区，一般GC发生在堆区，堆区可分为年轻代，老年代，以前有永久代，现在没有了。 年轻代分为eden和survior，新对象分配在eden，当年轻代满时触发minor gc，存活对象移至survivor区，然后两个区互换，等待下一场gc，当对象存活的阈值达到设定值时进入老年代，大对象也会直接进入老年代。 老年代空间较大，当老年代空间不足以存放年轻代过来的对象时，开始进行full gc。同时整理年轻代和老年代。一般年轻代使用停止复制，老年代使用标记清除。 3 垃圾收集器 serial串行 parallel并行 它们都有年轻代与老年代的不同实现。 然后是scanvage收集器，注重吞吐量，可以自己设置，不过不注重延迟。 cms垃圾收集器，注重延迟的缩短和控制，并且收集线程和系统线程可以并发。 cms收集步骤主要是，初次标记gc root，然后停顿进行并发标记，而后处理改变后的标记，最后停顿进行并发清除。 g1收集器和cms的收集方式类似，但是g1将堆内存划分成了大小相同的小块区域，并且将垃圾集中到一个区域，存活对象集中到另一个区域，然后进行收集，防止产生碎片，同时使分配方式更灵活，它还支持根据对象变化预测停顿时间，从而更好地帮用户解决延迟等问题。 JVM的锁优化在Java并发中讲述了synchronized重量级锁以及锁优化的方法，包括轻量级锁，偏向锁，自旋锁等。详细内容可以参考我的专栏：Java并发技术指南 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络编程与NIO学习总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJava%E7%BD%91%E7%BB%9C%E4%B8%8ENIO%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我之前Java网络编程与NIO系列文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：Java网络编程与NIO https://blog.csdn.net/column/details/21963.html Java IOJava IO的基础知识已在前面讲过 Socket编程socket是操作系统提供的网络编程接口，他封装了对于TCP/IP协议栈的支持，用于进程间的通信，当有连接接入主机以后，操作系统自动为其分配一个socket套接字，套接字绑定着一个IP与端口号。通过socket接口，可以获取tcp连接的输入流和输出流，并且通过他们进行读取和写入此操作。 Java提供了net包用于socket编程，同时支持像Inetaddress，URL等工具类，使用socket绑定一个endpoint（ip+端口号），可以用于客户端的请求处理和发送，使用serversocket绑定本地ip和端口号，可以用于服务端接收TCP请求。 客户端，服务端的线程模型一般客户端使用单线程模型即可，当有数据到来时启动线程读取，需要写入数据时开启线程进行数据写入。 服务端一般使用多线程模型，一个线程负责接收tcp连接请求，每当接收到请求后开启一个线程处理它的读写请求。 udp的客户端和服务端就比较简单了，由于udp数据报长度是确定的，只需要写入一个固定的缓存和读取一个固定的缓存空间即可。 一般通过DatagramPacket包装一个udp数据报，然后通过DatagramSocket发送 IO模型上述的socket在处理IO请求时使用的是阻塞模型。 于是我们还是得来探讨一下IO模型。 一般认为，应用程序处理IO请求需要将内核缓存区中的数据拷贝到用户缓冲区。这个步骤可以通过系统调用来完成，而用户程序处理IO请求的时候，需要先检查用户缓冲区是否准备好了数据，这个操作是系统调用recevfrom，如果数据没有准备好，默认会阻塞调用该方法的线程。 这样就导致了线程处理IO请求需要频繁进行阻塞，特别是并发量大的时候，线程切换的开销巨大。 一般认为有几种IO模型 1 阻塞IO ：就是线程会阻塞在系统调用recevfrom上，并且等待数据准备就绪以后才会返回。 2 非阻塞IO : 不阻塞在系统调用recevfrom，而是通过自旋忙等的方式不断询问缓冲区数据是否准备就绪，避免线程阻塞的开销。 3 IO多路复用 ：使用IO多路复用器管理socket，由于每个socket是一个文件描述符，操作系统可以维护socket和它的连接状态，一般分为可连接，可读和可写等状态。 每当用户程序接受到socket请求，将请求托管给多路复用器进行监控，当程序对请求感兴趣的事件发生时，多路复用器以某种方式通知或是用户程序自己轮询请求，以便获取就绪的socket，然后只需使用一个线程进行轮询，多个线程处理就绪请求即可。 IO多路复用避免了每个socket请求都需要一个线程去处理，而是使用事件驱动的方式，让少数的线程去处理多数socket的IO请求。 Linux操作系统对IO多路复用提供了较好的支持，select，poll，epoll是Linux提供的支持IO多路复用的API。一般用户程序基于这个API去开发自己的IO复用模型。比如NIO的非阻塞模型，就是采用了IO多路复用的方式，是基于epoll实现的。 3.1 select方式主要是使用数组来存储socket描述符，系统将发生事件的描述符做标记，然后IO复用器在轮询描述符数组的时候，就可以知道哪些请求是就绪了的。缺点是数组的长度只能到1024，并且需要不断地在内核空间和用户空间之间拷贝数组。 3.2 poll方式不采用数组存储描述符，而是使用独立的数据结构来描述，并且使用id来表示描述符，能支持更多的请求数量，缺点和select方式有点类似，就是轮询的效率很低，并且需要拷贝数据。 当然，上述两种方法适合在请求总数较少，并且活跃请求数较多的情况，这种场景下他们的性能还是不错的。 3.3 epoll epoll函数会在内核空间开辟一个特殊的数据结构，红黑树，树节点中存放的是一个socket描述符以及用户程序感兴趣的事件类型。同时epoll还会维护一个链表。用于存储已经就绪的socket描述符节点。 由Linux内核完成对红黑树的维护，当事件到达时，内核将就绪的socket节点加入链表中，用户程序可以直接访问这个链表以便获取就绪的socket。 当然了，这些操作都linux包装在epoll的api中了。 epoll_create函数会执行红黑树的创建操作。 epoll_ctl函数会将socket和感兴趣的事件注册到红黑树中。 epoll_wait函数会等待内核空间发来的链表，从而执行IO请求。 epoll的水平触发和边缘触发有所区别，水平触发的意思是，如果用户程序没有执行就绪链表里的任务，epoll仍会不断通知程序。 而边缘触发只会通知程序一次，之后socket的状态不发生改变epoll就不会再通知程序了。 4 信号驱动略 5 异步非阻塞 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 事实上就是，用户提交IO请求，然后直接返回，并且内核自动完成将数据从内核缓冲区复制到用户缓冲区，完成后再通知用户。 当然，内核通知我们以后我们还需要执行剩余的操作，但是我们的代码已经继续往下运行了，所以AIO采用了回调的机制，为每个socket注册一个回调事件或者是回调处理器，在处理器中完成数据的操作，也就是内核通知到用户的时候，会自动触发回调函数，完成剩余操作。这样的方式就是异步的网络编程。 但是，想要让操作系统支持这样的功能并非易事，windows的IOCP可以支持AIO方式，但是Linux的AIO支持并不是很好 NIO由于Java原生的socket只支持阻塞方式处理IO 所以Java后来推出了新版IO 也叫New IO = NIO NIO提出了socketChannel，serversocketchannel，bytebuffer，selector和selectedkey等概念。 1 socketchannel其实就是socket的替代品，他的好处是多个socket可以复用同一个bytebuffer，因为socket是从channel里打开的，所以多个socket都可以访问channel绑定着的buffer。 2 serversocketchannel顾名思义，是用在服务端的channel。 3 bytebuffer以前对用户是透明的，用户直接操作io流即可，所以之前的socket io操作都是阻塞的，引入bytebuffer以后，用户可以更灵活地进行io操作。 buffer可以分为不同数据类型的buffer，但是常用的还是bytebuffer。写入数据时按顺序写入，写入完使用flip方法反转缓冲区，让接收端反向读取。这个操作比较麻烦，后来的netty对缓冲区进行了重新封装，封装了这个经常容易出错的方法。 4 selector其实就是对io多路复用器的封装，一般基于linux的epoll来实现。socket把感兴趣的事件和描述符注册到selector上，然后通过遍历selectedKey来获取感兴趣的请求，进行IO操作。selectedkey应该就是epoll中就绪链表的实现了。 5 所以一般的流程是：新建一个serversocket，启动一个线程进行while循环，当有请求接入时，使用accept方法阻塞获取socket，然后将socket和感兴趣的事件注册到selector上。再开启一个线程轮询selectoredKey，当请求就绪时开启一个线程去处理即可。 AIO后来NIO发展到2.0，Java又推出了AIO 的API，与上面描述的异步非阻塞模型类似。 AIO使用回调的方式处理IO请求，在socket上注册一个回调函数，然后提交请求后直接返回。由操作系统完成数据拷贝操作，需要操作系统对AIO的支持。 AIO的具体使用方式还是比较复杂的，感兴趣的可以自己查阅资料。 Tomcat中的NIO模型Tomcat作为一个应用服务器，分为connector和container两个部分，connector负责接收请求，而container负责解析请求。 一般connector负责接收http请求，当然首先要建立tcp连接，所以涉及到了如何处理连接和IO请求。 Tomcat使用endpoint的概念来绑定一个ip+port，首先，使用acceptor循环等待连接请求。然后开启一个线程池，也叫poller池，每个请求绑定一个poller进行后续处理，poller将socket请求封装成一个事件，并且将这个事件注册到selector中。 poller还需要维护一个事件列表，以便获取selector上就绪的事件。然后poller再去列表中获取就绪的请求，将其封装成processor，交给后续的worker线程池，会有worker将其提交给container流程中进行处理。 当然，到达container之后还有非常复杂的处理过程，稍微提几个点。 Tomcat的containercontainer是一个多级容器，最外层到最内层依次是engine，host，context和wrapper 下面是个server.xml文件实例，Tomcat根据该文件进行部署 //顶层类元素，可以包括多个Service //顶层类元素，可包含一个Engine，多个Connecter //连接器类元素，代表通信接口 //容器类元素，为特定的Service组件处理客户请求，要包含多个Host //容器类元素，为特定的虚拟主机组件处理客户请求，可包含多个Context //容器类元素，为特定的Web应用处理所有的客户请求 根据配置文件初始化容器信息，当请求到达时进行容器间的请求传递，事实上整个链条被称作pipeline，pipeline连接了各个容器的入口，由于每个容器和组件都实现了lifecycle接口。 tomcat可以在任意流程中通过加监听器的方式监听组件的生命周期，也就能够控制整个运行的流程，通过在pipeline上增加valve可以增加一些自定义的操作。 一般到wrapper层才开始真正的请求解析，因为wrapper其实就是对servlet的简单封装，此时进来的请求和响应已经是httprequest和httpresponse，很多信息已经解析完毕，只需要按照service方法执行业务逻辑即可，当然在执行service方法之前，会调用filter链先执行过滤操作。 nettynetty我也不是很在行，这里简单总结一下 netty是一个基于事件驱动的网络编程框架。 因为直接基于Java NIO编程复杂度太高，而且容易出错，于是netty对NIO进行了改造和封装。形成了一个比较完整的网络框架，可以通过他实现rpc，http服务。 先了解一下两种线程模型。reactor和proactor。 1 reactor就是netty采用的模型，首先也是使用一个acceptor线程接收连接请求，然后开启一个线程组reactor thread pool。 server会事先在endpoint上注册一系列的回调方法，然后接收socket请求后交给底层的selector进行管理，当selector对应的事件响应以后，会通知用户进程，然后reactor工作线程会执行接下来的IO请求，执行操作是写在回调处理器中的。 其实netty 支持三种reactor模型1.1.Reactor单线程模型：Reactor单线程模型，指的是所有的I/O操作都在同一个NIO线程上面完成。对于一些小容量应用场景，可以使用单线程模型。 1.2.Reactor多线程模型：Rector多线程模型与单线程模型最大的区别就是有一组NIO线程处理I/O操作。主要用于高并发、大业务量场景。 1.3.主从Reactor多线程模型：主从Reactor线程模型的特点是服务端用于接收客户端连接的不再是个1个单独的NIO线程，而是一个独立的NIO线程池。利用主从NIO线程模型，可以解决1个服务端监听线程无法有效处理所有客户端连接的性能不足问题 2 proactor模型其实是基于异步非阻塞IO模型的，当accpetor接收到请求以后，直接提交异步的io请求给linux内核，内核完成io请求后会回写消息到proactor提供的事件队列中，此时工作线程查看到IO请求已完成，则会继续剩余的工作，也是通过回调处理器来进行的。 所以两者最大的差别是，前者基于epoll的IO多路复用，后者基于AIO实现。 3 netty的核心组件： bytebuf bytebuf是对NIO中Bytebuffer的优化和扩展，并且支持堆外内存分配，堆外内存避免gc，可以更好地与内核空间进行交换数据。 channel和NIO的channel类似，但是NIO的socket代码改成nio实现非常麻烦，所以netty优化了这个过程，只需替换几个类就可以实现不更新太多代码就完成旧IO和新IO的切换。 channelhandler就是任务的处理器了，使用回调函数的方式注册到channel中，更准确来说是注册到channelpipeline里。 channelpipeline是用来管理和连接多个channelhandler的容器，执行任务时，会根据channelpipeline的调用链完成处理器的顺序调用，启动服务器时只需要将需要的channelhandler注册在上面就可以了。 eventloop在Netty的线程模型中，一个EventLoop将由一个永远不会改变的Thread驱动，而一个Channel一生只会使用一个EventLoop（但是一个EventLoop可能会被指派用于服务多个Channel），在Channel中的所有I/O操作和事件都由EventLoop中的线程处理，也就是说一个Channel的一生之中都只会使用到一个线程。 bootstrap 在深入了解地Netty的核心组件之后，发现它们的设计都很模块化，如果想要实现你自己的应用程序，就需要将这些组件组装到一起。Netty通过Bootstrap类，以对一个Netty应用程序进行配置（组装各个组件），并最终使它运行起来。 对于客户端程序和服务器程序所使用到的Bootstrap类是不同的，后者需要使用ServerBootstrap，这样设计是因为，在如TCP这样有连接的协议中，服务器程序往往需要一个以上的Channel，通过父Channel来接受来自客户端的连接，然后创建子Channel用于它们之间的通信，而像UDP这样无连接的协议，它不需要每个连接都创建子Channel，只需要一个Channel即可。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Java网络编程</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJava%E5%B9%B6%E5%8F%91%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结主要是基于我Java并发技术系列的文章而形成的的。主要是把重要的知识点用自己的话说了一遍，可能会有一些错误，还望见谅和指点。谢谢 更多详细内容可以查看我的专栏文章：Java并发技术指南 https://blog.csdn.net/column/details/21961.html 线程安全线程安全一般指多线程之间的操作结果不会因为线程调度的顺序不同而发生改变。 互斥和同步互斥一般指资源的独占访问，同步则要求同步代码中的代码顺序执行，并且也是单线程独占的。JMM内存模型JVM中的内存分区包括堆，栈，方法区等区域，这些内存都是抽象出来的，实际上，系统中只有一个主内存，但是为了方便Java多线程语义的实现，以及降低程序员编写并发程序的难度，Java提出了JMM内存模型，将内存分为主内存和工作内存，工作内存是线程独占的，实际上它是一系列寄存器，编译器优化后的结果。as-if-Serial，happens-beforeas if serial语义提供单线程代码的顺序执行保证，虽然他允许指令重排序，但是前提是指令重排序不会改变执行结果。volatilevolatile语义实际上是在代码中插入一个内存屏障，内存屏障分为读写，写读，读读，写写四种，可以用来避免volatile变量的读写操作发生重排序，从而保证了volatile的语义，实际上，volatile修饰的变量强制要求线程写时将数据从缓存刷入主内存，读时强制要求线程从主内存中读取，因此保证了它的可见性。 而对于volatile修饰的64位类型数据，可以保证其原子性，不会因为指令重排序导致一个64位数据被分割成两个32位数据来读取。synchronized和锁优化synchronized是Java提供的同步标识，底层是操作系统的mutex lock调用，需要进行用户态到内核态的切换，开销比较大。 synchronized经过编译后的汇编代码会有monitor in和monitor out的字样，用于标识进入监视器模块和退出监视器模块， 监视器模块watcher会监控同步代码块中的线程号，只允线程号正确的线程进入。 Java在synchronized关键字中进行了多次优化。 比如轻量级锁优化，使用锁对象的对象头做文章，当一个线程需要获得该对象锁时，线程有一段空间叫做lock record，用于存储对象头的mask word，然后通过cas操作将对象头的mask word改成指向线程中的lockrecord。 如果成功了就是获取到了锁，否则就是发生了互斥。需要锁粗化，膨胀为互斥锁。 偏向锁，去掉了更多的同步措施，检查mask word是否是可偏向状态，然后检查mask word中的线程id是否是自己的id，如果是则执行同步代码，如果不是则cas修改其id，如果修改失败，则出现锁争用，偏向锁失效，膨胀为轻量级锁。 自旋锁，每个线程会被分配一段时间片，并且听候cpu调度，如果发生线程阻塞需要切换的开销，于是使用自旋锁不需要阻塞，而是忙等循环，一获取时间片就开始忙等，这样的锁就是自旋锁，一般用于并发量比较小，又担心切换开销的场景。CAS操作CAS操作是通过硬件实现的原子操作，通过一条指令完成比较和赋值的操作，防止发生因指令重排导致的非原子操作，在Java中通过unsafe包可以直接使用，在Java原子类中使用cas操作来完成一系列原子数据类型的构建，保证自加自减等依赖原值的操作不会出现并发问题。 cas操作也广泛用在其他并发类中，通过循环cas操作可以完成线程安全的并发赋值，也可以通过一次cas操作来避免使用互斥锁。Lock类AQSAQS是Lock类的基石，他是一个抽象类，通过操作一个变量state来判断线程锁争用的情况，通过一系列方法实现对该变量的修改。一般可以分为独占锁和互斥锁。 AQS维护着一个CLH阻塞队列，这个队列主要用来存放阻塞等待锁的线程节点。可以看做一个链表。 一：独占锁独占锁的state只有0和1两种情况（如果是可重入锁也可以把state一直往上加，这里不讨论），state = 1时说明已经有线程争用到锁。线程获取锁时一般是通过aqs的lock方法，如果state为0，首先尝试cas修改state=1，成功返回，失败时则加入阻塞队列。非公共锁使用时，线程节点加入阻塞队列时依然会尝试cas获取锁，最后如果还是失败再老老实实阻塞在队列中。 独占锁还可以分为公平锁和非公平锁，公平锁要求锁节点依据顺序加入阻塞队列，通过判断前置节点的状态来改变后置节点的状态，比如前置节点获取锁后，释放锁时会通知后置节点。 非公平锁则不一定会按照队列的节点顺序来获取锁，如上面所说，会先尝试cas操作，失败再进入阻塞队列。 二：共享锁共享锁的state状态可以是0到n。共享锁维护的阻塞队列和互斥锁不太一样，互斥锁的节点释放锁后只会通知后置节点，而共享锁获取锁后会通知所有的共享类型节点，让他们都来获取锁。共享锁用于countdownlatch工具类与cyliderbarrier等，可以很好地完成多线程的协调工作 锁Lock和ConditonLock 锁维护这两个内部类fairsync和unfairsync，都继承自aqs，重写了部分方法，实际上大部分方法还是aqs中的，Lock只是重新把AQS做了封装，让程序员更方便地使用Lock锁。 和Lock锁搭配使用的还有condition，由于Lock锁只维护着一个阻塞队列，有时候想分不同情况进行锁阻塞和锁通知怎么办，原来我们一般会使用多个锁对象，现在可以使用condition来完成这件事，比如线程A和线程B分别等待事件A和事件B，可以使用两个condition分别维护两个队列，A放在A队列，B放在B队列，由于Lock和condition是绑定使用的，当事件A触发，线程A被唤醒，此时他会加入Lock自己的CLH队列中进行锁争用，当然也分为公平锁和非公平锁两种，和上面的描述一样。 Lock和condtion的组合广泛用于JUC包中，比如生产者和消费者模型，再比如cyliderbarrier。 ###读写锁 读写锁也是Lock的一个子类，它在一个阻塞队列中同时存储读线程节点和写线程节点，读写锁采用state的高16位和低16位分别代表独占锁和共享锁的状态，如果共享锁的state &gt; 0可以继续获取读锁，并且state-1，如果=0,则加入到阻塞队列中，写锁节点和独占锁的处理一样，因此一个队列中会有两种类型的节点，唤醒读锁节点时不会唤醒写锁节点，唤醒写锁节点时，则会唤醒后续的节点。 因此读写锁一般用于读多写少的场景，写锁可以降级为读锁，就是在获取到写锁的情况下可以再获取读锁。 并发工具类countdownlatch主要通过AQS的共享模式实现，初始时设置state为N，N是countdownlatch初始化使用的size，每当有一个线程执行countdown，则state-1，state = 0之前所有线程阻塞在队列中，当state=0时唤醒队头节点，队头节点依次通知所有共享类型的节点，唤醒这些线程并执行后面的代码。 cycliderbarrier主要通过lock和condition结合实现，首先设置state为屏障等待的线程数，在某个节点设置一个屏障，所有线程运行到此处会阻塞等待，其实就是等待在一个condition的队列中，并且每当有一个线程到达，state -=1 则当所有线程到达时,state = 0，则唤醒condition队列的所有结点，去执行后面的代码。 samphere也是使用AQS的共享模式实现的，与countlatch大同小异，不再赘述。 exchanger就比较复杂了。使用exchanger时会开辟一段空间用来让两个线程进行交互操作，这个空间一般是一个栈或队列，一个线程进来时先把数据放到这个格子里，然后阻塞等待其他线程跟他交换，如果另一个线程也进来了，就会读取这个数据，并把自己的数据放到对方线程的格子里，然后双双离开。当然使用栈和队列的交互是不同的，使用栈的话匹配的是最晚进来的一个线程，队列则相反。 原子数据类型原子数据类型基本都是通过cas操作实现的，避免并发操作时出现的安全问题。 同步容器同步容器主要就是concurrenthashmap了，在集合类中我已经讲了chm了，所以在这里简单带过，chm1.7通过分段锁来实现锁粗化，使用的死LLock锁，而1.8则改用synchronized和cas的结合，性能更好一些。 还有就是concurrentlinkedlist，ConcurrentSkipListMap与CopyOnWriteArrayList。 第一个链表也是通过cas和synchronized实现。 而concurrentskiplistmap则是一个跳表，跳表分为很多层，每层都是一个链表，每个节点可以有向下和向右两个指针，先通过向右指针进行索引，再通过向下指针细化搜索，这个的搜索效率是很高的，可以达到logn，并且它的实现难度也比较低。通过跳表存map就是把entry节点放在链表中了。查询时按照跳表的查询规则即可。 CopyOnWriteArrayList是一个写时复制链表，查询时不加锁，而修改时则会复制一个新list进行操作，然后再赋值给原list即可。适合读多写少的场景。 阻塞队列 BlockingQueue 实现之 ArrayBlockingQueue ArrayBlockingQueue其实就是数组实现的阻塞队列，该阻塞队列通过一个lock和两个condition实现，一个condition负责从队头插入节点，一个condition负责队尾读取节点，通过这样的方式可以实现生产者消费者模型。 BlockingQueue 实现之 LinkedBlockingQueue LinkedBlockingQueue是用链表实现的阻塞队列，和arrayblockqueue有所区别，它支持实现为无界队列，并且它使用两个lock和对应的condition搭配使用，这是因为链表可以同时对头部和尾部进行操作，而数组进行操作后可能还要执行移位和扩容等操作。 所以链表实现更灵活，读写分别用两把锁，效率更高。 BlockingQueue 实现之 SynchronousQueue SynchronousQueue实现是一个不存储数据的队列，只会保留一个队列用于保存线程节点。详细请参加上面的exchanger实现类，它就是基于SynchronousQueue设计出来的工具类。 BlockingQueue 实现之 PriorityBlockingQueue PriorityBlockingQueue PriorityBlockingQueue是一个支持优先级的无界队列。默认情况下元素采取自然顺序排列，也可以通过比较器comparator来指定元素的排序规则。元素按照升序排列。 DelayQueue DelayQueue是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue来实现。队列中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。只有在延迟期满时才能从队列中提取元素。我们可以将DelayQueue运用在以下应用场景： 缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。 定时任务调度。使用DelayQueue保存当天将会执行的任务和执行时间，一旦从DelayQueue中获取到任务就开始执行，从比如TimerQueue就是使用DelayQueue实现的。线程池类图首先看看executor接口，只提供一个run方法，而他的一个子接口executorservice则提供了更多方法，比如提交任务，结束线程池等。 然后抽象类abstractexecutorservice提供了更多的实现了，最后我们最常使用的类ThreadPoolExecutor就是继承它来的。 ThreadPoolExecutor可以传入多种参数来自定义实现线程池。 而我们也可以使用Executors中的工厂方法来实例化常用的线程池。 常用线程池比如newFixedThreadPool newSingleThreadExecutor newCachedThreadPool newScheduledThreadPool等等，这些线程池即可以使用submit提交有返回结果的callable和futuretask任务，通过一个future来接收结果，或者通过callable中的回调函数call来回写执行结果。也可以用execute执行无返回值的runable任务。 在探讨这些线程池的区别之前，先看看线程池的几个核心概念。 任务队列：线程池中维护了一个任务队列，每当向线程池提交任务时，任务加入队列。 工作线程：也叫worker，从线程池中获取任务并执行，执行后被回收或者保留，因情况而定。 核心线程数和最大线程数，核心线程数是线程池需要保持存活的线程数量，以便接收任务，最大线程数是能创建的线程数上限。 newFixedThreadPool可以设置固定的核心线程数和最大线程数，一个任务进来以后，就会开启一个线程去执行，并且这部分线程不会被回收，当开启的线程达到核心线程数时，则把任务先放进任务队列。当任务队列已满时，才会继续开启线程去处理，如果线程总数打到最大线程数限制，任务队列又是满的时候，会执行对应的拒绝策略。 拒绝策略一般有几种常用的，比如丢弃任务，丢弃队尾任务，回退给调用者执行，或者抛出异常，也可以使用自定义的拒绝策略。 newSingleThreadExecutor是一个单线程执行的线程池，只会维护一个线程，他也有任务队列，当任务队列已满并且线程数已经是1个的时候，再提交任务就会执行拒绝策略。 newCachedThreadPool比较特别，第一个任务进来时会开启一个线程，而后如果线程还没执行完前面的任务又有新任务进来，就会再创建一个线程，这个线程池使用的是无容量的SynchronousQueue队列，要求请求线程和接受线程匹配时才会完成任务执行。所以如果一直提交任务，而接受线程来不及处理的话，就会导致线程池不断创建线程，导致cpu消耗很大。 ScheduledThreadPoolExecutor内部使用的是delayqueue队列，内部是一个优先级队列priorityqueue，也就是一个堆。通过这个delayqueue可以知道线程调度的先后顺序和执行时间点。 Fork/Join框架又称工作窃取线程池。 我们在大学算法课本上，学过的一种基本算法就是：分治。其基本思路就是：把一个大的任务分成若干个子任务，这些子任务分别计算，最后再Merge出最终结果。这个过程通常都会用到递归。 而Fork/Join其实就是一种利用多线程来实现“分治算法”的并行框架。 另外一方面，可以把Fori/Join看作一个单机版的Map/Reduce，只不过这里的并行不是多台机器并行计算，而是多个线程并行计算。 与ThreadPool的区别通过上面例子，我们可以看出，它在使用上，和ThreadPool有共同的地方，也有区别点：（1） ThreadPool只有“外部任务”，也就是调用者放到队列里的任务。 ForkJoinPool有“外部任务”，还有“内部任务”，也就是任务自身在执行过程中，分裂出”子任务“，递归，再次放入队列。（2）ForkJoinPool里面的任务通常有2类，RecusiveAction/RecusiveTask，这2个都是继承自FutureTask。在使用的时候，重写其compute算法。 工作窃取算法上面提到，ForkJoinPool里有”外部任务“，也有“内部任务”。其中外部任务，是放在ForkJoinPool的全局队列里面，而每个Worker线程，也有一个自己的队列，用于存放内部任务。 窃取的基本思路就是：当worker自己的任务队列里面没有任务时，就去scan别的线程的队列，把别人的任务拿过来执行 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Java并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合框架学习总结]]></title>
    <url>%2F2018%2F07%2F08%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJava%E9%9B%86%E5%90%88%E7%B1%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[这篇总结是基于之前博客内容的一个整理和回顾。 这里先简单地总结一下，更多详细内容请参考我的专栏：深入浅出Java核心技术 https://blog.csdn.net/column/details/21930.html 里面有包括Java集合类在内的众多Java核心技术系列文章。 以下总结不保证全对，如有错误，还望能够指出。谢谢 Colletion，iterator，comparable一般认为Collection是最上层接口，但是hashmap实际上实现的是Map接口。iterator是迭代器，是实现iterable接口的类必须要提供的一个东西，能够使用for(i : A) 这种方式实现的类型能提供迭代器，以前有一个enumeration，现在早弃用了。 ListList接口下的实现类有ArrayList，linkedlist，vector等等，一般就是用这两个，用法不多说，老生常谈。ArrayList的扩容方式是1.5倍扩容，这样扩容避免2倍扩容可能浪费空间，是一种折中的方案。另外他不是线程安全，vector则是线程安全的，它是两倍扩容的。 linkedlist没啥好说的，多用于实现链表。 Mapmap永远都是重头戏。 hashmap是数组和链表的组合结构，数组是一个Entry数组，entry是k-V键值对类型，所以一个entry数组存着很entry节点，一个entry的位置通过key的hashcode方法，再进行hash（移位等操作），最后与表长-1进行相与操作，其实就是取hash值到的后n - 1位，n代表表长是2的n次方。 hashmap的默认负载因子是0.75，阈值是16 * 0.75 = 12；初始长度为16； hashmap的增删改查方式比较简单，都是遍历，替换。有一点要注意的是key相等时，替换元素，不相等时连成链表。 除此之外，1.8jdk改进了hashmap，当链表上的元素个数超过8个时自动转化成红黑树，节点变成树节点，以提高搜索效率和插入效率到logn。 还有一点值得一提的是，hashmap的扩容操作，由于hashmap非线程安全，扩容时如果多线程并发进行操作，则可能有两个线程分别操作新表和旧表，导致节点成环，查询时会形成死锁。chm避免了这个问题。 另外，扩容时会将旧表元素移到新表，原来的版本移动时会有rehash操作，每个节点都要rehash，非常不方便，而1.8改成另一种方式，对于同一个index下的链表元素，由于一个元素的hash值在扩容后只有两种情况，要么是hash值不变，要么是hash值变为原来值+2^n次方，这是因为表长翻倍，所以hash值取后n位，第一位要么是0要么是1，所以hash值也只有两种情况。这两种情况的元素分别加到两个不同的链表。这两个链表也只需要分别放到新表的两个位置即可，是不是很酷。 最后有一个比较冷门的知识点，hashmap1.7版本链表使用的是节点的头插法，扩容时转移链表仍然使用头插法，这样的结果就是扩容后链表会倒置，而hashmap.1.8在插入时使用尾插法，扩容时使用头插法，这样可以保证顺序不变。 CHMconcurrenthashmap也稍微提一下把，chm1.7使用分段锁来控制并发，每个segment对应一个segmentmask，通过key的hash值相与这个segmentmask得到segment位置，然后在找到具体的entry数组下标。所以chm需要维护多个segment，每个segment对应一个数组。分段锁使用的是reetreetlock可重入锁实现。查询时不加锁。 1.8则放弃使用分段锁，改用cas+synchronized方式实现并发控制，查询时不加锁，插入时如果没有冲突直接cas到成功为止，有冲突则使用synchronized插入。 Setset就是hashmap将value固定为一个object，只存key元素包装成一个entry即可，其他不变。 Linkedhashmap在原来hashmap基础上将所有的节点依据插入的次序另外连成一个链表。用来保持顺序，可以使用它实现lru缓存，当访问命中时将节点移到队头，当插入元素超过长度时，删除队尾元素即可。 collections和Arrays工具类两个工具类分别操作集合和数组，可以进行常用的排序，合并等操作。 comparable和comparator实现comparable接口可以让一个类的实例互相使用compareTo方法进行比较大小，可以自定义比较规则，comparator则是一个通用的比较器，比较指定类型的两个元素之间的大小关系。 treemap和treeset主要是基于红黑树实现的两个数据结构，可以保证key序列是有序的，获取sortedset就可以顺序打印key值了。其中涉及到红黑树的插入和删除，调整等操作，比较复杂，这里就不细说了。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>后端</category>
        <category>技术总结</category>
      </categories>
      <tags>
        <tag>Java集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA后端开发学习之路]]></title>
    <url>%2F2018%2F04%2F20%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJava%2F</url>
    <content type="text"><![CDATA[本文主要记录了我从Java初学者到专注于Java后端技术栈的开发者的学习历程。主要分享了学习过程中的一些经验和教训，让后来人看到，少走弯路，与君共勉，共同进步。如有错误，还请见谅。 我的GitHub： https://github.com/h2pl/MyTech 喜欢的话麻烦点下星哈 &emsp;&emsp;不论你是不是网民，无论你远离互联网，还是沉浸其中；你的身影，都在这场伟大的迁徙洪流中。超越人类经验的大迁徙，温暖而无情地，开始了。&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;—–《互联网时代》 选择方向&emsp;&emsp;0上大学前的那些事，让它们随风逝去吧。 &emsp;&emsp;1 个人对计算机和互联网有情怀，有兴趣，本科时在专业和学校里选择了学校，当时专业不是计算机，只能接触到一点点计算机专业课程，所以选择了考研，花半年时间复习考进了一个还不错的985，考研经历有空会发到博客上。 &emsp;&emsp;2 本科阶段接触过Java和Android，感觉app蛮有趣的，所以研一的时候想做Android，起初花大量时间看了计算机专业课的教材，效果很差。但也稍微了解了一些计算机基础，如网络，操作系统，组成原理，数据库，软工等。 &emsp;&emsp;3 在没确定方向的迷茫时期看了大量视频和科普性文章，帮助理清头绪和方向。期间了解了诸如游戏开发，c++开发，Android，Java甚至前端等方向，其中还包含游戏策划岗。 &emsp;&emsp;4 后来综合自身条件以及行业发展等因素，开始锁定自己的目标在Java后台方向。于是乎各种百度，知乎，查阅该学什么该怎么学如此类的问题，学习别人的经验。当然只靠搜索引擎很难找到精品内容，那段时间可谓是病急乱投医，走了不少弯路。 夯实基础&emsp;&emsp;1 研一的工程实践课让我知道了我的基础不够扎实，由于并非科班，需要比别人更加勤奋，古语有云，天道酬勤，勤能补拙。赶上了17年的春招实习招聘，期间开始各种海投，各种大厂面试一问三不知，才知道自身差距很大，开始疯狂复习面试题，刷面经，看经验等。死记硬背，之乎者也，倒也是能应付一些小公司，可谓是临阵磨枪不快也光。 &emsp;&emsp;2 不过期间的屡屡受挫让我冷静思考了一段时间，我再度调研了岗位需求，学习方法，以及需要看的书等资料。再度开工时，我的桌上开始不断出现新的经典书籍。这还要归功于我的启蒙导师：江南白衣，在知乎上看到了他的一篇文章，我的Java后端书架。在这个书架里我找寻到了很多我想看的书，以及我需要学习的技术。 &emsp;&emsp;3 遥想研一我还在看的书：教材就不提了，脱离实际并且年代久远，而我选的入门书籍竟然还有Java web从入门到精通这种烂大街的书籍，然后就是什么Java编程思想啦，深入理解计算机系统，算法导论这种高深莫测的书，感觉有点高不成低不就的意思。要么太过难懂要么过于粗糙，这些书在当时基本上没能帮到我。 书籍选择&emsp;&emsp;1 江南白衣的后端书架真是救我于水火。他的书架里收录了许多Java后端需要用到的技术书籍，并且十分经典，虽不说每本都适合入门，但是只要你用心去看都会有收获，高质量的书籍给人的启发要优于普通书籍。 &emsp;&emsp;2 每个门类的书我都挑了一些。比如网络的两本（《tcp ip卷一》以及《计算机网络自顶向下》），操作系统两本（一本《Linux内核设计与实现》，一本高级操作系统，推荐先看完《深入理解计算机系统》再来看这两本），算法看的是《数据结构与算法（Java版）》，Java的四大件（《深入理解jvm虚拟机》，《java并发编程艺术》，《深入java web技术内幕》，《Java核心技术 卷一》这本没看）。 &emsp;&emsp;3 当然还有像《Effective Java》，《Java编程思想》，《Java性能调优指南》这种，不过新手不推荐，太不友好。接着是spring的两本《Spring实战》和《Spring源码剖析》。当然也包括一些redis，mq之类的书，还有就是一些介绍分布式组件的书籍，如zk等。 &emsp;&emsp;4 接下来就是扩展的内容了，比如分布式的三大件，《大型网站架构设计与实践》，《分布式网站架构设计与实践》，《Java中间件设计与实践》，外加一本《分布式服务框架设计与实践》。这几本书一看，绝对让你打开新世界的大门，醍醐灌顶，三月不知肉味。 &emsp;&emsp;5 你以为看完这些书你就无敌了，就满足了？想得倒是挺美。这些书最多就是把我从悬崖边拉回正途，能让我在正确的道路上行走了。毕竟技术书籍这种东西还是有门槛的，没有一定的知识储备，看书的过程也绝对是十分痛苦的。 &emsp;&emsp;&emsp;&emsp;6 比如《深入理解jvm虚拟机》和《java并发编程艺术》这两本书，我看了好几遍，第一遍基本当天书来看，第二遍挑着章节看，第三遍能把全部章节都看了。所以有时候你觉得你看完了一本书，对，你确实看完了，但过段时间是你能记得多少呢。可以说是很少了。 谈一谈学习方法&emsp;&emsp;1 人们在刚开始接触自己不熟悉的领域时，往往都会犯很多错误。刚开始学习Java时，就是摸着石头过河。从在极客学院慕课上看视频，到看书，再到看博客，再到工程实践，也是学习方式转变的一个过程。 &emsp;&emsp;2 看视频：适合0基础小白，视频给你构建一个世界观，让你对你要做的东西有个大概的了解，想要深入理解其中的技术原理，只看视频的话很难。 &emsp;&emsp;3 看书：就如上面一节所说，看书是一个很重要的环节。当你对技术只停留在大概的了解和基本会用的阶段时，经典书籍能够让你深入这些技术的原理，你可能会对书里的内容感到惊叹，也可能只是一知半解。所以第一遍的阅读一般读个大概就可以。一本书要吃透，不仅要看好几遍，还要多上手实践，才能变成自己的东西。 &emsp;&emsp;4 看博客，光看一些总结性的博客或者是科普性的博客可能还不够，一开始我也经常看这样的博客，后来只看这些东西，发现对技术的理解只能停留在表面。高质量的博客一般会把一个知识点讲得很透彻，比你看十篇总结都强，例如讲jdk源码的博文，可以很好地帮助你理解其原理，避免自己看的时候一脸懵逼。这里先推荐几个博客和网站，后面写复习计划的时候，会详细写出。博客：江南白衣、酷壳、战小狼。网站：并发编程网，importnew。 &emsp;&emsp;5 实践为王，Java后端毕竟还是工程方向，只是通过文字去理解技术点，可能有点纸上谈兵的感觉了。还有一个问题就是，没有进行上手实践的技术，一般很快就会忘了，做一些实践可以更好地巩固知识点。如果有项目中涉及不到的知识点，可以单独拿出来做一些demo，实在难以进行实践的技术点，可以参考别人的实践过程。 实习，提高工程能力的好机会&emsp;&emsp;1 这段时间以后就是实习期了，三个月的W厂实习经历。半年的B厂实习，让我着实过了一把大厂的瘾。但是其中做的工作无非就是增删改查写写业务逻辑，很难接触到比较核心的部分。 &emsp;&emsp;2 于是乎我花了许多时间学习部门的核心技术。比如在W厂参与数据平台的工作时，我学习了hadoop以及数据仓库的架构，也写了一些博客，并且向负责后端架构的导师请教了许多知识，收获颇丰。 &emsp;&emsp;3 在B厂实习期间则接触了许多云计算相关的技术。因为部门做的是私有云，所以业务代码和底层的服务也是息息相关的，比如平时的业务代码也会涉及到底层的接口调用，比如新建一个虚拟机或者启动一台虚拟机，需要通过多级的服务调用，首先是HTTP服务调用，经过多级的服务调用，最终完成流程。在这期间我花了一些时间学习了OpenStack的架构以及部门的实际应用情况，同时也玩了一下docker，看了kubenetes的一些书籍，算是入门。 &emsp;&emsp;4 但是这些东西其实离后台开发还是有一定距离的，比如后台开发的主要问题就是高并发，分布式，Linux服务器开发等。而我做的东西，只能稍微接触到这一部门的内容，因为主要是to b的内部业务。所以这段时间其实我的进步有限，虽然扩大了知识面并且积累了开发经验，但是对于后台岗位来说还是有所欠缺的。 &emsp;&emsp;5 不过将近一年的实习也让我收获了很多东西，大厂的实习体验很好，工作高效，团队合作，版本的快速迭代，技术氛围很不错。特别是在B厂了可以解到很多前沿的技术，对自己的视野扩展很有帮助。 实习转正，还是准备秋招？&emsp;&emsp;1 离职以后，在考虑是否还要找实习，因为有两份实习经历了，在考虑要不要静下心来刷刷题，复习一下基础，并且回顾一下实习时用到的技术。同一时期，我了解到腾讯和阿里等大厂的实习留用率不高，并且可能影响到秋招，所以当时的想法是直接复习等到秋招内推。因此，那段时间比较放松，没什么复习状态，也导致了我在今年春招内推的阶段比较艰难。 &emsp;&emsp;2 因为当时想着沉住气准备秋招，所以一开始对实习内推不太在意。但是由于AT招人的实习生转正比例较大，考虑到秋招的名额可能更少，所以还是不愿意错过这个机会。因为开始系统复习的时间比较晚，所以投的比较晚，担心准备不充分被刷。这次找实习主要是奔着转正去的，所以只投了bat和滴滴，京东，网易游戏等大厂。 &emsp;&emsp;3 由于投递时间原因，所以面试的流程特别慢。并且在笔试方面还是有所欠缺，刷题刷的比较少，在线编程的算法题还是屡屡受挫。这让我有点后悔实习结束后的那段时间没有好好刷题了。 调整心态，重新上路&emsp;&emsp;1 目前的状态是，一边刷题，一边复习基础，投了几家大厂的实习内推，打算选一个心仪的公司准备转正，但是事情总是没那么顺利，微软，头条等公司的笔试难度超过了我的能力范围，没能接到面试电话。腾讯投了一个自己比较喜欢的部门，可惜岗位没有匹配上，后台开发被转成了运营开发，最终没能通过。阿里面试的也不顺利，当时投了一个牛客上的蚂蚁金服内推，由于投的太晚，部门已经招满，只面了一面就没了下文，前几天接到了菜鸟的面试，这个未完待续。 &emsp;&emsp;2 目前的想法是，因为我不怎么需要实习经历来加分了，所以想多花些时间复习基础，刷题，并且巩固之前的项目经历。当然如果有好的岗位并且转正机会比较大的话，也是会考虑去实习的，那样的话可能需要多挤点时间来复习基础和刷题了。 &emsp;&emsp;3 在这期间，我会重新梳理一下自己的复习框架，有针对性地看一些高质量的博文，同时多做些项目实践，加深对知识的理解。当然这方面还会通过写博客进行跟进，写博客，做项目。前阵子在牛客上看到一位牛友CyC2018做的名为interview notebook的GitHub仓库，内容非常好，十分精品，我全部看完了，并且参考其LeetCode题解进行刷题。 &emsp;&emsp;4 受到这位大佬的启发，我也打算做一个类似的代码仓库或者是博客专栏，尽量在秋招之前把总结做完，并且把好的文章都放进去。上述内容只是本人个人的心得体会，如果有错误或者说的不合理的地方，还请谅解和指正。希望与广大牛友共勉，一起进步。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>个人总结</category>
      </categories>
      <tags>
        <tag>心路历程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我和技术博客的这一年]]></title>
    <url>%2F2018%2F04%2F20%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2Fblog%2F</url>
    <content type="text"><![CDATA[本文记录了我从Java初学者到专注于Java后端开发技术栈的成长历程，主要是与写博客相关的内容，其他内容还包括实习历程，后端技术学习历程，校招计划等内容，我会陆续发表并且提供链接。 我的GitHub： https://github.com/h2pl/MyTech 喜欢的话麻烦点下星哈 下面是正文： 梦开始的地方&emsp;&emsp;2017年初开了这个博客，转眼也一年多时间了。最早在博客园开的博客，后来感觉csdn生态更好一点，于是转移到csdn。恰逢这段时间在做学校的课题，于是最开始的时候记录了一些项目搭建以及开发中遇到的题以及解决方案。当时技术还比较稚嫩，属于刚刚入门Java web的阶段。所以博客内容也比较一般。 博客记录我的成长&emsp;&emsp;去年的春天我投入到浩瀚的春招大军中去了，也是那个时候确定了做Java开发的方向，当时对后台技术还不是太了解，主要从Java以及Java web入手，开始了一系列的学习和准备。这篇文章主要讲博客的历程，如果对我的学习历程有兴趣的朋友可以查看最上方的链接。&emsp;&emsp;项目结束以后，主要在复习Java基础，于是看了不少相关博客，记录了很多Java的基础知识点，比如异常，反射，序列化，集合类等等内容的一些总结，现在看来确实有点幼稚了。所以我最近也在删除一些低质量的文章，以便让大家能看到更好的内容。&emsp;&emsp;在准备春招实习面试期间，我花了大量时间阅读技术书籍以及博客，并且总结了一部分面经，同时将一些比较好的总结发在了博客上，以便我在复习期间能够阅读和复习，所以有一段时间发了大量的博文，多得连我自己都怕。当然我并不推荐这种做法，在后来的日子里，基本上是定期地发一些有一定质量的文章，尽量自己理清文章内容后再进行发布，否则可能有会滥竽充数的情况。&emsp;&emsp;除此之外也记录了一些工程方面的内容，例Maven，git，Tomcat，以及IDE的使用，以及MySQL的一些使用经验，由于有段时间在W厂实习，所以当时主要记录的是实习过程中用的技术栈以及相关开发tips。&emsp;&emsp;离开W厂之后，我来到了B厂，部门做的主要是云计算，于是记录了一些云计算相关的文章，比如OpenStack，docker，kubenetes等内容。B厂是技术为主导的公司，内部经常举办技术交流会以及分享会，我通常都会报名参加，了解了一部分AI和大数据的应用以及实现原理。所以这段时间主要会发一些AI以及Hadoop的文章，让我更全面了解相关技术。 最好的总结就是读书笔记&emsp;&emsp;在百度的这段时间里，我意识到了我的基础可能还是不够牢固。因为是非科班出身，虽然是硕士，但是基础还是有一些欠缺，这段时间我看了许多更加底层的东西，比如网络，操作系统，Linux内核，其中那一本《深入理解计算机系统》确实是不错的总结性书籍，基本可以带你概览计算机系统的全貌。&emsp;&emsp;因此，在这段时间里我写了不少的读书笔记等总结性文章，主要囊括了操作系统，计算机网络，Linux等内容。我发现写读书笔记是加深对原书理解的很好的途径，于是我把以前看过的一些书拿出来又翻了几遍，例如JVM虚拟机，java并发实战，大型网站架构滴滴，所以我干脆把其他书的读书笔记也整理出来了，不过有一些书过于晦涩或者是太厚，也借鉴了一些博友的读书笔记。当然有很多文章还不够成熟。 不积跬步无以至千里&emsp;&emsp;大公司面试时，会给你一种感觉，就是无孔不入，细节决定成败，往往粗浅的总结难以让你理解技术深层次的原理，缺乏实践或者是深入思考，可能会让你错过很多重要的知识点，而往往这些知识点是大厂面试官喜欢问的。&emsp;&emsp;就拿Java来说，jvm虚拟机垃圾回收器的具体回收过程，可以问的很深入，问到gcRoots，停顿多少次，是否并发回收等，这些问题可能不是对gc的浅显总结可以概括的。&emsp;&emsp;再比如，JUC中的Lock，平时可能只了解到lock的用法，condition，并发工具类的使用，但是Lock底层的AQS实现，可能很少去关注，AQS的相关源码晦涩难懂，推荐看大牛的解析，可以让你更好地理解lock类的实现。&emsp;&emsp;其实这个想法也是前阵子我才想到的，因为看到阿里的实习面经，Java相关的原理问的特别深，没有深入到源码去理解的话，往往就会被问住。结果可想而知。所以这段时间主要的想法是只记录高质量的内容，并且尽量覆盖重要的知识点。 纸上得来终觉浅，实践为王&emsp;&emsp;文章写得再好，毕竟是纸面上的东西，一旦上手，可能又是另一种情况，我虽然看了不少书，也阅读了许多优质的博客，但是对于有些技术细节总觉得还是差了点，或者说，书上看来的东西，很快就忘了。其实记忆本身就是这种特点，只有实战可以让书上的知识变成你自己的。用过这个技术并且能了解其原理，才能对这个技术有发言权。所以在未来的计划里，我打算更多地写一些实战性的文章。 回到原点，重新出发&emsp;&emsp;从第一次写博客到现在，经历了很多事，有了诸多感悟，与君共勉，至于对我的观点认同与否，那就见仁见智了。脚踏实地也不要忘了仰望星空。建议做开发的朋友们都要写博客，写博客的好处很多，方便记忆，便于交流，也是打造个人品牌的一种方式，有时间自己搭博客，效果更好。&emsp;&emsp;最近用b3log solo搭了博客，接下来打算用github pages + hexo来写博客。等到工作以后，可能会只用个人博客了。这可能也象征着学生时代的结束吧，新的博客不仅会有技术文章，还会分享人生感悟，csdn的话，还是主要发布技术文章。就说到这里了。希望有更多人看到。 微信公众号个人公众号：程序员黄小斜微信公众号【程序员黄小斜】新生代青年聚集地，程序员成长充电站。作者黄小斜，职业是阿里程序员，身份是斜杠青年，希望和更多的程序员交朋友，一起进步和成长！这一次，我们一起出发。 关注公众号后回复“2019”领取我这两年整理的学习资料，涵盖自学编程、求职面试、算法刷题、Java技术、计算机基础和考研等8000G资料合集。 技术公众号：Java技术江湖微信公众号【Java技术江湖】一位阿里 Java 工程师的技术小站，专注于 Java 相关技术：SSM、SpringBoot、MySQL、分布式、中间件、集群、Linux、网络、多线程，偶尔讲点Docker、ELK，同时也分享技术干货和学习经验，致力于Java全栈开发！ 关注公众号后回复“PDF”即可领取200+页的《Java工程师面试指南》强烈推荐，几乎涵盖所有Java工程师必知必会的知识点。 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '15310-1577469423472-640', name: '程序员黄小斜', qrcode: 'https://s2.ax1x.com/2019/12/28/le9CwT.jpg', keyword: '验证码', }); var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i); if (!isMobile) { var btw = new BTWPlugin(); btw.init({ "id": "vip-container", "blogId": "15310-1577469423472-640", "name": "黄小斜学Java", "qrcode": "https://s2.ax1x.com/2019/12/28/le9CwT.jpg", "keyword": "关键词" }); }]]></content>
      <categories>
        <category>个人总结</category>
      </categories>
      <tags>
        <tag>心路历程</tag>
      </tags>
  </entry>
</search>
